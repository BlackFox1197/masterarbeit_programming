{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 16:49:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-02-08 16:49:00.738282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 16:49:01.230416: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 16:49:01.230471: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 16:49:01.230476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/\"\n",
    "trials_per_model_type = 3\n",
    "epochs_per_model = 101\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.844142  [    0/ 5482]\n",
      "loss: 2.029996  [  800/ 5482]\n",
      "loss: 2.009137  [ 1600/ 5482]\n",
      "loss: 2.007756  [ 2400/ 5482]\n",
      "loss: 1.964097  [ 3200/ 5482]\n",
      "loss: 2.133368  [ 4000/ 5482]\n",
      "loss: 1.981885  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.108     0.000     0.000    99\n",
      " disgust     0.108     0.000     0.000    107\n",
      "    fear     0.108     0.000     0.000    80\n",
      "   happy     0.108     0.126     0.208    77\n",
      " neutral     0.108     0.000     0.000    95\n",
      "     sad     0.108     0.000     0.000    91\n",
      "surprise     0.108     0.106     0.820    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.108     0.033     0.147    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 1.955825 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.963555  [    0/ 5482]\n",
      "loss: 1.937844  [  800/ 5482]\n",
      "loss: 2.049615  [ 1600/ 5482]\n",
      "loss: 1.838875  [ 2400/ 5482]\n",
      "loss: 1.917204  [ 3200/ 5482]\n",
      "loss: 1.852369  [ 4000/ 5482]\n",
      "loss: 1.999173  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.200     0.000     0.000    99\n",
      " disgust     0.200     0.000     0.000    107\n",
      "    fear     0.200     0.186     0.100    80\n",
      "   happy     0.200     0.184     0.247    77\n",
      " neutral     0.200     0.000     0.000    95\n",
      "     sad     0.200     0.246     0.703    91\n",
      "surprise     0.200     0.152     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.200     0.110     0.223    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 20.0%, Avg loss: 1.891421 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.907214  [    0/ 5482]\n",
      "loss: 1.767782  [  800/ 5482]\n",
      "loss: 1.839783  [ 1600/ 5482]\n",
      "loss: 1.784438  [ 2400/ 5482]\n",
      "loss: 1.881686  [ 3200/ 5482]\n",
      "loss: 1.867823  [ 4000/ 5482]\n",
      "loss: 1.991162  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.302     0.000     0.000    99\n",
      " disgust     0.302     0.000     0.000    107\n",
      "    fear     0.302     0.250     0.075    80\n",
      "   happy     0.302     0.345     0.390    77\n",
      " neutral     0.302     0.508     0.705    95\n",
      "     sad     0.302     0.264     0.703    91\n",
      "surprise     0.302     0.136     0.279    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.302     0.215     0.307    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 1.843984 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.862096  [    0/ 5482]\n",
      "loss: 1.734271  [  800/ 5482]\n",
      "loss: 1.848278  [ 1600/ 5482]\n",
      "loss: 1.837461  [ 2400/ 5482]\n",
      "loss: 1.854794  [ 3200/ 5482]\n",
      "loss: 1.693991  [ 4000/ 5482]\n",
      "loss: 1.696829  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.385     0.000     0.000    99\n",
      " disgust     0.385     0.415     0.364    107\n",
      "    fear     0.385     0.383     0.225    80\n",
      "   happy     0.385     0.328     0.532    77\n",
      " neutral     0.385     0.536     0.789    95\n",
      "     sad     0.385     0.336     0.549    91\n",
      "surprise     0.385     0.218     0.197    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.385     0.316     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 1.785502 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.777278  [    0/ 5482]\n",
      "loss: 1.876450  [  800/ 5482]\n",
      "loss: 1.524098  [ 1600/ 5482]\n",
      "loss: 1.733956  [ 2400/ 5482]\n",
      "loss: 1.967651  [ 3200/ 5482]\n",
      "loss: 1.567707  [ 4000/ 5482]\n",
      "loss: 1.606992  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.421     0.000     0.000    99\n",
      " disgust     0.421     0.431     0.551    107\n",
      "    fear     0.421     0.417     0.375    80\n",
      "   happy     0.421     0.525     0.403    77\n",
      " neutral     0.421     0.698     0.632    95\n",
      "     sad     0.421     0.342     0.692    91\n",
      "surprise     0.421     0.194     0.230    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.421     0.372     0.412    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.750098 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.667216  [    0/ 5482]\n",
      "loss: 1.641199  [  800/ 5482]\n",
      "loss: 1.901199  [ 1600/ 5482]\n",
      "loss: 1.615652  [ 2400/ 5482]\n",
      "loss: 1.588446  [ 3200/ 5482]\n",
      "loss: 1.450071  [ 4000/ 5482]\n",
      "loss: 1.673841  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.410     0.000     0.000    99\n",
      " disgust     0.410     0.433     0.542    107\n",
      "    fear     0.410     0.368     0.400    80\n",
      "   happy     0.410     0.362     0.494    77\n",
      " neutral     0.410     0.726     0.642    95\n",
      "     sad     0.410     0.340     0.571    91\n",
      "surprise     0.410     0.191     0.148    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.410     0.346     0.400    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.714746 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.615166  [    0/ 5482]\n",
      "loss: 1.394949  [  800/ 5482]\n",
      "loss: 1.704299  [ 1600/ 5482]\n",
      "loss: 1.743499  [ 2400/ 5482]\n",
      "loss: 1.549769  [ 3200/ 5482]\n",
      "loss: 1.601752  [ 4000/ 5482]\n",
      "loss: 1.726425  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.436     0.000     0.000    99\n",
      " disgust     0.436     0.414     0.654    107\n",
      "    fear     0.436     0.636     0.350    80\n",
      "   happy     0.436     0.343     0.597    77\n",
      " neutral     0.436     0.831     0.674    95\n",
      "     sad     0.436     0.331     0.527    91\n",
      "surprise     0.436     0.244     0.164    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.436     0.400     0.424    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.688429 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.511276  [    0/ 5482]\n",
      "loss: 1.696563  [  800/ 5482]\n",
      "loss: 1.456124  [ 1600/ 5482]\n",
      "loss: 1.491773  [ 2400/ 5482]\n",
      "loss: 1.483937  [ 3200/ 5482]\n",
      "loss: 1.406361  [ 4000/ 5482]\n",
      "loss: 1.447079  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.446     1.000     0.010    99\n",
      " disgust     0.446     0.568     0.589    107\n",
      "    fear     0.446     0.396     0.450    80\n",
      "   happy     0.446     0.365     0.545    77\n",
      " neutral     0.446     0.613     0.768    95\n",
      "     sad     0.446     0.344     0.582    91\n",
      "surprise     0.446     0.211     0.066    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.446     0.500     0.430    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 1.661480 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.448362  [    0/ 5482]\n",
      "loss: 1.403199  [  800/ 5482]\n",
      "loss: 1.313673  [ 1600/ 5482]\n",
      "loss: 1.596171  [ 2400/ 5482]\n",
      "loss: 1.483723  [ 3200/ 5482]\n",
      "loss: 1.535372  [ 4000/ 5482]\n",
      "loss: 1.576829  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.452     0.667     0.020    99\n",
      " disgust     0.452     0.578     0.626    107\n",
      "    fear     0.452     0.356     0.463    80\n",
      "   happy     0.452     0.556     0.390    77\n",
      " neutral     0.452     0.730     0.684    95\n",
      "     sad     0.452     0.316     0.681    91\n",
      "surprise     0.452     0.271     0.213    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.452     0.496     0.440    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.630711 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.703178  [    0/ 5482]\n",
      "loss: 1.212268  [  800/ 5482]\n",
      "loss: 1.433164  [ 1600/ 5482]\n",
      "loss: 1.160279  [ 2400/ 5482]\n",
      "loss: 1.161605  [ 3200/ 5482]\n",
      "loss: 1.466926  [ 4000/ 5482]\n",
      "loss: 1.238879  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.431     0.333     0.010    99\n",
      " disgust     0.431     0.594     0.561    107\n",
      "    fear     0.431     0.325     0.487    80\n",
      "   happy     0.431     0.388     0.403    77\n",
      " neutral     0.431     0.654     0.716    95\n",
      "     sad     0.431     0.323     0.582    91\n",
      "surprise     0.431     0.289     0.180    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.431     0.415     0.420    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 1.629555 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.322058  [    0/ 5482]\n",
      "loss: 1.512213  [  800/ 5482]\n",
      "loss: 1.396774  [ 1600/ 5482]\n",
      "loss: 1.147655  [ 2400/ 5482]\n",
      "loss: 1.326792  [ 3200/ 5482]\n",
      "loss: 1.262424  [ 4000/ 5482]\n",
      "loss: 1.467746  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.417     0.051    99\n",
      " disgust     0.492     0.579     0.682    107\n",
      "    fear     0.492     0.388     0.588    80\n",
      "   happy     0.492     0.380     0.494    77\n",
      " neutral     0.492     0.710     0.695    95\n",
      "     sad     0.492     0.465     0.582    91\n",
      "surprise     0.492     0.409     0.295    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.478     0.484    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.580475 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.232786  [    0/ 5482]\n",
      "loss: 1.267095  [  800/ 5482]\n",
      "loss: 1.119038  [ 1600/ 5482]\n",
      "loss: 1.256865  [ 2400/ 5482]\n",
      "loss: 1.332707  [ 3200/ 5482]\n",
      "loss: 0.939848  [ 4000/ 5482]\n",
      "loss: 0.933227  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.421     0.081    99\n",
      " disgust     0.479     0.629     0.617    107\n",
      "    fear     0.479     0.339     0.500    80\n",
      "   happy     0.479     0.426     0.519    77\n",
      " neutral     0.479     0.609     0.705    95\n",
      "     sad     0.479     0.458     0.593    91\n",
      "surprise     0.479     0.370     0.279    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.464     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.542830 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.195101  [    0/ 5482]\n",
      "loss: 1.080965  [  800/ 5482]\n",
      "loss: 1.029226  [ 1600/ 5482]\n",
      "loss: 1.151805  [ 2400/ 5482]\n",
      "loss: 1.403065  [ 3200/ 5482]\n",
      "loss: 0.917663  [ 4000/ 5482]\n",
      "loss: 1.077115  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.536     0.152    99\n",
      " disgust     0.482     0.535     0.710    107\n",
      "    fear     0.482     0.351     0.412    80\n",
      "   happy     0.482     0.361     0.455    77\n",
      " neutral     0.482     0.674     0.674    95\n",
      "     sad     0.482     0.480     0.527    91\n",
      "surprise     0.482     0.426     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.480     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.539288 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.360327  [    0/ 5482]\n",
      "loss: 1.050403  [  800/ 5482]\n",
      "loss: 0.948175  [ 1600/ 5482]\n",
      "loss: 1.468817  [ 2400/ 5482]\n",
      "loss: 0.973431  [ 3200/ 5482]\n",
      "loss: 0.883525  [ 4000/ 5482]\n",
      "loss: 1.139932  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.512     0.212    99\n",
      " disgust     0.493     0.649     0.589    107\n",
      "    fear     0.493     0.350     0.450    80\n",
      "   happy     0.493     0.449     0.455    77\n",
      " neutral     0.493     0.714     0.684    95\n",
      "     sad     0.493     0.415     0.615    91\n",
      "surprise     0.493     0.385     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.496     0.488    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.539721 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.916940  [    0/ 5482]\n",
      "loss: 0.920639  [  800/ 5482]\n",
      "loss: 1.220995  [ 1600/ 5482]\n",
      "loss: 1.091150  [ 2400/ 5482]\n",
      "loss: 0.786147  [ 3200/ 5482]\n",
      "loss: 0.945366  [ 4000/ 5482]\n",
      "loss: 0.858530  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.549     0.588     0.505    99\n",
      " disgust     0.549     0.670     0.607    107\n",
      "    fear     0.549     0.455     0.438    80\n",
      "   happy     0.549     0.446     0.429    77\n",
      " neutral     0.549     0.670     0.684    95\n",
      "     sad     0.549     0.504     0.659    91\n",
      "surprise     0.549     0.443     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.549     0.539     0.538    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.498655 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.938908  [    0/ 5482]\n",
      "loss: 0.821564  [  800/ 5482]\n",
      "loss: 1.049516  [ 1600/ 5482]\n",
      "loss: 0.658149  [ 2400/ 5482]\n",
      "loss: 0.628602  [ 3200/ 5482]\n",
      "loss: 0.873139  [ 4000/ 5482]\n",
      "loss: 1.409938  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.549     0.500     0.596    99\n",
      " disgust     0.549     0.603     0.682    107\n",
      "    fear     0.549     0.429     0.412    80\n",
      "   happy     0.549     0.564     0.403    77\n",
      " neutral     0.549     0.718     0.642    95\n",
      "     sad     0.549     0.610     0.549    91\n",
      "surprise     0.549     0.389     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.549     0.545     0.535    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.515520 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.788202  [    0/ 5482]\n",
      "loss: 1.302617  [  800/ 5482]\n",
      "loss: 0.841320  [ 1600/ 5482]\n",
      "loss: 1.058523  [ 2400/ 5482]\n",
      "loss: 0.775658  [ 3200/ 5482]\n",
      "loss: 0.810447  [ 4000/ 5482]\n",
      "loss: 0.902522  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.505     0.515    99\n",
      " disgust     0.525     0.663     0.533    107\n",
      "    fear     0.525     0.529     0.338    80\n",
      "   happy     0.525     0.516     0.429    77\n",
      " neutral     0.525     0.696     0.674    95\n",
      "     sad     0.525     0.500     0.648    91\n",
      "surprise     0.525     0.296     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.529     0.516    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.506378 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.871138  [    0/ 5482]\n",
      "loss: 0.717625  [  800/ 5482]\n",
      "loss: 0.993123  [ 1600/ 5482]\n",
      "loss: 0.604798  [ 2400/ 5482]\n",
      "loss: 0.936541  [ 3200/ 5482]\n",
      "loss: 1.108650  [ 4000/ 5482]\n",
      "loss: 0.792914  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.504     0.636    99\n",
      " disgust     0.557     0.677     0.607    107\n",
      "    fear     0.557     0.565     0.438    80\n",
      "   happy     0.557     0.553     0.338    77\n",
      " neutral     0.557     0.853     0.674    95\n",
      "     sad     0.557     0.500     0.626    91\n",
      "surprise     0.557     0.330     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.569     0.544    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.480655 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.627865  [    0/ 5482]\n",
      "loss: 0.760656  [  800/ 5482]\n",
      "loss: 0.824651  [ 1600/ 5482]\n",
      "loss: 0.574424  [ 2400/ 5482]\n",
      "loss: 0.559157  [ 3200/ 5482]\n",
      "loss: 0.964761  [ 4000/ 5482]\n",
      "loss: 0.662594  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.614     0.515    99\n",
      " disgust     0.557     0.594     0.710    107\n",
      "    fear     0.557     0.500     0.388    80\n",
      "   happy     0.557     0.412     0.429    77\n",
      " neutral     0.557     0.626     0.705    95\n",
      "     sad     0.557     0.620     0.538    91\n",
      "surprise     0.557     0.465     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.547     0.547    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.465593 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.983647  [    0/ 5482]\n",
      "loss: 0.645089  [  800/ 5482]\n",
      "loss: 0.598933  [ 1600/ 5482]\n",
      "loss: 1.115956  [ 2400/ 5482]\n",
      "loss: 1.152075  [ 3200/ 5482]\n",
      "loss: 1.050358  [ 4000/ 5482]\n",
      "loss: 0.581666  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.552     0.528     0.566    99\n",
      " disgust     0.552     0.700     0.654    107\n",
      "    fear     0.552     0.516     0.412    80\n",
      "   happy     0.552     0.390     0.416    77\n",
      " neutral     0.552     0.602     0.747    95\n",
      "     sad     0.552     0.588     0.549    91\n",
      "surprise     0.552     0.455     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.552     0.540     0.536    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.483538 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.512968  [    0/ 5482]\n",
      "loss: 0.602940  [  800/ 5482]\n",
      "loss: 0.923375  [ 1600/ 5482]\n",
      "loss: 0.691606  [ 2400/ 5482]\n",
      "loss: 0.480390  [ 3200/ 5482]\n",
      "loss: 1.024586  [ 4000/ 5482]\n",
      "loss: 0.434866  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.569     0.604     0.586    99\n",
      " disgust     0.569     0.576     0.673    107\n",
      "    fear     0.569     0.596     0.350    80\n",
      "   happy     0.569     0.556     0.455    77\n",
      " neutral     0.569     0.774     0.684    95\n",
      "     sad     0.569     0.653     0.538    91\n",
      "surprise     0.569     0.333     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.569     0.585     0.563    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.485171 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.479225  [    0/ 5482]\n",
      "loss: 0.502788  [  800/ 5482]\n",
      "loss: 0.915793  [ 1600/ 5482]\n",
      "loss: 0.529612  [ 2400/ 5482]\n",
      "loss: 0.413022  [ 3200/ 5482]\n",
      "loss: 0.512323  [ 4000/ 5482]\n",
      "loss: 0.792363  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.564     0.500     0.576    99\n",
      " disgust     0.564     0.624     0.682    107\n",
      "    fear     0.564     0.625     0.375    80\n",
      "   happy     0.564     0.487     0.506    77\n",
      " neutral     0.564     0.721     0.653    95\n",
      "     sad     0.564     0.636     0.538    91\n",
      "surprise     0.564     0.386     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.564     0.569     0.555    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.507429 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.339264  [    0/ 5482]\n",
      "loss: 0.396059  [  800/ 5482]\n",
      "loss: 0.472728  [ 1600/ 5482]\n",
      "loss: 0.988228  [ 2400/ 5482]\n",
      "loss: 0.664175  [ 3200/ 5482]\n",
      "loss: 0.403081  [ 4000/ 5482]\n",
      "loss: 0.397448  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.564     0.566     0.606    99\n",
      " disgust     0.564     0.554     0.626    107\n",
      "    fear     0.564     0.531     0.425    80\n",
      "   happy     0.564     0.541     0.429    77\n",
      " neutral     0.564     0.673     0.695    95\n",
      "     sad     0.564     0.616     0.582    91\n",
      "surprise     0.564     0.419     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.564     0.557     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.541773 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.381462  [    0/ 5482]\n",
      "loss: 0.454613  [  800/ 5482]\n",
      "loss: 0.359085  [ 1600/ 5482]\n",
      "loss: 0.636493  [ 2400/ 5482]\n",
      "loss: 0.668827  [ 3200/ 5482]\n",
      "loss: 0.311931  [ 4000/ 5482]\n",
      "loss: 0.603877  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.564     0.557     0.495    99\n",
      " disgust     0.564     0.630     0.636    107\n",
      "    fear     0.564     0.608     0.388    80\n",
      "   happy     0.564     0.439     0.558    77\n",
      " neutral     0.564     0.787     0.663    95\n",
      "     sad     0.564     0.614     0.593    91\n",
      "surprise     0.564     0.371     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.564     0.572     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.575763 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.292530  [    0/ 5482]\n",
      "loss: 0.603109  [  800/ 5482]\n",
      "loss: 0.882660  [ 1600/ 5482]\n",
      "loss: 0.315623  [ 2400/ 5482]\n",
      "loss: 0.456338  [ 3200/ 5482]\n",
      "loss: 0.285851  [ 4000/ 5482]\n",
      "loss: 0.349797  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.548     0.576    99\n",
      " disgust     0.551     0.641     0.617    107\n",
      "    fear     0.551     0.514     0.463    80\n",
      "   happy     0.551     0.425     0.442    77\n",
      " neutral     0.551     0.646     0.653    95\n",
      "     sad     0.551     0.560     0.560    91\n",
      "surprise     0.551     0.453     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.541     0.541    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.595655 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.242898  [    0/ 5482]\n",
      "loss: 0.280825  [  800/ 5482]\n",
      "loss: 0.491789  [ 1600/ 5482]\n",
      "loss: 0.937550  [ 2400/ 5482]\n",
      "loss: 0.270761  [ 3200/ 5482]\n",
      "loss: 0.292241  [ 4000/ 5482]\n",
      "loss: 0.200534  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.637     0.586    99\n",
      " disgust     0.577     0.704     0.645    107\n",
      "    fear     0.577     0.481     0.463    80\n",
      "   happy     0.577     0.544     0.403    77\n",
      " neutral     0.577     0.657     0.705    95\n",
      "     sad     0.577     0.568     0.593    91\n",
      "surprise     0.577     0.400     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.570     0.569    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.566895 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.536416  [    0/ 5482]\n",
      "loss: 0.366015  [  800/ 5482]\n",
      "loss: 0.386038  [ 1600/ 5482]\n",
      "loss: 0.340465  [ 2400/ 5482]\n",
      "loss: 0.590927  [ 3200/ 5482]\n",
      "loss: 0.312019  [ 4000/ 5482]\n",
      "loss: 0.490468  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.689     0.626    99\n",
      " disgust     0.595     0.581     0.701    107\n",
      "    fear     0.595     0.607     0.463    80\n",
      "   happy     0.595     0.493     0.468    77\n",
      " neutral     0.595     0.710     0.695    95\n",
      "     sad     0.595     0.611     0.637    91\n",
      "surprise     0.595     0.420     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.587     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.544843 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.251200  [    0/ 5482]\n",
      "loss: 0.315742  [  800/ 5482]\n",
      "loss: 0.205003  [ 1600/ 5482]\n",
      "loss: 0.243245  [ 2400/ 5482]\n",
      "loss: 0.273852  [ 3200/ 5482]\n",
      "loss: 0.223866  [ 4000/ 5482]\n",
      "loss: 0.236665  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.549     0.626    99\n",
      " disgust     0.566     0.782     0.636    107\n",
      "    fear     0.566     0.467     0.438    80\n",
      "   happy     0.566     0.600     0.390    77\n",
      " neutral     0.566     0.695     0.695    95\n",
      "     sad     0.566     0.495     0.571    91\n",
      "surprise     0.566     0.376     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.566     0.554    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.544343 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.329214  [    0/ 5482]\n",
      "loss: 0.222046  [  800/ 5482]\n",
      "loss: 0.242960  [ 1600/ 5482]\n",
      "loss: 0.261995  [ 2400/ 5482]\n",
      "loss: 0.221837  [ 3200/ 5482]\n",
      "loss: 0.348228  [ 4000/ 5482]\n",
      "loss: 0.357637  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.507     0.707    99\n",
      " disgust     0.585     0.591     0.701    107\n",
      "    fear     0.585     0.535     0.475    80\n",
      "   happy     0.585     0.588     0.390    77\n",
      " neutral     0.585     0.843     0.621    95\n",
      "     sad     0.585     0.611     0.604    91\n",
      "surprise     0.585     0.476     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.593     0.570    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.640218 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.276115  [    0/ 5482]\n",
      "loss: 0.246663  [  800/ 5482]\n",
      "loss: 0.178105  [ 1600/ 5482]\n",
      "loss: 0.240350  [ 2400/ 5482]\n",
      "loss: 0.499330  [ 3200/ 5482]\n",
      "loss: 0.604351  [ 4000/ 5482]\n",
      "loss: 0.312096  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.644     0.657    99\n",
      " disgust     0.585     0.628     0.664    107\n",
      "    fear     0.585     0.610     0.450    80\n",
      "   happy     0.585     0.586     0.442    77\n",
      " neutral     0.585     0.776     0.621    95\n",
      "     sad     0.585     0.607     0.593    91\n",
      "surprise     0.585     0.333     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.598     0.578    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.602451 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.314578  [    0/ 5482]\n",
      "loss: 0.252120  [  800/ 5482]\n",
      "loss: 0.188002  [ 1600/ 5482]\n",
      "loss: 0.260722  [ 2400/ 5482]\n",
      "loss: 0.358494  [ 3200/ 5482]\n",
      "loss: 0.164365  [ 4000/ 5482]\n",
      "loss: 0.137660  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.720     0.596    99\n",
      " disgust     0.602     0.689     0.682    107\n",
      "    fear     0.602     0.538     0.525    80\n",
      "   happy     0.602     0.470     0.506    77\n",
      " neutral     0.602     0.652     0.768    95\n",
      "     sad     0.602     0.649     0.549    91\n",
      "surprise     0.602     0.431     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.593     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.559181 \n",
      "\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/Run_Nr_0_conv/emo_reco_best_ep30_acc_60\"!  new accuracy: 60.2\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.527312  [    0/ 5482]\n",
      "loss: 0.165034  [  800/ 5482]\n",
      "loss: 0.226815  [ 1600/ 5482]\n",
      "loss: 0.232713  [ 2400/ 5482]\n",
      "loss: 0.471618  [ 3200/ 5482]\n",
      "loss: 0.995287  [ 4000/ 5482]\n",
      "loss: 0.219216  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.658     0.525    99\n",
      " disgust     0.551     0.667     0.617    107\n",
      "    fear     0.551     0.427     0.475    80\n",
      "   happy     0.551     0.500     0.429    77\n",
      " neutral     0.551     0.797     0.621    95\n",
      "     sad     0.551     0.541     0.648    91\n",
      "surprise     0.551     0.309     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.557     0.541    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.710996 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.383835  [    0/ 5482]\n",
      "loss: 0.148269  [  800/ 5482]\n",
      "loss: 0.162592  [ 1600/ 5482]\n",
      "loss: 0.346746  [ 2400/ 5482]\n",
      "loss: 0.313946  [ 3200/ 5482]\n",
      "loss: 0.399177  [ 4000/ 5482]\n",
      "loss: 0.242546  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.765     0.525    99\n",
      " disgust     0.593     0.670     0.645    107\n",
      "    fear     0.593     0.613     0.475    80\n",
      "   happy     0.593     0.449     0.519    77\n",
      " neutral     0.593     0.750     0.695    95\n",
      "     sad     0.593     0.492     0.648    91\n",
      "surprise     0.593     0.475     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.602     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.635274 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.090907  [    0/ 5482]\n",
      "loss: 0.145293  [  800/ 5482]\n",
      "loss: 0.203539  [ 1600/ 5482]\n",
      "loss: 0.428915  [ 2400/ 5482]\n",
      "loss: 0.161602  [ 3200/ 5482]\n",
      "loss: 0.161225  [ 4000/ 5482]\n",
      "loss: 0.294255  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.594     0.606    99\n",
      " disgust     0.562     0.700     0.589    107\n",
      "    fear     0.562     0.536     0.463    80\n",
      "   happy     0.562     0.558     0.377    77\n",
      " neutral     0.562     0.701     0.642    95\n",
      "     sad     0.562     0.557     0.648    91\n",
      "surprise     0.562     0.324     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.567     0.555    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.818523 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.114150  [    0/ 5482]\n",
      "loss: 0.146719  [  800/ 5482]\n",
      "loss: 0.371286  [ 1600/ 5482]\n",
      "loss: 0.205408  [ 2400/ 5482]\n",
      "loss: 0.187176  [ 3200/ 5482]\n",
      "loss: 0.083881  [ 4000/ 5482]\n",
      "loss: 0.190956  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.642     0.616    99\n",
      " disgust     0.574     0.607     0.664    107\n",
      "    fear     0.574     0.554     0.512    80\n",
      "   happy     0.574     0.554     0.403    77\n",
      " neutral     0.574     0.701     0.642    95\n",
      "     sad     0.574     0.505     0.615    91\n",
      "surprise     0.574     0.414     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.568     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.748375 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.325850  [    0/ 5482]\n",
      "loss: 0.384716  [  800/ 5482]\n",
      "loss: 0.197354  [ 1600/ 5482]\n",
      "loss: 0.143243  [ 2400/ 5482]\n",
      "loss: 0.104115  [ 3200/ 5482]\n",
      "loss: 0.154327  [ 4000/ 5482]\n",
      "loss: 0.113415  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.674     0.646    99\n",
      " disgust     0.584     0.597     0.664    107\n",
      "    fear     0.584     0.550     0.412    80\n",
      "   happy     0.584     0.500     0.429    77\n",
      " neutral     0.584     0.793     0.684    95\n",
      "     sad     0.584     0.582     0.582    91\n",
      "surprise     0.584     0.381     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.582     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.788161 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.161281  [    0/ 5482]\n",
      "loss: 0.517226  [  800/ 5482]\n",
      "loss: 0.226443  [ 1600/ 5482]\n",
      "loss: 0.126100  [ 2400/ 5482]\n",
      "loss: 0.086076  [ 3200/ 5482]\n",
      "loss: 0.119902  [ 4000/ 5482]\n",
      "loss: 0.120303  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.632     0.606    99\n",
      " disgust     0.579     0.702     0.617    107\n",
      "    fear     0.579     0.608     0.388    80\n",
      "   happy     0.579     0.461     0.455    77\n",
      " neutral     0.579     0.750     0.726    95\n",
      "     sad     0.579     0.555     0.670    91\n",
      "surprise     0.579     0.337     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.578     0.567    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.802796 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.141239  [    0/ 5482]\n",
      "loss: 0.123364  [  800/ 5482]\n",
      "loss: 0.154644  [ 1600/ 5482]\n",
      "loss: 0.346782  [ 2400/ 5482]\n",
      "loss: 0.621976  [ 3200/ 5482]\n",
      "loss: 0.173077  [ 4000/ 5482]\n",
      "loss: 0.151005  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.569     0.594     0.576    99\n",
      " disgust     0.569     0.648     0.636    107\n",
      "    fear     0.569     0.567     0.475    80\n",
      "   happy     0.569     0.487     0.494    77\n",
      " neutral     0.569     0.769     0.632    95\n",
      "     sad     0.569     0.491     0.615    91\n",
      "surprise     0.569     0.417     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.569     0.568     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.811525 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.131903  [    0/ 5482]\n",
      "loss: 0.176070  [  800/ 5482]\n",
      "loss: 0.129641  [ 1600/ 5482]\n",
      "loss: 0.090903  [ 2400/ 5482]\n",
      "loss: 0.146827  [ 3200/ 5482]\n",
      "loss: 0.084462  [ 4000/ 5482]\n",
      "loss: 0.119717  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.671     0.556    99\n",
      " disgust     0.574     0.640     0.598    107\n",
      "    fear     0.574     0.571     0.450    80\n",
      "   happy     0.574     0.449     0.519    77\n",
      " neutral     0.574     0.688     0.674    95\n",
      "     sad     0.574     0.532     0.637    91\n",
      "surprise     0.574     0.446     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.571     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.854781 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.157796  [    0/ 5482]\n",
      "loss: 0.139769  [  800/ 5482]\n",
      "loss: 0.064398  [ 1600/ 5482]\n",
      "loss: 0.117214  [ 2400/ 5482]\n",
      "loss: 0.486589  [ 3200/ 5482]\n",
      "loss: 0.899024  [ 4000/ 5482]\n",
      "loss: 0.127768  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.569     0.573     0.636    99\n",
      " disgust     0.569     0.574     0.617    107\n",
      "    fear     0.569     0.653     0.400    80\n",
      "   happy     0.569     0.463     0.481    77\n",
      " neutral     0.569     0.897     0.642    95\n",
      "     sad     0.569     0.671     0.582    91\n",
      "surprise     0.569     0.321     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.569     0.593     0.562    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.911978 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.160965  [    0/ 5482]\n",
      "loss: 0.454763  [  800/ 5482]\n",
      "loss: 0.108527  [ 1600/ 5482]\n",
      "loss: 0.066825  [ 2400/ 5482]\n",
      "loss: 0.093974  [ 3200/ 5482]\n",
      "loss: 0.426709  [ 4000/ 5482]\n",
      "loss: 0.110305  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.740     0.545    99\n",
      " disgust     0.577     0.651     0.664    107\n",
      "    fear     0.577     0.578     0.463    80\n",
      "   happy     0.577     0.486     0.455    77\n",
      " neutral     0.577     0.776     0.695    95\n",
      "     sad     0.577     0.578     0.571    91\n",
      "surprise     0.577     0.316     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.589     0.571    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.912298 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.082960  [    0/ 5482]\n",
      "loss: 0.193709  [  800/ 5482]\n",
      "loss: 0.095575  [ 1600/ 5482]\n",
      "loss: 0.125005  [ 2400/ 5482]\n",
      "loss: 0.071743  [ 3200/ 5482]\n",
      "loss: 0.119332  [ 4000/ 5482]\n",
      "loss: 0.072477  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.610     0.616    99\n",
      " disgust     0.590     0.622     0.692    107\n",
      "    fear     0.590     0.603     0.475    80\n",
      "   happy     0.590     0.437     0.494    77\n",
      " neutral     0.590     0.730     0.684    95\n",
      "     sad     0.590     0.671     0.560    91\n",
      "surprise     0.590     0.434     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.587     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.868953 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.514234  [    0/ 5482]\n",
      "loss: 0.136287  [  800/ 5482]\n",
      "loss: 0.062865  [ 1600/ 5482]\n",
      "loss: 0.060316  [ 2400/ 5482]\n",
      "loss: 0.239475  [ 3200/ 5482]\n",
      "loss: 0.085183  [ 4000/ 5482]\n",
      "loss: 0.057273  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.604     0.586    99\n",
      " disgust     0.557     0.697     0.579    107\n",
      "    fear     0.557     0.487     0.475    80\n",
      "   happy     0.557     0.457     0.481    77\n",
      " neutral     0.557     0.762     0.642    95\n",
      "     sad     0.557     0.544     0.615    91\n",
      "surprise     0.557     0.337     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.555     0.548    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 2.023300 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.078078  [    0/ 5482]\n",
      "loss: 0.063768  [  800/ 5482]\n",
      "loss: 0.822505  [ 1600/ 5482]\n",
      "loss: 0.096935  [ 2400/ 5482]\n",
      "loss: 0.084249  [ 3200/ 5482]\n",
      "loss: 0.419761  [ 4000/ 5482]\n",
      "loss: 0.568775  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.539     0.710     0.444    99\n",
      " disgust     0.539     0.674     0.579    107\n",
      "    fear     0.539     0.471     0.500    80\n",
      "   happy     0.539     0.470     0.506    77\n",
      " neutral     0.539     0.685     0.642    95\n",
      "     sad     0.539     0.431     0.648    91\n",
      "surprise     0.539     0.387     0.393    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.539     0.547     0.531    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 2.197325 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.052760  [    0/ 5482]\n",
      "loss: 0.569688  [  800/ 5482]\n",
      "loss: 0.071853  [ 1600/ 5482]\n",
      "loss: 0.048168  [ 2400/ 5482]\n",
      "loss: 0.107570  [ 3200/ 5482]\n",
      "loss: 0.115608  [ 4000/ 5482]\n",
      "loss: 0.071669  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.656     0.596    99\n",
      " disgust     0.574     0.653     0.579    107\n",
      "    fear     0.574     0.678     0.500    80\n",
      "   happy     0.574     0.471     0.429    77\n",
      " neutral     0.574     0.630     0.716    95\n",
      "     sad     0.574     0.583     0.659    91\n",
      "surprise     0.574     0.329     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.571     0.563    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 2.004860 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.065622  [    0/ 5482]\n",
      "loss: 0.094378  [  800/ 5482]\n",
      "loss: 0.062879  [ 1600/ 5482]\n",
      "loss: 0.085994  [ 2400/ 5482]\n",
      "loss: 0.098616  [ 3200/ 5482]\n",
      "loss: 0.275174  [ 4000/ 5482]\n",
      "loss: 0.081291  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.644     0.586    99\n",
      " disgust     0.567     0.688     0.617    107\n",
      "    fear     0.567     0.534     0.487    80\n",
      "   happy     0.567     0.485     0.429    77\n",
      " neutral     0.567     0.808     0.663    95\n",
      "     sad     0.567     0.443     0.681    91\n",
      "surprise     0.567     0.385     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.570     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.940433 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.070321  [    0/ 5482]\n",
      "loss: 0.052508  [  800/ 5482]\n",
      "loss: 0.116990  [ 1600/ 5482]\n",
      "loss: 0.056648  [ 2400/ 5482]\n",
      "loss: 0.051155  [ 3200/ 5482]\n",
      "loss: 0.063230  [ 4000/ 5482]\n",
      "loss: 0.065697  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.624     0.586    99\n",
      " disgust     0.574     0.773     0.542    107\n",
      "    fear     0.574     0.587     0.463    80\n",
      "   happy     0.574     0.438     0.506    77\n",
      " neutral     0.574     0.708     0.716    95\n",
      "     sad     0.574     0.542     0.637    91\n",
      "surprise     0.574     0.368     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.577     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 2.037490 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.097147  [    0/ 5482]\n",
      "loss: 0.071137  [  800/ 5482]\n",
      "loss: 0.085105  [ 1600/ 5482]\n",
      "loss: 0.471106  [ 2400/ 5482]\n",
      "loss: 0.092649  [ 3200/ 5482]\n",
      "loss: 0.229619  [ 4000/ 5482]\n",
      "loss: 0.879536  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.591     0.556    99\n",
      " disgust     0.566     0.678     0.551    107\n",
      "    fear     0.566     0.590     0.450    80\n",
      "   happy     0.566     0.559     0.429    77\n",
      " neutral     0.566     0.738     0.653    95\n",
      "     sad     0.566     0.504     0.714    91\n",
      "surprise     0.566     0.361     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.575     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.067655 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.040011  [    0/ 5482]\n",
      "loss: 0.063856  [  800/ 5482]\n",
      "loss: 0.068364  [ 1600/ 5482]\n",
      "loss: 0.100083  [ 2400/ 5482]\n",
      "loss: 0.208915  [ 3200/ 5482]\n",
      "loss: 0.054814  [ 4000/ 5482]\n",
      "loss: 0.099389  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.609     0.535    99\n",
      " disgust     0.566     0.643     0.589    107\n",
      "    fear     0.566     0.574     0.438    80\n",
      "   happy     0.566     0.515     0.455    77\n",
      " neutral     0.566     0.744     0.674    95\n",
      "     sad     0.566     0.527     0.648    91\n",
      "surprise     0.566     0.367     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.568     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.183970 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.066116  [    0/ 5482]\n",
      "loss: 0.051547  [  800/ 5482]\n",
      "loss: 0.040652  [ 1600/ 5482]\n",
      "loss: 0.415136  [ 2400/ 5482]\n",
      "loss: 0.742446  [ 3200/ 5482]\n",
      "loss: 0.075057  [ 4000/ 5482]\n",
      "loss: 0.068562  [ 4800/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.704     0.576    99\n",
      " disgust     0.572     0.568     0.664    107\n",
      "    fear     0.572     0.614     0.438    80\n",
      "   happy     0.572     0.528     0.494    77\n",
      " neutral     0.572     0.698     0.632    95\n",
      "     sad     0.572     0.550     0.604    91\n",
      "surprise     0.572     0.371     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.576     0.564    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 2.047110 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.547814  [    0/ 5482]\n",
      "loss: 0.047295  [  800/ 5482]\n",
      "loss: 0.087431  [ 1600/ 5482]\n",
      "loss: 0.062279  [ 2400/ 5482]\n",
      "loss: 0.055878  [ 3200/ 5482]\n",
      "loss: 0.045947  [ 4000/ 5482]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
