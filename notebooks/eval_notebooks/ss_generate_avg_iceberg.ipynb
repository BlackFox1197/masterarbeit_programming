{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "from network_models.soundsream_models_and_utils.ss_model_dim_red import SSDimRedModel\n",
    "from network_models.soundsream_models_and_utils.ss_model_conv import SSConvModel3Sec\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings.pkl\", device=\"cuda\")\n",
    "\n",
    "json_path =\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/\"\n",
    "\n",
    "modelDimRed = SSDimRedModel(eval_mode=True).cuda()\n",
    "modelConv = SSConvModel3Sec(xSize=512, ySize=175, eval_mode=True).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelConv.load_state_dict(torch.load(\"../content/soundstream/experiments/nr2Run_Nr_0_conv/emo_reco_best_ep216_acc_62\"))\n",
    "modelDimRed.load_state_dict(torch.load(\"../content/soundstream/experiments/nr2Run_Nr_0_dimred/emo_reco_500.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "def generateParameters(device, model, dataset, labels):\n",
    "    loader = DataLoader(dataset, shuffle=False, batch_size=1)\n",
    "    d ={}\n",
    "\n",
    "    for label in labels:\n",
    "        d.update({label: []})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, z) in enumerate(loader):\n",
    "            X, z = X.to(device),  z.to(device)\n",
    "            dims, pred = model(X, return_with_dims= True)\n",
    "            label = data_set.encoded_dataset.label_list[z[0].nonzero().cpu().numpy()[0][0]]\n",
    "            dims = dims.cpu().numpy()[0]\n",
    "            d[label].append(dims)\n",
    "            if batch % 500 == 0:\n",
    "                print(f\"{batch*batch_size} of {len(dataset)}\")\n",
    "    return d\n",
    "\n",
    "\n",
    "def genAvgFull(fulldata: list[dict], label_list, modelNameList):\n",
    "    fullDict = {}\n",
    "    for label in label_list:\n",
    "        fullDict.update({label: genAvgAndVar([fd[label] for fd in fulldata], modelNameList)})\n",
    "    return fullDict\n",
    "\n",
    "def genAvgAndVar(data: list[list[ndarray[float]]], modelNameList):\n",
    "    d = {}\n",
    "    for i in range(len(modelNameList)):\n",
    "        mean = np.mean(np.asarray(data[i]), axis=0)\n",
    "        var = np.var(np.asarray(data[i]), axis=0)\n",
    "        d.update({modelNameList[i] : pd.Series([mean, var], index=[\"mean\", \"var\"])})\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "\n",
    "def generateJson(data):\n",
    "    str = \"{\"\n",
    "    for i in range(len(data)):\n",
    "        str += f\"\\\"x{i+1}\\\": {data[i]}\"\n",
    "        str += \",\" if i < len(data)-1 else \"\"\n",
    "    str += \"}\"\n",
    "    return(str)\n",
    "\n",
    "def writeJson(emotion, string, apendix=\"\"):\n",
    "    print(f\"writing: {string} \\n to {json_path+emotion}.json\")\n",
    "    file = open(json_path+f\"{emotion}{apendix}.json\", \"w\")\n",
    "    file.write(string)\n",
    "    file.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 6092\n",
      "500 of 6092\n",
      "1000 of 6092\n",
      "1500 of 6092\n",
      "2000 of 6092\n",
      "2500 of 6092\n",
      "3000 of 6092\n",
      "3500 of 6092\n",
      "4000 of 6092\n",
      "4500 of 6092\n",
      "5000 of 6092\n",
      "5500 of 6092\n",
      "6000 of 6092\n",
      "0 of 6092\n",
      "500 of 6092\n",
      "1000 of 6092\n",
      "1500 of 6092\n",
      "2000 of 6092\n",
      "2500 of 6092\n",
      "3000 of 6092\n",
      "3500 of 6092\n",
      "4000 of 6092\n",
      "4500 of 6092\n",
      "5000 of 6092\n",
      "5500 of 6092\n",
      "6000 of 6092\n"
     ]
    }
   ],
   "source": [
    "emo = generateParameters(\"cuda\", modelDimRed, data_set, data_set.encoded_dataset.label_list)\n",
    "conv_emo = generateParameters(\"cuda\", modelConv, data_set, data_set.encoded_dataset.label_list)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "fullDict = genAvgFull([emo, conv_emo], data_set.encoded_dataset.label_list, [\"red\", \"conv\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral :  redDim: [ 0.6616187   0.73795056 -0.744872   -0.633462  ], conv: [-0.40242827 -0.9487567   0.21536633 -0.91296506]\n",
      "happy   :  redDim: [-0.60835606 -0.6879876  -0.16170374 -0.5159379 ], conv: [-0.9162042   0.81222427 -0.94405884  0.55082613]\n",
      "sad     :  redDim: [ 0.7676968   0.53488284  0.55854934 -0.69721246], conv: [-0.32716918  0.8042437   0.89900833 -0.5743747 ]\n",
      "angry   :  redDim: [-0.6810923  -0.5163825   0.65164894  0.73342377], conv: [ 0.93623155 -0.94140714 -0.9219744   0.4013685 ]\n",
      "fear    :  redDim: [0.69507694 0.6124118  0.42723086 0.6004365 ], conv: [ 0.91713196  0.89425254 -0.6344462   0.9102133 ]\n",
      "disgust :  redDim: [ 0.6164171  -0.7668218   0.582811   -0.56092954], conv: [-0.79294395 -0.9409855   0.8996731   0.93437487]\n",
      "surprise:  redDim: [ 0.68390673 -0.65616286  0.49200478  0.62585956], conv: [ 0.89045614  0.79051965 -0.9374066  -0.9384939 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"neutral :  redDim: {fullDict['neutral']['red']['mean']}, conv: {fullDict['neutral']['conv']['mean']}\")\n",
    "print(f\"happy   :  redDim: {fullDict['happy']['red']['mean']}, conv: {fullDict['happy']['conv']['mean']}\")\n",
    "print(f\"sad     :  redDim: {fullDict['sad']['red']['mean']}, conv: {fullDict['sad']['conv']['mean']}\")\n",
    "print(f\"angry   :  redDim: {fullDict['angry']['red']['mean']}, conv: {fullDict['angry']['conv']['mean']}\")\n",
    "print(f\"fear    :  redDim: {fullDict['fear']['red']['mean']}, conv: {fullDict['fear']['conv']['mean']}\")\n",
    "print(f\"disgust :  redDim: {fullDict['disgust']['red']['mean']}, conv: {fullDict['disgust']['conv']['mean']}\")\n",
    "print(f\"surprise:  redDim: {fullDict['surprise']['red']['mean']}, conv: {fullDict['surprise']['conv']['mean']}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing: {\"x1\": -0.4024282693862915,\"x2\": -0.9487566947937012,\"x3\": 0.21536633372306824,\"x4\": -0.9129650592803955} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/neutral.json\n",
      "writing: {\"x1\": -0.9162042140960693,\"x2\": 0.812224268913269,\"x3\": -0.9440588355064392,\"x4\": 0.5508261322975159} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/happy.json\n",
      "writing: {\"x1\": -0.32716917991638184,\"x2\": 0.8042436838150024,\"x3\": 0.8990083336830139,\"x4\": -0.5743746757507324} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/sad.json\n",
      "writing: {\"x1\": 0.9362315535545349,\"x2\": -0.9414071440696716,\"x3\": -0.9219744205474854,\"x4\": 0.40136849880218506} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/angry.json\n",
      "writing: {\"x1\": 0.9171319603919983,\"x2\": 0.8942525386810303,\"x3\": -0.6344462037086487,\"x4\": 0.91021329164505} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/fear.json\n",
      "writing: {\"x1\": -0.7929439544677734,\"x2\": -0.9409855008125305,\"x3\": 0.8996731042861938,\"x4\": 0.9343748688697815} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/disgust.json\n",
      "writing: {\"x1\": 0.8904561400413513,\"x2\": 0.790519654750824,\"x3\": -0.937406599521637,\"x4\": -0.9384939074516296} \n",
      " to /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/surprise.json\n"
     ]
    }
   ],
   "source": [
    "writeJson(\"neutral\", generateJson(fullDict['neutral']['conv']['mean']))\n",
    "writeJson(\"happy\", generateJson(fullDict['happy']['conv']['mean']))\n",
    "writeJson(\"sad\", generateJson(fullDict['sad']['conv']['mean']))\n",
    "writeJson(\"angry\", generateJson(fullDict['angry']['conv']['mean']))\n",
    "writeJson(\"fear\", generateJson(fullDict['fear']['conv']['mean']))\n",
    "writeJson(\"disgust\", generateJson(fullDict['disgust']['conv']['mean']))\n",
    "writeJson(\"surprise\", generateJson(fullDict['surprise']['conv']['mean']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSConvModel3Sec\n"
     ]
    }
   ],
   "source": [
    "f\"{modelConv.__class__}\".split('.')[-1].split(\"'>\")[0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Umap Based"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "enc_ds= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/encoder_datasets/allEncodings_clip_like.pkl\", device=\"cuda\")\n",
    "\n",
    "lst = [tens.numpy() for tens in enc_ds.encoded_dataset.encodedData[\"encoded\"]]\n",
    "\n",
    "df = enc_ds.encoded_dataset.encodedData\n",
    "\n",
    "pca = umap.UMAP(n_components=4, n_neighbors=5000)\n",
    "out = pca.fit(np.asarray(lst))\n",
    "pcaOut = np.asarray([i for i in out.embedding_])\n",
    "pcaOut = 2 * (pcaOut - pcaOut.min(axis=0)) / (pcaOut.max(axis=0) - pcaOut.min(axis=0)) - 1\n",
    "#df[\"pca\"]=[i for i in out.embedding_]\n",
    "df[\"pca\"] = pcaOut.tolist()\n",
    "# pca = PCA(n_components=4)\n",
    "# out = pca.fit(np.asarray(lst).T)\n",
    "#\n",
    "# # np.asarray(lst).T\n",
    "# df[\"pca\"]=[i for i in out.components_.T]\n",
    "\n",
    "array = np.array(df.loc[df[\"clear_emotion\"].isin([\"happy\"])][\"pca\"].values.tolist(), dtype=object)\n",
    "emo_dict_mean =  {}\n",
    "\n",
    "# for label in enc_ds.encoded_dataset.label_list:\n",
    "#     pca = np.array(df.loc[df[\"clear_emotion\"].isin([label])][\"pca\"].values.tolist(), dtype=object)\n",
    "#     normalized = 2 * (pca - pca.min(axis=0)) / (pca.max(axis=0) - pca.min(axis=0)) - 1\n",
    "#     emo_dict_mean.update({label: normalized.mean(axis=0)})\n",
    "#     writeJson(label, generateJson(normalized.mean(axis=0)), apendix=\"pca\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "6092"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for label in enc_ds.encoded_dataset.label_list:\n",
    "    pca = np.array(df.loc[df[\"clear_emotion\"].isin([label])][\"pca\"].values.tolist(), dtype=object)\n",
    "    #normalized = 2 * (pca - pca.min(axis=0)) / (pca.max(axis=0) - pca.min(axis=0)) - 1\n",
    "    normalized = pca\n",
    "    emo_dict_mean.update({label: normalized.mean(axis=0)})\n",
    "    writeJson(label, generateJson(normalized.mean(axis=0)), apendix=\"pca\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
