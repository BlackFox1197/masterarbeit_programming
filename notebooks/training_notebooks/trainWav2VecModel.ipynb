{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:48:46.006330: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 13:48:46.687278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 13:48:46.687337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-02 13:48:46.687343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-02 13:48:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...generating encoding\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "# module_path = str(Path.cwd().parents[0] / \"utils\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/w2v_emotion_model\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_lstm\")\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "\n",
    "import utils.wav2Vec_utils as w2vU\n",
    "import utils.audio_dataset_utils as ADU\n",
    "import network_models.soundstream_lstm.LSTM_dataset as lds\n",
    "import network_models.soundstream_lstm.CombinedEmoDataset_7_Emo as ced\n",
    "import  network_models.w2v_emotion_model.custom_collator as cc\n",
    "import network_models.w2v_emotion_model.custom_model as cm\n",
    "import network_models.w2v_emotion_model.model_trainer as ct\n",
    "import gc\n",
    "\n",
    "model_name_or_path = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "pooling_mode = \"mean\"\n",
    "device = \"cuda\"\n",
    "emo_dataset = ced.CombinedEmoDataSet_7_emos(directory_tess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\",\n",
    "                                             directory_cafe=\"/home/ckwdani/Music/emotionDatasets/converted_mono/cafe\",\n",
    "                                             directory_ravdess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/RAVDESS Audio_Speech_Actors_01-24\",\n",
    "                                             directory_mesd=\"/home/ckwdani/Music/emotionDatasets/converted_mono/mesd\",\n",
    "                                             device=\"cpu\")\n",
    "\n",
    "emo_dataset = lds.AudioEmotionTessDataset(directory=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\", device=device)\n",
    "processor, sr = w2vU.init_w2v2(num_labels=len(emo_dataset.label_list), label_list=emo_dataset.label_list, device=device)\n",
    "newSet = lds.AudioEmotionTessWav2VecDataset(emo_dataset, processor= processor, sampling_rate=sr)\n",
    "gc.collect()\n",
    "model = cm.Wav2Vec2ForSpeechClassification(model_name_or_path=model_name_or_path, pooling_mode=\"mean\", device=device)\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "\n",
    "trainSet, evalSet = ADU.train_val_dataset(newSet, 0.1)\n",
    "#trainSet, evalSet = ADU.train_val_dataset(evalSet, 0.1)\n",
    "\n",
    "\n",
    "data_collator = cc.DataCollatorCTCWithPadding(processor=processor, padding=True, num_labels=len(newSet.dataSet.label_list))\n",
    "\n",
    "# trainer = ct.ModelTrainer(model=model, need_reshape=False, train_dataset=trainSet, eval_dataset=evalSet, device=device, batch_size=2, save_model_every=500, num_epochs=5001, model_path=\"../content/classifier/W2VClassifier/Nr1/\", data_collator=data_collator)\n",
    "# gc.collect()\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"../content/classifier/W2VClassifier/Nr1/emo_reco_950.pth\"))\n",
    "#model.from_pretrained()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/w2v_emotion_model/custom_model.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y = softmax(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.739951  [    0/ 2482]\n",
      "loss: 0.742032  [  400/ 2482]\n",
      "loss: 0.747976  [  800/ 2482]\n",
      "loss: 0.745455  [ 1200/ 2482]\n",
      "loss: 0.751447  [ 1600/ 2482]\n",
      "loss: 0.747257  [ 2000/ 2482]\n",
      "loss: 0.749066  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 13.0%, Avg loss: 0.747741 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.741612  [    0/ 2482]\n",
      "loss: 0.750302  [  400/ 2482]\n",
      "loss: 0.741826  [  800/ 2482]\n",
      "loss: 0.750142  [ 1200/ 2482]\n",
      "loss: 0.745453  [ 1600/ 2482]\n",
      "loss: 0.742933  [ 2000/ 2482]\n",
      "loss: 0.735791  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 13.4%, Avg loss: 0.747312 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.739388  [    0/ 2482]\n",
      "loss: 0.747516  [  400/ 2482]\n",
      "loss: 0.742116  [  800/ 2482]\n",
      "loss: 0.747162  [ 1200/ 2482]\n",
      "loss: 0.740377  [ 1600/ 2482]\n",
      "loss: 0.747455  [ 2000/ 2482]\n",
      "loss: 0.746063  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 12.3%, Avg loss: 0.747556 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.746844  [    0/ 2482]\n",
      "loss: 0.742499  [  400/ 2482]\n",
      "loss: 0.751830  [  800/ 2482]\n",
      "loss: 0.746748  [ 1200/ 2482]\n",
      "loss: 0.746766  [ 1600/ 2482]\n",
      "loss: 0.745212  [ 2000/ 2482]\n",
      "loss: 0.745771  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 0.747234 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.744893  [    0/ 2482]\n",
      "loss: 0.748155  [  400/ 2482]\n",
      "loss: 0.746793  [  800/ 2482]\n",
      "loss: 0.745575  [ 1200/ 2482]\n",
      "loss: 0.749757  [ 1600/ 2482]\n",
      "loss: 0.744406  [ 2000/ 2482]\n",
      "loss: 0.746951  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 14.5%, Avg loss: 0.746582 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.743099  [    0/ 2482]\n",
      "loss: 0.750271  [  400/ 2482]\n",
      "loss: 0.743992  [  800/ 2482]\n",
      "loss: 0.748810  [ 1200/ 2482]\n",
      "loss: 0.747234  [ 1600/ 2482]\n",
      "loss: 0.744492  [ 2000/ 2482]\n",
      "loss: 0.747015  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 13.4%, Avg loss: 0.747135 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.747130  [    0/ 2482]\n",
      "loss: 0.755762  [  400/ 2482]\n",
      "loss: 0.754031  [  800/ 2482]\n",
      "loss: 0.749453  [ 1200/ 2482]\n",
      "loss: 0.746930  [ 1600/ 2482]\n",
      "loss: 0.741677  [ 2000/ 2482]\n",
      "loss: 0.750395  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 12.0%, Avg loss: 0.746844 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.747286  [    0/ 2482]\n",
      "loss: 0.747101  [  400/ 2482]\n",
      "loss: 0.750664  [  800/ 2482]\n",
      "loss: 0.749799  [ 1200/ 2482]\n",
      "loss: 0.741667  [ 1600/ 2482]\n",
      "loss: 0.745211  [ 2000/ 2482]\n",
      "loss: 0.749044  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 0.746739 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.750674  [    0/ 2482]\n",
      "loss: 0.746678  [  400/ 2482]\n",
      "loss: 0.742027  [  800/ 2482]\n",
      "loss: 0.745087  [ 1200/ 2482]\n",
      "loss: 0.747628  [ 1600/ 2482]\n",
      "loss: 0.747220  [ 2000/ 2482]\n",
      "loss: 0.748639  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 12.0%, Avg loss: 0.746848 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.748320  [    0/ 2482]\n",
      "loss: 0.747918  [  400/ 2482]\n",
      "loss: 0.745530  [  800/ 2482]\n",
      "loss: 0.745000  [ 1200/ 2482]\n",
      "loss: 0.746866  [ 1600/ 2482]\n",
      "loss: 0.748152  [ 2000/ 2482]\n",
      "loss: 0.747478  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 12.3%, Avg loss: 0.747013 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.741631  [    0/ 2482]\n",
      "loss: 0.748154  [  400/ 2482]\n",
      "loss: 0.746045  [  800/ 2482]\n",
      "loss: 0.745999  [ 1200/ 2482]\n",
      "loss: 0.744397  [ 1600/ 2482]\n",
      "loss: 0.748483  [ 2000/ 2482]\n",
      "loss: 0.750703  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 12.3%, Avg loss: 0.746458 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.745577  [    0/ 2482]\n",
      "loss: 0.747517  [  400/ 2482]\n",
      "loss: 0.741553  [  800/ 2482]\n",
      "loss: 0.746580  [ 1200/ 2482]\n",
      "loss: 0.746931  [ 1600/ 2482]\n",
      "loss: 0.747192  [ 2000/ 2482]\n",
      "loss: 0.744813  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 0.747027 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.744220  [    0/ 2482]\n",
      "loss: 0.746484  [  400/ 2482]\n",
      "loss: 0.744741  [  800/ 2482]\n",
      "loss: 0.747595  [ 1200/ 2482]\n",
      "loss: 0.747857  [ 1600/ 2482]\n",
      "loss: 0.745311  [ 2000/ 2482]\n",
      "loss: 0.746885  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 0.746879 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.750206  [    0/ 2482]\n",
      "loss: 0.747022  [  400/ 2482]\n",
      "loss: 0.751114  [  800/ 2482]\n",
      "loss: 0.745002  [ 1200/ 2482]\n",
      "loss: 0.745270  [ 1600/ 2482]\n",
      "loss: 0.745090  [ 2000/ 2482]\n",
      "loss: 0.747139  [ 2400/ 2482]\n",
      "Test Error: \n",
      " Accuracy: 15.6%, Avg loss: 0.746710 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.747213  [    0/ 2482]\n",
      "loss: 0.741614  [  400/ 2482]\n",
      "loss: 0.751930  [  800/ 2482]\n",
      "loss: 0.742987  [ 1200/ 2482]\n",
      "loss: 0.749733  [ 1600/ 2482]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ct\u001B[38;5;241m.\u001B[39mModelTrainer(model\u001B[38;5;241m=\u001B[39mmodel, need_reshape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, train_dataset\u001B[38;5;241m=\u001B[39mtrainSet, eval_dataset\u001B[38;5;241m=\u001B[39mevalSet, device\u001B[38;5;241m=\u001B[39mdevice, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, save_model_every\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5001\u001B[39m, model_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../content/classifier/W2VClassifier/Nr1/\u001B[39m\u001B[38;5;124m\"\u001B[39m, data_collator\u001B[38;5;241m=\u001B[39mdata_collator)\n\u001B[1;32m      3\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/w2v_emotion_model/model_trainer.py:55\u001B[0m, in \u001B[0;36mModelTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     53\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memo_reco_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loop(test_dataloader, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/w2v_emotion_model/model_trainer.py:71\u001B[0m, in \u001B[0;36mModelTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Compute prediction and loss\u001B[39;00m\n\u001B[1;32m     70\u001B[0m z \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 71\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, z)\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/w2v_emotion_model/custom_model.py:89\u001B[0m, in \u001B[0;36mWav2Vec2ForSpeechClassification.forward\u001B[0;34m(self, input_values)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     87\u001B[0m         input_values,\n\u001B[1;32m     88\u001B[0m ):\n\u001B[0;32m---> 89\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwav2vec2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;66;03m#hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1311\u001B[0m, in \u001B[0;36mWav2Vec2Model.forward\u001B[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1306\u001B[0m hidden_states, extract_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_projection(extract_features)\n\u001B[1;32m   1307\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mask_hidden_states(\n\u001B[1;32m   1308\u001B[0m     hidden_states, mask_time_indices\u001B[38;5;241m=\u001B[39mmask_time_indices, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask\n\u001B[1;32m   1309\u001B[0m )\n\u001B[0;32m-> 1311\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1314\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1315\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1319\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madapter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:794\u001B[0m, in \u001B[0;36mWav2Vec2Encoder.forward\u001B[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    788\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    789\u001B[0m             create_custom_forward(layer),\n\u001B[1;32m    790\u001B[0m             hidden_states,\n\u001B[1;32m    791\u001B[0m             attention_mask,\n\u001B[1;32m    792\u001B[0m         )\n\u001B[1;32m    793\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 794\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    797\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    799\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m skip_the_layer:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:682\u001B[0m, in \u001B[0;36mWav2Vec2EncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, output_attentions)\u001B[0m\n\u001B[1;32m    679\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m attn_residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    681\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(hidden_states)\n\u001B[0;32m--> 682\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    683\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_layer_norm(hidden_states)\n\u001B[1;32m    685\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:654\u001B[0m, in \u001B[0;36mWav2Vec2FeedForward.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    651\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_act_fn(hidden_states)\n\u001B[1;32m    652\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_dropout(hidden_states)\n\u001B[0;32m--> 654\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_dense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    655\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_dropout(hidden_states)\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(ct)\n",
    "trainer = ct.ModelTrainer(model=model, need_reshape=False, train_dataset=trainSet, eval_dataset=evalSet, device=device, batch_size=4, save_model_every=20, num_epochs=5001, model_path=\"../content/classifier/W2VClassifier/Nr1/\", data_collator=data_collator)\n",
    "gc.collect()\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
