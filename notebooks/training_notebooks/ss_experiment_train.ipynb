{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 15:10:37.674273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 15:10:38.157946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 15:10:38.157996: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 15:10:38.158000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 6\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream\"\n",
    "trials_per_model_type = 4\n",
    "epochs_per_model = 200\n",
    "save_highest_acc_min_acc = 0.6\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient, save_highest_acc_min_acc=save_highest_acc_min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.973299  [    0/ 5482]\n",
      "loss: 1.860190  [  600/ 5482]\n",
      "loss: 1.914179  [ 1200/ 5482]\n",
      "loss: 1.934392  [ 1800/ 5482]\n",
      "loss: 1.946360  [ 2400/ 5482]\n",
      "loss: 1.989128  [ 3000/ 5482]\n",
      "loss: 1.871816  [ 3600/ 5482]\n",
      "loss: 1.968616  [ 4200/ 5482]\n",
      "loss: 1.888705  [ 4800/ 5482]\n",
      "loss: 1.892664  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.270     0.000     0.000    99\n",
      " disgust     0.270     0.000     0.000    107\n",
      "    fear     0.270     0.508     0.375    80\n",
      "   happy     0.270     0.202     0.260    77\n",
      " neutral     0.270     0.249     0.547    95\n",
      "     sad     0.270     0.259     0.692    91\n",
      "surprise     0.270     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.270     0.174     0.268    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 27.0%, Avg loss: 1.900247 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.997990  [    0/ 5482]\n",
      "loss: 1.951803  [  600/ 5482]\n",
      "loss: 1.870376  [ 1200/ 5482]\n",
      "loss: 1.900928  [ 1800/ 5482]\n",
      "loss: 1.959608  [ 2400/ 5482]\n",
      "loss: 1.865093  [ 3000/ 5482]\n",
      "loss: 1.836111  [ 3600/ 5482]\n",
      "loss: 1.775435  [ 4200/ 5482]\n",
      "loss: 1.953687  [ 4800/ 5482]\n",
      "loss: 1.844102  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.313     0.000     0.000    99\n",
      " disgust     0.313     0.000     0.000    107\n",
      "    fear     0.313     0.521     0.463    80\n",
      "   happy     0.313     0.278     0.416    77\n",
      " neutral     0.313     0.352     0.526    95\n",
      "     sad     0.313     0.255     0.791    91\n",
      "surprise     0.313     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.313     0.201     0.314    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 1.875541 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.967847  [    0/ 5482]\n",
      "loss: 1.853093  [  600/ 5482]\n",
      "loss: 1.946700  [ 1200/ 5482]\n",
      "loss: 1.977460  [ 1800/ 5482]\n",
      "loss: 1.993154  [ 2400/ 5482]\n",
      "loss: 1.726420  [ 3000/ 5482]\n",
      "loss: 1.779924  [ 3600/ 5482]\n",
      "loss: 1.794782  [ 4200/ 5482]\n",
      "loss: 1.988988  [ 4800/ 5482]\n",
      "loss: 1.952710  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.000     0.000    99\n",
      " disgust     0.336     0.000     0.000    107\n",
      "    fear     0.336     0.413     0.537    80\n",
      "   happy     0.336     0.246     0.558    77\n",
      " neutral     0.336     0.408     0.537    95\n",
      "     sad     0.336     0.330     0.747    91\n",
      "surprise     0.336     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.200     0.340    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.861244 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.731553  [    0/ 5482]\n",
      "loss: 1.771635  [  600/ 5482]\n",
      "loss: 1.774653  [ 1200/ 5482]\n",
      "loss: 1.744908  [ 1800/ 5482]\n",
      "loss: 1.946626  [ 2400/ 5482]\n",
      "loss: 1.752585  [ 3000/ 5482]\n",
      "loss: 1.897346  [ 3600/ 5482]\n",
      "loss: 1.892711  [ 4200/ 5482]\n",
      "loss: 1.683520  [ 4800/ 5482]\n",
      "loss: 1.703873  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.000     0.000    99\n",
      " disgust     0.338     0.000     0.000    107\n",
      "    fear     0.338     0.549     0.487    80\n",
      "   happy     0.338     0.384     0.429    77\n",
      " neutral     0.338     0.358     0.653    95\n",
      "     sad     0.338     0.257     0.791    91\n",
      "surprise     0.338     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.221     0.337    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.842792 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.944285  [    0/ 5482]\n",
      "loss: 1.890746  [  600/ 5482]\n",
      "loss: 1.783422  [ 1200/ 5482]\n",
      "loss: 1.779417  [ 1800/ 5482]\n",
      "loss: 1.899118  [ 2400/ 5482]\n",
      "loss: 1.833165  [ 3000/ 5482]\n",
      "loss: 1.783982  [ 3600/ 5482]\n",
      "loss: 1.743135  [ 4200/ 5482]\n",
      "loss: 1.704163  [ 4800/ 5482]\n",
      "loss: 1.778825  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.341     0.000     0.000    99\n",
      " disgust     0.341     0.000     0.000    107\n",
      "    fear     0.341     0.446     0.512    80\n",
      "   happy     0.341     0.323     0.545    77\n",
      " neutral     0.341     0.407     0.600    95\n",
      "     sad     0.341     0.274     0.747    91\n",
      "surprise     0.341     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.341     0.207     0.344    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.832502 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.997558  [    0/ 5482]\n",
      "loss: 1.871618  [  600/ 5482]\n",
      "loss: 1.909096  [ 1200/ 5482]\n",
      "loss: 1.613466  [ 1800/ 5482]\n",
      "loss: 1.794880  [ 2400/ 5482]\n",
      "loss: 1.723419  [ 3000/ 5482]\n",
      "loss: 1.763877  [ 3600/ 5482]\n",
      "loss: 1.771366  [ 4200/ 5482]\n",
      "loss: 1.734301  [ 4800/ 5482]\n",
      "loss: 2.008829  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.349     0.000     0.000    99\n",
      " disgust     0.349     0.000     0.000    107\n",
      "    fear     0.349     0.464     0.562    80\n",
      "   happy     0.349     0.438     0.545    77\n",
      " neutral     0.349     0.408     0.558    95\n",
      "     sad     0.349     0.254     0.802    91\n",
      "surprise     0.349     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.349     0.223     0.353    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 1.824409 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.776486  [    0/ 5482]\n",
      "loss: 1.814896  [  600/ 5482]\n",
      "loss: 1.830260  [ 1200/ 5482]\n",
      "loss: 1.719378  [ 1800/ 5482]\n",
      "loss: 1.926622  [ 2400/ 5482]\n",
      "loss: 1.628815  [ 3000/ 5482]\n",
      "loss: 1.848836  [ 3600/ 5482]\n",
      "loss: 1.663823  [ 4200/ 5482]\n",
      "loss: 1.695749  [ 4800/ 5482]\n",
      "loss: 1.800408  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.000     0.000    99\n",
      " disgust     0.336     0.000     0.000    107\n",
      "    fear     0.336     0.385     0.525    80\n",
      "   happy     0.336     0.371     0.506    77\n",
      " neutral     0.336     0.380     0.568    95\n",
      "     sad     0.336     0.276     0.769    91\n",
      "surprise     0.336     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.202     0.338    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.814176 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.833542  [    0/ 5482]\n",
      "loss: 1.727290  [  600/ 5482]\n",
      "loss: 1.961016  [ 1200/ 5482]\n",
      "loss: 1.755191  [ 1800/ 5482]\n",
      "loss: 1.741025  [ 2400/ 5482]\n",
      "loss: 1.818292  [ 3000/ 5482]\n",
      "loss: 1.785373  [ 3600/ 5482]\n",
      "loss: 1.669038  [ 4200/ 5482]\n",
      "loss: 1.819809  [ 4800/ 5482]\n",
      "loss: 1.967427  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.000     0.000    99\n",
      " disgust     0.354     0.000     0.000    107\n",
      "    fear     0.354     0.370     0.550    80\n",
      "   happy     0.354     0.336     0.597    77\n",
      " neutral     0.354     0.457     0.611    95\n",
      "     sad     0.354     0.300     0.747    91\n",
      "surprise     0.354     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.209     0.358    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.810546 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.578974  [    0/ 5482]\n",
      "loss: 1.699657  [  600/ 5482]\n",
      "loss: 1.884678  [ 1200/ 5482]\n",
      "loss: 1.872177  [ 1800/ 5482]\n",
      "loss: 1.749926  [ 2400/ 5482]\n",
      "loss: 1.890878  [ 3000/ 5482]\n",
      "loss: 1.624667  [ 3600/ 5482]\n",
      "loss: 2.154937  [ 4200/ 5482]\n",
      "loss: 1.891594  [ 4800/ 5482]\n",
      "loss: 1.998873  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.667     0.020    99\n",
      " disgust     0.352     0.000     0.000    107\n",
      "    fear     0.352     0.419     0.487    80\n",
      "   happy     0.352     0.338     0.571    77\n",
      " neutral     0.352     0.373     0.663    95\n",
      "     sad     0.352     0.312     0.736    91\n",
      "surprise     0.352     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.301     0.354    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.799299 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.774305  [    0/ 5482]\n",
      "loss: 1.607125  [  600/ 5482]\n",
      "loss: 1.801001  [ 1200/ 5482]\n",
      "loss: 1.829553  [ 1800/ 5482]\n",
      "loss: 1.752199  [ 2400/ 5482]\n",
      "loss: 1.758579  [ 3000/ 5482]\n",
      "loss: 1.901526  [ 3600/ 5482]\n",
      "loss: 1.805361  [ 4200/ 5482]\n",
      "loss: 1.768871  [ 4800/ 5482]\n",
      "loss: 1.687380  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.413     0.842     0.323    99\n",
      " disgust     0.413     0.000     0.000    107\n",
      "    fear     0.413     0.461     0.512    80\n",
      "   happy     0.413     0.396     0.571    77\n",
      " neutral     0.413     0.529     0.663    95\n",
      "     sad     0.413     0.285     0.791    91\n",
      "surprise     0.413     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.413     0.359     0.409    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.783234 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.840747  [    0/ 5482]\n",
      "loss: 1.965878  [  600/ 5482]\n",
      "loss: 1.911528  [ 1200/ 5482]\n",
      "loss: 1.759589  [ 1800/ 5482]\n",
      "loss: 1.783138  [ 2400/ 5482]\n",
      "loss: 1.859011  [ 3000/ 5482]\n",
      "loss: 1.676727  [ 3600/ 5482]\n",
      "loss: 1.603407  [ 4200/ 5482]\n",
      "loss: 1.856051  [ 4800/ 5482]\n",
      "loss: 1.861394  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.420     0.736     0.394    99\n",
      " disgust     0.420     0.000     0.000    107\n",
      "    fear     0.420     0.457     0.463    80\n",
      "   happy     0.420     0.301     0.636    77\n",
      " neutral     0.420     0.545     0.642    95\n",
      "     sad     0.420     0.348     0.769    91\n",
      "surprise     0.420     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.420     0.341     0.415    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.781366 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.650255  [    0/ 5482]\n",
      "loss: 1.729930  [  600/ 5482]\n",
      "loss: 1.922884  [ 1200/ 5482]\n",
      "loss: 1.768156  [ 1800/ 5482]\n",
      "loss: 1.747950  [ 2400/ 5482]\n",
      "loss: 1.649824  [ 3000/ 5482]\n",
      "loss: 1.681292  [ 3600/ 5482]\n",
      "loss: 1.799626  [ 4200/ 5482]\n",
      "loss: 1.910507  [ 4800/ 5482]\n",
      "loss: 1.662801  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.438     0.857     0.364    99\n",
      " disgust     0.438     0.000     0.000    107\n",
      "    fear     0.438     0.436     0.600    80\n",
      "   happy     0.438     0.381     0.584    77\n",
      " neutral     0.438     0.681     0.674    95\n",
      "     sad     0.438     0.301     0.813    91\n",
      "surprise     0.438     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.438     0.380     0.434    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.776556 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.816028  [    0/ 5482]\n",
      "loss: 1.872812  [  600/ 5482]\n",
      "loss: 1.851273  [ 1200/ 5482]\n",
      "loss: 1.678162  [ 1800/ 5482]\n",
      "loss: 1.707253  [ 2400/ 5482]\n",
      "loss: 1.836863  [ 3000/ 5482]\n",
      "loss: 1.793253  [ 3600/ 5482]\n",
      "loss: 1.815741  [ 4200/ 5482]\n",
      "loss: 1.498106  [ 4800/ 5482]\n",
      "loss: 1.618604  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.438     0.814     0.485    99\n",
      " disgust     0.438     0.000     0.000    107\n",
      "    fear     0.438     0.416     0.525    80\n",
      "   happy     0.438     0.418     0.494    77\n",
      " neutral     0.438     0.552     0.726    95\n",
      "     sad     0.438     0.299     0.769    91\n",
      "surprise     0.438     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.438     0.357     0.428    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.765931 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.700492  [    0/ 5482]\n",
      "loss: 1.711111  [  600/ 5482]\n",
      "loss: 1.578956  [ 1200/ 5482]\n",
      "loss: 1.651402  [ 1800/ 5482]\n",
      "loss: 1.691675  [ 2400/ 5482]\n",
      "loss: 1.737880  [ 3000/ 5482]\n",
      "loss: 1.509120  [ 3600/ 5482]\n",
      "loss: 1.744762  [ 4200/ 5482]\n",
      "loss: 1.773775  [ 4800/ 5482]\n",
      "loss: 1.608819  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.441     0.804     0.455    99\n",
      " disgust     0.441     0.000     0.000    107\n",
      "    fear     0.441     0.400     0.550    80\n",
      "   happy     0.441     0.410     0.558    77\n",
      " neutral     0.441     0.573     0.663    95\n",
      "     sad     0.441     0.317     0.791    91\n",
      "surprise     0.441     1.000     0.033    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.441     0.500     0.436    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 1.760677 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.768951  [    0/ 5482]\n",
      "loss: 1.587962  [  600/ 5482]\n",
      "loss: 1.649173  [ 1200/ 5482]\n",
      "loss: 1.584831  [ 1800/ 5482]\n",
      "loss: 1.863765  [ 2400/ 5482]\n",
      "loss: 1.705739  [ 3000/ 5482]\n",
      "loss: 1.774365  [ 3600/ 5482]\n",
      "loss: 1.643092  [ 4200/ 5482]\n",
      "loss: 1.721446  [ 4800/ 5482]\n",
      "loss: 1.913527  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.452     0.766     0.495    99\n",
      " disgust     0.452     0.000     0.000    107\n",
      "    fear     0.452     0.417     0.500    80\n",
      "   happy     0.452     0.358     0.701    77\n",
      " neutral     0.452     0.747     0.589    95\n",
      "     sad     0.452     0.338     0.813    91\n",
      "surprise     0.452     0.600     0.049    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.452     0.461     0.450    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.753848 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.677001  [    0/ 5482]\n",
      "loss: 1.757848  [  600/ 5482]\n",
      "loss: 1.792822  [ 1200/ 5482]\n",
      "loss: 1.760534  [ 1800/ 5482]\n",
      "loss: 1.708676  [ 2400/ 5482]\n",
      "loss: 1.805456  [ 3000/ 5482]\n",
      "loss: 1.558751  [ 3600/ 5482]\n",
      "loss: 1.864518  [ 4200/ 5482]\n",
      "loss: 1.539601  [ 4800/ 5482]\n",
      "loss: 1.729077  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.466     0.718     0.566    99\n",
      " disgust     0.466     0.000     0.000    107\n",
      "    fear     0.466     0.407     0.550    80\n",
      "   happy     0.466     0.447     0.545    77\n",
      " neutral     0.466     0.635     0.695    95\n",
      "     sad     0.466     0.332     0.813    91\n",
      "surprise     0.466     0.667     0.033    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.466     0.458     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 1.741570 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.467845  [    0/ 5482]\n",
      "loss: 1.665995  [  600/ 5482]\n",
      "loss: 1.769091  [ 1200/ 5482]\n",
      "loss: 1.871726  [ 1800/ 5482]\n",
      "loss: 1.544713  [ 2400/ 5482]\n",
      "loss: 1.761634  [ 3000/ 5482]\n",
      "loss: 1.893523  [ 3600/ 5482]\n",
      "loss: 1.821006  [ 4200/ 5482]\n",
      "loss: 1.551007  [ 4800/ 5482]\n",
      "loss: 1.917758  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.448     0.766     0.495    99\n",
      " disgust     0.448     0.000     0.000    107\n",
      "    fear     0.448     0.398     0.463    80\n",
      "   happy     0.448     0.363     0.688    77\n",
      " neutral     0.448     0.779     0.558    95\n",
      "     sad     0.448     0.330     0.824    91\n",
      "surprise     0.448     0.500     0.098    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.448     0.448     0.447    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.742028 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.725985  [    0/ 5482]\n",
      "loss: 1.803973  [  600/ 5482]\n",
      "loss: 1.656125  [ 1200/ 5482]\n",
      "loss: 1.599146  [ 1800/ 5482]\n",
      "loss: 1.568551  [ 2400/ 5482]\n",
      "loss: 1.636934  [ 3000/ 5482]\n",
      "loss: 1.681060  [ 3600/ 5482]\n",
      "loss: 1.493239  [ 4200/ 5482]\n",
      "loss: 1.704044  [ 4800/ 5482]\n",
      "loss: 1.633775  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.475     0.757     0.535    99\n",
      " disgust     0.475     0.000     0.000    107\n",
      "    fear     0.475     0.431     0.625    80\n",
      "   happy     0.475     0.463     0.571    77\n",
      " neutral     0.475     0.759     0.663    95\n",
      "     sad     0.475     0.311     0.813    91\n",
      "surprise     0.475     0.750     0.098    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.475     0.496     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.729014 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.584697  [    0/ 5482]\n",
      "loss: 1.667212  [  600/ 5482]\n",
      "loss: 1.742774  [ 1200/ 5482]\n",
      "loss: 1.763641  [ 1800/ 5482]\n",
      "loss: 1.595882  [ 2400/ 5482]\n",
      "loss: 1.652421  [ 3000/ 5482]\n",
      "loss: 1.493127  [ 3600/ 5482]\n",
      "loss: 1.819820  [ 4200/ 5482]\n",
      "loss: 1.788777  [ 4800/ 5482]\n",
      "loss: 1.631309  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.675     0.566    99\n",
      " disgust     0.479     0.000     0.000    107\n",
      "    fear     0.479     0.465     0.500    80\n",
      "   happy     0.479     0.413     0.649    77\n",
      " neutral     0.479     0.721     0.653    95\n",
      "     sad     0.479     0.338     0.791    91\n",
      "surprise     0.479     0.571     0.197    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.455     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.724698 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.763173  [    0/ 5482]\n",
      "loss: 1.969770  [  600/ 5482]\n",
      "loss: 1.619596  [ 1200/ 5482]\n",
      "loss: 1.633160  [ 1800/ 5482]\n",
      "loss: 1.533263  [ 2400/ 5482]\n",
      "loss: 1.596867  [ 3000/ 5482]\n",
      "loss: 1.708170  [ 3600/ 5482]\n",
      "loss: 1.716873  [ 4200/ 5482]\n",
      "loss: 1.563361  [ 4800/ 5482]\n",
      "loss: 1.484409  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.649     0.636    99\n",
      " disgust     0.485     0.000     0.000    107\n",
      "    fear     0.485     0.506     0.512    80\n",
      "   happy     0.485     0.387     0.597    77\n",
      " neutral     0.485     0.797     0.621    95\n",
      "     sad     0.485     0.339     0.824    91\n",
      "surprise     0.485     0.667     0.197    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.478     0.484    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.709741 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.615207  [    0/ 5482]\n",
      "loss: 1.775021  [  600/ 5482]\n",
      "loss: 1.498572  [ 1200/ 5482]\n",
      "loss: 1.688698  [ 1800/ 5482]\n",
      "loss: 1.634435  [ 2400/ 5482]\n",
      "loss: 1.503413  [ 3000/ 5482]\n",
      "loss: 1.605860  [ 3600/ 5482]\n",
      "loss: 1.491647  [ 4200/ 5482]\n",
      "loss: 1.651374  [ 4800/ 5482]\n",
      "loss: 1.626770  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.461     0.690     0.586    99\n",
      " disgust     0.461     0.000     0.000    107\n",
      "    fear     0.461     0.431     0.550    80\n",
      "   happy     0.461     0.526     0.532    77\n",
      " neutral     0.461     0.767     0.589    95\n",
      "     sad     0.461     0.292     0.835    91\n",
      "surprise     0.461     0.462     0.098    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.461     0.453     0.456    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 1.719465 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.641045  [    0/ 5482]\n",
      "loss: 1.486525  [  600/ 5482]\n",
      "loss: 1.348419  [ 1200/ 5482]\n",
      "loss: 1.671117  [ 1800/ 5482]\n",
      "loss: 1.491077  [ 2400/ 5482]\n",
      "loss: 1.730384  [ 3000/ 5482]\n",
      "loss: 1.801783  [ 3600/ 5482]\n",
      "loss: 1.633780  [ 4200/ 5482]\n",
      "loss: 1.762174  [ 4800/ 5482]\n",
      "loss: 1.515627  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.674     0.626    99\n",
      " disgust     0.477     0.000     0.000    107\n",
      "    fear     0.477     0.518     0.550    80\n",
      "   happy     0.477     0.585     0.494    77\n",
      " neutral     0.477     0.737     0.589    95\n",
      "     sad     0.477     0.297     0.923    91\n",
      "surprise     0.477     0.778     0.115    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.513     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.708275 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.699161  [    0/ 5482]\n",
      "loss: 1.447156  [  600/ 5482]\n",
      "loss: 1.568057  [ 1200/ 5482]\n",
      "loss: 1.606664  [ 1800/ 5482]\n",
      "loss: 1.634792  [ 2400/ 5482]\n",
      "loss: 1.353322  [ 3000/ 5482]\n",
      "loss: 1.643222  [ 3600/ 5482]\n",
      "loss: 1.732187  [ 4200/ 5482]\n",
      "loss: 1.691392  [ 4800/ 5482]\n",
      "loss: 1.563395  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.713     0.626    99\n",
      " disgust     0.500     0.000     0.000    107\n",
      "    fear     0.500     0.432     0.600    80\n",
      "   happy     0.500     0.506     0.545    77\n",
      " neutral     0.500     0.730     0.684    95\n",
      "     sad     0.500     0.353     0.846    91\n",
      "surprise     0.500     0.500     0.180    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.462     0.497    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.696768 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.522256  [    0/ 5482]\n",
      "loss: 1.678009  [  600/ 5482]\n",
      "loss: 1.489164  [ 1200/ 5482]\n",
      "loss: 1.518929  [ 1800/ 5482]\n",
      "loss: 1.555734  [ 2400/ 5482]\n",
      "loss: 1.739722  [ 3000/ 5482]\n",
      "loss: 1.735275  [ 3600/ 5482]\n",
      "loss: 1.433104  [ 4200/ 5482]\n",
      "loss: 1.638959  [ 4800/ 5482]\n",
      "loss: 1.712858  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.685     0.616    99\n",
      " disgust     0.510     0.000     0.000    107\n",
      "    fear     0.510     0.475     0.588    80\n",
      "   happy     0.510     0.515     0.649    77\n",
      " neutral     0.510     0.789     0.632    95\n",
      "     sad     0.510     0.338     0.835    91\n",
      "surprise     0.510     0.708     0.279    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.502     0.514    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.692439 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.761911  [    0/ 5482]\n",
      "loss: 1.575760  [  600/ 5482]\n",
      "loss: 1.635264  [ 1200/ 5482]\n",
      "loss: 1.731367  [ 1800/ 5482]\n",
      "loss: 1.662356  [ 2400/ 5482]\n",
      "loss: 1.650186  [ 3000/ 5482]\n",
      "loss: 1.843396  [ 3600/ 5482]\n",
      "loss: 1.786393  [ 4200/ 5482]\n",
      "loss: 1.829119  [ 4800/ 5482]\n",
      "loss: 1.548128  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.674     0.586    99\n",
      " disgust     0.493     0.000     0.000    107\n",
      "    fear     0.493     0.423     0.512    80\n",
      "   happy     0.493     0.441     0.584    77\n",
      " neutral     0.493     0.723     0.632    95\n",
      "     sad     0.493     0.363     0.857    91\n",
      "surprise     0.493     0.704     0.311    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.475     0.498    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.689017 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.716264  [    0/ 5482]\n",
      "loss: 1.730926  [  600/ 5482]\n",
      "loss: 1.560659  [ 1200/ 5482]\n",
      "loss: 1.302724  [ 1800/ 5482]\n",
      "loss: 1.667486  [ 2400/ 5482]\n",
      "loss: 1.761854  [ 3000/ 5482]\n",
      "loss: 1.495653  [ 3600/ 5482]\n",
      "loss: 1.474476  [ 4200/ 5482]\n",
      "loss: 1.799364  [ 4800/ 5482]\n",
      "loss: 1.497771  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.659     0.606    99\n",
      " disgust     0.515     0.000     0.000    107\n",
      "    fear     0.515     0.500     0.562    80\n",
      "   happy     0.515     0.523     0.597    77\n",
      " neutral     0.515     0.849     0.653    95\n",
      "     sad     0.515     0.347     0.901    91\n",
      "surprise     0.515     0.594     0.311    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.496     0.519    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.683540 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.629418  [    0/ 5482]\n",
      "loss: 1.721951  [  600/ 5482]\n",
      "loss: 1.548725  [ 1200/ 5482]\n",
      "loss: 1.786406  [ 1800/ 5482]\n",
      "loss: 1.668216  [ 2400/ 5482]\n",
      "loss: 1.536376  [ 3000/ 5482]\n",
      "loss: 1.635964  [ 3600/ 5482]\n",
      "loss: 1.708271  [ 4200/ 5482]\n",
      "loss: 1.565012  [ 4800/ 5482]\n",
      "loss: 1.574835  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.470     0.485     0.657    99\n",
      " disgust     0.470     0.000     0.000    107\n",
      "    fear     0.470     0.579     0.412    80\n",
      "   happy     0.470     0.367     0.701    77\n",
      " neutral     0.470     0.706     0.505    95\n",
      "     sad     0.470     0.407     0.791    91\n",
      "surprise     0.470     0.556     0.246    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.470     0.443     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 1.675003 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.699531  [    0/ 5482]\n",
      "loss: 1.532332  [  600/ 5482]\n",
      "loss: 1.482372  [ 1200/ 5482]\n",
      "loss: 1.687739  [ 1800/ 5482]\n",
      "loss: 1.731734  [ 2400/ 5482]\n",
      "loss: 1.588972  [ 3000/ 5482]\n",
      "loss: 1.635440  [ 3600/ 5482]\n",
      "loss: 1.632668  [ 4200/ 5482]\n",
      "loss: 1.491816  [ 4800/ 5482]\n",
      "loss: 1.783480  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.644     0.677    99\n",
      " disgust     0.525     0.000     0.000    107\n",
      "    fear     0.525     0.512     0.537    80\n",
      "   happy     0.525     0.431     0.610    77\n",
      " neutral     0.525     0.747     0.684    95\n",
      "     sad     0.525     0.396     0.857    91\n",
      "surprise     0.525     0.690     0.328    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.489     0.528    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.656720 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.472358  [    0/ 5482]\n",
      "loss: 1.639844  [  600/ 5482]\n",
      "loss: 1.525362  [ 1200/ 5482]\n",
      "loss: 1.580358  [ 1800/ 5482]\n",
      "loss: 1.649736  [ 2400/ 5482]\n",
      "loss: 1.477087  [ 3000/ 5482]\n",
      "loss: 1.604709  [ 3600/ 5482]\n",
      "loss: 1.536539  [ 4200/ 5482]\n",
      "loss: 1.765129  [ 4800/ 5482]\n",
      "loss: 1.699737  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.575     0.657    99\n",
      " disgust     0.500     0.000     0.000    107\n",
      "    fear     0.500     0.470     0.487    80\n",
      "   happy     0.500     0.426     0.597    77\n",
      " neutral     0.500     0.742     0.695    95\n",
      "     sad     0.500     0.389     0.813    91\n",
      "surprise     0.500     0.556     0.246    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.451     0.499    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.657686 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.507357  [    0/ 5482]\n",
      "loss: 1.667482  [  600/ 5482]\n",
      "loss: 1.350250  [ 1200/ 5482]\n",
      "loss: 1.468565  [ 1800/ 5482]\n",
      "loss: 1.656593  [ 2400/ 5482]\n",
      "loss: 1.594413  [ 3000/ 5482]\n",
      "loss: 1.606527  [ 3600/ 5482]\n",
      "loss: 1.591089  [ 4200/ 5482]\n",
      "loss: 1.394095  [ 4800/ 5482]\n",
      "loss: 1.604757  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.570     0.657    99\n",
      " disgust     0.502     0.000     0.000    107\n",
      "    fear     0.502     0.561     0.400    80\n",
      "   happy     0.502     0.404     0.571    77\n",
      " neutral     0.502     0.805     0.695    95\n",
      "     sad     0.502     0.378     0.868    91\n",
      "surprise     0.502     0.513     0.328    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.462     0.503    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.651784 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.579584  [    0/ 5482]\n",
      "loss: 1.553722  [  600/ 5482]\n",
      "loss: 1.535921  [ 1200/ 5482]\n",
      "loss: 1.718830  [ 1800/ 5482]\n",
      "loss: 1.267037  [ 2400/ 5482]\n",
      "loss: 1.648491  [ 3000/ 5482]\n",
      "loss: 1.641857  [ 3600/ 5482]\n",
      "loss: 1.449101  [ 4200/ 5482]\n",
      "loss: 1.769621  [ 4800/ 5482]\n",
      "loss: 2.054662  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.419     0.727    99\n",
      " disgust     0.477     0.000     0.000    107\n",
      "    fear     0.477     0.514     0.450    80\n",
      "   happy     0.477     0.381     0.662    77\n",
      " neutral     0.477     0.797     0.537    95\n",
      "     sad     0.477     0.448     0.714    91\n",
      "surprise     0.477     0.640     0.262    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.457     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.652253 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.530912  [    0/ 5482]\n",
      "loss: 1.596384  [  600/ 5482]\n",
      "loss: 1.435299  [ 1200/ 5482]\n",
      "loss: 1.517070  [ 1800/ 5482]\n",
      "loss: 1.619711  [ 2400/ 5482]\n",
      "loss: 1.596546  [ 3000/ 5482]\n",
      "loss: 1.700529  [ 3600/ 5482]\n",
      "loss: 1.506861  [ 4200/ 5482]\n",
      "loss: 1.484172  [ 4800/ 5482]\n",
      "loss: 1.543074  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.472     0.677    99\n",
      " disgust     0.518     0.000     0.000    107\n",
      "    fear     0.518     0.612     0.512    80\n",
      "   happy     0.518     0.422     0.636    77\n",
      " neutral     0.518     0.747     0.653    95\n",
      "     sad     0.518     0.437     0.835    91\n",
      "surprise     0.518     0.750     0.344    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.491     0.523    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.635840 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.553736  [    0/ 5482]\n",
      "loss: 1.573252  [  600/ 5482]\n",
      "loss: 1.600211  [ 1200/ 5482]\n",
      "loss: 1.509387  [ 1800/ 5482]\n",
      "loss: 1.599804  [ 2400/ 5482]\n",
      "loss: 1.536658  [ 3000/ 5482]\n",
      "loss: 1.620544  [ 3600/ 5482]\n",
      "loss: 1.332918  [ 4200/ 5482]\n",
      "loss: 1.895643  [ 4800/ 5482]\n",
      "loss: 1.402747  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.521     0.758    99\n",
      " disgust     0.516     0.000     0.000    107\n",
      "    fear     0.516     0.479     0.438    80\n",
      "   happy     0.516     0.537     0.558    77\n",
      " neutral     0.516     0.654     0.716    95\n",
      "     sad     0.516     0.407     0.813    91\n",
      "surprise     0.516     0.741     0.328    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.477     0.516    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.637231 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.522114  [    0/ 5482]\n",
      "loss: 1.585992  [  600/ 5482]\n",
      "loss: 1.434363  [ 1200/ 5482]\n",
      "loss: 1.694605  [ 1800/ 5482]\n",
      "loss: 1.564505  [ 2400/ 5482]\n",
      "loss: 1.566053  [ 3000/ 5482]\n",
      "loss: 1.603331  [ 3600/ 5482]\n",
      "loss: 1.445630  [ 4200/ 5482]\n",
      "loss: 1.310565  [ 4800/ 5482]\n",
      "loss: 1.623198  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.490     0.717    99\n",
      " disgust     0.515     0.000     0.000    107\n",
      "    fear     0.515     0.597     0.463    80\n",
      "   happy     0.515     0.431     0.610    77\n",
      " neutral     0.515     0.700     0.663    95\n",
      "     sad     0.515     0.431     0.791    91\n",
      "surprise     0.515     0.649     0.393    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.471     0.520    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.618128 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.556358  [    0/ 5482]\n",
      "loss: 1.623215  [  600/ 5482]\n",
      "loss: 1.528227  [ 1200/ 5482]\n",
      "loss: 1.825314  [ 1800/ 5482]\n",
      "loss: 1.388710  [ 2400/ 5482]\n",
      "loss: 1.622279  [ 3000/ 5482]\n",
      "loss: 1.300224  [ 3600/ 5482]\n",
      "loss: 1.447428  [ 4200/ 5482]\n",
      "loss: 1.547192  [ 4800/ 5482]\n",
      "loss: 1.734557  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.420     0.848    99\n",
      " disgust     0.485     0.000     0.000    107\n",
      "    fear     0.485     0.667     0.375    80\n",
      "   happy     0.485     0.427     0.494    77\n",
      " neutral     0.485     0.639     0.653    95\n",
      "     sad     0.485     0.432     0.769    91\n",
      "surprise     0.485     0.706     0.197    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.470     0.477    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.636712 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.740409  [    0/ 5482]\n",
      "loss: 1.636324  [  600/ 5482]\n",
      "loss: 1.242449  [ 1200/ 5482]\n",
      "loss: 1.269670  [ 1800/ 5482]\n",
      "loss: 1.567285  [ 2400/ 5482]\n",
      "loss: 1.402213  [ 3000/ 5482]\n",
      "loss: 1.296430  [ 3600/ 5482]\n",
      "loss: 1.532801  [ 4200/ 5482]\n",
      "loss: 1.673852  [ 4800/ 5482]\n",
      "loss: 1.528549  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.550     0.667    99\n",
      " disgust     0.533     0.000     0.000    107\n",
      "    fear     0.533     0.632     0.537    80\n",
      "   happy     0.533     0.475     0.623    77\n",
      " neutral     0.533     0.812     0.684    95\n",
      "     sad     0.533     0.396     0.857    91\n",
      "surprise     0.533     0.568     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.491     0.540    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.610228 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.508298  [    0/ 5482]\n",
      "loss: 1.781133  [  600/ 5482]\n",
      "loss: 1.464560  [ 1200/ 5482]\n",
      "loss: 1.352266  [ 1800/ 5482]\n",
      "loss: 1.350819  [ 2400/ 5482]\n",
      "loss: 1.738021  [ 3000/ 5482]\n",
      "loss: 1.696140  [ 3600/ 5482]\n",
      "loss: 1.488134  [ 4200/ 5482]\n",
      "loss: 1.719823  [ 4800/ 5482]\n",
      "loss: 1.495786  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.520     0.594     0.636    99\n",
      " disgust     0.520     0.000     0.000    107\n",
      "    fear     0.520     0.557     0.550    80\n",
      "   happy     0.520     0.530     0.571    77\n",
      " neutral     0.520     0.855     0.621    95\n",
      "     sad     0.520     0.353     0.857    91\n",
      "surprise     0.520     0.558     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.520     0.492     0.530    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.613661 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.013365  [    0/ 5482]\n",
      "loss: 1.377797  [  600/ 5482]\n",
      "loss: 1.453846  [ 1200/ 5482]\n",
      "loss: 1.624638  [ 1800/ 5482]\n",
      "loss: 1.576112  [ 2400/ 5482]\n",
      "loss: 1.470043  [ 3000/ 5482]\n",
      "loss: 1.437735  [ 3600/ 5482]\n",
      "loss: 1.456485  [ 4200/ 5482]\n",
      "loss: 1.450155  [ 4800/ 5482]\n",
      "loss: 1.421193  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.536     0.543     0.768    99\n",
      " disgust     0.536     0.000     0.000    107\n",
      "    fear     0.536     0.562     0.562    80\n",
      "   happy     0.536     0.452     0.494    77\n",
      " neutral     0.536     0.667     0.758    95\n",
      "     sad     0.536     0.434     0.791    91\n",
      "surprise     0.536     0.750     0.393    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.536     0.487     0.538    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.593609 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.579975  [    0/ 5482]\n",
      "loss: 1.543175  [  600/ 5482]\n",
      "loss: 1.441635  [ 1200/ 5482]\n",
      "loss: 1.382103  [ 1800/ 5482]\n",
      "loss: 1.546502  [ 2400/ 5482]\n",
      "loss: 1.809344  [ 3000/ 5482]\n",
      "loss: 1.449166  [ 3600/ 5482]\n",
      "loss: 1.522229  [ 4200/ 5482]\n",
      "loss: 1.444455  [ 4800/ 5482]\n",
      "loss: 1.457665  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.526     0.598     0.616    99\n",
      " disgust     0.526     0.000     0.000    107\n",
      "    fear     0.526     0.530     0.438    80\n",
      "   happy     0.526     0.505     0.649    77\n",
      " neutral     0.526     0.835     0.695    95\n",
      "     sad     0.526     0.377     0.846    91\n",
      "surprise     0.526     0.533     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.526     0.483     0.538    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.597355 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.681449  [    0/ 5482]\n",
      "loss: 1.502752  [  600/ 5482]\n",
      "loss: 1.456360  [ 1200/ 5482]\n",
      "loss: 1.405984  [ 1800/ 5482]\n",
      "loss: 1.562276  [ 2400/ 5482]\n",
      "loss: 1.644127  [ 3000/ 5482]\n",
      "loss: 1.439423  [ 3600/ 5482]\n",
      "loss: 1.509591  [ 4200/ 5482]\n",
      "loss: 1.553675  [ 4800/ 5482]\n",
      "loss: 1.522499  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.521     0.514     0.717    99\n",
      " disgust     0.521     0.000     0.000    107\n",
      "    fear     0.521     0.632     0.450    80\n",
      "   happy     0.521     0.437     0.584    77\n",
      " neutral     0.521     0.733     0.663    95\n",
      "     sad     0.521     0.404     0.857    91\n",
      "surprise     0.521     0.758     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.521     0.497     0.526    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.586047 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.566021  [    0/ 5482]\n",
      "loss: 1.572978  [  600/ 5482]\n",
      "loss: 1.484802  [ 1200/ 5482]\n",
      "loss: 1.466158  [ 1800/ 5482]\n",
      "loss: 1.487760  [ 2400/ 5482]\n",
      "loss: 1.344033  [ 3000/ 5482]\n",
      "loss: 1.437196  [ 3600/ 5482]\n",
      "loss: 1.564755  [ 4200/ 5482]\n",
      "loss: 1.539892  [ 4800/ 5482]\n",
      "loss: 1.305140  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.521     0.533     0.727    99\n",
      " disgust     0.521     0.000     0.000    107\n",
      "    fear     0.521     0.627     0.400    80\n",
      "   happy     0.521     0.410     0.649    77\n",
      " neutral     0.521     0.761     0.705    95\n",
      "     sad     0.521     0.410     0.802    91\n",
      "surprise     0.521     0.667     0.393    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.521     0.487     0.525    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.588998 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.648040  [    0/ 5482]\n",
      "loss: 1.269518  [  600/ 5482]\n",
      "loss: 1.683881  [ 1200/ 5482]\n",
      "loss: 1.598458  [ 1800/ 5482]\n",
      "loss: 1.470154  [ 2400/ 5482]\n",
      "loss: 1.313690  [ 3000/ 5482]\n",
      "loss: 1.352087  [ 3600/ 5482]\n",
      "loss: 1.493084  [ 4200/ 5482]\n",
      "loss: 1.335804  [ 4800/ 5482]\n",
      "loss: 1.565631  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.554     0.727    99\n",
      " disgust     0.533     0.000     0.000    107\n",
      "    fear     0.533     0.587     0.463    80\n",
      "   happy     0.533     0.517     0.584    77\n",
      " neutral     0.533     0.687     0.716    95\n",
      "     sad     0.533     0.415     0.857    91\n",
      "surprise     0.533     0.581     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.477     0.537    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.579376 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.273568  [    0/ 5482]\n",
      "loss: 1.607530  [  600/ 5482]\n",
      "loss: 1.513837  [ 1200/ 5482]\n",
      "loss: 1.454406  [ 1800/ 5482]\n",
      "loss: 1.607828  [ 2400/ 5482]\n",
      "loss: 1.397270  [ 3000/ 5482]\n",
      "loss: 1.263636  [ 3600/ 5482]\n",
      "loss: 1.485095  [ 4200/ 5482]\n",
      "loss: 1.646859  [ 4800/ 5482]\n",
      "loss: 1.725168  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.581     0.616    99\n",
      " disgust     0.533     0.000     0.000    107\n",
      "    fear     0.533     0.586     0.512    80\n",
      "   happy     0.533     0.542     0.584    77\n",
      " neutral     0.533     0.844     0.684    95\n",
      "     sad     0.533     0.361     0.868    91\n",
      "surprise     0.533     0.607     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.503     0.546    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.578520 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.468036  [    0/ 5482]\n",
      "loss: 1.557978  [  600/ 5482]\n",
      "loss: 1.376084  [ 1200/ 5482]\n",
      "loss: 1.590721  [ 1800/ 5482]\n",
      "loss: 1.701711  [ 2400/ 5482]\n",
      "loss: 1.346628  [ 3000/ 5482]\n",
      "loss: 1.515795  [ 3600/ 5482]\n",
      "loss: 1.424738  [ 4200/ 5482]\n",
      "loss: 1.592901  [ 4800/ 5482]\n",
      "loss: 1.188946  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.539     0.510     0.768    99\n",
      " disgust     0.539     0.000     0.000    107\n",
      "    fear     0.539     0.780     0.400    80\n",
      "   happy     0.539     0.512     0.545    77\n",
      " neutral     0.539     0.742     0.695    95\n",
      "     sad     0.539     0.404     0.879    91\n",
      "surprise     0.539     0.647     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.539     0.514     0.547    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.561147 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.370167  [    0/ 5482]\n",
      "loss: 1.201044  [  600/ 5482]\n",
      "loss: 1.344614  [ 1200/ 5482]\n",
      "loss: 1.492244  [ 1800/ 5482]\n",
      "loss: 1.300210  [ 2400/ 5482]\n",
      "loss: 1.634361  [ 3000/ 5482]\n",
      "loss: 1.668775  [ 3600/ 5482]\n",
      "loss: 1.492343  [ 4200/ 5482]\n",
      "loss: 1.391400  [ 4800/ 5482]\n",
      "loss: 1.448307  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.503     0.737    99\n",
      " disgust     0.525     0.000     0.000    107\n",
      "    fear     0.525     0.571     0.450    80\n",
      "   happy     0.525     0.467     0.545    77\n",
      " neutral     0.525     0.673     0.695    95\n",
      "     sad     0.525     0.443     0.846    91\n",
      "surprise     0.525     0.650     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.473     0.529    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.555090 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.291652  [    0/ 5482]\n",
      "loss: 1.538236  [  600/ 5482]\n",
      "loss: 1.444769  [ 1200/ 5482]\n",
      "loss: 1.538733  [ 1800/ 5482]\n",
      "loss: 1.458204  [ 2400/ 5482]\n",
      "loss: 1.414013  [ 3000/ 5482]\n",
      "loss: 1.521115  [ 3600/ 5482]\n",
      "loss: 1.417777  [ 4200/ 5482]\n",
      "loss: 1.318765  [ 4800/ 5482]\n",
      "loss: 1.436427  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.538     0.667     0.667    99\n",
      " disgust     0.538     0.000     0.000    107\n",
      "    fear     0.538     0.618     0.588    80\n",
      "   happy     0.538     0.683     0.558    77\n",
      " neutral     0.538     0.759     0.632    95\n",
      "     sad     0.538     0.332     0.879    91\n",
      "surprise     0.538     0.615     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.538     0.525     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.583043 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.451276  [    0/ 5482]\n",
      "loss: 1.680706  [  600/ 5482]\n",
      "loss: 1.501274  [ 1200/ 5482]\n",
      "loss: 1.380331  [ 1800/ 5482]\n",
      "loss: 1.489921  [ 2400/ 5482]\n",
      "loss: 1.499006  [ 3000/ 5482]\n",
      "loss: 1.355239  [ 3600/ 5482]\n",
      "loss: 1.289789  [ 4200/ 5482]\n",
      "loss: 1.428532  [ 4800/ 5482]\n",
      "loss: 1.474053  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.531     0.446     0.758    99\n",
      " disgust     0.531     0.000     0.000    107\n",
      "    fear     0.531     0.655     0.475    80\n",
      "   happy     0.531     0.452     0.675    77\n",
      " neutral     0.531     0.871     0.642    95\n",
      "     sad     0.531     0.440     0.769    91\n",
      "surprise     0.531     0.700     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.531     0.509     0.540    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.560179 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.618558  [    0/ 5482]\n",
      "loss: 1.755095  [  600/ 5482]\n",
      "loss: 1.351472  [ 1200/ 5482]\n",
      "loss: 1.575023  [ 1800/ 5482]\n",
      "loss: 1.221430  [ 2400/ 5482]\n",
      "loss: 1.019152  [ 3000/ 5482]\n",
      "loss: 1.258441  [ 3600/ 5482]\n",
      "loss: 1.309986  [ 4200/ 5482]\n",
      "loss: 1.591778  [ 4800/ 5482]\n",
      "loss: 1.234505  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.536     0.632     0.747    99\n",
      " disgust     0.536     0.000     0.000    107\n",
      "    fear     0.536     0.638     0.463    80\n",
      "   happy     0.536     0.759     0.532    77\n",
      " neutral     0.536     0.759     0.632    95\n",
      "     sad     0.536     0.339     0.934    91\n",
      "surprise     0.536     0.588     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.536     0.531     0.543    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.555573 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.543614  [    0/ 5482]\n",
      "loss: 1.417856  [  600/ 5482]\n",
      "loss: 1.316777  [ 1200/ 5482]\n",
      "loss: 1.263033  [ 1800/ 5482]\n",
      "loss: 1.342025  [ 2400/ 5482]\n",
      "loss: 1.489495  [ 3000/ 5482]\n",
      "loss: 1.645679  [ 3600/ 5482]\n",
      "loss: 1.660288  [ 4200/ 5482]\n",
      "loss: 1.496078  [ 4800/ 5482]\n",
      "loss: 1.358462  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.569     0.707    99\n",
      " disgust     0.556     0.000     0.000    107\n",
      "    fear     0.556     0.635     0.500    80\n",
      "   happy     0.556     0.600     0.584    77\n",
      " neutral     0.556     0.807     0.705    95\n",
      "     sad     0.556     0.382     0.857    91\n",
      "surprise     0.556     0.629     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.518     0.570    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.539285 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.836425  [    0/ 5482]\n",
      "loss: 1.495226  [  600/ 5482]\n",
      "loss: 1.402235  [ 1200/ 5482]\n",
      "loss: 1.262183  [ 1800/ 5482]\n",
      "loss: 1.656792  [ 2400/ 5482]\n",
      "loss: 1.199670  [ 3000/ 5482]\n",
      "loss: 1.278895  [ 3600/ 5482]\n",
      "loss: 1.493145  [ 4200/ 5482]\n",
      "loss: 1.252519  [ 4800/ 5482]\n",
      "loss: 1.334114  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.531     0.526     0.717    99\n",
      " disgust     0.531     0.000     0.000    107\n",
      "    fear     0.531     0.597     0.463    80\n",
      "   happy     0.531     0.484     0.597    77\n",
      " neutral     0.531     0.716     0.716    95\n",
      "     sad     0.531     0.411     0.791    91\n",
      "surprise     0.531     0.625     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.531     0.480     0.539    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.530610 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.349673  [    0/ 5482]\n",
      "loss: 1.609249  [  600/ 5482]\n",
      "loss: 1.307711  [ 1200/ 5482]\n",
      "loss: 1.402213  [ 1800/ 5482]\n",
      "loss: 1.344756  [ 2400/ 5482]\n",
      "loss: 1.486523  [ 3000/ 5482]\n",
      "loss: 1.538327  [ 3600/ 5482]\n",
      "loss: 1.626823  [ 4200/ 5482]\n",
      "loss: 1.178359  [ 4800/ 5482]\n",
      "loss: 1.590444  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.538     0.587     0.616    99\n",
      " disgust     0.538     0.000     0.000    107\n",
      "    fear     0.538     0.594     0.512    80\n",
      "   happy     0.538     0.597     0.558    77\n",
      " neutral     0.538     0.765     0.653    95\n",
      "     sad     0.538     0.381     0.912    91\n",
      "surprise     0.538     0.576     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.538     0.500     0.554    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.534142 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.821036  [    0/ 5482]\n",
      "loss: 1.349284  [  600/ 5482]\n",
      "loss: 1.377592  [ 1200/ 5482]\n",
      "loss: 1.328327  [ 1800/ 5482]\n",
      "loss: 1.486120  [ 2400/ 5482]\n",
      "loss: 1.314274  [ 3000/ 5482]\n",
      "loss: 1.130470  [ 3600/ 5482]\n",
      "loss: 1.210064  [ 4200/ 5482]\n",
      "loss: 1.435294  [ 4800/ 5482]\n",
      "loss: 1.213385  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.530     0.525     0.646    99\n",
      " disgust     0.530     0.000     0.000    107\n",
      "    fear     0.530     0.609     0.525    80\n",
      "   happy     0.530     0.500     0.571    77\n",
      " neutral     0.530     0.716     0.663    95\n",
      "     sad     0.530     0.414     0.824    91\n",
      "surprise     0.530     0.565     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.530     0.475     0.543    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.528515 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.287438  [    0/ 5482]\n",
      "loss: 1.440527  [  600/ 5482]\n",
      "loss: 1.571167  [ 1200/ 5482]\n",
      "loss: 1.205634  [ 1800/ 5482]\n",
      "loss: 1.338468  [ 2400/ 5482]\n",
      "loss: 1.311644  [ 3000/ 5482]\n",
      "loss: 1.460816  [ 3600/ 5482]\n",
      "loss: 1.294926  [ 4200/ 5482]\n",
      "loss: 1.642701  [ 4800/ 5482]\n",
      "loss: 1.469019  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.534     0.532     0.667    99\n",
      " disgust     0.534     0.000     0.000    107\n",
      "    fear     0.534     0.576     0.425    80\n",
      "   happy     0.534     0.531     0.558    77\n",
      " neutral     0.534     0.797     0.663    95\n",
      "     sad     0.534     0.397     0.868    91\n",
      "surprise     0.534     0.603     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.534     0.491     0.551    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.523516 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.090701  [    0/ 5482]\n",
      "loss: 1.207605  [  600/ 5482]\n",
      "loss: 1.335040  [ 1200/ 5482]\n",
      "loss: 1.139570  [ 1800/ 5482]\n",
      "loss: 1.149164  [ 2400/ 5482]\n",
      "loss: 1.027141  [ 3000/ 5482]\n",
      "loss: 1.353495  [ 3600/ 5482]\n",
      "loss: 1.506363  [ 4200/ 5482]\n",
      "loss: 1.254999  [ 4800/ 5482]\n",
      "loss: 1.223141  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.638     0.747    99\n",
      " disgust     0.556     0.000     0.000    107\n",
      "    fear     0.556     0.608     0.562    80\n",
      "   happy     0.556     0.614     0.558    77\n",
      " neutral     0.556     0.758     0.726    95\n",
      "     sad     0.556     0.369     0.901    91\n",
      "surprise     0.556     0.703     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.527     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.508912 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.465990  [    0/ 5482]\n",
      "loss: 1.229846  [  600/ 5482]\n",
      "loss: 1.276216  [ 1200/ 5482]\n",
      "loss: 1.240313  [ 1800/ 5482]\n",
      "loss: 1.479804  [ 2400/ 5482]\n",
      "loss: 1.570322  [ 3000/ 5482]\n",
      "loss: 1.355789  [ 3600/ 5482]\n",
      "loss: 1.295493  [ 4200/ 5482]\n",
      "loss: 1.269142  [ 4800/ 5482]\n",
      "loss: 1.219484  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.539     0.521     0.747    99\n",
      " disgust     0.539     0.000     0.000    107\n",
      "    fear     0.539     0.592     0.525    80\n",
      "   happy     0.539     0.512     0.532    77\n",
      " neutral     0.539     0.805     0.695    95\n",
      "     sad     0.539     0.422     0.868    91\n",
      "surprise     0.539     0.562     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.539     0.488     0.544    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.512102 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.305664  [    0/ 5482]\n",
      "loss: 1.465881  [  600/ 5482]\n",
      "loss: 1.361488  [ 1200/ 5482]\n",
      "loss: 1.246749  [ 1800/ 5482]\n",
      "loss: 1.154786  [ 2400/ 5482]\n",
      "loss: 1.181237  [ 3000/ 5482]\n",
      "loss: 1.473461  [ 3600/ 5482]\n",
      "loss: 1.456642  [ 4200/ 5482]\n",
      "loss: 1.543559  [ 4800/ 5482]\n",
      "loss: 1.680102  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.544     0.613     0.687    99\n",
      " disgust     0.544     0.000     0.000    107\n",
      "    fear     0.544     0.632     0.537    80\n",
      "   happy     0.544     0.714     0.455    77\n",
      " neutral     0.544     0.762     0.674    95\n",
      "     sad     0.544     0.361     0.912    91\n",
      "surprise     0.544     0.574     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.544     0.522     0.558    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.516762 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.208774  [    0/ 5482]\n",
      "loss: 1.294254  [  600/ 5482]\n",
      "loss: 1.408365  [ 1200/ 5482]\n",
      "loss: 1.314778  [ 1800/ 5482]\n",
      "loss: 1.306659  [ 2400/ 5482]\n",
      "loss: 1.261159  [ 3000/ 5482]\n",
      "loss: 1.498064  [ 3600/ 5482]\n",
      "loss: 1.222822  [ 4200/ 5482]\n",
      "loss: 1.325154  [ 4800/ 5482]\n",
      "loss: 1.460216  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.539     0.534     0.717    99\n",
      " disgust     0.539     0.000     0.000    107\n",
      "    fear     0.539     0.638     0.463    80\n",
      "   happy     0.539     0.505     0.636    77\n",
      " neutral     0.539     0.723     0.716    95\n",
      "     sad     0.539     0.409     0.791    91\n",
      "surprise     0.539     0.615     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.539     0.489     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.489187 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.502140  [    0/ 5482]\n",
      "loss: 1.346277  [  600/ 5482]\n",
      "loss: 1.204898  [ 1200/ 5482]\n",
      "loss: 0.974298  [ 1800/ 5482]\n",
      "loss: 1.282753  [ 2400/ 5482]\n",
      "loss: 1.397873  [ 3000/ 5482]\n",
      "loss: 1.194841  [ 3600/ 5482]\n",
      "loss: 1.333597  [ 4200/ 5482]\n",
      "loss: 1.121127  [ 4800/ 5482]\n",
      "loss: 1.168669  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.500     0.747    99\n",
      " disgust     0.556     0.000     0.000    107\n",
      "    fear     0.556     0.691     0.475    80\n",
      "   happy     0.556     0.547     0.610    77\n",
      " neutral     0.556     0.714     0.737    95\n",
      "     sad     0.556     0.450     0.846    91\n",
      "surprise     0.556     0.635     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.505     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.475211 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.205376  [    0/ 5482]\n",
      "loss: 1.155511  [  600/ 5482]\n",
      "loss: 1.640300  [ 1200/ 5482]\n",
      "loss: 1.649939  [ 1800/ 5482]\n",
      "loss: 1.457935  [ 2400/ 5482]\n",
      "loss: 1.406218  [ 3000/ 5482]\n",
      "loss: 1.222627  [ 3600/ 5482]\n",
      "loss: 1.112261  [ 4200/ 5482]\n",
      "loss: 1.081673  [ 4800/ 5482]\n",
      "loss: 1.175838  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.541     0.596     0.657    99\n",
      " disgust     0.541     0.000     0.000    107\n",
      "    fear     0.541     0.644     0.475    80\n",
      "   happy     0.541     0.580     0.610    77\n",
      " neutral     0.541     0.851     0.663    95\n",
      "     sad     0.541     0.367     0.802    91\n",
      "surprise     0.541     0.500     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.541     0.506     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 1.498519 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.279807  [    0/ 5482]\n",
      "loss: 1.465151  [  600/ 5482]\n",
      "loss: 1.146192  [ 1200/ 5482]\n",
      "loss: 1.343251  [ 1800/ 5482]\n",
      "loss: 1.459442  [ 2400/ 5482]\n",
      "loss: 1.225101  [ 3000/ 5482]\n",
      "loss: 1.295439  [ 3600/ 5482]\n",
      "loss: 1.354837  [ 4200/ 5482]\n",
      "loss: 1.253264  [ 4800/ 5482]\n",
      "loss: 1.148695  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.554     0.586     0.687    99\n",
      " disgust     0.554     0.000     0.000    107\n",
      "    fear     0.554     0.762     0.400    80\n",
      "   happy     0.554     0.477     0.675    77\n",
      " neutral     0.554     0.846     0.695    95\n",
      "     sad     0.554     0.404     0.791    91\n",
      "surprise     0.554     0.552     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.554     0.518     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.487446 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.550504  [    0/ 5482]\n",
      "loss: 1.282141  [  600/ 5482]\n",
      "loss: 1.207718  [ 1200/ 5482]\n",
      "loss: 1.119187  [ 1800/ 5482]\n",
      "loss: 1.296611  [ 2400/ 5482]\n",
      "loss: 1.314262  [ 3000/ 5482]\n",
      "loss: 1.387949  [ 3600/ 5482]\n",
      "loss: 1.488494  [ 4200/ 5482]\n",
      "loss: 1.160670  [ 4800/ 5482]\n",
      "loss: 1.216261  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.563     0.677    99\n",
      " disgust     0.551     0.000     0.000    107\n",
      "    fear     0.551     0.511     0.600    80\n",
      "   happy     0.551     0.719     0.532    77\n",
      " neutral     0.551     0.818     0.663    95\n",
      "     sad     0.551     0.396     0.857    91\n",
      "surprise     0.551     0.591     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.514     0.567    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.483546 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.387491  [    0/ 5482]\n",
      "loss: 1.212437  [  600/ 5482]\n",
      "loss: 1.192920  [ 1200/ 5482]\n",
      "loss: 1.240620  [ 1800/ 5482]\n",
      "loss: 1.192321  [ 2400/ 5482]\n",
      "loss: 1.226641  [ 3000/ 5482]\n",
      "loss: 1.258152  [ 3600/ 5482]\n",
      "loss: 1.182190  [ 4200/ 5482]\n",
      "loss: 1.296127  [ 4800/ 5482]\n",
      "loss: 1.303673  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.548     0.578     0.747    99\n",
      " disgust     0.548     0.000     0.000    107\n",
      "    fear     0.548     0.592     0.525    80\n",
      "   happy     0.548     0.615     0.519    77\n",
      " neutral     0.548     0.680     0.737    95\n",
      "     sad     0.548     0.387     0.813    91\n",
      "surprise     0.548     0.654     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.548     0.501     0.557    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.462958 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.237563  [    0/ 5482]\n",
      "loss: 1.399752  [  600/ 5482]\n",
      "loss: 1.171744  [ 1200/ 5482]\n",
      "loss: 1.380913  [ 1800/ 5482]\n",
      "loss: 1.410800  [ 2400/ 5482]\n",
      "loss: 1.353036  [ 3000/ 5482]\n",
      "loss: 1.246843  [ 3600/ 5482]\n",
      "loss: 1.330048  [ 4200/ 5482]\n",
      "loss: 1.370243  [ 4800/ 5482]\n",
      "loss: 1.334037  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.530     0.470     0.707    99\n",
      " disgust     0.530     0.000     0.000    107\n",
      "    fear     0.530     0.698     0.463    80\n",
      "   happy     0.530     0.469     0.597    77\n",
      " neutral     0.530     0.680     0.737    95\n",
      "     sad     0.530     0.439     0.747    91\n",
      "surprise     0.530     0.615     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.530     0.482     0.539    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.467070 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.453501  [    0/ 5482]\n",
      "loss: 1.200001  [  600/ 5482]\n",
      "loss: 1.447767  [ 1200/ 5482]\n",
      "loss: 1.101461  [ 1800/ 5482]\n",
      "loss: 1.423908  [ 2400/ 5482]\n",
      "loss: 1.284219  [ 3000/ 5482]\n",
      "loss: 1.167373  [ 3600/ 5482]\n",
      "loss: 1.373950  [ 4200/ 5482]\n",
      "loss: 1.231400  [ 4800/ 5482]\n",
      "loss: 1.110481  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.552     0.614     0.707    99\n",
      " disgust     0.552     0.000     0.000    107\n",
      "    fear     0.552     0.638     0.550    80\n",
      "   happy     0.552     0.717     0.494    77\n",
      " neutral     0.552     0.681     0.674    95\n",
      "     sad     0.552     0.380     0.923    91\n",
      "surprise     0.552     0.627     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.552     0.522     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.459805 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.238730  [    0/ 5482]\n",
      "loss: 1.480633  [  600/ 5482]\n",
      "loss: 1.139996  [ 1200/ 5482]\n",
      "loss: 1.471999  [ 1800/ 5482]\n",
      "loss: 1.133130  [ 2400/ 5482]\n",
      "loss: 1.328489  [ 3000/ 5482]\n",
      "loss: 1.219498  [ 3600/ 5482]\n",
      "loss: 1.011300  [ 4200/ 5482]\n",
      "loss: 1.305252  [ 4800/ 5482]\n",
      "loss: 1.240933  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.549     0.560     0.707    99\n",
      " disgust     0.549     0.000     0.000    107\n",
      "    fear     0.549     0.548     0.500    80\n",
      "   happy     0.549     0.557     0.506    77\n",
      " neutral     0.549     0.783     0.684    95\n",
      "     sad     0.549     0.421     0.846    91\n",
      "surprise     0.549     0.579     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.549     0.493     0.566    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.458312 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.537669  [    0/ 5482]\n",
      "loss: 1.073629  [  600/ 5482]\n",
      "loss: 1.264054  [ 1200/ 5482]\n",
      "loss: 1.373451  [ 1800/ 5482]\n",
      "loss: 1.212314  [ 2400/ 5482]\n",
      "loss: 1.085964  [ 3000/ 5482]\n",
      "loss: 1.311483  [ 3600/ 5482]\n",
      "loss: 1.472488  [ 4200/ 5482]\n",
      "loss: 0.975681  [ 4800/ 5482]\n",
      "loss: 1.216447  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.479     0.788    99\n",
      " disgust     0.551     0.000     0.000    107\n",
      "    fear     0.551     0.635     0.500    80\n",
      "   happy     0.551     0.571     0.571    77\n",
      " neutral     0.551     0.614     0.737    95\n",
      "     sad     0.551     0.474     0.813    91\n",
      "surprise     0.551     0.811     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.512     0.557    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.453605 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.238911  [    0/ 5482]\n",
      "loss: 1.114623  [  600/ 5482]\n",
      "loss: 1.223756  [ 1200/ 5482]\n",
      "loss: 1.583446  [ 1800/ 5482]\n",
      "loss: 1.273473  [ 2400/ 5482]\n",
      "loss: 1.141076  [ 3000/ 5482]\n",
      "loss: 0.970468  [ 3600/ 5482]\n",
      "loss: 1.374241  [ 4200/ 5482]\n",
      "loss: 1.209068  [ 4800/ 5482]\n",
      "loss: 1.393752  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.538     0.631     0.657    99\n",
      " disgust     0.538     0.000     0.000    107\n",
      "    fear     0.538     0.581     0.537    80\n",
      "   happy     0.538     0.816     0.519    77\n",
      " neutral     0.538     0.756     0.684    95\n",
      "     sad     0.538     0.328     0.945    91\n",
      "surprise     0.538     0.806     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.538     0.560     0.545    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.484925 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.202410  [    0/ 5482]\n",
      "loss: 1.131315  [  600/ 5482]\n",
      "loss: 1.104121  [ 1200/ 5482]\n",
      "loss: 1.152736  [ 1800/ 5482]\n",
      "loss: 1.130672  [ 2400/ 5482]\n",
      "loss: 1.190621  [ 3000/ 5482]\n",
      "loss: 1.309741  [ 3600/ 5482]\n",
      "loss: 1.108790  [ 4200/ 5482]\n",
      "loss: 1.113146  [ 4800/ 5482]\n",
      "loss: 1.415435  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.552     0.613     0.687    99\n",
      " disgust     0.552     0.000     0.000    107\n",
      "    fear     0.552     0.636     0.525    80\n",
      "   happy     0.552     0.589     0.558    77\n",
      " neutral     0.552     0.697     0.726    95\n",
      "     sad     0.552     0.392     0.780    91\n",
      "surprise     0.552     0.557     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.552     0.498     0.571    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.431750 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.557404  [    0/ 5482]\n",
      "loss: 1.061795  [  600/ 5482]\n",
      "loss: 1.302730  [ 1200/ 5482]\n",
      "loss: 1.179327  [ 1800/ 5482]\n",
      "loss: 1.019821  [ 2400/ 5482]\n",
      "loss: 1.213610  [ 3000/ 5482]\n",
      "loss: 1.128946  [ 3600/ 5482]\n",
      "loss: 1.373512  [ 4200/ 5482]\n",
      "loss: 1.161122  [ 4800/ 5482]\n",
      "loss: 1.022070  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.561     0.557     0.737    99\n",
      " disgust     0.561     1.000     0.009    107\n",
      "    fear     0.561     0.655     0.450    80\n",
      "   happy     0.561     0.515     0.662    77\n",
      " neutral     0.561     0.722     0.737    95\n",
      "     sad     0.561     0.441     0.780    91\n",
      "surprise     0.561     0.606     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.561     0.642     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.430674 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.108988  [    0/ 5482]\n",
      "loss: 1.346593  [  600/ 5482]\n",
      "loss: 1.382988  [ 1200/ 5482]\n",
      "loss: 0.931702  [ 1800/ 5482]\n",
      "loss: 0.989496  [ 2400/ 5482]\n",
      "loss: 1.282806  [ 3000/ 5482]\n",
      "loss: 1.395408  [ 3600/ 5482]\n",
      "loss: 1.154876  [ 4200/ 5482]\n",
      "loss: 1.273284  [ 4800/ 5482]\n",
      "loss: 1.070881  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.559     0.570     0.737    99\n",
      " disgust     0.559     0.500     0.009    107\n",
      "    fear     0.559     0.639     0.575    80\n",
      "   happy     0.559     0.655     0.494    77\n",
      " neutral     0.559     0.725     0.695    95\n",
      "     sad     0.559     0.394     0.835    91\n",
      "surprise     0.559     0.621     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.559     0.586     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.426186 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.205263  [    0/ 5482]\n",
      "loss: 1.207139  [  600/ 5482]\n",
      "loss: 1.006657  [ 1200/ 5482]\n",
      "loss: 0.943848  [ 1800/ 5482]\n",
      "loss: 1.257597  [ 2400/ 5482]\n",
      "loss: 0.940768  [ 3000/ 5482]\n",
      "loss: 1.222242  [ 3600/ 5482]\n",
      "loss: 1.181283  [ 4200/ 5482]\n",
      "loss: 1.210034  [ 4800/ 5482]\n",
      "loss: 0.974355  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.559     0.589     0.667    99\n",
      " disgust     0.559     0.750     0.028    107\n",
      "    fear     0.559     0.625     0.562    80\n",
      "   happy     0.559     0.627     0.545    77\n",
      " neutral     0.559     0.783     0.684    95\n",
      "     sad     0.559     0.402     0.791    91\n",
      "surprise     0.559     0.516     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.559     0.613     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.424983 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.044135  [    0/ 5482]\n",
      "loss: 1.271922  [  600/ 5482]\n",
      "loss: 1.244744  [ 1200/ 5482]\n",
      "loss: 1.223123  [ 1800/ 5482]\n",
      "loss: 1.116687  [ 2400/ 5482]\n",
      "loss: 1.308976  [ 3000/ 5482]\n",
      "loss: 0.908267  [ 3600/ 5482]\n",
      "loss: 1.081246  [ 4200/ 5482]\n",
      "loss: 1.243697  [ 4800/ 5482]\n",
      "loss: 0.992106  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.581     0.758    99\n",
      " disgust     0.589     0.889     0.075    107\n",
      "    fear     0.589     0.729     0.537    80\n",
      "   happy     0.589     0.710     0.571    77\n",
      " neutral     0.589     0.726     0.726    95\n",
      "     sad     0.589     0.409     0.868    91\n",
      "surprise     0.589     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.671     0.601    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.415515 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.134592  [    0/ 5482]\n",
      "loss: 1.194472  [  600/ 5482]\n",
      "loss: 1.132567  [ 1200/ 5482]\n",
      "loss: 1.084461  [ 1800/ 5482]\n",
      "loss: 0.984556  [ 2400/ 5482]\n",
      "loss: 0.995222  [ 3000/ 5482]\n",
      "loss: 1.043585  [ 3600/ 5482]\n",
      "loss: 1.104555  [ 4200/ 5482]\n",
      "loss: 1.162042  [ 4800/ 5482]\n",
      "loss: 1.126728  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.570     0.589     0.768    99\n",
      " disgust     0.570     0.750     0.028    107\n",
      "    fear     0.570     0.672     0.512    80\n",
      "   happy     0.570     0.577     0.532    77\n",
      " neutral     0.570     0.591     0.789    95\n",
      "     sad     0.570     0.466     0.824    91\n",
      "surprise     0.570     0.649     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.570     0.613     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.392911 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.917175  [    0/ 5482]\n",
      "loss: 1.584934  [  600/ 5482]\n",
      "loss: 0.902764  [ 1200/ 5482]\n",
      "loss: 1.391947  [ 1800/ 5482]\n",
      "loss: 0.987976  [ 2400/ 5482]\n",
      "loss: 1.337675  [ 3000/ 5482]\n",
      "loss: 1.107307  [ 3600/ 5482]\n",
      "loss: 1.161986  [ 4200/ 5482]\n",
      "loss: 0.953813  [ 4800/ 5482]\n",
      "loss: 1.081088  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.602     0.778    99\n",
      " disgust     0.567     1.000     0.028    107\n",
      "    fear     0.567     0.536     0.562    80\n",
      "   happy     0.567     0.667     0.545    77\n",
      " neutral     0.567     0.616     0.726    95\n",
      "     sad     0.567     0.436     0.824    91\n",
      "surprise     0.567     0.729     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.655     0.577    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.395402 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.075916  [    0/ 5482]\n",
      "loss: 1.126253  [  600/ 5482]\n",
      "loss: 1.183213  [ 1200/ 5482]\n",
      "loss: 1.045250  [ 1800/ 5482]\n",
      "loss: 1.189474  [ 2400/ 5482]\n",
      "loss: 1.255365  [ 3000/ 5482]\n",
      "loss: 1.411570  [ 3600/ 5482]\n",
      "loss: 1.397423  [ 4200/ 5482]\n",
      "loss: 1.252616  [ 4800/ 5482]\n",
      "loss: 1.127014  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.532     0.758    99\n",
      " disgust     0.587     1.000     0.131    107\n",
      "    fear     0.587     0.727     0.500    80\n",
      "   happy     0.587     0.571     0.571    77\n",
      " neutral     0.587     0.747     0.747    95\n",
      "     sad     0.587     0.462     0.868    91\n",
      "surprise     0.587     0.614     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.665     0.593    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.384743 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.040338  [    0/ 5482]\n",
      "loss: 1.170416  [  600/ 5482]\n",
      "loss: 1.051037  [ 1200/ 5482]\n",
      "loss: 1.279801  [ 1800/ 5482]\n",
      "loss: 1.337124  [ 2400/ 5482]\n",
      "loss: 1.113282  [ 3000/ 5482]\n",
      "loss: 0.979940  [ 3600/ 5482]\n",
      "loss: 1.117111  [ 4200/ 5482]\n",
      "loss: 1.064228  [ 4800/ 5482]\n",
      "loss: 1.216809  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.631     0.657    99\n",
      " disgust     0.572     0.778     0.065    107\n",
      "    fear     0.572     0.551     0.675    80\n",
      "   happy     0.572     0.573     0.610    77\n",
      " neutral     0.572     0.770     0.705    95\n",
      "     sad     0.572     0.412     0.725    91\n",
      "surprise     0.572     0.606     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.617     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.403787 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.412137  [    0/ 5482]\n",
      "loss: 1.428120  [  600/ 5482]\n",
      "loss: 1.101065  [ 1200/ 5482]\n",
      "loss: 0.974897  [ 1800/ 5482]\n",
      "loss: 1.158893  [ 2400/ 5482]\n",
      "loss: 1.207330  [ 3000/ 5482]\n",
      "loss: 1.240852  [ 3600/ 5482]\n",
      "loss: 0.972964  [ 4200/ 5482]\n",
      "loss: 1.228258  [ 4800/ 5482]\n",
      "loss: 0.994110  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.548     0.808    99\n",
      " disgust     0.580     0.818     0.084    107\n",
      "    fear     0.580     0.655     0.450    80\n",
      "   happy     0.580     0.636     0.545    77\n",
      " neutral     0.580     0.724     0.747    95\n",
      "     sad     0.580     0.449     0.824    91\n",
      "surprise     0.580     0.612     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.635     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.371895 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.981234  [    0/ 5482]\n",
      "loss: 1.182250  [  600/ 5482]\n",
      "loss: 1.687387  [ 1200/ 5482]\n",
      "loss: 1.303033  [ 1800/ 5482]\n",
      "loss: 0.887608  [ 2400/ 5482]\n",
      "loss: 1.081148  [ 3000/ 5482]\n",
      "loss: 1.479504  [ 3600/ 5482]\n",
      "loss: 1.155083  [ 4200/ 5482]\n",
      "loss: 1.152574  [ 4800/ 5482]\n",
      "loss: 1.260667  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.660     0.707    99\n",
      " disgust     0.579     1.000     0.065    107\n",
      "    fear     0.579     0.538     0.613    80\n",
      "   happy     0.579     0.784     0.519    77\n",
      " neutral     0.579     0.800     0.716    95\n",
      "     sad     0.579     0.386     0.912    91\n",
      "surprise     0.579     0.655     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.689     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.409521 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.046993  [    0/ 5482]\n",
      "loss: 1.056933  [  600/ 5482]\n",
      "loss: 1.056098  [ 1200/ 5482]\n",
      "loss: 1.167438  [ 1800/ 5482]\n",
      "loss: 1.191486  [ 2400/ 5482]\n",
      "loss: 1.149570  [ 3000/ 5482]\n",
      "loss: 1.011908  [ 3600/ 5482]\n",
      "loss: 1.275663  [ 4200/ 5482]\n",
      "loss: 1.274969  [ 4800/ 5482]\n",
      "loss: 1.015055  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.631     0.707    99\n",
      " disgust     0.597     0.957     0.206    107\n",
      "    fear     0.597     0.603     0.550    80\n",
      "   happy     0.597     0.584     0.584    77\n",
      " neutral     0.597     0.742     0.695    95\n",
      "     sad     0.597     0.442     0.835    91\n",
      "surprise     0.597     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.656     0.607    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.380485 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.189950  [    0/ 5482]\n",
      "loss: 1.048906  [  600/ 5482]\n",
      "loss: 0.965046  [ 1200/ 5482]\n",
      "loss: 1.065022  [ 1800/ 5482]\n",
      "loss: 1.125586  [ 2400/ 5482]\n",
      "loss: 0.932903  [ 3000/ 5482]\n",
      "loss: 1.162377  [ 3600/ 5482]\n",
      "loss: 1.104753  [ 4200/ 5482]\n",
      "loss: 1.171281  [ 4800/ 5482]\n",
      "loss: 1.032148  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.582     0.717    99\n",
      " disgust     0.589     0.900     0.168    107\n",
      "    fear     0.589     0.638     0.550    80\n",
      "   happy     0.589     0.667     0.545    77\n",
      " neutral     0.589     0.719     0.726    95\n",
      "     sad     0.589     0.417     0.824    91\n",
      "surprise     0.589     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.655     0.598    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.366928 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.262048  [    0/ 5482]\n",
      "loss: 0.873159  [  600/ 5482]\n",
      "loss: 1.169213  [ 1200/ 5482]\n",
      "loss: 1.130287  [ 1800/ 5482]\n",
      "loss: 1.074219  [ 2400/ 5482]\n",
      "loss: 0.919047  [ 3000/ 5482]\n",
      "loss: 1.326706  [ 3600/ 5482]\n",
      "loss: 1.218564  [ 4200/ 5482]\n",
      "loss: 1.064812  [ 4800/ 5482]\n",
      "loss: 1.197418  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.570     0.737    99\n",
      " disgust     0.602     0.909     0.187    107\n",
      "    fear     0.602     0.644     0.588    80\n",
      "   happy     0.602     0.595     0.571    77\n",
      " neutral     0.602     0.714     0.737    95\n",
      "     sad     0.602     0.480     0.780    91\n",
      "surprise     0.602     0.627     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.648     0.613    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.365444 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep80_acc_60.md \n",
      "\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep80_acc_60\"!  new accuracy: 60.2\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.188171  [    0/ 5482]\n",
      "loss: 1.137227  [  600/ 5482]\n",
      "loss: 0.892291  [ 1200/ 5482]\n",
      "loss: 1.145479  [ 1800/ 5482]\n",
      "loss: 1.230753  [ 2400/ 5482]\n",
      "loss: 1.171739  [ 3000/ 5482]\n",
      "loss: 1.032913  [ 3600/ 5482]\n",
      "loss: 1.211105  [ 4200/ 5482]\n",
      "loss: 0.980919  [ 4800/ 5482]\n",
      "loss: 1.028038  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.663     0.697    99\n",
      " disgust     0.600     0.913     0.196    107\n",
      "    fear     0.600     0.615     0.500    80\n",
      "   happy     0.600     0.615     0.623    77\n",
      " neutral     0.600     0.731     0.716    95\n",
      "     sad     0.600     0.442     0.791    91\n",
      "surprise     0.600     0.571     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.650     0.616    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.352454 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.197956  [    0/ 5482]\n",
      "loss: 1.039539  [  600/ 5482]\n",
      "loss: 1.117759  [ 1200/ 5482]\n",
      "loss: 1.215009  [ 1800/ 5482]\n",
      "loss: 0.895498  [ 2400/ 5482]\n",
      "loss: 0.844174  [ 3000/ 5482]\n",
      "loss: 1.145909  [ 3600/ 5482]\n",
      "loss: 1.227399  [ 4200/ 5482]\n",
      "loss: 1.015713  [ 4800/ 5482]\n",
      "loss: 1.142360  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.608     0.524     0.768    99\n",
      " disgust     0.608     0.931     0.252    107\n",
      "    fear     0.608     0.597     0.500    80\n",
      "   happy     0.608     0.511     0.610    77\n",
      " neutral     0.608     0.802     0.768    95\n",
      "     sad     0.608     0.539     0.758    91\n",
      "surprise     0.608     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.608     0.654     0.614    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.358049 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep82_acc_61.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep80_acc_60\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep82_acc_61\"! Old accuracy: 60.2, new accuracy: 60.8\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.355481  [    0/ 5482]\n",
      "loss: 1.530316  [  600/ 5482]\n",
      "loss: 0.995493  [ 1200/ 5482]\n",
      "loss: 1.271609  [ 1800/ 5482]\n",
      "loss: 1.109837  [ 2400/ 5482]\n",
      "loss: 1.031213  [ 3000/ 5482]\n",
      "loss: 1.309073  [ 3600/ 5482]\n",
      "loss: 1.118170  [ 4200/ 5482]\n",
      "loss: 0.976408  [ 4800/ 5482]\n",
      "loss: 0.979539  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.613     0.558     0.727    99\n",
      " disgust     0.613     0.867     0.243    107\n",
      "    fear     0.613     0.597     0.575    80\n",
      "   happy     0.613     0.629     0.571    77\n",
      " neutral     0.613     0.779     0.779    95\n",
      "     sad     0.613     0.500     0.769    91\n",
      "surprise     0.613     0.609     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.613     0.648     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.340263 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep83_acc_61.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep82_acc_61\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep83_acc_61\"! Old accuracy: 60.8, new accuracy: 61.3\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.165024  [    0/ 5482]\n",
      "loss: 0.957135  [  600/ 5482]\n",
      "loss: 0.924869  [ 1200/ 5482]\n",
      "loss: 1.179751  [ 1800/ 5482]\n",
      "loss: 1.164262  [ 2400/ 5482]\n",
      "loss: 1.187120  [ 3000/ 5482]\n",
      "loss: 0.908093  [ 3600/ 5482]\n",
      "loss: 1.259946  [ 4200/ 5482]\n",
      "loss: 0.905055  [ 4800/ 5482]\n",
      "loss: 1.070568  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.625     0.600     0.727    99\n",
      " disgust     0.625     0.914     0.299    107\n",
      "    fear     0.625     0.705     0.537    80\n",
      "   happy     0.625     0.638     0.571    77\n",
      " neutral     0.625     0.848     0.705    95\n",
      "     sad     0.625     0.450     0.846    91\n",
      "surprise     0.625     0.613     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.625     0.681     0.634    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.358147 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep84_acc_62.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep83_acc_61\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep84_acc_62\"! Old accuracy: 61.3, new accuracy: 62.5\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.107009  [    0/ 5482]\n",
      "loss: 1.100861  [  600/ 5482]\n",
      "loss: 0.777078  [ 1200/ 5482]\n",
      "loss: 0.930173  [ 1800/ 5482]\n",
      "loss: 1.041933  [ 2400/ 5482]\n",
      "loss: 0.983575  [ 3000/ 5482]\n",
      "loss: 0.957767  [ 3600/ 5482]\n",
      "loss: 1.191329  [ 4200/ 5482]\n",
      "loss: 1.171983  [ 4800/ 5482]\n",
      "loss: 1.288072  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.497     0.808    99\n",
      " disgust     0.633     0.923     0.336    107\n",
      "    fear     0.633     0.627     0.588    80\n",
      "   happy     0.633     0.697     0.597    77\n",
      " neutral     0.633     0.704     0.853    95\n",
      "     sad     0.633     0.577     0.703    91\n",
      "surprise     0.633     0.744     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.681     0.630    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.351576 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep85_acc_63.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep84_acc_62\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep85_acc_63\"! Old accuracy: 62.5, new accuracy: 63.3\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.904786  [    0/ 5482]\n",
      "loss: 1.293156  [  600/ 5482]\n",
      "loss: 1.062814  [ 1200/ 5482]\n",
      "loss: 0.827396  [ 1800/ 5482]\n",
      "loss: 1.218522  [ 2400/ 5482]\n",
      "loss: 0.779964  [ 3000/ 5482]\n",
      "loss: 0.969229  [ 3600/ 5482]\n",
      "loss: 0.838572  [ 4200/ 5482]\n",
      "loss: 1.006951  [ 4800/ 5482]\n",
      "loss: 1.064760  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.670     0.697    99\n",
      " disgust     0.595     0.950     0.178    107\n",
      "    fear     0.595     0.676     0.600    80\n",
      "   happy     0.595     0.724     0.545    77\n",
      " neutral     0.595     0.817     0.705    95\n",
      "     sad     0.595     0.385     0.846    91\n",
      "surprise     0.595     0.539     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.680     0.606    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.358234 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.050166  [    0/ 5482]\n",
      "loss: 0.894233  [  600/ 5482]\n",
      "loss: 1.003570  [ 1200/ 5482]\n",
      "loss: 0.902905  [ 1800/ 5482]\n",
      "loss: 1.068764  [ 2400/ 5482]\n",
      "loss: 0.872758  [ 3000/ 5482]\n",
      "loss: 0.968256  [ 3600/ 5482]\n",
      "loss: 1.172258  [ 4200/ 5482]\n",
      "loss: 1.293126  [ 4800/ 5482]\n",
      "loss: 0.839467  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.608     0.602     0.717    99\n",
      " disgust     0.608     0.852     0.215    107\n",
      "    fear     0.608     0.684     0.487    80\n",
      "   happy     0.608     0.634     0.584    77\n",
      " neutral     0.608     0.800     0.758    95\n",
      "     sad     0.608     0.445     0.802    91\n",
      "surprise     0.608     0.578     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.608     0.656     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.337533 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.895203  [    0/ 5482]\n",
      "loss: 0.896870  [  600/ 5482]\n",
      "loss: 0.986944  [ 1200/ 5482]\n",
      "loss: 1.226935  [ 1800/ 5482]\n",
      "loss: 1.313434  [ 2400/ 5482]\n",
      "loss: 1.074170  [ 3000/ 5482]\n",
      "loss: 1.311012  [ 3600/ 5482]\n",
      "loss: 1.027789  [ 4200/ 5482]\n",
      "loss: 1.067005  [ 4800/ 5482]\n",
      "loss: 1.021161  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.612     0.798    99\n",
      " disgust     0.652     0.882     0.421    107\n",
      "    fear     0.652     0.691     0.588    80\n",
      "   happy     0.652     0.701     0.610    77\n",
      " neutral     0.652     0.639     0.800    95\n",
      "     sad     0.652     0.546     0.780    91\n",
      "surprise     0.652     0.717     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.684     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.310946 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep88_acc_65.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep85_acc_63\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep88_acc_65\"! Old accuracy: 63.3, new accuracy: 65.2\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.875388  [    0/ 5482]\n",
      "loss: 0.993938  [  600/ 5482]\n",
      "loss: 1.011176  [ 1200/ 5482]\n",
      "loss: 0.922502  [ 1800/ 5482]\n",
      "loss: 0.943948  [ 2400/ 5482]\n",
      "loss: 0.919446  [ 3000/ 5482]\n",
      "loss: 0.822809  [ 3600/ 5482]\n",
      "loss: 1.084548  [ 4200/ 5482]\n",
      "loss: 1.076157  [ 4800/ 5482]\n",
      "loss: 1.159614  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.615     0.808    99\n",
      " disgust     0.643     0.818     0.336    107\n",
      "    fear     0.643     0.672     0.537    80\n",
      "   happy     0.643     0.636     0.636    77\n",
      " neutral     0.643     0.779     0.779    95\n",
      "     sad     0.643     0.526     0.780    91\n",
      "surprise     0.643     0.600     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.664     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.301580 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.128580  [    0/ 5482]\n",
      "loss: 1.208227  [  600/ 5482]\n",
      "loss: 1.038244  [ 1200/ 5482]\n",
      "loss: 1.135842  [ 1800/ 5482]\n",
      "loss: 1.211201  [ 2400/ 5482]\n",
      "loss: 0.842862  [ 3000/ 5482]\n",
      "loss: 0.975518  [ 3600/ 5482]\n",
      "loss: 1.163471  [ 4200/ 5482]\n",
      "loss: 1.118451  [ 4800/ 5482]\n",
      "loss: 0.841917  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.630     0.603     0.768    99\n",
      " disgust     0.630     0.943     0.308    107\n",
      "    fear     0.630     0.588     0.588    80\n",
      "   happy     0.630     0.574     0.701    77\n",
      " neutral     0.630     0.787     0.737    95\n",
      "     sad     0.630     0.535     0.758    91\n",
      "surprise     0.630     0.614     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.630     0.663     0.633    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.310166 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.963826  [    0/ 5482]\n",
      "loss: 1.179758  [  600/ 5482]\n",
      "loss: 0.912789  [ 1200/ 5482]\n",
      "loss: 1.094868  [ 1800/ 5482]\n",
      "loss: 1.120193  [ 2400/ 5482]\n",
      "loss: 0.939080  [ 3000/ 5482]\n",
      "loss: 1.160955  [ 3600/ 5482]\n",
      "loss: 0.861828  [ 4200/ 5482]\n",
      "loss: 0.894188  [ 4800/ 5482]\n",
      "loss: 0.932896  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.571     0.808    99\n",
      " disgust     0.659     0.868     0.430    107\n",
      "    fear     0.659     0.679     0.662    80\n",
      "   happy     0.659     0.695     0.532    77\n",
      " neutral     0.659     0.724     0.800    95\n",
      "     sad     0.659     0.538     0.780    91\n",
      "surprise     0.659     0.814     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.698     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.315943 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep91_acc_66.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep88_acc_65\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep91_acc_66\"! Old accuracy: 65.2, new accuracy: 65.9\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.066498  [    0/ 5482]\n",
      "loss: 1.091733  [  600/ 5482]\n",
      "loss: 1.024596  [ 1200/ 5482]\n",
      "loss: 0.762979  [ 1800/ 5482]\n",
      "loss: 1.011058  [ 2400/ 5482]\n",
      "loss: 1.032353  [ 3000/ 5482]\n",
      "loss: 1.029017  [ 3600/ 5482]\n",
      "loss: 0.887445  [ 4200/ 5482]\n",
      "loss: 0.983823  [ 4800/ 5482]\n",
      "loss: 1.275368  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.600     0.758    99\n",
      " disgust     0.648     0.849     0.421    107\n",
      "    fear     0.648     0.571     0.650    80\n",
      "   happy     0.648     0.616     0.584    77\n",
      " neutral     0.648     0.740     0.811    95\n",
      "     sad     0.648     0.602     0.747    91\n",
      "surprise     0.648     0.647     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.661     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.284329 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.728488  [    0/ 5482]\n",
      "loss: 0.885923  [  600/ 5482]\n",
      "loss: 0.937108  [ 1200/ 5482]\n",
      "loss: 0.858166  [ 1800/ 5482]\n",
      "loss: 1.125990  [ 2400/ 5482]\n",
      "loss: 1.408697  [ 3000/ 5482]\n",
      "loss: 0.923594  [ 3600/ 5482]\n",
      "loss: 0.958813  [ 4200/ 5482]\n",
      "loss: 0.821342  [ 4800/ 5482]\n",
      "loss: 1.090480  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.614     0.818    99\n",
      " disgust     0.662     0.833     0.421    107\n",
      "    fear     0.662     0.683     0.537    80\n",
      "   happy     0.662     0.667     0.597    77\n",
      " neutral     0.662     0.791     0.758    95\n",
      "     sad     0.662     0.525     0.813    91\n",
      "surprise     0.662     0.717     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.690     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.283377 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep93_acc_66.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep91_acc_66\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep93_acc_66\"! Old accuracy: 65.9, new accuracy: 66.2\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.919318  [    0/ 5482]\n",
      "loss: 1.207774  [  600/ 5482]\n",
      "loss: 0.938315  [ 1200/ 5482]\n",
      "loss: 0.972891  [ 1800/ 5482]\n",
      "loss: 1.036205  [ 2400/ 5482]\n",
      "loss: 1.129164  [ 3000/ 5482]\n",
      "loss: 0.925426  [ 3600/ 5482]\n",
      "loss: 0.730270  [ 4200/ 5482]\n",
      "loss: 1.061572  [ 4800/ 5482]\n",
      "loss: 1.098646  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.623     0.610     0.727    99\n",
      " disgust     0.623     0.796     0.364    107\n",
      "    fear     0.623     0.566     0.588    80\n",
      "   happy     0.623     0.745     0.532    77\n",
      " neutral     0.623     0.760     0.768    95\n",
      "     sad     0.623     0.457     0.813    91\n",
      "surprise     0.623     0.723     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.623     0.665     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.316851 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.812400  [    0/ 5482]\n",
      "loss: 0.958755  [  600/ 5482]\n",
      "loss: 1.133376  [ 1200/ 5482]\n",
      "loss: 0.856558  [ 1800/ 5482]\n",
      "loss: 1.097517  [ 2400/ 5482]\n",
      "loss: 1.056685  [ 3000/ 5482]\n",
      "loss: 0.834674  [ 3600/ 5482]\n",
      "loss: 0.929376  [ 4200/ 5482]\n",
      "loss: 0.917313  [ 4800/ 5482]\n",
      "loss: 0.898527  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.583     0.747    99\n",
      " disgust     0.646     0.939     0.430    107\n",
      "    fear     0.646     0.639     0.575    80\n",
      "   happy     0.646     0.629     0.571    77\n",
      " neutral     0.646     0.742     0.758    95\n",
      "     sad     0.646     0.555     0.780    91\n",
      "surprise     0.646     0.612     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.671     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.301014 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.835533  [    0/ 5482]\n",
      "loss: 0.978701  [  600/ 5482]\n",
      "loss: 0.853095  [ 1200/ 5482]\n",
      "loss: 0.925370  [ 1800/ 5482]\n",
      "loss: 1.006848  [ 2400/ 5482]\n",
      "loss: 1.033685  [ 3000/ 5482]\n",
      "loss: 1.100958  [ 3600/ 5482]\n",
      "loss: 0.922590  [ 4200/ 5482]\n",
      "loss: 1.144933  [ 4800/ 5482]\n",
      "loss: 0.847577  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.603     0.737    99\n",
      " disgust     0.657     0.815     0.495    107\n",
      "    fear     0.657     0.656     0.525    80\n",
      "   happy     0.657     0.737     0.545    77\n",
      " neutral     0.657     0.778     0.737    95\n",
      "     sad     0.657     0.516     0.868    91\n",
      "surprise     0.657     0.700     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.687     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.282186 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.861622  [    0/ 5482]\n",
      "loss: 0.923161  [  600/ 5482]\n",
      "loss: 1.259822  [ 1200/ 5482]\n",
      "loss: 0.886900  [ 1800/ 5482]\n",
      "loss: 0.907067  [ 2400/ 5482]\n",
      "loss: 0.922398  [ 3000/ 5482]\n",
      "loss: 0.959016  [ 3600/ 5482]\n",
      "loss: 1.038145  [ 4200/ 5482]\n",
      "loss: 0.870651  [ 4800/ 5482]\n",
      "loss: 1.077054  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.682     0.737    99\n",
      " disgust     0.643     0.857     0.336    107\n",
      "    fear     0.643     0.683     0.512    80\n",
      "   happy     0.643     0.714     0.584    77\n",
      " neutral     0.643     0.896     0.726    95\n",
      "     sad     0.643     0.465     0.879    91\n",
      "surprise     0.643     0.539     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.691     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.292177 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.958543  [    0/ 5482]\n",
      "loss: 1.117816  [  600/ 5482]\n",
      "loss: 0.990270  [ 1200/ 5482]\n",
      "loss: 1.024474  [ 1800/ 5482]\n",
      "loss: 0.946955  [ 2400/ 5482]\n",
      "loss: 0.936483  [ 3000/ 5482]\n",
      "loss: 0.889481  [ 3600/ 5482]\n",
      "loss: 0.989610  [ 4200/ 5482]\n",
      "loss: 0.844949  [ 4800/ 5482]\n",
      "loss: 0.953727  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.641     0.848    99\n",
      " disgust     0.649     0.825     0.439    107\n",
      "    fear     0.649     0.579     0.550    80\n",
      "   happy     0.649     0.776     0.494    77\n",
      " neutral     0.649     0.805     0.737    95\n",
      "     sad     0.649     0.490     0.824    91\n",
      "surprise     0.649     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.683     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.270483 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.829334  [    0/ 5482]\n",
      "loss: 0.863535  [  600/ 5482]\n",
      "loss: 0.729380  [ 1200/ 5482]\n",
      "loss: 0.947537  [ 1800/ 5482]\n",
      "loss: 0.992869  [ 2400/ 5482]\n",
      "loss: 0.971940  [ 3000/ 5482]\n",
      "loss: 0.876611  [ 3600/ 5482]\n",
      "loss: 0.750606  [ 4200/ 5482]\n",
      "loss: 0.892982  [ 4800/ 5482]\n",
      "loss: 0.997588  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.585     0.768    99\n",
      " disgust     0.652     0.864     0.477    107\n",
      "    fear     0.652     0.661     0.512    80\n",
      "   happy     0.652     0.593     0.623    77\n",
      " neutral     0.652     0.841     0.726    95\n",
      "     sad     0.652     0.561     0.758    91\n",
      "surprise     0.652     0.603     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.673     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.275345 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.971541  [    0/ 5482]\n",
      "loss: 0.994569  [  600/ 5482]\n",
      "loss: 0.742605  [ 1200/ 5482]\n",
      "loss: 0.839802  [ 1800/ 5482]\n",
      "loss: 0.854355  [ 2400/ 5482]\n",
      "loss: 1.085596  [ 3000/ 5482]\n",
      "loss: 0.891502  [ 3600/ 5482]\n",
      "loss: 0.900422  [ 4200/ 5482]\n",
      "loss: 0.986814  [ 4800/ 5482]\n",
      "loss: 0.838540  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.659     0.818    99\n",
      " disgust     0.661     0.824     0.523    107\n",
      "    fear     0.661     0.644     0.475    80\n",
      "   happy     0.661     0.682     0.584    77\n",
      " neutral     0.661     0.798     0.705    95\n",
      "     sad     0.661     0.528     0.835    91\n",
      "surprise     0.661     0.606     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.677     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.270713 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.928378  [    0/ 5482]\n",
      "loss: 0.802822  [  600/ 5482]\n",
      "loss: 0.854226  [ 1200/ 5482]\n",
      "loss: 0.753628  [ 1800/ 5482]\n",
      "loss: 0.860044  [ 2400/ 5482]\n",
      "loss: 0.838182  [ 3000/ 5482]\n",
      "loss: 0.750400  [ 3600/ 5482]\n",
      "loss: 1.020701  [ 4200/ 5482]\n",
      "loss: 0.871157  [ 4800/ 5482]\n",
      "loss: 0.904942  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.603     0.798    99\n",
      " disgust     0.662     0.850     0.477    107\n",
      "    fear     0.662     0.618     0.588    80\n",
      "   happy     0.662     0.759     0.532    77\n",
      " neutral     0.662     0.873     0.726    95\n",
      "     sad     0.662     0.528     0.835    91\n",
      "surprise     0.662     0.621     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.693     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.253782 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.752784  [    0/ 5482]\n",
      "loss: 1.005272  [  600/ 5482]\n",
      "loss: 1.032874  [ 1200/ 5482]\n",
      "loss: 0.960364  [ 1800/ 5482]\n",
      "loss: 0.970551  [ 2400/ 5482]\n",
      "loss: 0.939916  [ 3000/ 5482]\n",
      "loss: 0.877139  [ 3600/ 5482]\n",
      "loss: 0.775045  [ 4200/ 5482]\n",
      "loss: 0.854533  [ 4800/ 5482]\n",
      "loss: 0.930712  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.661     0.747    99\n",
      " disgust     0.662     0.795     0.542    107\n",
      "    fear     0.662     0.721     0.550    80\n",
      "   happy     0.662     0.625     0.584    77\n",
      " neutral     0.662     0.791     0.716    95\n",
      "     sad     0.662     0.536     0.813    91\n",
      "surprise     0.662     0.603     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.676     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.274397 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.918951  [    0/ 5482]\n",
      "loss: 0.851708  [  600/ 5482]\n",
      "loss: 0.845571  [ 1200/ 5482]\n",
      "loss: 0.755449  [ 1800/ 5482]\n",
      "loss: 0.873351  [ 2400/ 5482]\n",
      "loss: 0.729625  [ 3000/ 5482]\n",
      "loss: 0.758573  [ 3600/ 5482]\n",
      "loss: 0.933272  [ 4200/ 5482]\n",
      "loss: 0.851987  [ 4800/ 5482]\n",
      "loss: 0.731196  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.667     0.727    99\n",
      " disgust     0.652     0.831     0.458    107\n",
      "    fear     0.652     0.649     0.625    80\n",
      "   happy     0.652     0.721     0.571    77\n",
      " neutral     0.652     0.772     0.747    95\n",
      "     sad     0.652     0.493     0.813    91\n",
      "surprise     0.652     0.603     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.677     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.266624 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.758911  [    0/ 5482]\n",
      "loss: 0.784618  [  600/ 5482]\n",
      "loss: 0.800305  [ 1200/ 5482]\n",
      "loss: 0.849021  [ 1800/ 5482]\n",
      "loss: 0.710263  [ 2400/ 5482]\n",
      "loss: 0.657338  [ 3000/ 5482]\n",
      "loss: 0.678904  [ 3600/ 5482]\n",
      "loss: 1.134881  [ 4200/ 5482]\n",
      "loss: 0.691213  [ 4800/ 5482]\n",
      "loss: 0.851483  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.587     0.717    99\n",
      " disgust     0.646     0.873     0.514    107\n",
      "    fear     0.646     0.617     0.625    80\n",
      "   happy     0.646     0.500     0.597    77\n",
      " neutral     0.646     0.892     0.695    95\n",
      "     sad     0.646     0.632     0.659    91\n",
      "surprise     0.646     0.548     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.664     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.263287 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.841240  [    0/ 5482]\n",
      "loss: 0.932910  [  600/ 5482]\n",
      "loss: 0.738197  [ 1200/ 5482]\n",
      "loss: 0.786872  [ 1800/ 5482]\n",
      "loss: 0.912290  [ 2400/ 5482]\n",
      "loss: 0.897778  [ 3000/ 5482]\n",
      "loss: 0.849578  [ 3600/ 5482]\n",
      "loss: 1.046499  [ 4200/ 5482]\n",
      "loss: 0.862851  [ 4800/ 5482]\n",
      "loss: 0.778493  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.696     0.717    99\n",
      " disgust     0.679     0.767     0.523    107\n",
      "    fear     0.679     0.667     0.625    80\n",
      "   happy     0.679     0.804     0.584    77\n",
      " neutral     0.679     0.800     0.758    95\n",
      "     sad     0.679     0.531     0.857    91\n",
      "surprise     0.679     0.627     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.699     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.236812 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep105_acc_68.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep93_acc_66\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep105_acc_68\"! Old accuracy: 66.2, new accuracy: 67.9\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.785174  [    0/ 5482]\n",
      "loss: 0.663675  [  600/ 5482]\n",
      "loss: 0.875749  [ 1200/ 5482]\n",
      "loss: 0.975216  [ 1800/ 5482]\n",
      "loss: 0.848185  [ 2400/ 5482]\n",
      "loss: 0.631366  [ 3000/ 5482]\n",
      "loss: 1.264878  [ 3600/ 5482]\n",
      "loss: 1.070212  [ 4200/ 5482]\n",
      "loss: 1.040228  [ 4800/ 5482]\n",
      "loss: 0.752908  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.624     0.737    99\n",
      " disgust     0.672     0.800     0.598    107\n",
      "    fear     0.672     0.587     0.675    80\n",
      "   happy     0.672     0.657     0.597    77\n",
      " neutral     0.672     0.814     0.737    95\n",
      "     sad     0.672     0.611     0.758    91\n",
      "surprise     0.672     0.654     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.678     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.256511 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.867947  [    0/ 5482]\n",
      "loss: 0.765905  [  600/ 5482]\n",
      "loss: 1.171360  [ 1200/ 5482]\n",
      "loss: 0.917736  [ 1800/ 5482]\n",
      "loss: 0.784927  [ 2400/ 5482]\n",
      "loss: 0.872400  [ 3000/ 5482]\n",
      "loss: 0.823967  [ 3600/ 5482]\n",
      "loss: 0.815307  [ 4200/ 5482]\n",
      "loss: 0.881095  [ 4800/ 5482]\n",
      "loss: 0.987455  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.683     0.828    99\n",
      " disgust     0.705     0.863     0.589    107\n",
      "    fear     0.705     0.607     0.637    80\n",
      "   happy     0.705     0.754     0.597    77\n",
      " neutral     0.705     0.841     0.779    95\n",
      "     sad     0.705     0.611     0.725    91\n",
      "surprise     0.705     0.632     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.713     0.706    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.208064 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep107_acc_70.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep105_acc_68\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep107_acc_70\"! Old accuracy: 67.9, new accuracy: 70.5\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.766344  [    0/ 5482]\n",
      "loss: 0.702089  [  600/ 5482]\n",
      "loss: 0.943212  [ 1200/ 5482]\n",
      "loss: 0.989434  [ 1800/ 5482]\n",
      "loss: 0.852234  [ 2400/ 5482]\n",
      "loss: 0.699459  [ 3000/ 5482]\n",
      "loss: 0.839035  [ 3600/ 5482]\n",
      "loss: 0.851158  [ 4200/ 5482]\n",
      "loss: 0.828553  [ 4800/ 5482]\n",
      "loss: 1.089382  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.572     0.838    99\n",
      " disgust     0.689     0.733     0.692    107\n",
      "    fear     0.689     0.662     0.562    80\n",
      "   happy     0.689     0.759     0.532    77\n",
      " neutral     0.689     0.758     0.789    95\n",
      "     sad     0.689     0.683     0.758    91\n",
      "surprise     0.689     0.786     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.708     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.278081 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.005119  [    0/ 5482]\n",
      "loss: 0.864694  [  600/ 5482]\n",
      "loss: 0.930713  [ 1200/ 5482]\n",
      "loss: 0.886986  [ 1800/ 5482]\n",
      "loss: 0.862502  [ 2400/ 5482]\n",
      "loss: 0.853574  [ 3000/ 5482]\n",
      "loss: 1.062375  [ 3600/ 5482]\n",
      "loss: 0.974054  [ 4200/ 5482]\n",
      "loss: 0.655432  [ 4800/ 5482]\n",
      "loss: 0.805601  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.679     0.747    99\n",
      " disgust     0.669     0.863     0.589    107\n",
      "    fear     0.669     0.638     0.550    80\n",
      "   happy     0.669     0.649     0.649    77\n",
      " neutral     0.669     0.853     0.674    95\n",
      "     sad     0.669     0.555     0.725    91\n",
      "surprise     0.669     0.534     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.682     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.231836 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 1.015564  [    0/ 5482]\n",
      "loss: 0.814517  [  600/ 5482]\n",
      "loss: 0.827837  [ 1200/ 5482]\n",
      "loss: 0.764281  [ 1800/ 5482]\n",
      "loss: 0.934379  [ 2400/ 5482]\n",
      "loss: 0.746417  [ 3000/ 5482]\n",
      "loss: 0.820724  [ 3600/ 5482]\n",
      "loss: 0.962954  [ 4200/ 5482]\n",
      "loss: 0.948046  [ 4800/ 5482]\n",
      "loss: 0.859409  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.716     0.737    99\n",
      " disgust     0.692     0.829     0.636    107\n",
      "    fear     0.692     0.640     0.713    80\n",
      "   happy     0.692     0.676     0.597    77\n",
      " neutral     0.692     0.829     0.716    95\n",
      "     sad     0.692     0.546     0.780    91\n",
      "surprise     0.692     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.703     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.234741 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.941145  [    0/ 5482]\n",
      "loss: 1.272914  [  600/ 5482]\n",
      "loss: 0.747760  [ 1200/ 5482]\n",
      "loss: 0.698943  [ 1800/ 5482]\n",
      "loss: 0.711429  [ 2400/ 5482]\n",
      "loss: 0.758733  [ 3000/ 5482]\n",
      "loss: 0.831700  [ 3600/ 5482]\n",
      "loss: 0.673642  [ 4200/ 5482]\n",
      "loss: 0.817297  [ 4800/ 5482]\n",
      "loss: 0.942849  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.506     0.788    99\n",
      " disgust     0.654     0.867     0.607    107\n",
      "    fear     0.654     0.660     0.438    80\n",
      "   happy     0.654     0.520     0.662    77\n",
      " neutral     0.654     0.793     0.726    95\n",
      "     sad     0.654     0.691     0.714    91\n",
      "surprise     0.654     0.735     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.682     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.263211 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.907255  [    0/ 5482]\n",
      "loss: 0.725936  [  600/ 5482]\n",
      "loss: 0.780722  [ 1200/ 5482]\n",
      "loss: 0.750444  [ 1800/ 5482]\n",
      "loss: 0.934007  [ 2400/ 5482]\n",
      "loss: 0.711740  [ 3000/ 5482]\n",
      "loss: 0.958147  [ 3600/ 5482]\n",
      "loss: 0.847582  [ 4200/ 5482]\n",
      "loss: 0.696959  [ 4800/ 5482]\n",
      "loss: 0.766201  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.602     0.747    99\n",
      " disgust     0.705     0.864     0.654    107\n",
      "    fear     0.705     0.704     0.625    80\n",
      "   happy     0.705     0.602     0.649    77\n",
      " neutral     0.705     0.910     0.747    95\n",
      "     sad     0.705     0.679     0.791    91\n",
      "surprise     0.705     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.713     0.703    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.206786 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.828676  [    0/ 5482]\n",
      "loss: 0.706471  [  600/ 5482]\n",
      "loss: 0.894036  [ 1200/ 5482]\n",
      "loss: 0.902538  [ 1800/ 5482]\n",
      "loss: 0.827880  [ 2400/ 5482]\n",
      "loss: 0.904459  [ 3000/ 5482]\n",
      "loss: 0.857665  [ 3600/ 5482]\n",
      "loss: 0.899684  [ 4200/ 5482]\n",
      "loss: 0.743784  [ 4800/ 5482]\n",
      "loss: 0.835180  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.653     0.818    99\n",
      " disgust     0.703     0.855     0.664    107\n",
      "    fear     0.703     0.657     0.550    80\n",
      "   happy     0.703     0.653     0.636    77\n",
      " neutral     0.703     0.820     0.768    95\n",
      "     sad     0.703     0.636     0.769    91\n",
      "surprise     0.703     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.705     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.220307 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.871888  [    0/ 5482]\n",
      "loss: 0.829420  [  600/ 5482]\n",
      "loss: 0.743270  [ 1200/ 5482]\n",
      "loss: 0.912139  [ 1800/ 5482]\n",
      "loss: 0.762007  [ 2400/ 5482]\n",
      "loss: 0.904141  [ 3000/ 5482]\n",
      "loss: 0.905264  [ 3600/ 5482]\n",
      "loss: 0.791820  [ 4200/ 5482]\n",
      "loss: 0.862054  [ 4800/ 5482]\n",
      "loss: 0.840478  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.658     0.758    99\n",
      " disgust     0.700     0.809     0.673    107\n",
      "    fear     0.700     0.634     0.650    80\n",
      "   happy     0.700     0.667     0.571    77\n",
      " neutral     0.700     0.835     0.747    95\n",
      "     sad     0.700     0.640     0.802    91\n",
      "surprise     0.700     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.701     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.207744 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.798184  [    0/ 5482]\n",
      "loss: 0.774972  [  600/ 5482]\n",
      "loss: 0.709924  [ 1200/ 5482]\n",
      "loss: 0.766568  [ 1800/ 5482]\n",
      "loss: 0.975445  [ 2400/ 5482]\n",
      "loss: 0.908668  [ 3000/ 5482]\n",
      "loss: 0.778665  [ 3600/ 5482]\n",
      "loss: 0.761796  [ 4200/ 5482]\n",
      "loss: 0.698659  [ 4800/ 5482]\n",
      "loss: 0.712605  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.648     0.818    99\n",
      " disgust     0.698     0.819     0.636    107\n",
      "    fear     0.698     0.658     0.625    80\n",
      "   happy     0.698     0.635     0.610    77\n",
      " neutral     0.698     0.866     0.747    95\n",
      "     sad     0.698     0.609     0.769    91\n",
      "surprise     0.698     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.706     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.205884 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.739899  [    0/ 5482]\n",
      "loss: 0.852176  [  600/ 5482]\n",
      "loss: 0.642876  [ 1200/ 5482]\n",
      "loss: 0.616113  [ 1800/ 5482]\n",
      "loss: 0.802120  [ 2400/ 5482]\n",
      "loss: 0.871721  [ 3000/ 5482]\n",
      "loss: 0.870204  [ 3600/ 5482]\n",
      "loss: 0.910354  [ 4200/ 5482]\n",
      "loss: 0.877549  [ 4800/ 5482]\n",
      "loss: 0.656422  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.708     0.758    99\n",
      " disgust     0.690     0.764     0.636    107\n",
      "    fear     0.690     0.653     0.588    80\n",
      "   happy     0.690     0.824     0.545    77\n",
      " neutral     0.690     0.781     0.789    95\n",
      "     sad     0.690     0.519     0.879    91\n",
      "surprise     0.690     0.810     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.723     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.247622 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.728558  [    0/ 5482]\n",
      "loss: 0.672829  [  600/ 5482]\n",
      "loss: 0.800500  [ 1200/ 5482]\n",
      "loss: 0.805499  [ 1800/ 5482]\n",
      "loss: 0.733499  [ 2400/ 5482]\n",
      "loss: 0.664138  [ 3000/ 5482]\n",
      "loss: 0.785229  [ 3600/ 5482]\n",
      "loss: 0.753910  [ 4200/ 5482]\n",
      "loss: 0.709802  [ 4800/ 5482]\n",
      "loss: 0.605958  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.604     0.848    99\n",
      " disgust     0.695     0.843     0.654    107\n",
      "    fear     0.695     0.709     0.487    80\n",
      "   happy     0.695     0.625     0.584    77\n",
      " neutral     0.695     0.802     0.768    95\n",
      "     sad     0.695     0.676     0.824    91\n",
      "surprise     0.695     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.701     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.191159 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.676994  [    0/ 5482]\n",
      "loss: 0.750422  [  600/ 5482]\n",
      "loss: 0.735134  [ 1200/ 5482]\n",
      "loss: 0.759889  [ 1800/ 5482]\n",
      "loss: 0.852267  [ 2400/ 5482]\n",
      "loss: 0.603973  [ 3000/ 5482]\n",
      "loss: 0.618703  [ 3600/ 5482]\n",
      "loss: 0.764331  [ 4200/ 5482]\n",
      "loss: 0.843861  [ 4800/ 5482]\n",
      "loss: 0.735916  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.679     0.768    99\n",
      " disgust     0.698     0.814     0.654    107\n",
      "    fear     0.698     0.598     0.650    80\n",
      "   happy     0.698     0.681     0.610    77\n",
      " neutral     0.698     0.833     0.737    95\n",
      "     sad     0.698     0.610     0.791    91\n",
      "surprise     0.698     0.722     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.705     0.693    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.209859 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.673493  [    0/ 5482]\n",
      "loss: 0.756507  [  600/ 5482]\n",
      "loss: 0.838663  [ 1200/ 5482]\n",
      "loss: 0.596801  [ 1800/ 5482]\n",
      "loss: 0.662136  [ 2400/ 5482]\n",
      "loss: 0.762514  [ 3000/ 5482]\n",
      "loss: 0.802392  [ 3600/ 5482]\n",
      "loss: 0.695690  [ 4200/ 5482]\n",
      "loss: 0.761877  [ 4800/ 5482]\n",
      "loss: 0.750343  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.684     0.788    99\n",
      " disgust     0.690     0.805     0.654    107\n",
      "    fear     0.690     0.651     0.512    80\n",
      "   happy     0.690     0.627     0.610    77\n",
      " neutral     0.690     0.863     0.726    95\n",
      "     sad     0.690     0.602     0.813    91\n",
      "surprise     0.690     0.618     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.693     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.195986 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.995058  [    0/ 5482]\n",
      "loss: 0.728902  [  600/ 5482]\n",
      "loss: 0.791948  [ 1200/ 5482]\n",
      "loss: 0.723878  [ 1800/ 5482]\n",
      "loss: 0.749760  [ 2400/ 5482]\n",
      "loss: 0.764420  [ 3000/ 5482]\n",
      "loss: 0.656734  [ 3600/ 5482]\n",
      "loss: 0.947848  [ 4200/ 5482]\n",
      "loss: 0.732103  [ 4800/ 5482]\n",
      "loss: 0.781372  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.636     0.778    99\n",
      " disgust     0.695     0.835     0.664    107\n",
      "    fear     0.695     0.680     0.637    80\n",
      "   happy     0.695     0.567     0.662    77\n",
      " neutral     0.695     0.816     0.747    95\n",
      "     sad     0.695     0.667     0.747    91\n",
      "surprise     0.695     0.700     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.700     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.182353 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.808343  [    0/ 5482]\n",
      "loss: 0.903458  [  600/ 5482]\n",
      "loss: 0.761372  [ 1200/ 5482]\n",
      "loss: 0.846129  [ 1800/ 5482]\n",
      "loss: 0.848458  [ 2400/ 5482]\n",
      "loss: 0.768593  [ 3000/ 5482]\n",
      "loss: 0.949108  [ 3600/ 5482]\n",
      "loss: 0.647340  [ 4200/ 5482]\n",
      "loss: 0.994918  [ 4800/ 5482]\n",
      "loss: 0.687479  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.664     0.778    99\n",
      " disgust     0.700     0.825     0.617    107\n",
      "    fear     0.700     0.671     0.588    80\n",
      "   happy     0.700     0.731     0.636    77\n",
      " neutral     0.700     0.831     0.726    95\n",
      "     sad     0.700     0.615     0.824    91\n",
      "surprise     0.700     0.611     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.707     0.699    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.170745 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.655991  [    0/ 5482]\n",
      "loss: 0.739767  [  600/ 5482]\n",
      "loss: 0.815480  [ 1200/ 5482]\n",
      "loss: 0.741871  [ 1800/ 5482]\n",
      "loss: 0.619903  [ 2400/ 5482]\n",
      "loss: 0.729093  [ 3000/ 5482]\n",
      "loss: 0.739746  [ 3600/ 5482]\n",
      "loss: 0.807698  [ 4200/ 5482]\n",
      "loss: 0.627837  [ 4800/ 5482]\n",
      "loss: 0.784577  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.628     0.768    99\n",
      " disgust     0.705     0.817     0.710    107\n",
      "    fear     0.705     0.667     0.650    80\n",
      "   happy     0.705     0.640     0.623    77\n",
      " neutral     0.705     0.814     0.737    95\n",
      "     sad     0.705     0.708     0.747    91\n",
      "surprise     0.705     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.704     0.699    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.167646 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.630738  [    0/ 5482]\n",
      "loss: 0.734875  [  600/ 5482]\n",
      "loss: 0.776571  [ 1200/ 5482]\n",
      "loss: 0.815211  [ 1800/ 5482]\n",
      "loss: 0.705219  [ 2400/ 5482]\n",
      "loss: 0.795115  [ 3000/ 5482]\n",
      "loss: 0.771131  [ 3600/ 5482]\n",
      "loss: 0.597376  [ 4200/ 5482]\n",
      "loss: 0.757209  [ 4800/ 5482]\n",
      "loss: 0.684958  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.650     0.768    99\n",
      " disgust     0.698     0.833     0.701    107\n",
      "    fear     0.698     0.574     0.675    80\n",
      "   happy     0.698     0.623     0.623    77\n",
      " neutral     0.698     0.857     0.695    95\n",
      "     sad     0.698     0.708     0.692    91\n",
      "surprise     0.698     0.667     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.702     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.176299 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.607294  [    0/ 5482]\n",
      "loss: 0.662981  [  600/ 5482]\n",
      "loss: 0.704057  [ 1200/ 5482]\n",
      "loss: 0.830866  [ 1800/ 5482]\n",
      "loss: 0.805975  [ 2400/ 5482]\n",
      "loss: 0.819714  [ 3000/ 5482]\n",
      "loss: 0.588403  [ 3600/ 5482]\n",
      "loss: 0.696742  [ 4200/ 5482]\n",
      "loss: 0.631043  [ 4800/ 5482]\n",
      "loss: 0.696853  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.650     0.788    99\n",
      " disgust     0.705     0.785     0.682    107\n",
      "    fear     0.705     0.667     0.625    80\n",
      "   happy     0.705     0.652     0.584    77\n",
      " neutral     0.705     0.821     0.726    95\n",
      "     sad     0.705     0.698     0.813    91\n",
      "surprise     0.705     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.703     0.699    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.157787 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.595925  [    0/ 5482]\n",
      "loss: 0.647376  [  600/ 5482]\n",
      "loss: 0.901358  [ 1200/ 5482]\n",
      "loss: 0.738936  [ 1800/ 5482]\n",
      "loss: 0.706717  [ 2400/ 5482]\n",
      "loss: 0.655440  [ 3000/ 5482]\n",
      "loss: 0.919206  [ 3600/ 5482]\n",
      "loss: 0.653510  [ 4200/ 5482]\n",
      "loss: 0.659523  [ 4800/ 5482]\n",
      "loss: 0.923555  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.710     0.635     0.737    99\n",
      " disgust     0.710     0.798     0.664    107\n",
      "    fear     0.710     0.761     0.675    80\n",
      "   happy     0.710     0.657     0.597    77\n",
      " neutral     0.710     0.857     0.758    95\n",
      "     sad     0.710     0.655     0.813    91\n",
      "surprise     0.710     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.710     0.714     0.707    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.174222 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep125_acc_71.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep107_acc_70\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep125_acc_71\"! Old accuracy: 70.5, new accuracy: 71.0\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.656306  [    0/ 5482]\n",
      "loss: 0.686730  [  600/ 5482]\n",
      "loss: 0.817602  [ 1200/ 5482]\n",
      "loss: 0.732623  [ 1800/ 5482]\n",
      "loss: 0.621553  [ 2400/ 5482]\n",
      "loss: 0.824361  [ 3000/ 5482]\n",
      "loss: 0.681679  [ 3600/ 5482]\n",
      "loss: 0.629426  [ 4200/ 5482]\n",
      "loss: 0.714190  [ 4800/ 5482]\n",
      "loss: 0.905539  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.707     0.705     0.747    99\n",
      " disgust     0.707     0.855     0.664    107\n",
      "    fear     0.707     0.552     0.662    80\n",
      "   happy     0.707     0.754     0.597    77\n",
      " neutral     0.707     0.845     0.747    95\n",
      "     sad     0.707     0.643     0.791    91\n",
      "surprise     0.707     0.638     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.707     0.713     0.704    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.162796 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.646370  [    0/ 5482]\n",
      "loss: 0.678323  [  600/ 5482]\n",
      "loss: 0.750478  [ 1200/ 5482]\n",
      "loss: 0.773865  [ 1800/ 5482]\n",
      "loss: 0.995349  [ 2400/ 5482]\n",
      "loss: 0.657734  [ 3000/ 5482]\n",
      "loss: 0.642518  [ 3600/ 5482]\n",
      "loss: 0.862520  [ 4200/ 5482]\n",
      "loss: 0.780418  [ 4800/ 5482]\n",
      "loss: 0.709866  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.716     0.639     0.788    99\n",
      " disgust     0.716     0.845     0.664    107\n",
      "    fear     0.716     0.684     0.675    80\n",
      "   happy     0.716     0.685     0.649    77\n",
      " neutral     0.716     0.845     0.747    95\n",
      "     sad     0.716     0.716     0.747    91\n",
      "surprise     0.716     0.616     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.716     0.719     0.715    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.138648 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep127_acc_72.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep125_acc_71\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep127_acc_72\"! Old accuracy: 71.0, new accuracy: 71.6\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.710300  [    0/ 5482]\n",
      "loss: 0.592848  [  600/ 5482]\n",
      "loss: 0.619657  [ 1200/ 5482]\n",
      "loss: 0.716746  [ 1800/ 5482]\n",
      "loss: 0.619371  [ 2400/ 5482]\n",
      "loss: 0.626892  [ 3000/ 5482]\n",
      "loss: 0.857613  [ 3600/ 5482]\n",
      "loss: 0.801014  [ 4200/ 5482]\n",
      "loss: 0.604040  [ 4800/ 5482]\n",
      "loss: 0.654472  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.720     0.630     0.808    99\n",
      " disgust     0.720     0.762     0.720    107\n",
      "    fear     0.720     0.704     0.625    80\n",
      "   happy     0.720     0.770     0.610    77\n",
      " neutral     0.720     0.818     0.758    95\n",
      "     sad     0.720     0.685     0.813    91\n",
      "surprise     0.720     0.722     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.720     0.728     0.711    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.148240 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep128_acc_72.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep127_acc_72\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep128_acc_72\"! Old accuracy: 71.6, new accuracy: 72.0\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.849600  [    0/ 5482]\n",
      "loss: 0.684320  [  600/ 5482]\n",
      "loss: 0.741542  [ 1200/ 5482]\n",
      "loss: 0.759671  [ 1800/ 5482]\n",
      "loss: 0.784289  [ 2400/ 5482]\n",
      "loss: 0.581655  [ 3000/ 5482]\n",
      "loss: 0.748312  [ 3600/ 5482]\n",
      "loss: 0.623946  [ 4200/ 5482]\n",
      "loss: 0.570690  [ 4800/ 5482]\n",
      "loss: 0.736291  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.708     0.667     0.727    99\n",
      " disgust     0.708     0.899     0.664    107\n",
      "    fear     0.708     0.604     0.688    80\n",
      "   happy     0.708     0.703     0.584    77\n",
      " neutral     0.708     0.837     0.758    95\n",
      "     sad     0.708     0.628     0.780    91\n",
      "surprise     0.708     0.667     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.708     0.715     0.708    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.143905 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.672682  [    0/ 5482]\n",
      "loss: 0.772157  [  600/ 5482]\n",
      "loss: 0.820233  [ 1200/ 5482]\n",
      "loss: 0.940048  [ 1800/ 5482]\n",
      "loss: 0.751664  [ 2400/ 5482]\n",
      "loss: 0.598743  [ 3000/ 5482]\n",
      "loss: 0.582869  [ 3600/ 5482]\n",
      "loss: 0.567897  [ 4200/ 5482]\n",
      "loss: 0.714174  [ 4800/ 5482]\n",
      "loss: 0.622793  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.683     0.717    99\n",
      " disgust     0.700     0.758     0.701    107\n",
      "    fear     0.700     0.657     0.575    80\n",
      "   happy     0.700     0.746     0.571    77\n",
      " neutral     0.700     0.909     0.737    95\n",
      "     sad     0.700     0.633     0.835    91\n",
      "surprise     0.700     0.556     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.706     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.136244 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.668365  [    0/ 5482]\n",
      "loss: 0.748992  [  600/ 5482]\n",
      "loss: 0.712864  [ 1200/ 5482]\n",
      "loss: 0.680583  [ 1800/ 5482]\n",
      "loss: 0.686256  [ 2400/ 5482]\n",
      "loss: 0.692344  [ 3000/ 5482]\n",
      "loss: 0.853501  [ 3600/ 5482]\n",
      "loss: 0.514228  [ 4200/ 5482]\n",
      "loss: 0.633960  [ 4800/ 5482]\n",
      "loss: 0.757616  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.664     0.758    99\n",
      " disgust     0.702     0.866     0.664    107\n",
      "    fear     0.702     0.617     0.625    80\n",
      "   happy     0.702     0.723     0.610    77\n",
      " neutral     0.702     0.826     0.747    95\n",
      "     sad     0.702     0.625     0.824    91\n",
      "surprise     0.702     0.619     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.706     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.148692 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.630596  [    0/ 5482]\n",
      "loss: 0.671875  [  600/ 5482]\n",
      "loss: 0.767489  [ 1200/ 5482]\n",
      "loss: 0.712234  [ 1800/ 5482]\n",
      "loss: 0.547707  [ 2400/ 5482]\n",
      "loss: 0.644710  [ 3000/ 5482]\n",
      "loss: 0.611993  [ 3600/ 5482]\n",
      "loss: 0.524105  [ 4200/ 5482]\n",
      "loss: 0.637092  [ 4800/ 5482]\n",
      "loss: 0.577973  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.723     0.743     0.818    99\n",
      " disgust     0.723     0.784     0.710    107\n",
      "    fear     0.723     0.726     0.562    80\n",
      "   happy     0.723     0.754     0.636    77\n",
      " neutral     0.723     0.854     0.737    95\n",
      "     sad     0.723     0.597     0.846    91\n",
      "surprise     0.723     0.652     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.723     0.730     0.716    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.140590 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep132_acc_72.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep128_acc_72\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep132_acc_72\"! Old accuracy: 72.0, new accuracy: 72.3\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.598878  [    0/ 5482]\n",
      "loss: 0.535311  [  600/ 5482]\n",
      "loss: 0.565197  [ 1200/ 5482]\n",
      "loss: 0.583922  [ 1800/ 5482]\n",
      "loss: 0.805629  [ 2400/ 5482]\n",
      "loss: 0.727049  [ 3000/ 5482]\n",
      "loss: 0.611096  [ 3600/ 5482]\n",
      "loss: 0.617492  [ 4200/ 5482]\n",
      "loss: 0.799281  [ 4800/ 5482]\n",
      "loss: 0.581928  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.716     0.616     0.778    99\n",
      " disgust     0.716     0.849     0.682    107\n",
      "    fear     0.716     0.712     0.650    80\n",
      "   happy     0.716     0.691     0.610    77\n",
      " neutral     0.716     0.843     0.789    95\n",
      "     sad     0.716     0.680     0.769    91\n",
      "surprise     0.716     0.652     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.716     0.720     0.712    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.130954 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.588249  [    0/ 5482]\n",
      "loss: 0.717531  [  600/ 5482]\n",
      "loss: 0.777872  [ 1200/ 5482]\n",
      "loss: 0.738714  [ 1800/ 5482]\n",
      "loss: 0.609888  [ 2400/ 5482]\n",
      "loss: 0.801729  [ 3000/ 5482]\n",
      "loss: 0.795577  [ 3600/ 5482]\n",
      "loss: 0.658895  [ 4200/ 5482]\n",
      "loss: 0.894690  [ 4800/ 5482]\n",
      "loss: 0.909771  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.710     0.750     0.727    99\n",
      " disgust     0.710     0.875     0.654    107\n",
      "    fear     0.710     0.612     0.750    80\n",
      "   happy     0.710     0.746     0.571    77\n",
      " neutral     0.710     0.869     0.768    95\n",
      "     sad     0.710     0.573     0.780    91\n",
      "surprise     0.710     0.623     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.710     0.721     0.708    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.138835 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.503284  [    0/ 5482]\n",
      "loss: 0.751884  [  600/ 5482]\n",
      "loss: 0.672071  [ 1200/ 5482]\n",
      "loss: 0.643408  [ 1800/ 5482]\n",
      "loss: 0.522959  [ 2400/ 5482]\n",
      "loss: 0.624598  [ 3000/ 5482]\n",
      "loss: 0.769480  [ 3600/ 5482]\n",
      "loss: 0.509527  [ 4200/ 5482]\n",
      "loss: 0.680620  [ 4800/ 5482]\n",
      "loss: 0.607666  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.645     0.788    99\n",
      " disgust     0.697     0.852     0.645    107\n",
      "    fear     0.697     0.591     0.650    80\n",
      "   happy     0.697     0.529     0.701    77\n",
      " neutral     0.697     0.866     0.747    95\n",
      "     sad     0.697     0.831     0.703    91\n",
      "surprise     0.697     0.627     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.706     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.159788 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.798859  [    0/ 5482]\n",
      "loss: 0.758623  [  600/ 5482]\n",
      "loss: 0.582763  [ 1200/ 5482]\n",
      "loss: 0.527039  [ 1800/ 5482]\n",
      "loss: 0.698067  [ 2400/ 5482]\n",
      "loss: 0.714754  [ 3000/ 5482]\n",
      "loss: 0.608831  [ 3600/ 5482]\n",
      "loss: 0.628680  [ 4200/ 5482]\n",
      "loss: 0.717403  [ 4800/ 5482]\n",
      "loss: 0.616414  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.715     0.710     0.717    99\n",
      " disgust     0.715     0.832     0.738    107\n",
      "    fear     0.715     0.635     0.675    80\n",
      "   happy     0.715     0.627     0.610    77\n",
      " neutral     0.715     0.919     0.716    95\n",
      "     sad     0.715     0.673     0.769    91\n",
      "surprise     0.715     0.610     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.715     0.715     0.714    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.134519 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.675889  [    0/ 5482]\n",
      "loss: 0.588116  [  600/ 5482]\n",
      "loss: 0.723429  [ 1200/ 5482]\n",
      "loss: 0.557571  [ 1800/ 5482]\n",
      "loss: 0.580844  [ 2400/ 5482]\n",
      "loss: 0.757552  [ 3000/ 5482]\n",
      "loss: 0.836590  [ 3600/ 5482]\n",
      "loss: 0.605938  [ 4200/ 5482]\n",
      "loss: 0.667619  [ 4800/ 5482]\n",
      "loss: 0.734031  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.742     0.727    99\n",
      " disgust     0.721     0.800     0.710    107\n",
      "    fear     0.721     0.696     0.688    80\n",
      "   happy     0.721     0.763     0.584    77\n",
      " neutral     0.721     0.893     0.705    95\n",
      "     sad     0.721     0.634     0.857    91\n",
      "surprise     0.721     0.573     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.729     0.720    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.137683 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.606896  [    0/ 5482]\n",
      "loss: 0.589147  [  600/ 5482]\n",
      "loss: 0.715455  [ 1200/ 5482]\n",
      "loss: 0.646548  [ 1800/ 5482]\n",
      "loss: 0.638805  [ 2400/ 5482]\n",
      "loss: 0.750449  [ 3000/ 5482]\n",
      "loss: 0.610833  [ 3600/ 5482]\n",
      "loss: 0.563975  [ 4200/ 5482]\n",
      "loss: 0.743843  [ 4800/ 5482]\n",
      "loss: 0.608337  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.720     0.656     0.808    99\n",
      " disgust     0.720     0.839     0.729    107\n",
      "    fear     0.720     0.575     0.625    80\n",
      "   happy     0.720     0.730     0.597    77\n",
      " neutral     0.720     0.864     0.800    95\n",
      "     sad     0.720     0.693     0.769    91\n",
      "surprise     0.720     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.720     0.722     0.710    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.117609 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.566265  [    0/ 5482]\n",
      "loss: 0.573474  [  600/ 5482]\n",
      "loss: 0.675955  [ 1200/ 5482]\n",
      "loss: 0.627307  [ 1800/ 5482]\n",
      "loss: 0.668339  [ 2400/ 5482]\n",
      "loss: 0.684204  [ 3000/ 5482]\n",
      "loss: 0.556686  [ 3600/ 5482]\n",
      "loss: 0.662426  [ 4200/ 5482]\n",
      "loss: 0.635935  [ 4800/ 5482]\n",
      "loss: 0.672067  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.707     0.646     0.828    99\n",
      " disgust     0.707     0.784     0.748    107\n",
      "    fear     0.707     0.617     0.625    80\n",
      "   happy     0.707     0.702     0.519    77\n",
      " neutral     0.707     0.775     0.726    95\n",
      "     sad     0.707     0.686     0.769    91\n",
      "surprise     0.707     0.769     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.707     0.711     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.141867 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.592692  [    0/ 5482]\n",
      "loss: 0.646561  [  600/ 5482]\n",
      "loss: 0.696788  [ 1200/ 5482]\n",
      "loss: 0.632766  [ 1800/ 5482]\n",
      "loss: 0.707920  [ 2400/ 5482]\n",
      "loss: 0.689025  [ 3000/ 5482]\n",
      "loss: 0.753347  [ 3600/ 5482]\n",
      "loss: 0.799666  [ 4200/ 5482]\n",
      "loss: 0.539176  [ 4800/ 5482]\n",
      "loss: 0.532308  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.715     0.718     0.798    99\n",
      " disgust     0.715     0.765     0.701    107\n",
      "    fear     0.715     0.646     0.637    80\n",
      "   happy     0.715     0.733     0.571    77\n",
      " neutral     0.715     0.936     0.768    95\n",
      "     sad     0.715     0.623     0.835    91\n",
      "surprise     0.715     0.603     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.715     0.718     0.705    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.110194 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.668709  [    0/ 5482]\n",
      "loss: 0.643896  [  600/ 5482]\n",
      "loss: 0.614968  [ 1200/ 5482]\n",
      "loss: 0.714351  [ 1800/ 5482]\n",
      "loss: 0.670538  [ 2400/ 5482]\n",
      "loss: 0.684435  [ 3000/ 5482]\n",
      "loss: 0.568761  [ 3600/ 5482]\n",
      "loss: 0.645653  [ 4200/ 5482]\n",
      "loss: 0.549965  [ 4800/ 5482]\n",
      "loss: 0.684790  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.726     0.697     0.768    99\n",
      " disgust     0.726     0.712     0.785    107\n",
      "    fear     0.726     0.712     0.650    80\n",
      "   happy     0.726     0.804     0.584    77\n",
      " neutral     0.726     0.843     0.737    95\n",
      "     sad     0.726     0.685     0.835    91\n",
      "surprise     0.726     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.726     0.731     0.716    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.100841 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep141_acc_73.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep132_acc_72\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep141_acc_73\"! Old accuracy: 72.3, new accuracy: 72.6\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.550707  [    0/ 5482]\n",
      "loss: 0.620930  [  600/ 5482]\n",
      "loss: 0.697173  [ 1200/ 5482]\n",
      "loss: 0.555825  [ 1800/ 5482]\n",
      "loss: 0.634339  [ 2400/ 5482]\n",
      "loss: 0.726973  [ 3000/ 5482]\n",
      "loss: 0.619737  [ 3600/ 5482]\n",
      "loss: 0.548808  [ 4200/ 5482]\n",
      "loss: 0.495014  [ 4800/ 5482]\n",
      "loss: 0.666222  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.723     0.675     0.778    99\n",
      " disgust     0.723     0.794     0.757    107\n",
      "    fear     0.723     0.675     0.650    80\n",
      "   happy     0.723     0.641     0.649    77\n",
      " neutral     0.723     0.800     0.758    95\n",
      "     sad     0.723     0.729     0.769    91\n",
      "surprise     0.723     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.723     0.722     0.714    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.109200 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.646639  [    0/ 5482]\n",
      "loss: 0.601211  [  600/ 5482]\n",
      "loss: 0.674111  [ 1200/ 5482]\n",
      "loss: 0.556690  [ 1800/ 5482]\n",
      "loss: 0.480547  [ 2400/ 5482]\n",
      "loss: 0.656452  [ 3000/ 5482]\n",
      "loss: 0.613319  [ 3600/ 5482]\n",
      "loss: 0.651893  [ 4200/ 5482]\n",
      "loss: 0.632197  [ 4800/ 5482]\n",
      "loss: 0.595605  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.681     0.778    99\n",
      " disgust     0.730     0.848     0.729    107\n",
      "    fear     0.730     0.596     0.700    80\n",
      "   happy     0.730     0.729     0.662    77\n",
      " neutral     0.730     0.880     0.768    95\n",
      "     sad     0.730     0.717     0.780    91\n",
      "surprise     0.730     0.661     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.730     0.722    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.099060 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep143_acc_73.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep141_acc_73\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep143_acc_73\"! Old accuracy: 72.6, new accuracy: 73.0\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.618866  [    0/ 5482]\n",
      "loss: 0.615047  [  600/ 5482]\n",
      "loss: 1.026319  [ 1200/ 5482]\n",
      "loss: 0.543863  [ 1800/ 5482]\n",
      "loss: 0.614685  [ 2400/ 5482]\n",
      "loss: 0.664614  [ 3000/ 5482]\n",
      "loss: 0.613921  [ 3600/ 5482]\n",
      "loss: 0.526256  [ 4200/ 5482]\n",
      "loss: 0.601905  [ 4800/ 5482]\n",
      "loss: 0.623498  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.726     0.733     0.778    99\n",
      " disgust     0.726     0.812     0.766    107\n",
      "    fear     0.726     0.679     0.688    80\n",
      "   happy     0.726     0.800     0.571    77\n",
      " neutral     0.726     0.826     0.747    95\n",
      "     sad     0.726     0.618     0.835    91\n",
      "surprise     0.726     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.726     0.730     0.716    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.117981 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.651411  [    0/ 5482]\n",
      "loss: 0.721579  [  600/ 5482]\n",
      "loss: 0.614465  [ 1200/ 5482]\n",
      "loss: 0.645741  [ 1800/ 5482]\n",
      "loss: 0.698737  [ 2400/ 5482]\n",
      "loss: 0.626737  [ 3000/ 5482]\n",
      "loss: 0.658191  [ 3600/ 5482]\n",
      "loss: 0.584860  [ 4200/ 5482]\n",
      "loss: 0.649604  [ 4800/ 5482]\n",
      "loss: 0.514218  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.655     0.727    99\n",
      " disgust     0.702     0.830     0.729    107\n",
      "    fear     0.702     0.578     0.650    80\n",
      "   happy     0.702     0.742     0.636    77\n",
      " neutral     0.702     0.855     0.747    95\n",
      "     sad     0.702     0.656     0.692    91\n",
      "surprise     0.702     0.606     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.703     0.698    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.131832 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.628192  [    0/ 5482]\n",
      "loss: 0.511444  [  600/ 5482]\n",
      "loss: 0.555527  [ 1200/ 5482]\n",
      "loss: 0.604872  [ 1800/ 5482]\n",
      "loss: 0.509095  [ 2400/ 5482]\n",
      "loss: 0.581422  [ 3000/ 5482]\n",
      "loss: 0.490835  [ 3600/ 5482]\n",
      "loss: 0.603666  [ 4200/ 5482]\n",
      "loss: 0.685995  [ 4800/ 5482]\n",
      "loss: 0.543556  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.733     0.667     0.788    99\n",
      " disgust     0.733     0.818     0.757    107\n",
      "    fear     0.733     0.655     0.713    80\n",
      "   happy     0.733     0.677     0.571    77\n",
      " neutral     0.733     0.884     0.800    95\n",
      "     sad     0.733     0.711     0.758    91\n",
      "surprise     0.733     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.733     0.732     0.725    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.082014 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep146_acc_73.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep143_acc_73\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep146_acc_73\"! Old accuracy: 73.0, new accuracy: 73.3\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.677046  [    0/ 5482]\n",
      "loss: 0.600650  [  600/ 5482]\n",
      "loss: 0.580591  [ 1200/ 5482]\n",
      "loss: 0.635177  [ 1800/ 5482]\n",
      "loss: 0.718480  [ 2400/ 5482]\n",
      "loss: 0.472613  [ 3000/ 5482]\n",
      "loss: 0.506840  [ 3600/ 5482]\n",
      "loss: 0.497824  [ 4200/ 5482]\n",
      "loss: 0.756512  [ 4800/ 5482]\n",
      "loss: 0.611519  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.710     0.641     0.758    99\n",
      " disgust     0.710     0.767     0.738    107\n",
      "    fear     0.710     0.621     0.675    80\n",
      "   happy     0.710     0.697     0.597    77\n",
      " neutral     0.710     0.818     0.758    95\n",
      "     sad     0.710     0.763     0.780    91\n",
      "surprise     0.710     0.643     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.710     0.707     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.103013 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.639034  [    0/ 5482]\n",
      "loss: 0.486560  [  600/ 5482]\n",
      "loss: 0.526694  [ 1200/ 5482]\n",
      "loss: 0.669667  [ 1800/ 5482]\n",
      "loss: 0.492587  [ 2400/ 5482]\n",
      "loss: 0.576548  [ 3000/ 5482]\n",
      "loss: 0.543533  [ 3600/ 5482]\n",
      "loss: 0.628246  [ 4200/ 5482]\n",
      "loss: 0.574628  [ 4800/ 5482]\n",
      "loss: 0.613308  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.707     0.707    99\n",
      " disgust     0.703     0.770     0.720    107\n",
      "    fear     0.703     0.658     0.600    80\n",
      "   happy     0.703     0.759     0.571    77\n",
      " neutral     0.703     0.816     0.747    95\n",
      "     sad     0.703     0.651     0.780    91\n",
      "surprise     0.703     0.571     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.705     0.702    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.115651 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.579931  [    0/ 5482]\n",
      "loss: 0.624711  [  600/ 5482]\n",
      "loss: 0.560312  [ 1200/ 5482]\n",
      "loss: 0.506829  [ 1800/ 5482]\n",
      "loss: 0.529690  [ 2400/ 5482]\n",
      "loss: 0.519426  [ 3000/ 5482]\n",
      "loss: 0.592621  [ 3600/ 5482]\n",
      "loss: 0.540792  [ 4200/ 5482]\n",
      "loss: 0.514017  [ 4800/ 5482]\n",
      "loss: 0.589157  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.725     0.664     0.798    99\n",
      " disgust     0.725     0.771     0.757    107\n",
      "    fear     0.725     0.678     0.738    80\n",
      "   happy     0.725     0.708     0.597    77\n",
      " neutral     0.725     0.809     0.758    95\n",
      "     sad     0.725     0.701     0.747    91\n",
      "surprise     0.725     0.771     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.725     0.729     0.715    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.114813 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.551186  [    0/ 5482]\n",
      "loss: 0.614892  [  600/ 5482]\n",
      "loss: 0.503379  [ 1200/ 5482]\n",
      "loss: 0.706481  [ 1800/ 5482]\n",
      "loss: 0.615818  [ 2400/ 5482]\n",
      "loss: 0.716688  [ 3000/ 5482]\n",
      "loss: 0.720497  [ 3600/ 5482]\n",
      "loss: 0.682231  [ 4200/ 5482]\n",
      "loss: 0.512980  [ 4800/ 5482]\n",
      "loss: 0.684544  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.716     0.687    99\n",
      " disgust     0.721     0.784     0.748    107\n",
      "    fear     0.721     0.667     0.700    80\n",
      "   happy     0.721     0.793     0.597    77\n",
      " neutral     0.721     0.828     0.758    95\n",
      "     sad     0.721     0.679     0.813    91\n",
      "surprise     0.721     0.587     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.722     0.718    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.091972 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.618666  [    0/ 5482]\n",
      "loss: 0.490722  [  600/ 5482]\n",
      "loss: 0.533506  [ 1200/ 5482]\n",
      "loss: 0.448030  [ 1800/ 5482]\n",
      "loss: 0.696914  [ 2400/ 5482]\n",
      "loss: 0.585548  [ 3000/ 5482]\n",
      "loss: 0.566743  [ 3600/ 5482]\n",
      "loss: 0.518706  [ 4200/ 5482]\n",
      "loss: 0.589545  [ 4800/ 5482]\n",
      "loss: 0.438014  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.643     0.818    99\n",
      " disgust     0.721     0.772     0.729    107\n",
      "    fear     0.721     0.663     0.713    80\n",
      "   happy     0.721     0.703     0.584    77\n",
      " neutral     0.721     0.782     0.832    95\n",
      "     sad     0.721     0.744     0.703    91\n",
      "surprise     0.721     0.783     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.727     0.710    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.105452 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.636943  [    0/ 5482]\n",
      "loss: 0.503234  [  600/ 5482]\n",
      "loss: 0.583906  [ 1200/ 5482]\n",
      "loss: 0.756767  [ 1800/ 5482]\n",
      "loss: 0.591018  [ 2400/ 5482]\n",
      "loss: 0.650015  [ 3000/ 5482]\n",
      "loss: 0.558210  [ 3600/ 5482]\n",
      "loss: 0.584998  [ 4200/ 5482]\n",
      "loss: 0.501076  [ 4800/ 5482]\n",
      "loss: 0.536368  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.710     0.594     0.828    99\n",
      " disgust     0.710     0.741     0.776    107\n",
      "    fear     0.710     0.697     0.575    80\n",
      "   happy     0.710     0.687     0.597    77\n",
      " neutral     0.710     0.820     0.768    95\n",
      "     sad     0.710     0.779     0.736    91\n",
      "surprise     0.710     0.692     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.710     0.716     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1.123672 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.611458  [    0/ 5482]\n",
      "loss: 0.619809  [  600/ 5482]\n",
      "loss: 0.617153  [ 1200/ 5482]\n",
      "loss: 0.479485  [ 1800/ 5482]\n",
      "loss: 0.730899  [ 2400/ 5482]\n",
      "loss: 0.559339  [ 3000/ 5482]\n",
      "loss: 0.458188  [ 3600/ 5482]\n",
      "loss: 0.471083  [ 4200/ 5482]\n",
      "loss: 0.622778  [ 4800/ 5482]\n",
      "loss: 0.547894  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.718     0.682     0.758    99\n",
      " disgust     0.718     0.741     0.776    107\n",
      "    fear     0.718     0.651     0.675    80\n",
      "   happy     0.718     0.717     0.558    77\n",
      " neutral     0.718     0.878     0.758    95\n",
      "     sad     0.718     0.731     0.747    91\n",
      "surprise     0.718     0.614     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.718     0.716     0.711    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.096774 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.478245  [    0/ 5482]\n",
      "loss: 0.612016  [  600/ 5482]\n",
      "loss: 0.632909  [ 1200/ 5482]\n",
      "loss: 0.543568  [ 1800/ 5482]\n",
      "loss: 0.545853  [ 2400/ 5482]\n",
      "loss: 0.571754  [ 3000/ 5482]\n",
      "loss: 0.514308  [ 3600/ 5482]\n",
      "loss: 0.518482  [ 4200/ 5482]\n",
      "loss: 0.610876  [ 4800/ 5482]\n",
      "loss: 0.653336  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.723     0.688     0.778    99\n",
      " disgust     0.723     0.777     0.748    107\n",
      "    fear     0.723     0.769     0.625    80\n",
      "   happy     0.723     0.558     0.623    77\n",
      " neutral     0.723     0.885     0.726    95\n",
      "     sad     0.723     0.766     0.791    91\n",
      "surprise     0.723     0.625     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.723     0.724     0.718    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.094478 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.461822  [    0/ 5482]\n",
      "loss: 0.563614  [  600/ 5482]\n",
      "loss: 0.567478  [ 1200/ 5482]\n",
      "loss: 0.603935  [ 1800/ 5482]\n",
      "loss: 0.397865  [ 2400/ 5482]\n",
      "loss: 0.493754  [ 3000/ 5482]\n",
      "loss: 0.614760  [ 3600/ 5482]\n",
      "loss: 0.665907  [ 4200/ 5482]\n",
      "loss: 0.536296  [ 4800/ 5482]\n",
      "loss: 0.740095  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.725     0.709     0.737    99\n",
      " disgust     0.725     0.812     0.766    107\n",
      "    fear     0.725     0.675     0.700    80\n",
      "   happy     0.725     0.678     0.519    77\n",
      " neutral     0.725     0.880     0.768    95\n",
      "     sad     0.725     0.683     0.780    91\n",
      "surprise     0.725     0.610     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.725     0.721     0.720    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.093924 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.553887  [    0/ 5482]\n",
      "loss: 0.499829  [  600/ 5482]\n",
      "loss: 0.488725  [ 1200/ 5482]\n",
      "loss: 0.616309  [ 1800/ 5482]\n",
      "loss: 0.564569  [ 2400/ 5482]\n",
      "loss: 0.560905  [ 3000/ 5482]\n",
      "loss: 0.618665  [ 3600/ 5482]\n",
      "loss: 0.486169  [ 4200/ 5482]\n",
      "loss: 0.498694  [ 4800/ 5482]\n",
      "loss: 0.632138  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.736     0.702     0.737    99\n",
      " disgust     0.736     0.792     0.785    107\n",
      "    fear     0.736     0.667     0.700    80\n",
      "   happy     0.736     0.807     0.597    77\n",
      " neutral     0.736     0.878     0.758    95\n",
      "     sad     0.736     0.641     0.824    91\n",
      "surprise     0.736     0.717     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.736     0.743     0.730    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.072915 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep156_acc_74.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep146_acc_73\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep156_acc_74\"! Old accuracy: 73.3, new accuracy: 73.6\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.479170  [    0/ 5482]\n",
      "loss: 0.547803  [  600/ 5482]\n",
      "loss: 0.478388  [ 1200/ 5482]\n",
      "loss: 0.456358  [ 1800/ 5482]\n",
      "loss: 0.558122  [ 2400/ 5482]\n",
      "loss: 0.584878  [ 3000/ 5482]\n",
      "loss: 0.481359  [ 3600/ 5482]\n",
      "loss: 0.465201  [ 4200/ 5482]\n",
      "loss: 0.548111  [ 4800/ 5482]\n",
      "loss: 0.554902  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.709     0.737    99\n",
      " disgust     0.721     0.832     0.738    107\n",
      "    fear     0.721     0.654     0.637    80\n",
      "   happy     0.721     0.738     0.623    77\n",
      " neutral     0.721     0.819     0.716    95\n",
      "     sad     0.721     0.705     0.813    91\n",
      "surprise     0.721     0.580     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.720     0.719    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.099128 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.566925  [    0/ 5482]\n",
      "loss: 0.573305  [  600/ 5482]\n",
      "loss: 0.452919  [ 1200/ 5482]\n",
      "loss: 0.525424  [ 1800/ 5482]\n",
      "loss: 0.576927  [ 2400/ 5482]\n",
      "loss: 0.534301  [ 3000/ 5482]\n",
      "loss: 0.679339  [ 3600/ 5482]\n",
      "loss: 0.503721  [ 4200/ 5482]\n",
      "loss: 0.523914  [ 4800/ 5482]\n",
      "loss: 0.483517  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.708     0.667     0.788    99\n",
      " disgust     0.708     0.816     0.748    107\n",
      "    fear     0.708     0.608     0.600    80\n",
      "   happy     0.708     0.689     0.545    77\n",
      " neutral     0.708     0.830     0.768    95\n",
      "     sad     0.708     0.696     0.780    91\n",
      "surprise     0.708     0.615     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.708     0.703     0.698    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 1.091021 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.495103  [    0/ 5482]\n",
      "loss: 0.456734  [  600/ 5482]\n",
      "loss: 0.539506  [ 1200/ 5482]\n",
      "loss: 0.474256  [ 1800/ 5482]\n",
      "loss: 0.515359  [ 2400/ 5482]\n",
      "loss: 0.479142  [ 3000/ 5482]\n",
      "loss: 0.493786  [ 3600/ 5482]\n",
      "loss: 0.591078  [ 4200/ 5482]\n",
      "loss: 0.541346  [ 4800/ 5482]\n",
      "loss: 0.573954  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.738     0.758     0.758    99\n",
      " disgust     0.738     0.830     0.776    107\n",
      "    fear     0.738     0.636     0.787    80\n",
      "   happy     0.738     0.789     0.584    77\n",
      " neutral     0.738     0.773     0.789    95\n",
      "     sad     0.738     0.677     0.736    91\n",
      "surprise     0.738     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.738     0.739     0.731    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.070940 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep159_acc_74.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep156_acc_74\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep159_acc_74\"! Old accuracy: 73.6, new accuracy: 73.8\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.465372  [    0/ 5482]\n",
      "loss: 0.446445  [  600/ 5482]\n",
      "loss: 0.660580  [ 1200/ 5482]\n",
      "loss: 0.694026  [ 1800/ 5482]\n",
      "loss: 0.401780  [ 2400/ 5482]\n",
      "loss: 0.545541  [ 3000/ 5482]\n",
      "loss: 0.518220  [ 3600/ 5482]\n",
      "loss: 0.772881  [ 4200/ 5482]\n",
      "loss: 0.413400  [ 4800/ 5482]\n",
      "loss: 0.497447  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.721     0.808    99\n",
      " disgust     0.730     0.782     0.804    107\n",
      "    fear     0.730     0.554     0.637    80\n",
      "   happy     0.730     0.710     0.636    77\n",
      " neutral     0.730     0.833     0.789    95\n",
      "     sad     0.730     0.807     0.736    91\n",
      "surprise     0.730     0.673     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.726     0.717    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.096379 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.531116  [    0/ 5482]\n",
      "loss: 0.493560  [  600/ 5482]\n",
      "loss: 0.497117  [ 1200/ 5482]\n",
      "loss: 0.479324  [ 1800/ 5482]\n",
      "loss: 0.560997  [ 2400/ 5482]\n",
      "loss: 0.479823  [ 3000/ 5482]\n",
      "loss: 0.497392  [ 3600/ 5482]\n",
      "loss: 0.521182  [ 4200/ 5482]\n",
      "loss: 0.493649  [ 4800/ 5482]\n",
      "loss: 0.519343  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.734     0.690     0.808    99\n",
      " disgust     0.734     0.847     0.776    107\n",
      "    fear     0.734     0.625     0.625    80\n",
      "   happy     0.734     0.708     0.662    77\n",
      " neutral     0.734     0.880     0.768    95\n",
      "     sad     0.734     0.719     0.758    91\n",
      "surprise     0.734     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.734     0.731     0.727    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.072121 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.473246  [    0/ 5482]\n",
      "loss: 0.505742  [  600/ 5482]\n",
      "loss: 0.484541  [ 1200/ 5482]\n",
      "loss: 0.540123  [ 1800/ 5482]\n",
      "loss: 0.401292  [ 2400/ 5482]\n",
      "loss: 0.482608  [ 3000/ 5482]\n",
      "loss: 0.458959  [ 3600/ 5482]\n",
      "loss: 0.612112  [ 4200/ 5482]\n",
      "loss: 0.562900  [ 4800/ 5482]\n",
      "loss: 0.571632  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.728     0.740     0.747    99\n",
      " disgust     0.728     0.781     0.766    107\n",
      "    fear     0.728     0.659     0.725    80\n",
      "   happy     0.728     0.793     0.597    77\n",
      " neutral     0.728     0.809     0.800    95\n",
      "     sad     0.728     0.653     0.725    91\n",
      "surprise     0.728     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.728     0.727     0.721    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.076838 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.629218  [    0/ 5482]\n",
      "loss: 0.546204  [  600/ 5482]\n",
      "loss: 0.465820  [ 1200/ 5482]\n",
      "loss: 0.565561  [ 1800/ 5482]\n",
      "loss: 0.516318  [ 2400/ 5482]\n",
      "loss: 0.613028  [ 3000/ 5482]\n",
      "loss: 0.438060  [ 3600/ 5482]\n",
      "loss: 0.505626  [ 4200/ 5482]\n",
      "loss: 0.569007  [ 4800/ 5482]\n",
      "loss: 0.426153  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.698     0.747    99\n",
      " disgust     0.721     0.739     0.794    107\n",
      "    fear     0.721     0.708     0.575    80\n",
      "   happy     0.721     0.767     0.597    77\n",
      " neutral     0.721     0.826     0.747    95\n",
      "     sad     0.721     0.716     0.802    91\n",
      "surprise     0.721     0.592     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.721     0.715    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.068758 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.742249  [    0/ 5482]\n",
      "loss: 0.463455  [  600/ 5482]\n",
      "loss: 0.599658  [ 1200/ 5482]\n",
      "loss: 0.421025  [ 1800/ 5482]\n",
      "loss: 0.446508  [ 2400/ 5482]\n",
      "loss: 0.484547  [ 3000/ 5482]\n",
      "loss: 0.544576  [ 3600/ 5482]\n",
      "loss: 0.543022  [ 4200/ 5482]\n",
      "loss: 0.720417  [ 4800/ 5482]\n",
      "loss: 0.537194  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.766     0.727    99\n",
      " disgust     0.721     0.804     0.766    107\n",
      "    fear     0.721     0.654     0.662    80\n",
      "   happy     0.721     0.734     0.610    77\n",
      " neutral     0.721     0.768     0.800    95\n",
      "     sad     0.721     0.694     0.747    91\n",
      "surprise     0.721     0.583     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.715     0.715    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.055922 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.550998  [    0/ 5482]\n",
      "loss: 0.531900  [  600/ 5482]\n",
      "loss: 0.417828  [ 1200/ 5482]\n",
      "loss: 0.561547  [ 1800/ 5482]\n",
      "loss: 0.451349  [ 2400/ 5482]\n",
      "loss: 0.540103  [ 3000/ 5482]\n",
      "loss: 0.472897  [ 3600/ 5482]\n",
      "loss: 0.462050  [ 4200/ 5482]\n",
      "loss: 0.472575  [ 4800/ 5482]\n",
      "loss: 0.548062  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.743     0.709     0.838    99\n",
      " disgust     0.743     0.800     0.785    107\n",
      "    fear     0.743     0.648     0.713    80\n",
      "   happy     0.743     0.797     0.610    77\n",
      " neutral     0.743     0.818     0.758    95\n",
      "     sad     0.743     0.747     0.780    91\n",
      "surprise     0.743     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.743     0.742     0.732    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.035765 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep165_acc_74.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep159_acc_74\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv/emo_reco_best_ep165_acc_74\"! Old accuracy: 73.8, new accuracy: 74.3\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.375288  [    0/ 5482]\n",
      "loss: 0.428066  [  600/ 5482]\n",
      "loss: 0.558551  [ 1200/ 5482]\n",
      "loss: 0.489260  [ 1800/ 5482]\n",
      "loss: 0.477058  [ 2400/ 5482]\n",
      "loss: 0.665248  [ 3000/ 5482]\n",
      "loss: 0.557218  [ 3600/ 5482]\n",
      "loss: 0.455688  [ 4200/ 5482]\n",
      "loss: 0.410625  [ 4800/ 5482]\n",
      "loss: 0.469497  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.726     0.675     0.798    99\n",
      " disgust     0.726     0.750     0.813    107\n",
      "    fear     0.726     0.625     0.562    80\n",
      "   happy     0.726     0.733     0.571    77\n",
      " neutral     0.726     0.894     0.800    95\n",
      "     sad     0.726     0.732     0.780    91\n",
      "surprise     0.726     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.726     0.723     0.714    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.068757 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.437427  [    0/ 5482]\n",
      "loss: 0.504559  [  600/ 5482]\n",
      "loss: 0.575832  [ 1200/ 5482]\n",
      "loss: 0.532206  [ 1800/ 5482]\n",
      "loss: 0.615785  [ 2400/ 5482]\n",
      "loss: 0.479722  [ 3000/ 5482]\n",
      "loss: 0.504077  [ 3600/ 5482]\n",
      "loss: 0.640753  [ 4200/ 5482]\n",
      "loss: 0.532808  [ 4800/ 5482]\n",
      "loss: 0.497817  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.715     0.731     0.768    99\n",
      " disgust     0.715     0.802     0.757    107\n",
      "    fear     0.715     0.691     0.588    80\n",
      "   happy     0.715     0.667     0.597    77\n",
      " neutral     0.715     0.816     0.747    95\n",
      "     sad     0.715     0.654     0.769    91\n",
      "surprise     0.715     0.608     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.715     0.710     0.709    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.063940 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.405830  [    0/ 5482]\n",
      "loss: 0.480269  [  600/ 5482]\n",
      "loss: 0.395309  [ 1200/ 5482]\n",
      "loss: 0.441663  [ 1800/ 5482]\n",
      "loss: 0.580251  [ 2400/ 5482]\n",
      "loss: 0.426638  [ 3000/ 5482]\n",
      "loss: 0.431902  [ 3600/ 5482]\n",
      "loss: 0.408054  [ 4200/ 5482]\n",
      "loss: 0.548421  [ 4800/ 5482]\n",
      "loss: 0.457328  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.700     0.778    99\n",
      " disgust     0.730     0.798     0.776    107\n",
      "    fear     0.730     0.727     0.600    80\n",
      "   happy     0.730     0.729     0.662    77\n",
      " neutral     0.730     0.837     0.758    95\n",
      "     sad     0.730     0.664     0.780    91\n",
      "surprise     0.730     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.728     0.723    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.064750 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.485062  [    0/ 5482]\n",
      "loss: 0.384645  [  600/ 5482]\n",
      "loss: 0.444935  [ 1200/ 5482]\n",
      "loss: 0.430437  [ 1800/ 5482]\n",
      "loss: 0.548941  [ 2400/ 5482]\n",
      "loss: 0.513451  [ 3000/ 5482]\n",
      "loss: 0.433471  [ 3600/ 5482]\n",
      "loss: 0.616391  [ 4200/ 5482]\n",
      "loss: 0.481035  [ 4800/ 5482]\n",
      "loss: 0.469905  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.725     0.729     0.788    99\n",
      " disgust     0.725     0.850     0.794    107\n",
      "    fear     0.725     0.649     0.600    80\n",
      "   happy     0.725     0.833     0.649    77\n",
      " neutral     0.725     0.798     0.747    95\n",
      "     sad     0.725     0.615     0.736    91\n",
      "surprise     0.725     0.606     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.725     0.726     0.717    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.074476 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.511553  [    0/ 5482]\n",
      "loss: 0.452036  [  600/ 5482]\n",
      "loss: 0.483708  [ 1200/ 5482]\n",
      "loss: 0.409792  [ 1800/ 5482]\n",
      "loss: 0.492974  [ 2400/ 5482]\n",
      "loss: 0.687732  [ 3000/ 5482]\n",
      "loss: 0.413533  [ 3600/ 5482]\n",
      "loss: 0.447064  [ 4200/ 5482]\n",
      "loss: 0.515547  [ 4800/ 5482]\n",
      "loss: 0.491636  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.734     0.757     0.788    99\n",
      " disgust     0.734     0.860     0.748    107\n",
      "    fear     0.734     0.611     0.725    80\n",
      "   happy     0.734     0.696     0.623    77\n",
      " neutral     0.734     0.882     0.789    95\n",
      "     sad     0.734     0.725     0.725    91\n",
      "surprise     0.734     0.581     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.734     0.730     0.729    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.037540 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.493000  [    0/ 5482]\n",
      "loss: 0.447507  [  600/ 5482]\n",
      "loss: 0.446569  [ 1200/ 5482]\n",
      "loss: 0.531649  [ 1800/ 5482]\n",
      "loss: 0.585181  [ 2400/ 5482]\n",
      "loss: 0.472491  [ 3000/ 5482]\n",
      "loss: 0.444040  [ 3600/ 5482]\n",
      "loss: 0.521057  [ 4200/ 5482]\n",
      "loss: 0.376814  [ 4800/ 5482]\n",
      "loss: 0.396616  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.739     0.701     0.758    99\n",
      " disgust     0.739     0.824     0.832    107\n",
      "    fear     0.739     0.667     0.650    80\n",
      "   happy     0.739     0.763     0.584    77\n",
      " neutral     0.739     0.843     0.737    95\n",
      "     sad     0.739     0.745     0.802    91\n",
      "surprise     0.739     0.610     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.739     0.736     0.733    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 1.032003 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.437493  [    0/ 5482]\n",
      "loss: 0.505955  [  600/ 5482]\n",
      "loss: 0.453022  [ 1200/ 5482]\n",
      "loss: 0.394147  [ 1800/ 5482]\n",
      "loss: 0.478657  [ 2400/ 5482]\n",
      "loss: 0.444014  [ 3000/ 5482]\n",
      "loss: 0.469614  [ 3600/ 5482]\n",
      "loss: 0.483784  [ 4200/ 5482]\n",
      "loss: 0.486656  [ 4800/ 5482]\n",
      "loss: 0.497372  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.738     0.725     0.798    99\n",
      " disgust     0.738     0.832     0.785    107\n",
      "    fear     0.738     0.571     0.700    80\n",
      "   happy     0.738     0.692     0.584    77\n",
      " neutral     0.738     0.814     0.832    95\n",
      "     sad     0.738     0.782     0.747    91\n",
      "surprise     0.738     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.738     0.736     0.727    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.012301 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.451691  [    0/ 5482]\n",
      "loss: 0.458211  [  600/ 5482]\n",
      "loss: 0.474375  [ 1200/ 5482]\n",
      "loss: 0.443038  [ 1800/ 5482]\n",
      "loss: 0.382887  [ 2400/ 5482]\n",
      "loss: 0.432278  [ 3000/ 5482]\n",
      "loss: 0.491565  [ 3600/ 5482]\n",
      "loss: 0.479543  [ 4200/ 5482]\n",
      "loss: 0.435297  [ 4800/ 5482]\n",
      "loss: 0.455557  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.713     0.683     0.717    99\n",
      " disgust     0.713     0.784     0.748    107\n",
      "    fear     0.713     0.739     0.637    80\n",
      "   happy     0.713     0.627     0.610    77\n",
      " neutral     0.713     0.867     0.758    95\n",
      "     sad     0.713     0.742     0.791    91\n",
      "surprise     0.713     0.525     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.713     0.710     0.707    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.059680 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.408271  [    0/ 5482]\n",
      "loss: 0.529285  [  600/ 5482]\n",
      "loss: 0.448036  [ 1200/ 5482]\n",
      "loss: 0.386466  [ 1800/ 5482]\n",
      "loss: 0.432988  [ 2400/ 5482]\n",
      "loss: 0.481057  [ 3000/ 5482]\n",
      "loss: 0.480863  [ 3600/ 5482]\n",
      "loss: 0.496548  [ 4200/ 5482]\n",
      "loss: 0.481318  [ 4800/ 5482]\n",
      "loss: 0.438128  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.708     0.758    99\n",
      " disgust     0.721     0.786     0.757    107\n",
      "    fear     0.721     0.559     0.650    80\n",
      "   happy     0.721     0.723     0.610    77\n",
      " neutral     0.721     0.846     0.811    95\n",
      "     sad     0.721     0.716     0.747    91\n",
      "surprise     0.721     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.720     0.713    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.070194 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.459876  [    0/ 5482]\n",
      "loss: 0.376228  [  600/ 5482]\n",
      "loss: 0.503967  [ 1200/ 5482]\n",
      "loss: 0.547208  [ 1800/ 5482]\n",
      "loss: 0.416887  [ 2400/ 5482]\n",
      "loss: 0.384020  [ 3000/ 5482]\n",
      "loss: 0.422197  [ 3600/ 5482]\n",
      "loss: 0.441130  [ 4200/ 5482]\n",
      "loss: 0.417498  [ 4800/ 5482]\n",
      "loss: 0.526276  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.736     0.711     0.818    99\n",
      " disgust     0.736     0.830     0.729    107\n",
      "    fear     0.736     0.646     0.637    80\n",
      "   happy     0.736     0.696     0.623    77\n",
      " neutral     0.736     0.841     0.779    95\n",
      "     sad     0.736     0.710     0.725    91\n",
      "surprise     0.736     0.699     0.836    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.736     0.733     0.735    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.042802 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.397948  [    0/ 5482]\n",
      "loss: 0.451867  [  600/ 5482]\n",
      "loss: 0.558265  [ 1200/ 5482]\n",
      "loss: 0.570443  [ 1800/ 5482]\n",
      "loss: 0.425180  [ 2400/ 5482]\n",
      "loss: 0.496916  [ 3000/ 5482]\n",
      "loss: 0.491575  [ 3600/ 5482]\n",
      "loss: 0.439122  [ 4200/ 5482]\n",
      "loss: 0.584358  [ 4800/ 5482]\n",
      "loss: 0.461213  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.731     0.737     0.737    99\n",
      " disgust     0.731     0.786     0.757    107\n",
      "    fear     0.731     0.718     0.700    80\n",
      "   happy     0.731     0.780     0.597    77\n",
      " neutral     0.731     0.843     0.789    95\n",
      "     sad     0.731     0.687     0.747    91\n",
      "surprise     0.731     0.566     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.731     0.731     0.728    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.002491 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.394457  [    0/ 5482]\n",
      "loss: 0.435796  [  600/ 5482]\n",
      "loss: 0.477074  [ 1200/ 5482]\n",
      "loss: 0.469607  [ 1800/ 5482]\n",
      "loss: 0.417953  [ 2400/ 5482]\n",
      "loss: 0.487340  [ 3000/ 5482]\n",
      "loss: 0.578482  [ 3600/ 5482]\n",
      "loss: 0.474063  [ 4200/ 5482]\n",
      "loss: 0.432168  [ 4800/ 5482]\n",
      "loss: 0.561139  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.733     0.712     0.747    99\n",
      " disgust     0.733     0.800     0.785    107\n",
      "    fear     0.733     0.675     0.675    80\n",
      "   happy     0.733     0.780     0.597    77\n",
      " neutral     0.733     0.855     0.747    95\n",
      "     sad     0.733     0.679     0.813    91\n",
      "surprise     0.733     0.629     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.733     0.733     0.727    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.034966 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.494574  [    0/ 5482]\n",
      "loss: 0.389657  [  600/ 5482]\n",
      "loss: 0.413360  [ 1200/ 5482]\n",
      "loss: 0.544220  [ 1800/ 5482]\n",
      "loss: 0.616119  [ 2400/ 5482]\n",
      "loss: 0.511121  [ 3000/ 5482]\n",
      "loss: 0.421632  [ 3600/ 5482]\n",
      "loss: 0.555132  [ 4200/ 5482]\n",
      "loss: 0.505349  [ 4800/ 5482]\n",
      "loss: 0.429249  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.718     0.743     0.758    99\n",
      " disgust     0.718     0.792     0.748    107\n",
      "    fear     0.718     0.591     0.650    80\n",
      "   happy     0.718     0.800     0.623    77\n",
      " neutral     0.718     0.866     0.747    95\n",
      "     sad     0.718     0.705     0.736    91\n",
      "surprise     0.718     0.542     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.718     0.720     0.714    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.022320 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.487277  [    0/ 5482]\n",
      "loss: 0.537377  [  600/ 5482]\n",
      "loss: 0.403896  [ 1200/ 5482]\n",
      "loss: 0.371178  [ 1800/ 5482]\n",
      "loss: 0.427705  [ 2400/ 5482]\n",
      "loss: 0.466059  [ 3000/ 5482]\n",
      "loss: 0.439553  [ 3600/ 5482]\n",
      "loss: 0.647954  [ 4200/ 5482]\n",
      "loss: 0.395335  [ 4800/ 5482]\n",
      "loss: 0.475752  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.725     0.686     0.838    99\n",
      " disgust     0.725     0.729     0.804    107\n",
      "    fear     0.725     0.672     0.537    80\n",
      "   happy     0.725     0.723     0.610    77\n",
      " neutral     0.725     0.817     0.800    95\n",
      "     sad     0.725     0.722     0.769    91\n",
      "surprise     0.725     0.712     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.725     0.723     0.709    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.058729 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.487495  [    0/ 5482]\n",
      "loss: 0.490366  [  600/ 5482]\n",
      "loss: 0.432922  [ 1200/ 5482]\n",
      "loss: 0.452904  [ 1800/ 5482]\n",
      "loss: 0.379556  [ 2400/ 5482]\n",
      "loss: 0.420078  [ 3000/ 5482]\n",
      "loss: 0.519154  [ 3600/ 5482]\n",
      "loss: 0.429824  [ 4200/ 5482]\n",
      "loss: 0.432792  [ 4800/ 5482]\n",
      "loss: 0.482537  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.716     0.688     0.758    99\n",
      " disgust     0.716     0.759     0.766    107\n",
      "    fear     0.716     0.614     0.675    80\n",
      "   happy     0.716     0.783     0.610    77\n",
      " neutral     0.716     0.831     0.726    95\n",
      "     sad     0.716     0.711     0.758    91\n",
      "surprise     0.716     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.716     0.717     0.709    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.032117 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.513126  [    0/ 5482]\n",
      "loss: 0.381740  [  600/ 5482]\n",
      "loss: 0.551197  [ 1200/ 5482]\n",
      "loss: 0.446084  [ 1800/ 5482]\n",
      "loss: 0.362522  [ 2400/ 5482]\n",
      "loss: 0.401852  [ 3000/ 5482]\n",
      "loss: 0.427741  [ 3600/ 5482]\n",
      "loss: 0.349927  [ 4200/ 5482]\n",
      "loss: 0.481825  [ 4800/ 5482]\n",
      "loss: 0.407574  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.725     0.649     0.747    99\n",
      " disgust     0.725     0.800     0.785    107\n",
      "    fear     0.725     0.671     0.613    80\n",
      "   happy     0.725     0.780     0.597    77\n",
      " neutral     0.725     0.811     0.811    95\n",
      "     sad     0.725     0.719     0.758    91\n",
      "surprise     0.725     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.725     0.723     0.717    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 1.028365 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.794176  [    0/ 5482]\n",
      "loss: 0.348722  [  600/ 5482]\n",
      "loss: 0.399319  [ 1200/ 5482]\n",
      "loss: 0.366912  [ 1800/ 5482]\n",
      "loss: 0.470101  [ 2400/ 5482]\n",
      "loss: 0.420620  [ 3000/ 5482]\n",
      "loss: 0.471497  [ 3600/ 5482]\n",
      "loss: 0.472269  [ 4200/ 5482]\n",
      "loss: 0.448712  [ 4800/ 5482]\n",
      "loss: 0.454541  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.721     0.699     0.798    99\n",
      " disgust     0.721     0.766     0.794    107\n",
      "    fear     0.721     0.644     0.588    80\n",
      "   happy     0.721     0.712     0.610    77\n",
      " neutral     0.721     0.841     0.779    95\n",
      "     sad     0.721     0.711     0.758    91\n",
      "surprise     0.721     0.629     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.721     0.715     0.710    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.079929 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.586022  [    0/ 5482]\n",
      "loss: 0.366625  [  600/ 5482]\n",
      "loss: 0.529232  [ 1200/ 5482]\n",
      "loss: 0.396156  [ 1800/ 5482]\n",
      "loss: 0.463890  [ 2400/ 5482]\n",
      "loss: 0.542792  [ 3000/ 5482]\n",
      "loss: 0.443014  [ 3600/ 5482]\n",
      "loss: 0.383256  [ 4200/ 5482]\n",
      "loss: 0.520612  [ 4800/ 5482]\n",
      "loss: 0.443511  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.723     0.747     0.717    99\n",
      " disgust     0.723     0.828     0.766    107\n",
      "    fear     0.723     0.644     0.588    80\n",
      "   happy     0.723     0.628     0.636    77\n",
      " neutral     0.723     0.845     0.747    95\n",
      "     sad     0.723     0.707     0.769    91\n",
      "surprise     0.723     0.622     0.836    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.723     0.717     0.723    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.026916 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.448186  [    0/ 5482]\n",
      "loss: 0.516862  [  600/ 5482]\n",
      "loss: 0.403830  [ 1200/ 5482]\n",
      "loss: 0.370971  [ 1800/ 5482]\n",
      "loss: 0.479898  [ 2400/ 5482]\n",
      "loss: 0.471537  [ 3000/ 5482]\n",
      "loss: 0.436019  [ 3600/ 5482]\n",
      "loss: 0.487421  [ 4200/ 5482]\n",
      "loss: 0.448169  [ 4800/ 5482]\n",
      "loss: 0.446705  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.728     0.716     0.788    99\n",
      " disgust     0.728     0.813     0.813    107\n",
      "    fear     0.728     0.621     0.675    80\n",
      "   happy     0.728     0.742     0.636    77\n",
      " neutral     0.728     0.828     0.758    95\n",
      "     sad     0.728     0.742     0.725    91\n",
      "surprise     0.728     0.585     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.728     0.721     0.717    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.032937 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.445393  [    0/ 5482]\n",
      "loss: 0.432483  [  600/ 5482]\n",
      "loss: 0.373132  [ 1200/ 5482]\n",
      "loss: 0.331498  [ 1800/ 5482]\n",
      "loss: 0.422414  [ 2400/ 5482]\n",
      "loss: 0.458593  [ 3000/ 5482]\n",
      "loss: 0.339248  [ 3600/ 5482]\n",
      "loss: 0.387647  [ 4200/ 5482]\n",
      "loss: 0.539638  [ 4800/ 5482]\n",
      "loss: 0.391424  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.713     0.716     0.737    99\n",
      " disgust     0.713     0.713     0.813    107\n",
      "    fear     0.713     0.703     0.562    80\n",
      "   happy     0.713     0.846     0.571    77\n",
      " neutral     0.713     0.765     0.789    95\n",
      "     sad     0.713     0.649     0.791    91\n",
      "surprise     0.713     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.713     0.719     0.701    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.073792 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.531253  [    0/ 5482]\n",
      "loss: 0.449799  [  600/ 5482]\n",
      "loss: 0.404488  [ 1200/ 5482]\n",
      "loss: 0.357562  [ 1800/ 5482]\n",
      "loss: 0.525138  [ 2400/ 5482]\n",
      "loss: 0.427546  [ 3000/ 5482]\n",
      "loss: 0.382080  [ 3600/ 5482]\n",
      "loss: 0.419920  [ 4200/ 5482]\n",
      "loss: 0.347115  [ 4800/ 5482]\n",
      "loss: 0.389434  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.670     0.717    99\n",
      " disgust     0.730     0.804     0.804    107\n",
      "    fear     0.730     0.615     0.700    80\n",
      "   happy     0.730     0.807     0.597    77\n",
      " neutral     0.730     0.800     0.800    95\n",
      "     sad     0.730     0.821     0.758    91\n",
      "surprise     0.730     0.586     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.729     0.721    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.016915 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.420197  [    0/ 5482]\n",
      "loss: 0.499216  [  600/ 5482]\n",
      "loss: 0.390695  [ 1200/ 5482]\n",
      "loss: 0.489086  [ 1800/ 5482]\n",
      "loss: 0.438589  [ 2400/ 5482]\n",
      "loss: 0.445729  [ 3000/ 5482]\n",
      "loss: 0.429872  [ 3600/ 5482]\n",
      "loss: 0.502241  [ 4200/ 5482]\n",
      "loss: 0.416932  [ 4800/ 5482]\n",
      "loss: 0.496486  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.741     0.740     0.778    99\n",
      " disgust     0.741     0.769     0.776    107\n",
      "    fear     0.741     0.663     0.738    80\n",
      "   happy     0.741     0.780     0.597    77\n",
      " neutral     0.741     0.841     0.779    95\n",
      "     sad     0.741     0.724     0.780    91\n",
      "surprise     0.741     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.741     0.739     0.734    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.994947 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.408109  [    0/ 5482]\n",
      "loss: 0.373430  [  600/ 5482]\n",
      "loss: 0.431238  [ 1200/ 5482]\n",
      "loss: 0.428592  [ 1800/ 5482]\n",
      "loss: 0.342130  [ 2400/ 5482]\n",
      "loss: 0.402720  [ 3000/ 5482]\n",
      "loss: 0.383800  [ 3600/ 5482]\n",
      "loss: 0.341129  [ 4200/ 5482]\n",
      "loss: 0.397817  [ 4800/ 5482]\n",
      "loss: 0.486972  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.720     0.642     0.798    99\n",
      " disgust     0.720     0.729     0.804    107\n",
      "    fear     0.720     0.649     0.600    80\n",
      "   happy     0.720     0.734     0.610    77\n",
      " neutral     0.720     0.772     0.821    95\n",
      "     sad     0.720     0.792     0.670    91\n",
      "surprise     0.720     0.755     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.720     0.725     0.708    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 1.060397 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.435558  [    0/ 5482]\n",
      "loss: 0.410461  [  600/ 5482]\n",
      "loss: 0.374204  [ 1200/ 5482]\n",
      "loss: 0.592647  [ 1800/ 5482]\n",
      "loss: 0.357899  [ 2400/ 5482]\n",
      "loss: 0.433110  [ 3000/ 5482]\n",
      "loss: 0.456094  [ 3600/ 5482]\n",
      "loss: 0.510136  [ 4200/ 5482]\n",
      "loss: 0.380200  [ 4800/ 5482]\n",
      "loss: 0.403321  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.713     0.778    99\n",
      " disgust     0.730     0.810     0.794    107\n",
      "    fear     0.730     0.662     0.613    80\n",
      "   happy     0.730     0.733     0.571    77\n",
      " neutral     0.730     0.811     0.768    95\n",
      "     sad     0.730     0.699     0.791    91\n",
      "surprise     0.730     0.643     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.724     0.722    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.011227 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.366122  [    0/ 5482]\n",
      "loss: 0.367736  [  600/ 5482]\n",
      "loss: 0.428178  [ 1200/ 5482]\n",
      "loss: 0.337959  [ 1800/ 5482]\n",
      "loss: 0.426911  [ 2400/ 5482]\n",
      "loss: 0.375898  [ 3000/ 5482]\n",
      "loss: 0.377984  [ 3600/ 5482]\n",
      "loss: 0.415207  [ 4200/ 5482]\n",
      "loss: 0.366729  [ 4800/ 5482]\n",
      "loss: 0.336226  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.713     0.783     0.727    99\n",
      " disgust     0.713     0.739     0.794    107\n",
      "    fear     0.713     0.610     0.625    80\n",
      "   happy     0.713     0.692     0.584    77\n",
      " neutral     0.713     0.835     0.747    95\n",
      "     sad     0.713     0.686     0.769    91\n",
      "surprise     0.713     0.609     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.713     0.708     0.705    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.057915 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.402353  [    0/ 5482]\n",
      "loss: 0.491224  [  600/ 5482]\n",
      "loss: 0.480198  [ 1200/ 5482]\n",
      "loss: 0.433205  [ 1800/ 5482]\n",
      "loss: 0.371797  [ 2400/ 5482]\n",
      "loss: 0.366470  [ 3000/ 5482]\n",
      "loss: 0.420585  [ 3600/ 5482]\n",
      "loss: 0.468481  [ 4200/ 5482]\n",
      "loss: 0.358842  [ 4800/ 5482]\n",
      "loss: 0.349744  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.728     0.712     0.747    99\n",
      " disgust     0.728     0.757     0.813    107\n",
      "    fear     0.728     0.646     0.637    80\n",
      "   happy     0.728     0.719     0.597    77\n",
      " neutral     0.728     0.828     0.811    95\n",
      "     sad     0.728     0.744     0.736    91\n",
      "surprise     0.728     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.728     0.722     0.719    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.012034 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.395860  [    0/ 5482]\n",
      "loss: 0.328161  [  600/ 5482]\n",
      "loss: 0.367471  [ 1200/ 5482]\n",
      "loss: 0.406624  [ 1800/ 5482]\n",
      "loss: 0.448351  [ 2400/ 5482]\n",
      "loss: 0.397240  [ 3000/ 5482]\n",
      "loss: 0.470995  [ 3600/ 5482]\n",
      "loss: 0.409197  [ 4200/ 5482]\n",
      "loss: 0.372282  [ 4800/ 5482]\n",
      "loss: 0.396712  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.703     0.788    99\n",
      " disgust     0.730     0.814     0.776    107\n",
      "    fear     0.730     0.649     0.625    80\n",
      "   happy     0.730     0.742     0.636    77\n",
      " neutral     0.730     0.843     0.789    95\n",
      "     sad     0.730     0.724     0.692    91\n",
      "surprise     0.730     0.603     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.725     0.725    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 1.007574 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.478260  [    0/ 5482]\n",
      "loss: 0.402801  [  600/ 5482]\n",
      "loss: 0.391069  [ 1200/ 5482]\n",
      "loss: 0.379151  [ 1800/ 5482]\n",
      "loss: 0.368431  [ 2400/ 5482]\n",
      "loss: 0.425511  [ 3000/ 5482]\n",
      "loss: 0.335705  [ 3600/ 5482]\n",
      "loss: 0.450853  [ 4200/ 5482]\n",
      "loss: 0.417098  [ 4800/ 5482]\n",
      "loss: 0.438322  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.723     0.750     0.697    99\n",
      " disgust     0.723     0.796     0.766    107\n",
      "    fear     0.723     0.691     0.700    80\n",
      "   happy     0.723     0.776     0.584    77\n",
      " neutral     0.723     0.860     0.779    95\n",
      "     sad     0.723     0.660     0.769    91\n",
      "surprise     0.723     0.536     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.723     0.724     0.719    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 1.012595 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.389392  [    0/ 5482]\n",
      "loss: 0.355363  [  600/ 5482]\n",
      "loss: 0.408119  [ 1200/ 5482]\n",
      "loss: 0.405919  [ 1800/ 5482]\n",
      "loss: 0.383579  [ 2400/ 5482]\n",
      "loss: 0.374642  [ 3000/ 5482]\n",
      "loss: 0.371369  [ 3600/ 5482]\n",
      "loss: 0.452341  [ 4200/ 5482]\n",
      "loss: 0.358580  [ 4800/ 5482]\n",
      "loss: 0.319119  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.731     0.706     0.778    99\n",
      " disgust     0.731     0.798     0.776    107\n",
      "    fear     0.731     0.643     0.675    80\n",
      "   happy     0.731     0.734     0.610    77\n",
      " neutral     0.731     0.851     0.779    95\n",
      "     sad     0.731     0.677     0.736    91\n",
      "surprise     0.731     0.698     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.731     0.730     0.725    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.007063 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.369295  [    0/ 5482]\n",
      "loss: 0.445134  [  600/ 5482]\n",
      "loss: 0.389461  [ 1200/ 5482]\n",
      "loss: 0.328829  [ 1800/ 5482]\n",
      "loss: 0.344918  [ 2400/ 5482]\n",
      "loss: 0.463478  [ 3000/ 5482]\n",
      "loss: 0.463708  [ 3600/ 5482]\n",
      "loss: 0.342501  [ 4200/ 5482]\n",
      "loss: 0.360946  [ 4800/ 5482]\n",
      "loss: 0.334664  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.741     0.760     0.768    99\n",
      " disgust     0.741     0.874     0.776    107\n",
      "    fear     0.741     0.578     0.738    80\n",
      "   happy     0.741     0.690     0.636    77\n",
      " neutral     0.741     0.851     0.779    95\n",
      "     sad     0.741     0.819     0.747    91\n",
      "surprise     0.741     0.597     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.741     0.738     0.735    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.001857 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.429373  [    0/ 5482]\n",
      "loss: 0.369918  [  600/ 5482]\n",
      "loss: 0.431857  [ 1200/ 5482]\n",
      "loss: 0.430669  [ 1800/ 5482]\n",
      "loss: 0.363707  [ 2400/ 5482]\n",
      "loss: 0.417827  [ 3000/ 5482]\n",
      "loss: 0.355451  [ 3600/ 5482]\n",
      "loss: 0.366116  [ 4200/ 5482]\n",
      "loss: 0.388843  [ 4800/ 5482]\n",
      "loss: 0.404598  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.718     0.730     0.737    99\n",
      " disgust     0.718     0.796     0.804    107\n",
      "    fear     0.718     0.623     0.600    80\n",
      "   happy     0.718     0.658     0.649    77\n",
      " neutral     0.718     0.805     0.737    95\n",
      "     sad     0.718     0.713     0.736    91\n",
      "surprise     0.718     0.647     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.718     0.710     0.712    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 1.045055 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.388705  [    0/ 5482]\n",
      "loss: 0.317107  [  600/ 5482]\n",
      "loss: 0.395048  [ 1200/ 5482]\n",
      "loss: 0.404574  [ 1800/ 5482]\n",
      "loss: 0.416843  [ 2400/ 5482]\n",
      "loss: 0.423019  [ 3000/ 5482]\n",
      "loss: 0.448366  [ 3600/ 5482]\n",
      "loss: 0.367531  [ 4200/ 5482]\n",
      "loss: 0.469676  [ 4800/ 5482]\n",
      "loss: 0.309681  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.741     0.752     0.768    99\n",
      " disgust     0.741     0.761     0.776    107\n",
      "    fear     0.741     0.690     0.750    80\n",
      "   happy     0.741     0.723     0.610    77\n",
      " neutral     0.741     0.818     0.758    95\n",
      "     sad     0.741     0.767     0.758    91\n",
      "surprise     0.741     0.643     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.741     0.736     0.737    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.987360 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.414290  [    0/ 5482]\n",
      "loss: 0.398449  [  600/ 5482]\n",
      "loss: 0.389246  [ 1200/ 5482]\n",
      "loss: 0.431282  [ 1800/ 5482]\n",
      "loss: 0.448193  [ 2400/ 5482]\n",
      "loss: 0.337660  [ 3000/ 5482]\n",
      "loss: 0.361351  [ 3600/ 5482]\n",
      "loss: 0.374912  [ 4200/ 5482]\n",
      "loss: 0.338345  [ 4800/ 5482]\n",
      "loss: 0.420165  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.716     0.699     0.727    99\n",
      " disgust     0.716     0.757     0.757    107\n",
      "    fear     0.716     0.650     0.650    80\n",
      "   happy     0.716     0.730     0.597    77\n",
      " neutral     0.716     0.816     0.747    95\n",
      "     sad     0.716     0.747     0.780    91\n",
      "surprise     0.716     0.587     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.716     0.712     0.712    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1.000188 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.427263  [    0/ 5482]\n",
      "loss: 0.417961  [  600/ 5482]\n",
      "loss: 0.362811  [ 1200/ 5482]\n",
      "loss: 0.384690  [ 1800/ 5482]\n",
      "loss: 0.382751  [ 2400/ 5482]\n",
      "loss: 0.316156  [ 3000/ 5482]\n",
      "loss: 0.327674  [ 3600/ 5482]\n",
      "loss: 0.353529  [ 4200/ 5482]\n",
      "loss: 0.380718  [ 4800/ 5482]\n",
      "loss: 0.338264  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.730     0.753     0.737    99\n",
      " disgust     0.730     0.811     0.804    107\n",
      "    fear     0.730     0.616     0.662    80\n",
      "   happy     0.730     0.725     0.649    77\n",
      " neutral     0.730     0.752     0.800    95\n",
      "     sad     0.730     0.798     0.736    91\n",
      "surprise     0.730     0.597     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.730     0.722     0.721    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.991493 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/newSoundstream/Run_Nr_0/conv.md \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.970980  [    0/ 5482]\n",
      "loss: 1.967479  [ 1200/ 5482]\n",
      "loss: 1.975447  [ 2400/ 5482]\n",
      "loss: 1.988427  [ 3600/ 5482]\n",
      "loss: 1.945810  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.159     0.296     0.081    99\n",
      " disgust     0.159     0.000     0.000    107\n",
      "    fear     0.159     0.000     0.000    80\n",
      "   happy     0.159     0.000     0.000    77\n",
      " neutral     0.159     0.000     0.000    95\n",
      "     sad     0.159     0.153     0.978    91\n",
      "surprise     0.159     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.159     0.064     0.151    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 1.939328 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.920393  [    0/ 5482]\n",
      "loss: 1.913957  [ 1200/ 5482]\n",
      "loss: 1.964776  [ 2400/ 5482]\n",
      "loss: 1.955820  [ 3600/ 5482]\n",
      "loss: 1.928524  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.177     0.367     0.111    99\n",
      " disgust     0.177     0.000     0.000    107\n",
      "    fear     0.177     0.235     0.150    80\n",
      "   happy     0.177     0.000     0.000    77\n",
      " neutral     0.177     0.000     0.000    95\n",
      "     sad     0.177     0.161     0.934    91\n",
      "surprise     0.177     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.177     0.109     0.171    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 17.7%, Avg loss: 1.932051 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.956156  [    0/ 5482]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mexp_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_em\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:40\u001B[0m, in \u001B[0;36mExperimentsTrainer.train_em\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     37\u001B[0m SSGenModelTrainer\u001B[38;5;241m.\u001B[39mgenAndSaveEvaluation(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrail\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/conv\u001B[39m\u001B[38;5;124m\"\u001B[39m, higest_true_c, higest_pred_c, highest_acc_c, higest_epoch_c, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvolutional\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mencoded_dataset\u001B[38;5;241m.\u001B[39mlabel_list)\n\u001B[1;32m     39\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m---> 40\u001B[0m highest_acc_dr, higest_epoch_dr, higest_true_dr, higest_pred_dr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_dim_red_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs_per_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrail\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# generate Report\u001B[39;00m\n\u001B[1;32m     42\u001B[0m SSGenModelTrainer\u001B[38;5;241m.\u001B[39mgenAndSaveEvaluation(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrail\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/dimred\u001B[39m\u001B[38;5;124m\"\u001B[39m, higest_true_dr, higest_pred_dr, highest_acc_dr, higest_epoch_dr, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimension Reduced\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mencoded_dataset\u001B[38;5;241m.\u001B[39mlabel_list)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:60\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_dim_red_model_test\u001B[0;34m(self, lr, epochs, current_run)\u001B[0m\n\u001B[1;32m     58\u001B[0m model \u001B[38;5;241m=\u001B[39m SSDimRedModel(num_emotions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list))\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     59\u001B[0m save_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_run\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/dimred/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:79\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_model_test\u001B[0;34m(self, lr, epochs, model, save_dir, bs)\u001B[0m\n\u001B[1;32m     71\u001B[0m trainDS, testDs \u001B[38;5;241m=\u001B[39m train_val_dataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, val_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m     72\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SSGenModelTrainer(lr\u001B[38;5;241m=\u001B[39mlr, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model\u001B[38;5;241m=\u001B[39mmodel, train_dataset\u001B[38;5;241m=\u001B[39mtrainDS,\n\u001B[1;32m     73\u001B[0m                             eval_dataset\u001B[38;5;241m=\u001B[39mtestDs,\n\u001B[1;32m     74\u001B[0m                             device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, labelList\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m                             save_highest_acc_min_acc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_highest_acc_min_acc,\n\u001B[1;32m     78\u001B[0m                             model_path \u001B[38;5;241m=\u001B[39m save_dir)\n\u001B[0;32m---> 79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:64\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     62\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memo_reco_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m acc, true, preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loop(test_dataloader, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(acc \u001B[38;5;241m>\u001B[39m highest_acc):\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:95\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     94\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 95\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
