{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 21:30:47.071355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 21:30:47.576465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 21:30:47.576557: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 21:30:47.576564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_bottleneck_encoder import SSBottelneckLayer\n",
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 10\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/AUTO_encoder/Nr1\"\n",
    "safe_model_every = 500\n",
    "epochs = 19001\n",
    "save_highest_acc_min_acc = 0.5\n",
    "lr = 1e-2\n",
    "save_every = 50\n",
    "start_lr = 4e-3\n",
    "gc.collect()\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncs_betterSS_12_1_wrap_music.pkl\", device=device)\n",
    "trainDS, testDs = train_val_dataset(data_set, val_split=0.1, seed=100)\n",
    "\n",
    "torch.manual_seed(33333)\n",
    "model = SSBottelneckLayer(num_cols = 175,bottleneck_size =  4, dropout=0.1)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.574778  [    0/ 6092]\n",
      "loss: 2.308557  [ 1000/ 6092]\n",
      "loss: 2.306574  [ 2000/ 6092]\n",
      "loss: 2.302569  [ 3000/ 6092]\n",
      "loss: 2.304477  [ 4000/ 6092]\n",
      "loss: 2.309453  [ 5000/ 6092]\n",
      "loss: 2.301938  [ 6000/ 6092]\n",
      "1405.0880396366122\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.308906  [    0/ 6092]\n",
      "loss: 2.311700  [ 1000/ 6092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m      7\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SSAutoencoderTrainer(batch_size\u001B[38;5;241m=\u001B[39mbatch_size, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model_path\u001B[38;5;241m=\u001B[39mmodels_dir, save_model_every\u001B[38;5;241m=\u001B[39msave_every, lr\u001B[38;5;241m=\u001B[39mstart_lr, dataset\u001B[38;5;241m=\u001B[39mdata_set, device\u001B[38;5;241m=\u001B[39mdevice, model\u001B[38;5;241m=\u001B[39mmodel, loss_fn\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss())\n\u001B[0;32m----> 8\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/encoder/ss_trainer_bottleneck_encoder.py:54\u001B[0m, in \u001B[0;36mSSAutoencoderTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     52\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/encoder/ss_trainer_bottleneck_encoder.py:83\u001B[0m, in \u001B[0;36mSSAutoencoderTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     80\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     81\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 83\u001B[0m main_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     86\u001B[0m     loss, current \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem(), batch \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(X)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_trainer_bottleneck_encoder import SSAutoencoderTrainer\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/AUTO_encoder/Nr2_25001/more/\"\n",
    "epochs = 25001\n",
    "safe_model_every = 500\n",
    "start_lr = 1e-2\n",
    "gc.collect()\n",
    "trainer = SSAutoencoderTrainer(batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=model, loss_fn=torch.nn.CrossEntropyLoss())\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
