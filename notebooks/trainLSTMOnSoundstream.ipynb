{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 16:10:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-01-10 16:10:57.649162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 16:10:58.196649: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 16:10:58.196701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-10 16:10:58.196706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": "SoundStream(\n  (encoder): Sequential(\n    (0): CausalConv1d(\n      (conv): Conv1d(1, 32, kernel_size=(7,), stride=(1,))\n    )\n    (1): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n      )\n    )\n    (2): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(64, 128, kernel_size=(8,), stride=(4,))\n      )\n    )\n    (3): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(128, 256, kernel_size=(10,), stride=(5,))\n      )\n    )\n    (4): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(256, 512, kernel_size=(16,), stride=(8,))\n      )\n    )\n    (5): CausalConv1d(\n      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n    )\n  )\n  (encoder_attn): LocalMHA(\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n    (attn_fn): LocalAttention(\n      (dropout): Dropout(p=0.0, inplace=False)\n      (rel_pos): SinusoidalEmbeddings()\n    )\n    (to_out): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (rq): ResidualVQ(\n    (layers): ModuleList(\n      (0): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (1): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (2): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (3): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (4): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (5): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (6): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (7): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n    )\n  )\n  (decoder_attn): LocalMHA(\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n    (attn_fn): LocalAttention(\n      (dropout): Dropout(p=0.0, inplace=False)\n      (rel_pos): SinusoidalEmbeddings()\n    )\n    (to_out): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (decoder): Sequential(\n    (0): CausalConv1d(\n      (conv): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n    )\n    (1): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (2): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(256, 128, kernel_size=(10,), stride=(5,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (3): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (4): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (5): CausalConv1d(\n      (conv): Conv1d(32, 1, kernel_size=(7,), stride=(1,))\n    )\n  )\n  (discriminators): ModuleList(\n    (0): MultiScaleDiscriminator(\n      (init_conv): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n      (conv_layers): ModuleList(\n        (0): Sequential(\n          (0): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (1): Sequential(\n          (0): Conv1d(64, 256, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (2): Sequential(\n          (0): Conv1d(256, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (3): Sequential(\n          (0): Conv1d(1024, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n      )\n      (final_conv): Sequential(\n        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,))\n        (1): LeakyReLU(negative_slope=0.1)\n        (2): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (1): MultiScaleDiscriminator(\n      (init_conv): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n      (conv_layers): ModuleList(\n        (0): Sequential(\n          (0): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (1): Sequential(\n          (0): Conv1d(64, 256, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (2): Sequential(\n          (0): Conv1d(256, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (3): Sequential(\n          (0): Conv1d(1024, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n      )\n      (final_conv): Sequential(\n        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,))\n        (1): LeakyReLU(negative_slope=0.1)\n        (2): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (2): MultiScaleDiscriminator(\n      (init_conv): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n      (conv_layers): ModuleList(\n        (0): Sequential(\n          (0): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (1): Sequential(\n          (0): Conv1d(64, 256, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (2): Sequential(\n          (0): Conv1d(256, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (3): Sequential(\n          (0): Conv1d(1024, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n      )\n      (final_conv): Sequential(\n        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,))\n        (1): LeakyReLU(negative_slope=0.1)\n        (2): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n  (stft_discriminator): STFTDiscriminator(\n    (init_conv): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n    (layers): ModuleList(\n      (0): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 2))\n      )\n      (1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n      )\n      (2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(64, 128, kernel_size=(3, 4), stride=(1, 2), padding=(1, 2))\n      )\n      (3): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n      )\n      (4): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(128, 256, kernel_size=(3, 4), stride=(1, 2), padding=(1, 2))\n      )\n      (5): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n      )\n    )\n    (final_conv): Conv2d(256, 1, kernel_size=(16, 1), stride=(1, 1))\n  )\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "soundstream.load(\"content/soundstream/results3/soundstream.9000.pt\")\n",
    "soundstream.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...generating encoding\r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from importlib import reload\n",
    "import network_models.soundstream_lstm.LSTM_dataset as lds\n",
    "import torch.utils.data as data_utils\n",
    "#from network_models.soundstream_lstm.LSTM_dataset  import AudioEmotionTessDataset\n",
    "reload(lds)\n",
    "\n",
    "\n",
    "tess_dataset = lds.AudioEmotionTessDataset(\"../tess\")\n",
    "#encoder_dataset = lds.NewAudioEmotionTessDataset(\"../tess\", soundstream)\n",
    "encoded_dataset = lds.AudioEmotionTessSoundStreamEncodedDataset(dataSet= tess_dataset, soundStream=soundstream)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_lstm\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundstream_lstm.small_model as sm\n",
    "importlib.reload(sm)\n",
    "\n",
    "\n",
    "import network_models.soundstream_lstm.soundstream_datacollator as datacollator\n",
    "importlib.reload(datacollator)\n",
    "data_collator = datacollator.SoundStreamDataCollator(200)\n",
    "\n",
    "model = sm.EmotionClassifierSevenEmos().to(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from network_models.soundstream_lstm.soundstream_datacollator import SoundstreamModelTrainer\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "\n",
    "is_regression = False\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    trainSet = Subset(dataset, train_idx)\n",
    "    valSet = Subset(dataset, val_idx)\n",
    "    return trainSet, valSet\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=0)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "\n",
    "train_dataset, eval_dataset = train_val_dataset(encoded_dataset, val_split=0.25)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 0.30641523, -0.30414137, -0.3276682 , ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.40829998, -0.06142293, -0.00129516, ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.08494987,  0.24296378, -0.02997825, ...,  0.        ,\n          0.        ,  0.        ],\n        ...,\n        [-0.32030046,  0.27021587,  0.36861932, ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.02692641, -0.13035715, -0.1913992 , ...,  0.        ,\n          0.        ,  0.        ],\n        [-0.07453994, -0.16956666, -0.19240046, ...,  0.        ,\n          0.        ,  0.        ]], dtype=float32),\n tensor([0., 0., 0., 0., 1., 0., 0.]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(train_dataset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "evalLoader = torch.utils.data.DataLoader(eval_dataset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "train_dataset[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.937051  [    0/ 2100]\n",
      "loss: 1.952227  [ 1000/ 2100]\n",
      "loss: 1.939513  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 1.946134 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.931537  [    0/ 2100]\n",
      "loss: 1.943762  [ 1000/ 2100]\n",
      "loss: 1.923206  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 17.0%, Avg loss: 1.944619 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.925678  [    0/ 2100]\n",
      "loss: 1.934024  [ 1000/ 2100]\n",
      "loss: 1.904628  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 17.3%, Avg loss: 1.943027 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.919361  [    0/ 2100]\n",
      "loss: 1.923257  [ 1000/ 2100]\n",
      "loss: 1.886316  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 1.941338 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.912491  [    0/ 2100]\n",
      "loss: 1.911494  [ 1000/ 2100]\n",
      "loss: 1.869779  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 19.7%, Avg loss: 1.939552 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.904943  [    0/ 2100]\n",
      "loss: 1.898868  [ 1000/ 2100]\n",
      "loss: 1.854784  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 20.1%, Avg loss: 1.937691 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.896660  [    0/ 2100]\n",
      "loss: 1.885621  [ 1000/ 2100]\n",
      "loss: 1.840725  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 21.1%, Avg loss: 1.935764 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.887394  [    0/ 2100]\n",
      "loss: 1.872746  [ 1000/ 2100]\n",
      "loss: 1.826646  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 22.7%, Avg loss: 1.933770 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.876882  [    0/ 2100]\n",
      "loss: 1.859547  [ 1000/ 2100]\n",
      "loss: 1.812368  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 23.6%, Avg loss: 1.931711 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.865052  [    0/ 2100]\n",
      "loss: 1.845677  [ 1000/ 2100]\n",
      "loss: 1.797826  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 23.9%, Avg loss: 1.929617 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.851788  [    0/ 2100]\n",
      "loss: 1.830812  [ 1000/ 2100]\n",
      "loss: 1.783133  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 1.927510 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.837733  [    0/ 2100]\n",
      "loss: 1.814798  [ 1000/ 2100]\n",
      "loss: 1.768882  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 25.1%, Avg loss: 1.925427 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.823312  [    0/ 2100]\n",
      "loss: 1.797793  [ 1000/ 2100]\n",
      "loss: 1.755318  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 1.923334 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.809697  [    0/ 2100]\n",
      "loss: 1.779592  [ 1000/ 2100]\n",
      "loss: 1.742975  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 1.921226 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.797375  [    0/ 2100]\n",
      "loss: 1.760124  [ 1000/ 2100]\n",
      "loss: 1.731684  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 27.6%, Avg loss: 1.919100 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.786499  [    0/ 2100]\n",
      "loss: 1.739620  [ 1000/ 2100]\n",
      "loss: 1.721548  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 28.1%, Avg loss: 1.916939 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.776810  [    0/ 2100]\n",
      "loss: 1.718654  [ 1000/ 2100]\n",
      "loss: 1.712516  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 29.0%, Avg loss: 1.914744 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.767977  [    0/ 2100]\n",
      "loss: 1.697950  [ 1000/ 2100]\n",
      "loss: 1.704411  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 29.9%, Avg loss: 1.912553 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.759652  [    0/ 2100]\n",
      "loss: 1.678144  [ 1000/ 2100]\n",
      "loss: 1.697156  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 1.910341 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.751945  [    0/ 2100]\n",
      "loss: 1.660498  [ 1000/ 2100]\n",
      "loss: 1.690405  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 1.908125 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.744605  [    0/ 2100]\n",
      "loss: 1.645081  [ 1000/ 2100]\n",
      "loss: 1.684265  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 31.4%, Avg loss: 1.905916 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.737442  [    0/ 2100]\n",
      "loss: 1.631895  [ 1000/ 2100]\n",
      "loss: 1.678569  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.903697 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.730541  [    0/ 2100]\n",
      "loss: 1.620298  [ 1000/ 2100]\n",
      "loss: 1.673409  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.901488 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.723862  [    0/ 2100]\n",
      "loss: 1.610302  [ 1000/ 2100]\n",
      "loss: 1.668714  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.899286 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.717383  [    0/ 2100]\n",
      "loss: 1.601303  [ 1000/ 2100]\n",
      "loss: 1.664404  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.897086 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.711179  [    0/ 2100]\n",
      "loss: 1.593247  [ 1000/ 2100]\n",
      "loss: 1.660554  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 1.894893 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.705280  [    0/ 2100]\n",
      "loss: 1.585556  [ 1000/ 2100]\n",
      "loss: 1.657112  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 1.892737 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.699528  [    0/ 2100]\n",
      "loss: 1.578342  [ 1000/ 2100]\n",
      "loss: 1.653935  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 1.890641 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.694083  [    0/ 2100]\n",
      "loss: 1.571470  [ 1000/ 2100]\n",
      "loss: 1.650959  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.888598 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.688721  [    0/ 2100]\n",
      "loss: 1.564715  [ 1000/ 2100]\n",
      "loss: 1.648216  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 1.886602 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.683654  [    0/ 2100]\n",
      "loss: 1.558173  [ 1000/ 2100]\n",
      "loss: 1.645665  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 1.884641 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.678801  [    0/ 2100]\n",
      "loss: 1.551855  [ 1000/ 2100]\n",
      "loss: 1.643173  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 36.3%, Avg loss: 1.882741 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.674202  [    0/ 2100]\n",
      "loss: 1.545714  [ 1000/ 2100]\n",
      "loss: 1.640822  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.880859 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.669783  [    0/ 2100]\n",
      "loss: 1.539589  [ 1000/ 2100]\n",
      "loss: 1.638587  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 36.3%, Avg loss: 1.879018 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.665517  [    0/ 2100]\n",
      "loss: 1.533612  [ 1000/ 2100]\n",
      "loss: 1.636499  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.877208 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.661565  [    0/ 2100]\n",
      "loss: 1.527807  [ 1000/ 2100]\n",
      "loss: 1.634444  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 1.875442 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.657773  [    0/ 2100]\n",
      "loss: 1.522127  [ 1000/ 2100]\n",
      "loss: 1.632462  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 1.873719 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.654040  [    0/ 2100]\n",
      "loss: 1.516598  [ 1000/ 2100]\n",
      "loss: 1.630610  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 1.872039 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.650503  [    0/ 2100]\n",
      "loss: 1.511315  [ 1000/ 2100]\n",
      "loss: 1.628777  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 1.870405 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.647296  [    0/ 2100]\n",
      "loss: 1.506213  [ 1000/ 2100]\n",
      "loss: 1.627024  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 1.868818 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.644150  [    0/ 2100]\n",
      "loss: 1.501197  [ 1000/ 2100]\n",
      "loss: 1.625365  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 1.867274 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.641243  [    0/ 2100]\n",
      "loss: 1.496370  [ 1000/ 2100]\n",
      "loss: 1.623783  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.865780 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.638383  [    0/ 2100]\n",
      "loss: 1.491712  [ 1000/ 2100]\n",
      "loss: 1.622235  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.864335 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.635693  [    0/ 2100]\n",
      "loss: 1.487213  [ 1000/ 2100]\n",
      "loss: 1.620791  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.862940 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.633123  [    0/ 2100]\n",
      "loss: 1.482957  [ 1000/ 2100]\n",
      "loss: 1.619339  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.861584 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.630773  [    0/ 2100]\n",
      "loss: 1.478807  [ 1000/ 2100]\n",
      "loss: 1.617967  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.860272 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.628508  [    0/ 2100]\n",
      "loss: 1.474901  [ 1000/ 2100]\n",
      "loss: 1.616719  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.859008 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.626385  [    0/ 2100]\n",
      "loss: 1.471147  [ 1000/ 2100]\n",
      "loss: 1.615493  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 1.857769 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.624352  [    0/ 2100]\n",
      "loss: 1.467611  [ 1000/ 2100]\n",
      "loss: 1.614290  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 1.856573 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.622504  [    0/ 2100]\n",
      "loss: 1.464184  [ 1000/ 2100]\n",
      "loss: 1.613154  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.855412 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.620750  [    0/ 2100]\n",
      "loss: 1.460913  [ 1000/ 2100]\n",
      "loss: 1.612069  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 1.854296 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.619040  [    0/ 2100]\n",
      "loss: 1.457892  [ 1000/ 2100]\n",
      "loss: 1.611036  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.853218 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.617452  [    0/ 2100]\n",
      "loss: 1.454967  [ 1000/ 2100]\n",
      "loss: 1.610071  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.852177 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.615967  [    0/ 2100]\n",
      "loss: 1.452228  [ 1000/ 2100]\n",
      "loss: 1.609133  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.851162 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.614537  [    0/ 2100]\n",
      "loss: 1.449681  [ 1000/ 2100]\n",
      "loss: 1.608256  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.6%, Avg loss: 1.850190 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.613222  [    0/ 2100]\n",
      "loss: 1.447270  [ 1000/ 2100]\n",
      "loss: 1.607419  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.6%, Avg loss: 1.849241 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.611939  [    0/ 2100]\n",
      "loss: 1.445001  [ 1000/ 2100]\n",
      "loss: 1.606648  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.6%, Avg loss: 1.848326 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.610780  [    0/ 2100]\n",
      "loss: 1.442928  [ 1000/ 2100]\n",
      "loss: 1.605869  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.847436 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.609632  [    0/ 2100]\n",
      "loss: 1.440921  [ 1000/ 2100]\n",
      "loss: 1.605142  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.846576 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.608560  [    0/ 2100]\n",
      "loss: 1.439054  [ 1000/ 2100]\n",
      "loss: 1.604461  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.845738 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.607537  [    0/ 2100]\n",
      "loss: 1.437313  [ 1000/ 2100]\n",
      "loss: 1.603787  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 1.844925 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.606577  [    0/ 2100]\n",
      "loss: 1.435704  [ 1000/ 2100]\n",
      "loss: 1.603152  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 1.844136 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.605657  [    0/ 2100]\n",
      "loss: 1.434166  [ 1000/ 2100]\n",
      "loss: 1.602517  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.843367 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.604781  [    0/ 2100]\n",
      "loss: 1.432731  [ 1000/ 2100]\n",
      "loss: 1.601933  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.842622 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.603948  [    0/ 2100]\n",
      "loss: 1.431377  [ 1000/ 2100]\n",
      "loss: 1.601339  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.841897 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.603161  [    0/ 2100]\n",
      "loss: 1.430093  [ 1000/ 2100]\n",
      "loss: 1.600776  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.841200 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.602403  [    0/ 2100]\n",
      "loss: 1.428916  [ 1000/ 2100]\n",
      "loss: 1.600220  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.840520 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.601689  [    0/ 2100]\n",
      "loss: 1.427767  [ 1000/ 2100]\n",
      "loss: 1.599715  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.839865 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.601001  [    0/ 2100]\n",
      "loss: 1.426699  [ 1000/ 2100]\n",
      "loss: 1.599201  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.839228 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.600360  [    0/ 2100]\n",
      "loss: 1.425681  [ 1000/ 2100]\n",
      "loss: 1.598713  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.838605 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.599728  [    0/ 2100]\n",
      "loss: 1.424739  [ 1000/ 2100]\n",
      "loss: 1.598256  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.838003 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.599146  [    0/ 2100]\n",
      "loss: 1.423838  [ 1000/ 2100]\n",
      "loss: 1.597793  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.837417 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.598564  [    0/ 2100]\n",
      "loss: 1.422991  [ 1000/ 2100]\n",
      "loss: 1.597362  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.836846 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.598012  [    0/ 2100]\n",
      "loss: 1.422179  [ 1000/ 2100]\n",
      "loss: 1.596931  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.836289 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.597483  [    0/ 2100]\n",
      "loss: 1.421410  [ 1000/ 2100]\n",
      "loss: 1.596530  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.835751 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.596985  [    0/ 2100]\n",
      "loss: 1.420675  [ 1000/ 2100]\n",
      "loss: 1.596143  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.835221 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.596501  [    0/ 2100]\n",
      "loss: 1.419976  [ 1000/ 2100]\n",
      "loss: 1.595768  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 1.834704 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.596024  [    0/ 2100]\n",
      "loss: 1.419317  [ 1000/ 2100]\n",
      "loss: 1.595407  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.834206 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.595578  [    0/ 2100]\n",
      "loss: 1.418680  [ 1000/ 2100]\n",
      "loss: 1.595072  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.833715 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.595142  [    0/ 2100]\n",
      "loss: 1.418085  [ 1000/ 2100]\n",
      "loss: 1.594731  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.833238 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.594723  [    0/ 2100]\n",
      "loss: 1.417510  [ 1000/ 2100]\n",
      "loss: 1.594407  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.832772 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.594330  [    0/ 2100]\n",
      "loss: 1.416968  [ 1000/ 2100]\n",
      "loss: 1.594089  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.832320 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.593944  [    0/ 2100]\n",
      "loss: 1.416442  [ 1000/ 2100]\n",
      "loss: 1.593792  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.831877 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.593570  [    0/ 2100]\n",
      "loss: 1.415933  [ 1000/ 2100]\n",
      "loss: 1.593493  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 1.831450 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.593212  [    0/ 2100]\n",
      "loss: 1.415448  [ 1000/ 2100]\n",
      "loss: 1.593207  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 1.831030 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.592869  [    0/ 2100]\n",
      "loss: 1.414981  [ 1000/ 2100]\n",
      "loss: 1.592927  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.830621 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.592533  [    0/ 2100]\n",
      "loss: 1.414536  [ 1000/ 2100]\n",
      "loss: 1.592661  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.830220 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.592216  [    0/ 2100]\n",
      "loss: 1.414110  [ 1000/ 2100]\n",
      "loss: 1.592407  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.829827 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.591900  [    0/ 2100]\n",
      "loss: 1.413696  [ 1000/ 2100]\n",
      "loss: 1.592162  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.829444 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.591601  [    0/ 2100]\n",
      "loss: 1.413306  [ 1000/ 2100]\n",
      "loss: 1.591917  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.829062 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.591309  [    0/ 2100]\n",
      "loss: 1.412928  [ 1000/ 2100]\n",
      "loss: 1.591688  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.828692 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.591030  [    0/ 2100]\n",
      "loss: 1.412562  [ 1000/ 2100]\n",
      "loss: 1.591459  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.828328 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.590756  [    0/ 2100]\n",
      "loss: 1.412208  [ 1000/ 2100]\n",
      "loss: 1.591235  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.827971 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.590491  [    0/ 2100]\n",
      "loss: 1.411875  [ 1000/ 2100]\n",
      "loss: 1.591030  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.827622 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.590229  [    0/ 2100]\n",
      "loss: 1.411546  [ 1000/ 2100]\n",
      "loss: 1.590816  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.827279 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.589981  [    0/ 2100]\n",
      "loss: 1.411236  [ 1000/ 2100]\n",
      "loss: 1.590616  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.826947 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.589738  [    0/ 2100]\n",
      "loss: 1.410930  [ 1000/ 2100]\n",
      "loss: 1.590408  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.826617 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.589505  [    0/ 2100]\n",
      "loss: 1.410641  [ 1000/ 2100]\n",
      "loss: 1.590219  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.826299 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.589277  [    0/ 2100]\n",
      "loss: 1.410359  [ 1000/ 2100]\n",
      "loss: 1.590023  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.825982 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.589056  [    0/ 2100]\n",
      "loss: 1.410088  [ 1000/ 2100]\n",
      "loss: 1.589840  [ 2000/ 2100]\n",
      "Test Error: \n",
      " Accuracy: 41.7%, Avg loss: 1.825673 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import OneHotEncoder# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "def encodeLabel(label):\n",
    "    return enc.fit_transform(label).toarray()\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "test_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.reshape(-1, 512*400).to(\"cuda\")\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        y = y.to(\"cuda\")\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.reshape(-1, 512*400).to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += loss_fn(pred, y.to(\"cuda\")).item()\n",
    "            y = torch.tensor([a.nonzero() for a in y]).to(\"cuda\")\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0598, 0.0175, 0.1067, 0.7351, 0.0032, 0.0755, 0.0023],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "'happy'"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "# trainer.train()\n",
    "#torch.save(model.state_dict(), 'content/classifierSS/model_weights.pth')\n",
    "audio = torchaudio.load(\"../Stimuli_Intensitätsmorphs/nf01_hap_w01_o_100_70dB.wav\")\n",
    "\n",
    "data = soundstream.encoder(audio[0].to(\"cuda\"))\n",
    "data = torch.nn.functional.pad(data, (0, 400 - data.shape[1], 0, 0))\n",
    "pred = model(data.flatten())\n",
    "print(pred)\n",
    "encoded_dataset.getEmotionFromId(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
