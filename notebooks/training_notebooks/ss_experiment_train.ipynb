{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:07:17.632426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 17:07:18.176516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:07:18.176572: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:07:18.176576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 12\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final/\"\n",
    "trials_per_model_type = 1\n",
    "epochs_per_model = 1000\n",
    "save_highest_acc_min_acc = 0.5\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks_clip/content/datasets/soundstream_encoded/allEncodings_noInducednoStimuli_3_5_sec_v12_1_basic.pkl\", device=\"cuda\")\n",
    "    #csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient, save_highest_acc_min_acc=save_highest_acc_min_acc, seed=3333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.917340  [ 1200/ 4873]\n",
      "loss: 1.958150  [ 2400/ 4873]\n",
      "loss: 1.974627  [ 3600/ 4873]\n",
      "loss: 2.007561  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.970926 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.103434  [ 1200/ 4873]\n",
      "loss: 1.944358  [ 2400/ 4873]\n",
      "loss: 1.953886  [ 3600/ 4873]\n",
      "loss: 1.990515  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.967790 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.909411  [ 1200/ 4873]\n",
      "loss: 1.963987  [ 2400/ 4873]\n",
      "loss: 1.909258  [ 3600/ 4873]\n",
      "loss: 1.879341  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.965278 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.908756  [ 1200/ 4873]\n",
      "loss: 1.971192  [ 2400/ 4873]\n",
      "loss: 1.873519  [ 3600/ 4873]\n",
      "loss: 2.012936  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.962752 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.005588  [ 1200/ 4873]\n",
      "loss: 1.915216  [ 2400/ 4873]\n",
      "loss: 1.917898  [ 3600/ 4873]\n",
      "loss: 1.913776  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.960494 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.037873  [ 1200/ 4873]\n",
      "loss: 1.967522  [ 2400/ 4873]\n",
      "loss: 1.979019  [ 3600/ 4873]\n",
      "loss: 1.928265  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.958417 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.004150  [ 1200/ 4873]\n",
      "loss: 1.945599  [ 2400/ 4873]\n",
      "loss: 2.000841  [ 3600/ 4873]\n",
      "loss: 1.883033  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.956569 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.946801  [ 1200/ 4873]\n",
      "loss: 1.912569  [ 2400/ 4873]\n",
      "loss: 1.930702  [ 3600/ 4873]\n",
      "loss: 1.984101  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.959604 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.981075  [ 1200/ 4873]\n",
      "loss: 1.941069  [ 2400/ 4873]\n",
      "loss: 1.907350  [ 3600/ 4873]\n",
      "loss: 1.947439  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.953386 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.944523  [ 1200/ 4873]\n",
      "loss: 1.908881  [ 2400/ 4873]\n",
      "loss: 1.968382  [ 3600/ 4873]\n",
      "loss: 1.994812  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.960135 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.997939  [ 1200/ 4873]\n",
      "loss: 2.039158  [ 2400/ 4873]\n",
      "loss: 1.956025  [ 3600/ 4873]\n",
      "loss: 1.949127  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.950908 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.939952  [ 1200/ 4873]\n",
      "loss: 1.946803  [ 2400/ 4873]\n",
      "loss: 1.942753  [ 3600/ 4873]\n",
      "loss: 1.961398  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.949798 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.989424  [ 1200/ 4873]\n",
      "loss: 1.947215  [ 2400/ 4873]\n",
      "loss: 1.960899  [ 3600/ 4873]\n",
      "loss: 1.934715  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.949037 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.898114  [ 1200/ 4873]\n",
      "loss: 1.870255  [ 2400/ 4873]\n",
      "loss: 1.994040  [ 3600/ 4873]\n",
      "loss: 1.964637  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.947957 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.892273  [ 1200/ 4873]\n",
      "loss: 1.983973  [ 2400/ 4873]\n",
      "loss: 1.941343  [ 3600/ 4873]\n",
      "loss: 1.912450  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.947283 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.945746  [ 1200/ 4873]\n",
      "loss: 1.992150  [ 2400/ 4873]\n",
      "loss: 1.915405  [ 3600/ 4873]\n",
      "loss: 1.920768  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.952914 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.937102  [ 1200/ 4873]\n",
      "loss: 1.923514  [ 2400/ 4873]\n",
      "loss: 1.974207  [ 3600/ 4873]\n",
      "loss: 1.919920  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.946077 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.885319  [ 1200/ 4873]\n",
      "loss: 1.907740  [ 2400/ 4873]\n",
      "loss: 1.953705  [ 3600/ 4873]\n",
      "loss: 1.954749  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.945685 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.880056  [ 1200/ 4873]\n",
      "loss: 1.988558  [ 2400/ 4873]\n",
      "loss: 1.916998  [ 3600/ 4873]\n",
      "loss: 1.911828  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.945380 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.985252  [ 1200/ 4873]\n",
      "loss: 1.896672  [ 2400/ 4873]\n",
      "loss: 1.938352  [ 3600/ 4873]\n",
      "loss: 1.985892  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.945138 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.884123  [ 1200/ 4873]\n",
      "loss: 1.899890  [ 2400/ 4873]\n",
      "loss: 1.927524  [ 3600/ 4873]\n",
      "loss: 1.988552  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.945914 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.958316  [ 1200/ 4873]\n",
      "loss: 1.919157  [ 2400/ 4873]\n",
      "loss: 1.908629  [ 3600/ 4873]\n",
      "loss: 1.941214  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.143     0.000     0.000    105\n",
      " disgust     0.143     0.000     0.000    109\n",
      "    fear     0.143     0.000     0.000    80\n",
      "   happy     0.143     0.000     0.000    81\n",
      " neutral     0.143     0.000     0.000    84\n",
      "     sad     0.143     0.143     1.000    87\n",
      "surprise     0.143     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.143     0.020     0.143    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.945645 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.957721  [ 1200/ 4873]\n",
      "loss: 1.890163  [ 2400/ 4873]\n",
      "loss: 1.993159  [ 3600/ 4873]\n",
      "loss: 1.821005  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.174     0.000     0.000    105\n",
      " disgust     0.174     0.000     0.000    109\n",
      "    fear     0.174     0.000     0.000    80\n",
      "   happy     0.174     1.000     0.235    81\n",
      " neutral     0.174     0.000     0.000    84\n",
      "     sad     0.174     0.147     1.000    87\n",
      "surprise     0.174     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.174     0.164     0.176    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 17.4%, Avg loss: 1.920859 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.968894  [ 1200/ 4873]\n",
      "loss: 1.942749  [ 2400/ 4873]\n",
      "loss: 1.813055  [ 3600/ 4873]\n",
      "loss: 1.920614  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.230     0.000     0.000    105\n",
      " disgust     0.230     0.251     0.495    109\n",
      "    fear     0.230     0.000     0.000    80\n",
      "   happy     0.230     0.268     0.679    81\n",
      " neutral     0.230     0.000     0.000    84\n",
      "     sad     0.230     0.163     0.356    87\n",
      "surprise     0.230     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.230     0.098     0.219    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.0%, Avg loss: 1.883545 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.886070  [ 1200/ 4873]\n",
      "loss: 1.890660  [ 2400/ 4873]\n",
      "loss: 1.752716  [ 3600/ 4873]\n",
      "loss: 1.942166  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.280     0.000     0.000    105\n",
      " disgust     0.280     0.327     0.780    109\n",
      "    fear     0.280     0.000     0.000    80\n",
      "   happy     0.280     0.285     0.802    81\n",
      " neutral     0.280     0.000     0.000    84\n",
      "     sad     0.280     0.172     0.241    87\n",
      "surprise     0.280     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.280     0.112     0.261    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.0%, Avg loss: 1.859118 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.772561  [ 1200/ 4873]\n",
      "loss: 1.889682  [ 2400/ 4873]\n",
      "loss: 1.822937  [ 3600/ 4873]\n",
      "loss: 1.757341  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.328     0.786     0.210    105\n",
      " disgust     0.328     0.321     0.872    109\n",
      "    fear     0.328     0.000     0.000    80\n",
      "   happy     0.328     0.286     0.790    81\n",
      " neutral     0.328     0.000     0.000    84\n",
      "     sad     0.328     0.306     0.218    87\n",
      "surprise     0.328     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.328     0.243     0.299    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.838682 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.673551  [ 1200/ 4873]\n",
      "loss: 1.828215  [ 2400/ 4873]\n",
      "loss: 1.769824  [ 3600/ 4873]\n",
      "loss: 1.916170  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.313     0.295     0.124    105\n",
      " disgust     0.313     0.313     0.899    109\n",
      "    fear     0.313     0.000     0.000    80\n",
      "   happy     0.313     0.382     0.778    81\n",
      " neutral     0.313     0.000     0.000    84\n",
      "     sad     0.313     0.193     0.195    87\n",
      "surprise     0.313     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.313     0.169     0.285    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 1.813612 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.609771  [ 1200/ 4873]\n",
      "loss: 1.860574  [ 2400/ 4873]\n",
      "loss: 1.915500  [ 3600/ 4873]\n",
      "loss: 1.787490  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.361     0.676     0.457    105\n",
      " disgust     0.361     0.306     0.899    109\n",
      "    fear     0.361     0.000     0.000    80\n",
      "   happy     0.361     0.335     0.741    81\n",
      " neutral     0.361     0.000     0.000    84\n",
      "     sad     0.361     0.406     0.149    87\n",
      "surprise     0.361     0.125     0.016    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.361     0.264     0.323    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 1.775318 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.924245  [ 1200/ 4873]\n",
      "loss: 1.806890  [ 2400/ 4873]\n",
      "loss: 1.679205  [ 3600/ 4873]\n",
      "loss: 1.501253  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.349     0.764     0.524    105\n",
      " disgust     0.349     0.315     0.789    109\n",
      "    fear     0.349     0.000     0.000    80\n",
      "   happy     0.349     0.262     0.815    81\n",
      " neutral     0.349     0.000     0.000    84\n",
      "     sad     0.349     0.750     0.069    87\n",
      "surprise     0.349     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.349     0.299     0.314    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 1.766646 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.809436  [ 1200/ 4873]\n",
      "loss: 1.650471  [ 2400/ 4873]\n",
      "loss: 1.720460  [ 3600/ 4873]\n",
      "loss: 1.438956  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.349     0.778     0.533    105\n",
      " disgust     0.349     0.310     0.844    109\n",
      "    fear     0.349     0.000     0.000    80\n",
      "   happy     0.349     0.294     0.802    81\n",
      " neutral     0.349     0.000     0.000    84\n",
      "     sad     0.349     0.000     0.000    87\n",
      "surprise     0.349     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.349     0.197     0.311    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 1.722197 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.688587  [ 1200/ 4873]\n",
      "loss: 1.859304  [ 2400/ 4873]\n",
      "loss: 1.422156  [ 3600/ 4873]\n",
      "loss: 1.782906  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.361     0.696     0.610    105\n",
      " disgust     0.361     0.310     0.771    109\n",
      "    fear     0.361     0.000     0.000    80\n",
      "   happy     0.361     0.309     0.778    81\n",
      " neutral     0.361     0.000     0.000    84\n",
      "     sad     0.361     0.143     0.011    87\n",
      "surprise     0.361     0.222     0.125    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.361     0.240     0.328    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 1.690807 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.521467  [ 1200/ 4873]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 10:00:38.227854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 10:00:38.701976: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 10:00:38.702028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 10:00:38.702032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 267178 samples and validating with randomly splitted 14063 samples\n",
      "0: soundstream total loss: 545563.781, soundstream recon loss: 0.603 | discr (scale 1) loss: 2.001 | discr (scale 0.5) loss: 2.002 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to ../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32\n",
      "0: saving model to ../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32\n",
      "1: soundstream total loss: 660274.273, soundstream recon loss: 0.702 | discr (scale 1) loss: 1.989 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 1.997\n",
      "2: soundstream total loss: 526542.078, soundstream recon loss: 0.547 | discr (scale 1) loss: 1.966 | discr (scale 0.5) loss: 1.990 | discr (scale 0.25) loss: 2.001\n",
      "3: soundstream total loss: 158701.781, soundstream recon loss: 0.173 | discr (scale 1) loss: 1.958 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 2.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 37\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#soundstream.load(\"../content/soundstream/verision0.12.1/00_10000_1e-4_bs6_gae8_dml320-32/soundstream.42500.pt\")\u001B[39;00m\n\u001B[1;32m     20\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SoundStreamTrainer(\n\u001B[1;32m     21\u001B[0m     soundstream,\n\u001B[1;32m     22\u001B[0m     folder \u001B[38;5;241m=\u001B[39m  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/home/ckwdani/Music/libri\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m#standard lr 3e-4,\u001B[39;00m\n\u001B[1;32m     35\u001B[0m )\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m---> 37\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:455\u001B[0m, in \u001B[0;36mSoundStreamTrainer.train\u001B[0;34m(self, log_fn)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, log_fn \u001B[38;5;241m=\u001B[39m noop):\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_train_steps:\n\u001B[0;32m--> 455\u001B[0m         logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m         log_fn(logs)\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining complete\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:357\u001B[0m, in \u001B[0;36mSoundStreamTrainer.train_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    354\u001B[0m     multiscale_discr_optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad_accum_every):\n\u001B[0;32m--> 357\u001B[0m     wave, \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdl_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m     wave \u001B[38;5;241m=\u001B[39m wave\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    360\u001B[0m     discr_losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoundstream(\n\u001B[1;32m    361\u001B[0m         wave,\n\u001B[1;32m    362\u001B[0m         apply_grad_penalty \u001B[38;5;241m=\u001B[39m apply_grad_penalty,\n\u001B[1;32m    363\u001B[0m         return_discr_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    364\u001B[0m         return_discr_losses_separately \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    365\u001B[0m     )\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:72\u001B[0m, in \u001B[0;36mcycle\u001B[0;34m(dl)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcycle\u001B[39m(dl):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 72\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dl:\n\u001B[1;32m     73\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/accelerate/data_loader.py:383\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    382\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m send_to_device(current_batch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 383\u001B[0m next_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m current_batch\n\u001B[1;32m    385\u001B[0m current_batch \u001B[38;5;241m=\u001B[39m next_batch\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/data.py:67\u001B[0m, in \u001B[0;36mSoundDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     65\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfiles[idx]\n\u001B[0;32m---> 67\u001B[0m     data, sample_hz \u001B[38;5;241m=\u001B[39m \u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m data\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mone of your audio file (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) is empty. please remove it from your folder\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;66;03m# the audio has more than 1 channel, convert to mono\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:222\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _fallback_load_fileobj(filepath, frame_offset, num_frames, normalize, channels_first, \u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    221\u001B[0m     filepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(filepath)\n\u001B[0;32m--> 222\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msox_io_load_audio_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\n\u001B[1;32m    224\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_ops.py:442\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "#\n",
    "# import numpy as np\n",
    "# from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "#\n",
    "# soundstream = SoundStream(\n",
    "#\n",
    "#     codebook_size = 2048,\n",
    "#     rq_num_quantizers = 12,\n",
    "#     attn_window_size = 128,       # local attention receptive field at bottleneck\n",
    "#     attn_depth = 3\n",
    "#     #target_sample_hz=16000\n",
    "# )\n",
    "#\n",
    "# #soundstream.load(\"../content/soundstream/verision0.12.1/00_10000_1e-4_bs6_gae8_dml320-32/soundstream.42500.pt\")\n",
    "#\n",
    "# trainer = SoundStreamTrainer(\n",
    "#     soundstream,\n",
    "#     folder =  '/home/ckwdani/Music/libri',\n",
    "#     #'/home/ckwdani/Music/train-clean-100',\n",
    "#     batch_size = 6,\n",
    "#     grad_accum_every = 8,         # effective batch size of 32\n",
    "#     data_max_length = 320 * 32,\n",
    "#     save_results_every=1000,\n",
    "#     num_train_steps = 60001,\n",
    "#     save_model_every= 500,\n",
    "#     results_folder = '../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32',\n",
    "#     #lr=1e-4\n",
    "# #    lr = 5e-5,\n",
    "#     #lr = 5e-5\n",
    "#     #standard lr 3e-4,\n",
    "# ).cuda()\n",
    "#\n",
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
