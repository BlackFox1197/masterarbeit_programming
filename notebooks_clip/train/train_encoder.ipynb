{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 17:39:18.063730: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 17:39:18.497624: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:39:18.497674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:39:18.497678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from network_models.clip.models.ss_encoder_downmapping import EncoderDownmapping\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks_clip/content/models/clip_models/clip_full_dataset_5_sec_\"\n",
    "# epochs =200\n",
    "# save_every = 40\n",
    "# start_lr = 2e-6\n",
    "gc.collect()\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks_clip/content/datasets/soundstream_encoded/allEncodings_noInducednoStimuli_5_sec_v12_1_basic.pkl\", device=device)\n",
    "\n",
    "enc_model = EncoderDownmapping(embed_dim=512, n_heads=4, ff_dim=2, n_layers=1, dropout=0.2, output=1024, max_seq_len=250).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.939615  [    0/ 6092]\n",
      "loss: 2.034612  [ 1400/ 6092]\n",
      "loss: 1.928231  [ 2800/ 6092]\n",
      "loss: 1.944877  [ 4200/ 6092]\n",
      "loss: 1.835991  [ 5600/ 6092]\n",
      "873.3010689020157\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.819953  [    0/ 6092]\n",
      "loss: 1.760646  [ 1400/ 6092]\n",
      "loss: 1.903033  [ 2800/ 6092]\n",
      "loss: 1.939355  [ 4200/ 6092]\n",
      "loss: 1.427097  [ 5600/ 6092]\n",
      "818.6753878593445\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.632241  [    0/ 6092]\n",
      "loss: 1.796509  [ 1400/ 6092]\n",
      "loss: 1.685769  [ 2800/ 6092]\n",
      "loss: 1.586994  [ 4200/ 6092]\n",
      "loss: 1.752129  [ 5600/ 6092]\n",
      "804.2670874595642\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.798676  [    0/ 6092]\n",
      "loss: 1.605760  [ 1400/ 6092]\n",
      "loss: 1.663527  [ 2800/ 6092]\n",
      "loss: 1.773258  [ 4200/ 6092]\n",
      "loss: 1.718079  [ 5600/ 6092]\n",
      "794.5637493133545\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.759920  [    0/ 6092]\n",
      "loss: 1.521178  [ 1400/ 6092]\n",
      "loss: 1.866092  [ 2800/ 6092]\n",
      "loss: 1.678475  [ 4200/ 6092]\n",
      "loss: 1.770130  [ 5600/ 6092]\n",
      "777.5672776699066\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.588595  [    0/ 6092]\n",
      "loss: 1.498844  [ 1400/ 6092]\n",
      "loss: 1.648396  [ 2800/ 6092]\n",
      "loss: 1.513877  [ 4200/ 6092]\n",
      "loss: 1.864573  [ 5600/ 6092]\n",
      "770.5519338846207\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.582845  [    0/ 6092]\n",
      "loss: 1.522253  [ 1400/ 6092]\n",
      "loss: 1.768203  [ 2800/ 6092]\n",
      "loss: 1.432729  [ 4200/ 6092]\n",
      "loss: 1.461654  [ 5600/ 6092]\n",
      "763.7238613367081\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.081556  [    0/ 6092]\n",
      "loss: 1.595858  [ 1400/ 6092]\n",
      "loss: 1.568437  [ 2800/ 6092]\n",
      "loss: 1.461329  [ 4200/ 6092]\n",
      "loss: 1.784717  [ 5600/ 6092]\n",
      "754.0074697732925\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.626542  [    0/ 6092]\n",
      "loss: 1.703110  [ 1400/ 6092]\n",
      "loss: 1.481245  [ 2800/ 6092]\n",
      "loss: 1.377260  [ 4200/ 6092]\n",
      "loss: 1.760831  [ 5600/ 6092]\n",
      "748.6089477539062\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.614308  [    0/ 6092]\n",
      "loss: 1.480370  [ 1400/ 6092]\n",
      "loss: 1.588582  [ 2800/ 6092]\n",
      "loss: 1.472048  [ 4200/ 6092]\n",
      "loss: 1.756009  [ 5600/ 6092]\n",
      "741.2980180978775\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.361155  [    0/ 6092]\n",
      "loss: 1.611121  [ 1400/ 6092]\n",
      "loss: 1.802642  [ 2800/ 6092]\n",
      "loss: 1.429836  [ 4200/ 6092]\n",
      "loss: 1.576447  [ 5600/ 6092]\n",
      "732.7867078781128\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.601594  [    0/ 6092]\n",
      "loss: 1.407360  [ 1400/ 6092]\n",
      "loss: 1.468598  [ 2800/ 6092]\n",
      "loss: 1.562992  [ 4200/ 6092]\n",
      "loss: 1.618716  [ 5600/ 6092]\n",
      "726.479998588562\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.487574  [    0/ 6092]\n",
      "loss: 1.506525  [ 1400/ 6092]\n",
      "loss: 1.693934  [ 2800/ 6092]\n",
      "loss: 1.692607  [ 4200/ 6092]\n",
      "loss: 1.787584  [ 5600/ 6092]\n",
      "724.0321683883667\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.848699  [    0/ 6092]\n",
      "loss: 1.553315  [ 1400/ 6092]\n",
      "loss: 1.472428  [ 2800/ 6092]\n",
      "loss: 1.663245  [ 4200/ 6092]\n",
      "loss: 1.602530  [ 5600/ 6092]\n",
      "719.9186680316925\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.420344  [    0/ 6092]\n",
      "loss: 1.752317  [ 1400/ 6092]\n",
      "loss: 1.385935  [ 2800/ 6092]\n",
      "loss: 1.516760  [ 4200/ 6092]\n",
      "loss: 1.368554  [ 5600/ 6092]\n",
      "711.7093821763992\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.642537  [    0/ 6092]\n",
      "loss: 1.389600  [ 1400/ 6092]\n",
      "loss: 1.440164  [ 2800/ 6092]\n",
      "loss: 1.469634  [ 4200/ 6092]\n",
      "loss: 1.412288  [ 5600/ 6092]\n",
      "707.8773653507233\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.583627  [    0/ 6092]\n",
      "loss: 1.264524  [ 1400/ 6092]\n",
      "loss: 1.504653  [ 2800/ 6092]\n",
      "loss: 1.518910  [ 4200/ 6092]\n",
      "loss: 1.512048  [ 5600/ 6092]\n",
      "707.6665569543839\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.606593  [    0/ 6092]\n",
      "loss: 1.425977  [ 1400/ 6092]\n",
      "loss: 1.720133  [ 2800/ 6092]\n",
      "loss: 1.400820  [ 4200/ 6092]\n",
      "loss: 1.487753  [ 5600/ 6092]\n",
      "702.3171293735504\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.389542  [    0/ 6092]\n",
      "loss: 1.609364  [ 1400/ 6092]\n",
      "loss: 1.729908  [ 2800/ 6092]\n",
      "loss: 1.352022  [ 4200/ 6092]\n",
      "loss: 1.686428  [ 5600/ 6092]\n",
      "695.3938380479813\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.427052  [    0/ 6092]\n",
      "loss: 1.450471  [ 1400/ 6092]\n",
      "loss: 1.594452  [ 2800/ 6092]\n",
      "loss: 1.479684  [ 4200/ 6092]\n",
      "loss: 1.401983  [ 5600/ 6092]\n",
      "697.1763589382172\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.460631  [    0/ 6092]\n",
      "loss: 1.608227  [ 1400/ 6092]\n",
      "loss: 1.390268  [ 2800/ 6092]\n",
      "loss: 1.437110  [ 4200/ 6092]\n",
      "loss: 1.474166  [ 5600/ 6092]\n",
      "689.3162770271301\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.398848  [    0/ 6092]\n",
      "loss: 1.328381  [ 1400/ 6092]\n",
      "loss: 1.564875  [ 2800/ 6092]\n",
      "loss: 1.397659  [ 4200/ 6092]\n",
      "loss: 1.407362  [ 5600/ 6092]\n",
      "684.6624932289124\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.427278  [    0/ 6092]\n",
      "loss: 1.471543  [ 1400/ 6092]\n",
      "loss: 1.671301  [ 2800/ 6092]\n",
      "loss: 1.565192  [ 4200/ 6092]\n",
      "loss: 1.422351  [ 5600/ 6092]\n",
      "677.3038501739502\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.715367  [    0/ 6092]\n",
      "loss: 1.702411  [ 1400/ 6092]\n",
      "loss: 1.333748  [ 2800/ 6092]\n",
      "loss: 1.628547  [ 4200/ 6092]\n",
      "loss: 1.727025  [ 5600/ 6092]\n",
      "674.6503393650055\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.596640  [    0/ 6092]\n",
      "loss: 1.521264  [ 1400/ 6092]\n",
      "loss: 1.311246  [ 2800/ 6092]\n",
      "loss: 1.511324  [ 4200/ 6092]\n",
      "loss: 1.329528  [ 5600/ 6092]\n",
      "672.5766812562943\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.385507  [    0/ 6092]\n",
      "loss: 1.477749  [ 1400/ 6092]\n",
      "loss: 1.438304  [ 2800/ 6092]\n",
      "loss: 1.335228  [ 4200/ 6092]\n",
      "loss: 1.418219  [ 5600/ 6092]\n",
      "666.2574505805969\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.461729  [    0/ 6092]\n",
      "loss: 1.371543  [ 1400/ 6092]\n",
      "loss: 1.278504  [ 2800/ 6092]\n",
      "loss: 1.202059  [ 4200/ 6092]\n",
      "loss: 1.627467  [ 5600/ 6092]\n",
      "664.3382649421692\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.281927  [    0/ 6092]\n",
      "loss: 1.404619  [ 1400/ 6092]\n",
      "loss: 1.397468  [ 2800/ 6092]\n",
      "loss: 1.376470  [ 4200/ 6092]\n",
      "loss: 1.408378  [ 5600/ 6092]\n",
      "659.8736319541931\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.373730  [    0/ 6092]\n",
      "loss: 1.329954  [ 1400/ 6092]\n",
      "loss: 1.417443  [ 2800/ 6092]\n",
      "loss: 1.296383  [ 4200/ 6092]\n",
      "loss: 1.489529  [ 5600/ 6092]\n",
      "657.1402630805969\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.331827  [    0/ 6092]\n",
      "loss: 1.523827  [ 1400/ 6092]\n",
      "loss: 1.446825  [ 2800/ 6092]\n",
      "loss: 1.375200  [ 4200/ 6092]\n",
      "loss: 1.300452  [ 5600/ 6092]\n",
      "651.8445625305176\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.310750  [    0/ 6092]\n",
      "loss: 1.225220  [ 1400/ 6092]\n",
      "loss: 1.430514  [ 2800/ 6092]\n",
      "loss: 1.449256  [ 4200/ 6092]\n",
      "loss: 1.316139  [ 5600/ 6092]\n",
      "652.2604691982269\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.303516  [    0/ 6092]\n",
      "loss: 1.349344  [ 1400/ 6092]\n",
      "loss: 1.337319  [ 2800/ 6092]\n",
      "loss: 1.418604  [ 4200/ 6092]\n",
      "loss: 1.367105  [ 5600/ 6092]\n",
      "648.739032626152\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.299963  [    0/ 6092]\n",
      "loss: 1.364841  [ 1400/ 6092]\n",
      "loss: 1.262769  [ 2800/ 6092]\n",
      "loss: 1.313633  [ 4200/ 6092]\n",
      "loss: 1.458318  [ 5600/ 6092]\n",
      "645.0967475175858\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.382719  [    0/ 6092]\n",
      "loss: 1.306151  [ 1400/ 6092]\n",
      "loss: 1.415096  [ 2800/ 6092]\n",
      "loss: 1.460482  [ 4200/ 6092]\n",
      "loss: 1.308837  [ 5600/ 6092]\n",
      "641.2900496721268\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.375196  [    0/ 6092]\n",
      "loss: 1.391112  [ 1400/ 6092]\n",
      "loss: 1.378749  [ 2800/ 6092]\n",
      "loss: 1.404201  [ 4200/ 6092]\n",
      "loss: 1.330002  [ 5600/ 6092]\n",
      "641.212681055069\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.241783  [    0/ 6092]\n",
      "loss: 1.359083  [ 1400/ 6092]\n",
      "loss: 1.318931  [ 2800/ 6092]\n",
      "loss: 1.390291  [ 4200/ 6092]\n",
      "loss: 1.401678  [ 5600/ 6092]\n",
      "635.1292666196823\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.509718  [    0/ 6092]\n",
      "loss: 1.462340  [ 1400/ 6092]\n",
      "loss: 1.312763  [ 2800/ 6092]\n",
      "loss: 1.320256  [ 4200/ 6092]\n",
      "loss: 1.411475  [ 5600/ 6092]\n",
      "634.5702759027481\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.307211  [    0/ 6092]\n",
      "loss: 1.363202  [ 1400/ 6092]\n",
      "loss: 1.338850  [ 2800/ 6092]\n",
      "loss: 1.378982  [ 4200/ 6092]\n",
      "loss: 1.501543  [ 5600/ 6092]\n",
      "629.2642245292664\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.290760  [    0/ 6092]\n",
      "loss: 1.477107  [ 1400/ 6092]\n",
      "loss: 1.399944  [ 2800/ 6092]\n",
      "loss: 1.376754  [ 4200/ 6092]\n",
      "loss: 1.228716  [ 5600/ 6092]\n",
      "628.7467036247253\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.358075  [    0/ 6092]\n",
      "loss: 1.304168  [ 1400/ 6092]\n",
      "loss: 1.414889  [ 2800/ 6092]\n",
      "loss: 1.412087  [ 4200/ 6092]\n",
      "loss: 1.326629  [ 5600/ 6092]\n",
      "628.3175729513168\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.321284  [    0/ 6092]\n",
      "loss: 1.420595  [ 1400/ 6092]\n",
      "loss: 1.263609  [ 2800/ 6092]\n",
      "loss: 1.298987  [ 4200/ 6092]\n",
      "loss: 1.670449  [ 5600/ 6092]\n",
      "626.7749857902527\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.535870  [    0/ 6092]\n",
      "loss: 1.276860  [ 1400/ 6092]\n",
      "loss: 1.347103  [ 2800/ 6092]\n",
      "loss: 1.370317  [ 4200/ 6092]\n",
      "loss: 1.205900  [ 5600/ 6092]\n",
      "621.0996006727219\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.255362  [    0/ 6092]\n",
      "loss: 1.420185  [ 1400/ 6092]\n",
      "loss: 1.246328  [ 2800/ 6092]\n",
      "loss: 1.261845  [ 4200/ 6092]\n",
      "loss: 1.350710  [ 5600/ 6092]\n",
      "620.9137063026428\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.338159  [    0/ 6092]\n",
      "loss: 1.374069  [ 1400/ 6092]\n",
      "loss: 1.382440  [ 2800/ 6092]\n",
      "loss: 1.336703  [ 4200/ 6092]\n",
      "loss: 1.297529  [ 5600/ 6092]\n",
      "618.2585574388504\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.245447  [    0/ 6092]\n",
      "loss: 1.272422  [ 1400/ 6092]\n",
      "loss: 1.510923  [ 2800/ 6092]\n",
      "loss: 1.288899  [ 4200/ 6092]\n",
      "loss: 1.239127  [ 5600/ 6092]\n",
      "618.4947279691696\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.239056  [    0/ 6092]\n",
      "loss: 1.252490  [ 1400/ 6092]\n",
      "loss: 1.194665  [ 2800/ 6092]\n",
      "loss: 1.276850  [ 4200/ 6092]\n",
      "loss: 1.244986  [ 5600/ 6092]\n",
      "614.2836369276047\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.336001  [    0/ 6092]\n",
      "loss: 1.292912  [ 1400/ 6092]\n",
      "loss: 1.455969  [ 2800/ 6092]\n",
      "loss: 1.272661  [ 4200/ 6092]\n",
      "loss: 1.312877  [ 5600/ 6092]\n",
      "611.3286312818527\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.251732  [    0/ 6092]\n",
      "loss: 1.434991  [ 1400/ 6092]\n",
      "loss: 1.282387  [ 2800/ 6092]\n",
      "loss: 1.283444  [ 4200/ 6092]\n",
      "loss: 1.506931  [ 5600/ 6092]\n",
      "611.9202719926834\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.349523  [    0/ 6092]\n",
      "loss: 1.340459  [ 1400/ 6092]\n",
      "loss: 1.290540  [ 2800/ 6092]\n",
      "loss: 1.251735  [ 4200/ 6092]\n",
      "loss: 1.279290  [ 5600/ 6092]\n",
      "610.6744283437729\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.226013  [    0/ 6092]\n",
      "loss: 1.235756  [ 1400/ 6092]\n",
      "loss: 1.285010  [ 2800/ 6092]\n",
      "loss: 1.384355  [ 4200/ 6092]\n",
      "loss: 1.289922  [ 5600/ 6092]\n",
      "607.0322920084\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.337292  [    0/ 6092]\n",
      "loss: 1.320027  [ 1400/ 6092]\n",
      "loss: 1.316179  [ 2800/ 6092]\n",
      "loss: 1.417725  [ 4200/ 6092]\n",
      "loss: 1.361892  [ 5600/ 6092]\n",
      "605.3689107894897\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.278666  [    0/ 6092]\n",
      "loss: 1.281112  [ 1400/ 6092]\n",
      "loss: 1.197901  [ 2800/ 6092]\n",
      "loss: 1.260010  [ 4200/ 6092]\n",
      "loss: 1.330237  [ 5600/ 6092]\n",
      "604.307062625885\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.258353  [    0/ 6092]\n",
      "loss: 1.315327  [ 1400/ 6092]\n",
      "loss: 1.253413  [ 2800/ 6092]\n",
      "loss: 1.349166  [ 4200/ 6092]\n",
      "loss: 1.233145  [ 5600/ 6092]\n",
      "602.5854380130768\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.252118  [    0/ 6092]\n",
      "loss: 1.401680  [ 1400/ 6092]\n",
      "loss: 1.194926  [ 2800/ 6092]\n",
      "loss: 1.287089  [ 4200/ 6092]\n",
      "loss: 1.296266  [ 5600/ 6092]\n",
      "602.3993223905563\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.303284  [    0/ 6092]\n",
      "loss: 1.251075  [ 1400/ 6092]\n",
      "loss: 1.505839  [ 2800/ 6092]\n",
      "loss: 1.319970  [ 4200/ 6092]\n",
      "loss: 1.359508  [ 5600/ 6092]\n",
      "600.2724651098251\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.234172  [    0/ 6092]\n",
      "loss: 1.291736  [ 1400/ 6092]\n",
      "loss: 1.256798  [ 2800/ 6092]\n",
      "loss: 1.281197  [ 4200/ 6092]\n",
      "loss: 1.376908  [ 5600/ 6092]\n",
      "595.3845421075821\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.212292  [    0/ 6092]\n",
      "loss: 1.218308  [ 1400/ 6092]\n",
      "loss: 1.283848  [ 2800/ 6092]\n",
      "loss: 1.316547  [ 4200/ 6092]\n",
      "loss: 1.318371  [ 5600/ 6092]\n",
      "595.2060739994049\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.274910  [    0/ 6092]\n",
      "loss: 1.271525  [ 1400/ 6092]\n",
      "loss: 1.479817  [ 2800/ 6092]\n",
      "loss: 1.262470  [ 4200/ 6092]\n",
      "loss: 1.342351  [ 5600/ 6092]\n",
      "594.1957639455795\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.287579  [    0/ 6092]\n",
      "loss: 1.284631  [ 1400/ 6092]\n",
      "loss: 1.304883  [ 2800/ 6092]\n",
      "loss: 1.393634  [ 4200/ 6092]\n",
      "loss: 1.257057  [ 5600/ 6092]\n",
      "594.3350976705551\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.282220  [    0/ 6092]\n",
      "loss: 1.300348  [ 1400/ 6092]\n",
      "loss: 1.205117  [ 2800/ 6092]\n",
      "loss: 1.363485  [ 4200/ 6092]\n",
      "loss: 1.306062  [ 5600/ 6092]\n",
      "593.2883265018463\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.274745  [    0/ 6092]\n",
      "loss: 1.289368  [ 1400/ 6092]\n",
      "loss: 1.209271  [ 2800/ 6092]\n",
      "loss: 1.348578  [ 4200/ 6092]\n",
      "loss: 1.229385  [ 5600/ 6092]\n",
      "595.1346981525421\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.221133  [    0/ 6092]\n",
      "loss: 1.197701  [ 1400/ 6092]\n",
      "loss: 1.211518  [ 2800/ 6092]\n",
      "loss: 1.540009  [ 4200/ 6092]\n",
      "loss: 1.270351  [ 5600/ 6092]\n",
      "588.296778678894\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.344915  [    0/ 6092]\n",
      "loss: 1.370052  [ 1400/ 6092]\n",
      "loss: 1.206047  [ 2800/ 6092]\n",
      "loss: 1.236555  [ 4200/ 6092]\n",
      "loss: 1.267143  [ 5600/ 6092]\n",
      "587.8372370004654\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.237553  [    0/ 6092]\n",
      "loss: 1.203041  [ 1400/ 6092]\n",
      "loss: 1.327736  [ 2800/ 6092]\n",
      "loss: 1.252283  [ 4200/ 6092]\n",
      "loss: 1.303185  [ 5600/ 6092]\n",
      "588.4892576932907\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.250696  [    0/ 6092]\n",
      "loss: 1.199674  [ 1400/ 6092]\n",
      "loss: 1.299160  [ 2800/ 6092]\n",
      "loss: 1.250673  [ 4200/ 6092]\n",
      "loss: 1.242012  [ 5600/ 6092]\n",
      "584.6472542285919\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.243024  [    0/ 6092]\n",
      "loss: 1.265648  [ 1400/ 6092]\n",
      "loss: 1.224839  [ 2800/ 6092]\n",
      "loss: 1.340753  [ 4200/ 6092]\n",
      "loss: 1.280087  [ 5600/ 6092]\n",
      "583.1572570800781\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.196640  [    0/ 6092]\n",
      "loss: 1.284437  [ 1400/ 6092]\n",
      "loss: 1.262542  [ 2800/ 6092]\n",
      "loss: 1.199365  [ 4200/ 6092]\n",
      "loss: 1.283700  [ 5600/ 6092]\n",
      "583.0846117734909\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.417342  [    0/ 6092]\n",
      "loss: 1.186702  [ 1400/ 6092]\n",
      "loss: 1.192619  [ 2800/ 6092]\n",
      "loss: 1.322196  [ 4200/ 6092]\n",
      "loss: 1.285605  [ 5600/ 6092]\n",
      "581.4494473934174\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.190594  [    0/ 6092]\n",
      "loss: 1.280680  [ 1400/ 6092]\n",
      "loss: 1.167075  [ 2800/ 6092]\n",
      "loss: 1.218822  [ 4200/ 6092]\n",
      "loss: 1.253884  [ 5600/ 6092]\n",
      "581.7261805534363\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.267805  [    0/ 6092]\n",
      "loss: 1.210556  [ 1400/ 6092]\n",
      "loss: 1.343168  [ 2800/ 6092]\n",
      "loss: 1.323268  [ 4200/ 6092]\n",
      "loss: 1.287334  [ 5600/ 6092]\n",
      "580.9442375898361\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.207897  [    0/ 6092]\n",
      "loss: 1.231560  [ 1400/ 6092]\n",
      "loss: 1.225614  [ 2800/ 6092]\n",
      "loss: 1.210947  [ 4200/ 6092]\n",
      "loss: 1.162280  [ 5600/ 6092]\n",
      "581.2538315057755\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.206301  [    0/ 6092]\n",
      "loss: 1.221064  [ 1400/ 6092]\n",
      "loss: 1.260780  [ 2800/ 6092]\n",
      "loss: 1.188752  [ 4200/ 6092]\n",
      "loss: 1.194444  [ 5600/ 6092]\n",
      "577.3277168273926\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.218097  [    0/ 6092]\n",
      "loss: 1.201602  [ 1400/ 6092]\n",
      "loss: 1.246415  [ 2800/ 6092]\n",
      "loss: 1.314001  [ 4200/ 6092]\n",
      "loss: 1.259462  [ 5600/ 6092]\n",
      "577.5421994924545\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.329111  [    0/ 6092]\n",
      "loss: 1.196917  [ 1400/ 6092]\n",
      "loss: 1.267968  [ 2800/ 6092]\n",
      "loss: 1.260250  [ 4200/ 6092]\n",
      "loss: 1.249055  [ 5600/ 6092]\n",
      "575.1121851205826\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.251526  [    0/ 6092]\n",
      "loss: 1.163209  [ 1400/ 6092]\n",
      "loss: 1.247543  [ 2800/ 6092]\n",
      "loss: 1.169355  [ 4200/ 6092]\n",
      "loss: 1.247448  [ 5600/ 6092]\n",
      "575.1694880723953\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.326698  [    0/ 6092]\n",
      "loss: 1.203793  [ 1400/ 6092]\n",
      "loss: 1.281450  [ 2800/ 6092]\n",
      "loss: 1.300323  [ 4200/ 6092]\n",
      "loss: 1.213343  [ 5600/ 6092]\n",
      "573.0862272977829\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.200447  [    0/ 6092]\n",
      "loss: 1.327724  [ 1400/ 6092]\n",
      "loss: 1.239717  [ 2800/ 6092]\n",
      "loss: 1.133287  [ 4200/ 6092]\n",
      "loss: 1.243336  [ 5600/ 6092]\n",
      "571.8758745193481\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.222954  [    0/ 6092]\n",
      "loss: 1.156780  [ 1400/ 6092]\n",
      "loss: 1.225338  [ 2800/ 6092]\n",
      "loss: 1.237620  [ 4200/ 6092]\n",
      "loss: 1.332223  [ 5600/ 6092]\n",
      "571.1182420253754\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.209808  [    0/ 6092]\n",
      "loss: 1.205603  [ 1400/ 6092]\n",
      "loss: 1.193027  [ 2800/ 6092]\n",
      "loss: 1.207546  [ 4200/ 6092]\n",
      "loss: 1.233103  [ 5600/ 6092]\n",
      "570.6204615831375\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.139588  [    0/ 6092]\n",
      "loss: 1.216034  [ 1400/ 6092]\n",
      "loss: 1.168376  [ 2800/ 6092]\n",
      "loss: 1.237653  [ 4200/ 6092]\n",
      "loss: 1.188607  [ 5600/ 6092]\n",
      "571.6781738996506\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.178152  [    0/ 6092]\n",
      "loss: 1.292101  [ 1400/ 6092]\n",
      "loss: 1.224904  [ 2800/ 6092]\n",
      "loss: 1.259520  [ 4200/ 6092]\n",
      "loss: 1.220081  [ 5600/ 6092]\n",
      "567.9652421474457\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.262895  [    0/ 6092]\n",
      "loss: 1.212442  [ 1400/ 6092]\n",
      "loss: 1.194159  [ 2800/ 6092]\n",
      "loss: 1.228086  [ 4200/ 6092]\n",
      "loss: 1.187072  [ 5600/ 6092]\n",
      "567.1156998872757\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.199343  [    0/ 6092]\n",
      "loss: 1.190426  [ 1400/ 6092]\n",
      "loss: 1.180392  [ 2800/ 6092]\n",
      "loss: 1.196518  [ 4200/ 6092]\n",
      "loss: 1.191796  [ 5600/ 6092]\n",
      "567.7233136892319\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.195943  [    0/ 6092]\n",
      "loss: 1.245437  [ 1400/ 6092]\n",
      "loss: 1.167577  [ 2800/ 6092]\n",
      "loss: 1.167550  [ 4200/ 6092]\n",
      "loss: 1.198990  [ 5600/ 6092]\n",
      "568.742182970047\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.183979  [    0/ 6092]\n",
      "loss: 1.162987  [ 1400/ 6092]\n",
      "loss: 1.198623  [ 2800/ 6092]\n",
      "loss: 1.221977  [ 4200/ 6092]\n",
      "loss: 1.188169  [ 5600/ 6092]\n",
      "566.2169616222382\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.187804  [    0/ 6092]\n",
      "loss: 1.188166  [ 1400/ 6092]\n",
      "loss: 1.222002  [ 2800/ 6092]\n",
      "loss: 1.157373  [ 4200/ 6092]\n",
      "loss: 1.240490  [ 5600/ 6092]\n",
      "565.0683796405792\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.176572  [    0/ 6092]\n",
      "loss: 1.369116  [ 1400/ 6092]\n",
      "loss: 1.160496  [ 2800/ 6092]\n",
      "loss: 1.180099  [ 4200/ 6092]\n",
      "loss: 1.358392  [ 5600/ 6092]\n",
      "564.2659450769424\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.209016  [    0/ 6092]\n",
      "loss: 1.214802  [ 1400/ 6092]\n",
      "loss: 1.194588  [ 2800/ 6092]\n",
      "loss: 1.130945  [ 4200/ 6092]\n",
      "loss: 1.254595  [ 5600/ 6092]\n",
      "564.3261337280273\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.194360  [    0/ 6092]\n",
      "loss: 1.196100  [ 1400/ 6092]\n",
      "loss: 1.277545  [ 2800/ 6092]\n",
      "loss: 1.228837  [ 4200/ 6092]\n",
      "loss: 1.224675  [ 5600/ 6092]\n",
      "561.3162195682526\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.203872  [    0/ 6092]\n",
      "loss: 1.188156  [ 1400/ 6092]\n",
      "loss: 1.212418  [ 2800/ 6092]\n",
      "loss: 1.229193  [ 4200/ 6092]\n",
      "loss: 1.344314  [ 5600/ 6092]\n",
      "562.4644978046417\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.256556  [    0/ 6092]\n",
      "loss: 1.218901  [ 1400/ 6092]\n",
      "loss: 1.206500  [ 2800/ 6092]\n",
      "loss: 1.206104  [ 4200/ 6092]\n",
      "loss: 1.184386  [ 5600/ 6092]\n",
      "560.9579297304153\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.217999  [    0/ 6092]\n",
      "loss: 1.173510  [ 1400/ 6092]\n",
      "loss: 1.149593  [ 2800/ 6092]\n",
      "loss: 1.206015  [ 4200/ 6092]\n",
      "loss: 1.202327  [ 5600/ 6092]\n",
      "561.2794407606125\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.187909  [    0/ 6092]\n",
      "loss: 1.146873  [ 1400/ 6092]\n",
      "loss: 1.191971  [ 2800/ 6092]\n",
      "loss: 1.186144  [ 4200/ 6092]\n",
      "loss: 1.278651  [ 5600/ 6092]\n",
      "559.9096835851669\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.244658  [    0/ 6092]\n",
      "loss: 1.203766  [ 1400/ 6092]\n",
      "loss: 1.195354  [ 2800/ 6092]\n",
      "loss: 1.325416  [ 4200/ 6092]\n",
      "loss: 1.263300  [ 5600/ 6092]\n",
      "558.5937715768814\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.211705  [    0/ 6092]\n",
      "loss: 1.191194  [ 1400/ 6092]\n",
      "loss: 1.155523  [ 2800/ 6092]\n",
      "loss: 1.157380  [ 4200/ 6092]\n",
      "loss: 1.241873  [ 5600/ 6092]\n",
      "557.5454778671265\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.282566  [    0/ 6092]\n",
      "loss: 1.156874  [ 1400/ 6092]\n",
      "loss: 1.173765  [ 2800/ 6092]\n",
      "loss: 1.274762  [ 4200/ 6092]\n",
      "loss: 1.326016  [ 5600/ 6092]\n",
      "558.6743261814117\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.191388  [    0/ 6092]\n",
      "loss: 1.205513  [ 1400/ 6092]\n",
      "loss: 1.162833  [ 2800/ 6092]\n",
      "loss: 1.193527  [ 4200/ 6092]\n",
      "loss: 1.191579  [ 5600/ 6092]\n",
      "557.3603595495224\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.255387  [    0/ 6092]\n",
      "loss: 1.162459  [ 1400/ 6092]\n",
      "loss: 1.196759  [ 2800/ 6092]\n",
      "loss: 1.187848  [ 4200/ 6092]\n",
      "loss: 1.173788  [ 5600/ 6092]\n",
      "556.0534375905991\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.145748  [    0/ 6092]\n",
      "loss: 1.227750  [ 1400/ 6092]\n",
      "loss: 1.179646  [ 2800/ 6092]\n",
      "loss: 1.148773  [ 4200/ 6092]\n",
      "loss: 1.212913  [ 5600/ 6092]\n",
      "553.6553236246109\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.184721  [    0/ 6092]\n",
      "loss: 1.164799  [ 1400/ 6092]\n",
      "loss: 1.159845  [ 2800/ 6092]\n",
      "loss: 1.181641  [ 4200/ 6092]\n",
      "loss: 1.170765  [ 5600/ 6092]\n",
      "554.8864481449127\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.203720  [    0/ 6092]\n",
      "loss: 1.175603  [ 1400/ 6092]\n",
      "loss: 1.147958  [ 2800/ 6092]\n",
      "loss: 1.167430  [ 4200/ 6092]\n",
      "loss: 1.256150  [ 5600/ 6092]\n",
      "554.1230632066727\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 1.178969  [    0/ 6092]\n",
      "loss: 1.255187  [ 1400/ 6092]\n",
      "loss: 1.156190  [ 2800/ 6092]\n",
      "loss: 1.164221  [ 4200/ 6092]\n",
      "loss: 1.259979  [ 5600/ 6092]\n",
      "552.671627163887\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 1.206946  [    0/ 6092]\n",
      "loss: 1.160673  [ 1400/ 6092]\n",
      "loss: 1.197230  [ 2800/ 6092]\n",
      "loss: 1.228924  [ 4200/ 6092]\n",
      "loss: 1.188374  [ 5600/ 6092]\n",
      "554.3780628442764\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 1.149618  [    0/ 6092]\n",
      "loss: 1.233549  [ 1400/ 6092]\n",
      "loss: 1.207193  [ 2800/ 6092]\n",
      "loss: 1.167360  [ 4200/ 6092]\n",
      "loss: 1.157456  [ 5600/ 6092]\n",
      "553.3640955686569\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 1.214101  [    0/ 6092]\n",
      "loss: 1.204723  [ 1400/ 6092]\n",
      "loss: 1.304623  [ 2800/ 6092]\n",
      "loss: 1.167327  [ 4200/ 6092]\n",
      "loss: 1.252532  [ 5600/ 6092]\n",
      "552.8300580978394\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 1.263836  [    0/ 6092]\n",
      "loss: 1.170989  [ 1400/ 6092]\n",
      "loss: 1.186623  [ 2800/ 6092]\n",
      "loss: 1.195157  [ 4200/ 6092]\n",
      "loss: 1.217551  [ 5600/ 6092]\n",
      "551.1956868171692\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 1.156648  [    0/ 6092]\n",
      "loss: 1.166795  [ 1400/ 6092]\n",
      "loss: 1.223550  [ 2800/ 6092]\n",
      "loss: 1.232636  [ 4200/ 6092]\n",
      "loss: 1.160339  [ 5600/ 6092]\n",
      "552.8169976472855\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 1.163706  [    0/ 6092]\n",
      "loss: 1.156201  [ 1400/ 6092]\n",
      "loss: 1.162336  [ 2800/ 6092]\n",
      "loss: 1.163293  [ 4200/ 6092]\n",
      "loss: 1.236738  [ 5600/ 6092]\n",
      "550.0281771421432\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 1.140389  [    0/ 6092]\n",
      "loss: 1.175233  [ 1400/ 6092]\n",
      "loss: 1.224286  [ 2800/ 6092]\n",
      "loss: 1.221826  [ 4200/ 6092]\n",
      "loss: 1.177233  [ 5600/ 6092]\n",
      "550.1899346113205\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.168799  [    0/ 6092]\n",
      "loss: 1.165132  [ 1400/ 6092]\n",
      "loss: 1.174421  [ 2800/ 6092]\n",
      "loss: 1.193206  [ 4200/ 6092]\n",
      "loss: 1.149140  [ 5600/ 6092]\n",
      "550.4177820682526\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 1.181643  [    0/ 6092]\n",
      "loss: 1.198270  [ 1400/ 6092]\n",
      "loss: 1.165482  [ 2800/ 6092]\n",
      "loss: 1.146805  [ 4200/ 6092]\n",
      "loss: 1.154249  [ 5600/ 6092]\n",
      "548.8791601657867\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 1.212576  [    0/ 6092]\n",
      "loss: 1.177623  [ 1400/ 6092]\n",
      "loss: 1.188815  [ 2800/ 6092]\n",
      "loss: 1.234551  [ 4200/ 6092]\n",
      "loss: 1.200523  [ 5600/ 6092]\n",
      "548.8479545116425\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 1.213438  [    0/ 6092]\n",
      "loss: 1.213585  [ 1400/ 6092]\n",
      "loss: 1.161576  [ 2800/ 6092]\n",
      "loss: 1.204097  [ 4200/ 6092]\n",
      "loss: 1.167722  [ 5600/ 6092]\n",
      "546.25068795681\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 1.144561  [    0/ 6092]\n",
      "loss: 1.294386  [ 1400/ 6092]\n",
      "loss: 1.170758  [ 2800/ 6092]\n",
      "loss: 1.157499  [ 4200/ 6092]\n",
      "loss: 1.184074  [ 5600/ 6092]\n",
      "549.6606703996658\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 1.182097  [    0/ 6092]\n",
      "loss: 1.136242  [ 1400/ 6092]\n",
      "loss: 1.150585  [ 2800/ 6092]\n",
      "loss: 1.126271  [ 4200/ 6092]\n",
      "loss: 1.342971  [ 5600/ 6092]\n",
      "546.9971092939377\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 1.215038  [    0/ 6092]\n",
      "loss: 1.168454  [ 1400/ 6092]\n",
      "loss: 1.200609  [ 2800/ 6092]\n",
      "loss: 1.180390  [ 4200/ 6092]\n",
      "loss: 1.162797  [ 5600/ 6092]\n",
      "546.1833589076996\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 1.147853  [    0/ 6092]\n",
      "loss: 1.192622  [ 1400/ 6092]\n",
      "loss: 1.174291  [ 2800/ 6092]\n",
      "loss: 1.187217  [ 4200/ 6092]\n",
      "loss: 1.159384  [ 5600/ 6092]\n",
      "546.6787062883377\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 1.168517  [    0/ 6092]\n",
      "loss: 1.138282  [ 1400/ 6092]\n",
      "loss: 1.191769  [ 2800/ 6092]\n",
      "loss: 1.167891  [ 4200/ 6092]\n",
      "loss: 1.151043  [ 5600/ 6092]\n",
      "545.0664292573929\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 1.125497  [    0/ 6092]\n",
      "loss: 1.154458  [ 1400/ 6092]\n",
      "loss: 1.177341  [ 2800/ 6092]\n",
      "loss: 1.173086  [ 4200/ 6092]\n",
      "loss: 1.194873  [ 5600/ 6092]\n",
      "545.3684099912643\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 1.144661  [    0/ 6092]\n",
      "loss: 1.176190  [ 1400/ 6092]\n",
      "loss: 1.248640  [ 2800/ 6092]\n",
      "loss: 1.222848  [ 4200/ 6092]\n",
      "loss: 1.131956  [ 5600/ 6092]\n",
      "544.9617688655853\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 1.191383  [    0/ 6092]\n",
      "loss: 1.169258  [ 1400/ 6092]\n",
      "loss: 1.146158  [ 2800/ 6092]\n",
      "loss: 1.157608  [ 4200/ 6092]\n",
      "loss: 1.184363  [ 5600/ 6092]\n",
      "542.7327625751495\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 1.200301  [    0/ 6092]\n",
      "loss: 1.188617  [ 1400/ 6092]\n",
      "loss: 1.144764  [ 2800/ 6092]\n",
      "loss: 1.151479  [ 4200/ 6092]\n",
      "loss: 1.189393  [ 5600/ 6092]\n",
      "543.8411663770676\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 1.182393  [    0/ 6092]\n",
      "loss: 1.184878  [ 1400/ 6092]\n",
      "loss: 1.156947  [ 2800/ 6092]\n",
      "loss: 1.129063  [ 4200/ 6092]\n",
      "loss: 1.176973  [ 5600/ 6092]\n",
      "542.7623897790909\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 1.152965  [    0/ 6092]\n",
      "loss: 1.200812  [ 1400/ 6092]\n",
      "loss: 1.156471  [ 2800/ 6092]\n",
      "loss: 1.130910  [ 4200/ 6092]\n",
      "loss: 1.124282  [ 5600/ 6092]\n",
      "542.7484681606293\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 1.160382  [    0/ 6092]\n",
      "loss: 1.126966  [ 1400/ 6092]\n",
      "loss: 1.149089  [ 2800/ 6092]\n",
      "loss: 1.166403  [ 4200/ 6092]\n",
      "loss: 1.151983  [ 5600/ 6092]\n",
      "541.5132151842117\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 1.151820  [    0/ 6092]\n",
      "loss: 1.154487  [ 1400/ 6092]\n",
      "loss: 1.175815  [ 2800/ 6092]\n",
      "loss: 1.181483  [ 4200/ 6092]\n",
      "loss: 1.228362  [ 5600/ 6092]\n",
      "543.055471777916\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 1.162334  [    0/ 6092]\n",
      "loss: 1.130145  [ 1400/ 6092]\n",
      "loss: 1.159884  [ 2800/ 6092]\n",
      "loss: 1.176750  [ 4200/ 6092]\n",
      "loss: 1.150355  [ 5600/ 6092]\n",
      "542.2924498319626\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 1.184580  [    0/ 6092]\n",
      "loss: 1.130127  [ 1400/ 6092]\n",
      "loss: 1.186888  [ 2800/ 6092]\n",
      "loss: 1.202460  [ 4200/ 6092]\n",
      "loss: 1.167623  [ 5600/ 6092]\n",
      "541.0024874210358\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 1.137396  [    0/ 6092]\n",
      "loss: 1.156914  [ 1400/ 6092]\n",
      "loss: 1.141085  [ 2800/ 6092]\n",
      "loss: 1.155858  [ 4200/ 6092]\n",
      "loss: 1.155501  [ 5600/ 6092]\n",
      "540.5477393865585\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 1.166850  [    0/ 6092]\n",
      "loss: 1.218426  [ 1400/ 6092]\n",
      "loss: 1.131863  [ 2800/ 6092]\n",
      "loss: 1.140645  [ 4200/ 6092]\n",
      "loss: 1.200803  [ 5600/ 6092]\n",
      "541.3210737705231\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 1.180290  [    0/ 6092]\n",
      "loss: 1.166304  [ 1400/ 6092]\n",
      "loss: 1.182943  [ 2800/ 6092]\n",
      "loss: 1.161937  [ 4200/ 6092]\n",
      "loss: 1.189786  [ 5600/ 6092]\n",
      "539.5109311342239\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 1.151842  [    0/ 6092]\n",
      "loss: 1.143508  [ 1400/ 6092]\n",
      "loss: 1.138221  [ 2800/ 6092]\n",
      "loss: 1.178946  [ 4200/ 6092]\n",
      "loss: 1.129064  [ 5600/ 6092]\n",
      "540.3970640897751\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 1.145513  [    0/ 6092]\n",
      "loss: 1.116286  [ 1400/ 6092]\n",
      "loss: 1.145420  [ 2800/ 6092]\n",
      "loss: 1.147196  [ 4200/ 6092]\n",
      "loss: 1.169580  [ 5600/ 6092]\n",
      "540.8373520374298\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 1.152887  [    0/ 6092]\n",
      "loss: 1.164359  [ 1400/ 6092]\n",
      "loss: 1.172380  [ 2800/ 6092]\n",
      "loss: 1.146866  [ 4200/ 6092]\n",
      "loss: 1.202291  [ 5600/ 6092]\n",
      "539.3664435148239\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 1.194329  [    0/ 6092]\n",
      "loss: 1.141860  [ 1400/ 6092]\n",
      "loss: 1.140622  [ 2800/ 6092]\n",
      "loss: 1.156978  [ 4200/ 6092]\n",
      "loss: 1.157020  [ 5600/ 6092]\n",
      "539.2817690372467\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 1.109387  [    0/ 6092]\n",
      "loss: 1.130109  [ 1400/ 6092]\n",
      "loss: 1.150299  [ 2800/ 6092]\n",
      "loss: 1.151792  [ 4200/ 6092]\n",
      "loss: 1.167511  [ 5600/ 6092]\n",
      "540.1944687366486\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 1.168516  [    0/ 6092]\n",
      "loss: 1.184534  [ 1400/ 6092]\n",
      "loss: 1.164218  [ 2800/ 6092]\n",
      "loss: 1.131819  [ 4200/ 6092]\n",
      "loss: 1.160188  [ 5600/ 6092]\n",
      "539.1178418397903\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 1.152753  [    0/ 6092]\n",
      "loss: 1.136165  [ 1400/ 6092]\n",
      "loss: 1.120432  [ 2800/ 6092]\n",
      "loss: 1.121333  [ 4200/ 6092]\n",
      "loss: 1.133275  [ 5600/ 6092]\n",
      "537.8183957338333\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 1.193130  [    0/ 6092]\n",
      "loss: 1.122811  [ 1400/ 6092]\n",
      "loss: 1.147969  [ 2800/ 6092]\n",
      "loss: 1.161178  [ 4200/ 6092]\n",
      "loss: 1.141580  [ 5600/ 6092]\n",
      "537.3069548606873\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 1.191644  [    0/ 6092]\n",
      "loss: 1.277557  [ 1400/ 6092]\n",
      "loss: 1.117538  [ 2800/ 6092]\n",
      "loss: 1.179403  [ 4200/ 6092]\n",
      "loss: 1.174097  [ 5600/ 6092]\n",
      "537.8393006324768\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 1.117249  [    0/ 6092]\n",
      "loss: 1.144294  [ 1400/ 6092]\n",
      "loss: 1.205112  [ 2800/ 6092]\n",
      "loss: 1.239470  [ 4200/ 6092]\n",
      "loss: 1.127872  [ 5600/ 6092]\n",
      "538.0589703321457\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 1.139577  [    0/ 6092]\n",
      "loss: 1.164358  [ 1400/ 6092]\n",
      "loss: 1.120514  [ 2800/ 6092]\n",
      "loss: 1.182232  [ 4200/ 6092]\n",
      "loss: 1.159705  [ 5600/ 6092]\n",
      "536.436923623085\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 1.121060  [    0/ 6092]\n",
      "loss: 1.108799  [ 1400/ 6092]\n",
      "loss: 1.111775  [ 2800/ 6092]\n",
      "loss: 1.152121  [ 4200/ 6092]\n",
      "loss: 1.249048  [ 5600/ 6092]\n",
      "537.156173825264\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 1.151572  [    0/ 6092]\n",
      "loss: 1.138333  [ 1400/ 6092]\n",
      "loss: 1.116416  [ 2800/ 6092]\n",
      "loss: 1.166221  [ 4200/ 6092]\n",
      "loss: 1.187346  [ 5600/ 6092]\n",
      "536.7703512907028\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 1.225609  [    0/ 6092]\n",
      "loss: 1.147188  [ 1400/ 6092]\n",
      "loss: 1.156051  [ 2800/ 6092]\n",
      "loss: 1.124967  [ 4200/ 6092]\n",
      "loss: 1.120242  [ 5600/ 6092]\n",
      "535.8221805095673\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 1.158868  [    0/ 6092]\n",
      "loss: 1.134575  [ 1400/ 6092]\n",
      "loss: 1.139354  [ 2800/ 6092]\n",
      "loss: 1.178187  [ 4200/ 6092]\n",
      "loss: 1.111153  [ 5600/ 6092]\n",
      "536.605193734169\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 1.108208  [    0/ 6092]\n",
      "loss: 1.146040  [ 1400/ 6092]\n",
      "loss: 1.126859  [ 2800/ 6092]\n",
      "loss: 1.133840  [ 4200/ 6092]\n",
      "loss: 1.111764  [ 5600/ 6092]\n",
      "535.41013276577\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 1.141380  [    0/ 6092]\n",
      "loss: 1.182269  [ 1400/ 6092]\n",
      "loss: 1.169994  [ 2800/ 6092]\n",
      "loss: 1.159560  [ 4200/ 6092]\n",
      "loss: 1.135184  [ 5600/ 6092]\n",
      "534.7224953174591\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 1.137913  [    0/ 6092]\n",
      "loss: 1.149459  [ 1400/ 6092]\n",
      "loss: 1.162803  [ 2800/ 6092]\n",
      "loss: 1.153622  [ 4200/ 6092]\n",
      "loss: 1.155153  [ 5600/ 6092]\n",
      "534.5494307279587\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 1.112096  [    0/ 6092]\n",
      "loss: 1.129339  [ 1400/ 6092]\n",
      "loss: 1.142647  [ 2800/ 6092]\n",
      "loss: 1.230010  [ 4200/ 6092]\n",
      "loss: 1.125328  [ 5600/ 6092]\n",
      "533.4454015493393\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 1.144290  [    0/ 6092]\n",
      "loss: 1.126418  [ 1400/ 6092]\n",
      "loss: 1.172982  [ 2800/ 6092]\n",
      "loss: 1.159355  [ 4200/ 6092]\n",
      "loss: 1.127459  [ 5600/ 6092]\n",
      "534.2606387138367\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 1.180414  [    0/ 6092]\n",
      "loss: 1.179035  [ 1400/ 6092]\n",
      "loss: 1.127944  [ 2800/ 6092]\n",
      "loss: 1.115401  [ 4200/ 6092]\n",
      "loss: 1.147046  [ 5600/ 6092]\n",
      "534.3503074645996\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 1.148142  [    0/ 6092]\n",
      "loss: 1.150180  [ 1400/ 6092]\n",
      "loss: 1.115699  [ 2800/ 6092]\n",
      "loss: 1.178333  [ 4200/ 6092]\n",
      "loss: 1.115638  [ 5600/ 6092]\n",
      "532.5080435276031\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 1.121791  [    0/ 6092]\n",
      "loss: 1.113295  [ 1400/ 6092]\n",
      "loss: 1.169063  [ 2800/ 6092]\n",
      "loss: 1.148428  [ 4200/ 6092]\n",
      "loss: 1.120669  [ 5600/ 6092]\n",
      "533.4588018655777\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 1.127111  [    0/ 6092]\n",
      "loss: 1.159508  [ 1400/ 6092]\n",
      "loss: 1.137939  [ 2800/ 6092]\n",
      "loss: 1.116013  [ 4200/ 6092]\n",
      "loss: 1.175708  [ 5600/ 6092]\n",
      "532.6842440366745\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 1.119471  [    0/ 6092]\n",
      "loss: 1.134896  [ 1400/ 6092]\n",
      "loss: 1.124101  [ 2800/ 6092]\n",
      "loss: 1.167210  [ 4200/ 6092]\n",
      "loss: 1.188519  [ 5600/ 6092]\n",
      "532.8104650974274\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 1.125290  [    0/ 6092]\n",
      "loss: 1.142648  [ 1400/ 6092]\n",
      "loss: 1.125381  [ 2800/ 6092]\n",
      "loss: 1.153352  [ 4200/ 6092]\n",
      "loss: 1.154185  [ 5600/ 6092]\n",
      "533.4308681488037\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 1.173625  [    0/ 6092]\n",
      "loss: 1.142647  [ 1400/ 6092]\n",
      "loss: 1.173359  [ 2800/ 6092]\n",
      "loss: 1.121071  [ 4200/ 6092]\n",
      "loss: 1.117105  [ 5600/ 6092]\n",
      "533.2853583097458\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 1.143837  [    0/ 6092]\n",
      "loss: 1.111708  [ 1400/ 6092]\n",
      "loss: 1.150041  [ 2800/ 6092]\n",
      "loss: 1.147639  [ 4200/ 6092]\n",
      "loss: 1.154477  [ 5600/ 6092]\n",
      "532.5138055086136\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 1.191971  [    0/ 6092]\n",
      "loss: 1.135168  [ 1400/ 6092]\n",
      "loss: 1.155223  [ 2800/ 6092]\n",
      "loss: 1.152474  [ 4200/ 6092]\n",
      "loss: 1.129843  [ 5600/ 6092]\n",
      "532.7154488563538\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 1.099092  [    0/ 6092]\n",
      "loss: 1.153915  [ 1400/ 6092]\n",
      "loss: 1.142704  [ 2800/ 6092]\n",
      "loss: 1.110840  [ 4200/ 6092]\n",
      "loss: 1.184398  [ 5600/ 6092]\n",
      "531.1374518871307\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 1.115299  [    0/ 6092]\n",
      "loss: 1.121557  [ 1400/ 6092]\n",
      "loss: 1.117323  [ 2800/ 6092]\n",
      "loss: 1.114369  [ 4200/ 6092]\n",
      "loss: 1.137844  [ 5600/ 6092]\n",
      "530.5078268051147\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 1.175836  [    0/ 6092]\n",
      "loss: 1.107018  [ 1400/ 6092]\n",
      "loss: 1.196162  [ 2800/ 6092]\n",
      "loss: 1.124647  [ 4200/ 6092]\n",
      "loss: 1.156389  [ 5600/ 6092]\n",
      "530.6571953296661\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 1.159866  [    0/ 6092]\n",
      "loss: 1.127805  [ 1400/ 6092]\n",
      "loss: 1.190889  [ 2800/ 6092]\n",
      "loss: 1.132700  [ 4200/ 6092]\n",
      "loss: 1.135076  [ 5600/ 6092]\n",
      "531.2782640457153\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 1.184387  [    0/ 6092]\n",
      "loss: 1.124460  [ 1400/ 6092]\n",
      "loss: 1.126339  [ 2800/ 6092]\n",
      "loss: 1.118408  [ 4200/ 6092]\n",
      "loss: 1.130595  [ 5600/ 6092]\n",
      "530.2181481122971\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 1.118034  [    0/ 6092]\n",
      "loss: 1.124037  [ 1400/ 6092]\n",
      "loss: 1.162349  [ 2800/ 6092]\n",
      "loss: 1.170593  [ 4200/ 6092]\n",
      "loss: 1.134468  [ 5600/ 6092]\n",
      "530.8166207075119\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 1.132303  [    0/ 6092]\n",
      "loss: 1.150449  [ 1400/ 6092]\n",
      "loss: 1.129064  [ 2800/ 6092]\n",
      "loss: 1.166523  [ 4200/ 6092]\n",
      "loss: 1.142885  [ 5600/ 6092]\n",
      "530.5243657827377\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 1.123392  [    0/ 6092]\n",
      "loss: 1.135145  [ 1400/ 6092]\n",
      "loss: 1.161927  [ 2800/ 6092]\n",
      "loss: 1.155427  [ 4200/ 6092]\n",
      "loss: 1.126829  [ 5600/ 6092]\n",
      "529.3107855319977\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 1.134925  [    0/ 6092]\n",
      "loss: 1.127941  [ 1400/ 6092]\n",
      "loss: 1.120924  [ 2800/ 6092]\n",
      "loss: 1.129725  [ 4200/ 6092]\n",
      "loss: 1.156277  [ 5600/ 6092]\n",
      "531.042688369751\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 1.112903  [    0/ 6092]\n",
      "loss: 1.167756  [ 1400/ 6092]\n",
      "loss: 1.183393  [ 2800/ 6092]\n",
      "loss: 1.118599  [ 4200/ 6092]\n",
      "loss: 1.147913  [ 5600/ 6092]\n",
      "530.1987457275391\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 1.155809  [    0/ 6092]\n",
      "loss: 1.147828  [ 1400/ 6092]\n",
      "loss: 1.161289  [ 2800/ 6092]\n",
      "loss: 1.135015  [ 4200/ 6092]\n",
      "loss: 1.149976  [ 5600/ 6092]\n",
      "531.6674968004227\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 1.135832  [    0/ 6092]\n",
      "loss: 1.142567  [ 1400/ 6092]\n",
      "loss: 1.103122  [ 2800/ 6092]\n",
      "loss: 1.103848  [ 4200/ 6092]\n",
      "loss: 1.159652  [ 5600/ 6092]\n",
      "528.7896157503128\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 1.137098  [    0/ 6092]\n",
      "loss: 1.122763  [ 1400/ 6092]\n",
      "loss: 1.122861  [ 2800/ 6092]\n",
      "loss: 1.184490  [ 4200/ 6092]\n",
      "loss: 1.177737  [ 5600/ 6092]\n",
      "528.4339092969894\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 1.116720  [    0/ 6092]\n",
      "loss: 1.143121  [ 1400/ 6092]\n",
      "loss: 1.131102  [ 2800/ 6092]\n",
      "loss: 1.126185  [ 4200/ 6092]\n",
      "loss: 1.110174  [ 5600/ 6092]\n",
      "530.002804517746\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 1.182858  [    0/ 6092]\n",
      "loss: 1.106640  [ 1400/ 6092]\n",
      "loss: 1.169636  [ 2800/ 6092]\n",
      "loss: 1.116165  [ 4200/ 6092]\n",
      "loss: 1.101838  [ 5600/ 6092]\n",
      "528.5185135602951\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 1.094355  [    0/ 6092]\n",
      "loss: 1.213912  [ 1400/ 6092]\n",
      "loss: 1.098709  [ 2800/ 6092]\n",
      "loss: 1.095947  [ 4200/ 6092]\n",
      "loss: 1.178835  [ 5600/ 6092]\n",
      "528.0392634868622\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 1.176425  [    0/ 6092]\n",
      "loss: 1.125755  [ 1400/ 6092]\n",
      "loss: 1.127202  [ 2800/ 6092]\n",
      "loss: 1.148880  [ 4200/ 6092]\n",
      "loss: 1.133200  [ 5600/ 6092]\n",
      "527.9060944318771\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 1.131893  [    0/ 6092]\n",
      "loss: 1.173292  [ 1400/ 6092]\n",
      "loss: 1.214526  [ 2800/ 6092]\n",
      "loss: 1.139483  [ 4200/ 6092]\n",
      "loss: 1.131998  [ 5600/ 6092]\n",
      "527.5901854038239\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 1.192411  [    0/ 6092]\n",
      "loss: 1.128046  [ 1400/ 6092]\n",
      "loss: 1.109482  [ 2800/ 6092]\n",
      "loss: 1.117087  [ 4200/ 6092]\n",
      "loss: 1.122407  [ 5600/ 6092]\n",
      "528.3623028993607\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 1.171269  [    0/ 6092]\n",
      "loss: 1.140839  [ 1400/ 6092]\n",
      "loss: 1.107680  [ 2800/ 6092]\n",
      "loss: 1.132934  [ 4200/ 6092]\n",
      "loss: 1.158309  [ 5600/ 6092]\n",
      "527.0509042739868\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 1.153328  [    0/ 6092]\n",
      "loss: 1.110293  [ 1400/ 6092]\n",
      "loss: 1.109282  [ 2800/ 6092]\n",
      "loss: 1.131233  [ 4200/ 6092]\n",
      "loss: 1.170268  [ 5600/ 6092]\n",
      "526.5486429929733\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 1.110524  [    0/ 6092]\n",
      "loss: 1.163529  [ 1400/ 6092]\n",
      "loss: 1.134315  [ 2800/ 6092]\n",
      "loss: 1.132235  [ 4200/ 6092]\n",
      "loss: 1.136960  [ 5600/ 6092]\n",
      "527.7163611650467\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 1.157961  [    0/ 6092]\n",
      "loss: 1.111213  [ 1400/ 6092]\n",
      "loss: 1.168002  [ 2800/ 6092]\n",
      "loss: 1.120153  [ 4200/ 6092]\n",
      "loss: 1.129840  [ 5600/ 6092]\n",
      "526.5414180755615\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 1.151841  [    0/ 6092]\n",
      "loss: 1.124552  [ 1400/ 6092]\n",
      "loss: 1.141100  [ 2800/ 6092]\n",
      "loss: 1.157099  [ 4200/ 6092]\n",
      "loss: 1.133596  [ 5600/ 6092]\n",
      "526.566149353981\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 1.136933  [    0/ 6092]\n",
      "loss: 1.109374  [ 1400/ 6092]\n",
      "loss: 1.115371  [ 2800/ 6092]\n",
      "loss: 1.155580  [ 4200/ 6092]\n",
      "loss: 1.143936  [ 5600/ 6092]\n",
      "526.7485389709473\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 1.135619  [    0/ 6092]\n",
      "loss: 1.138302  [ 1400/ 6092]\n",
      "loss: 1.138525  [ 2800/ 6092]\n",
      "loss: 1.128720  [ 4200/ 6092]\n",
      "loss: 1.106090  [ 5600/ 6092]\n",
      "527.2297959327698\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 1.140047  [    0/ 6092]\n",
      "loss: 1.122611  [ 1400/ 6092]\n",
      "loss: 1.187674  [ 2800/ 6092]\n",
      "loss: 1.140185  [ 4200/ 6092]\n",
      "loss: 1.120274  [ 5600/ 6092]\n",
      "526.1374772787094\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 1.130235  [    0/ 6092]\n",
      "loss: 1.141855  [ 1400/ 6092]\n",
      "loss: 1.102546  [ 2800/ 6092]\n",
      "loss: 1.104501  [ 4200/ 6092]\n",
      "loss: 1.116672  [ 5600/ 6092]\n",
      "528.1909538507462\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 1.120945  [    0/ 6092]\n",
      "loss: 1.158428  [ 1400/ 6092]\n",
      "loss: 1.176475  [ 2800/ 6092]\n",
      "loss: 1.103178  [ 4200/ 6092]\n",
      "loss: 1.113443  [ 5600/ 6092]\n",
      "525.875666975975\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 1.112318  [    0/ 6092]\n",
      "loss: 1.112891  [ 1400/ 6092]\n",
      "loss: 1.151577  [ 2800/ 6092]\n",
      "loss: 1.116275  [ 4200/ 6092]\n",
      "loss: 1.123535  [ 5600/ 6092]\n",
      "524.6070637702942\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 1.135847  [    0/ 6092]\n",
      "loss: 1.166118  [ 1400/ 6092]\n",
      "loss: 1.108843  [ 2800/ 6092]\n",
      "loss: 1.139255  [ 4200/ 6092]\n",
      "loss: 1.092825  [ 5600/ 6092]\n",
      "525.1567384004593\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 1.134737  [    0/ 6092]\n",
      "loss: 1.108263  [ 1400/ 6092]\n",
      "loss: 1.128570  [ 2800/ 6092]\n",
      "loss: 1.153553  [ 4200/ 6092]\n",
      "loss: 1.146626  [ 5600/ 6092]\n",
      "525.6168016195297\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 1.125175  [    0/ 6092]\n",
      "loss: 1.104100  [ 1400/ 6092]\n",
      "loss: 1.196310  [ 2800/ 6092]\n",
      "loss: 1.105902  [ 4200/ 6092]\n",
      "loss: 1.129510  [ 5600/ 6092]\n",
      "525.712519288063\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 1.121396  [    0/ 6092]\n",
      "loss: 1.133442  [ 1400/ 6092]\n",
      "loss: 1.117311  [ 2800/ 6092]\n",
      "loss: 1.107391  [ 4200/ 6092]\n",
      "loss: 1.116589  [ 5600/ 6092]\n",
      "524.934673666954\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 1.129306  [    0/ 6092]\n",
      "loss: 1.123074  [ 1400/ 6092]\n",
      "loss: 1.133724  [ 2800/ 6092]\n",
      "loss: 1.128654  [ 4200/ 6092]\n",
      "loss: 1.127113  [ 5600/ 6092]\n",
      "524.670196056366\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 1.138376  [    0/ 6092]\n",
      "loss: 1.105532  [ 1400/ 6092]\n",
      "loss: 1.113966  [ 2800/ 6092]\n",
      "loss: 1.110762  [ 4200/ 6092]\n",
      "loss: 1.123805  [ 5600/ 6092]\n",
      "525.3005284070969\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 1.144075  [    0/ 6092]\n",
      "loss: 1.115749  [ 1400/ 6092]\n",
      "loss: 1.124739  [ 2800/ 6092]\n",
      "loss: 1.119421  [ 4200/ 6092]\n",
      "loss: 1.124794  [ 5600/ 6092]\n",
      "524.2525398731232\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 1.105282  [    0/ 6092]\n",
      "loss: 1.154946  [ 1400/ 6092]\n",
      "loss: 1.120657  [ 2800/ 6092]\n",
      "loss: 1.129586  [ 4200/ 6092]\n",
      "loss: 1.184689  [ 5600/ 6092]\n",
      "523.9437906742096\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 1.113328  [    0/ 6092]\n",
      "loss: 1.093445  [ 1400/ 6092]\n",
      "loss: 1.159490  [ 2800/ 6092]\n",
      "loss: 1.146658  [ 4200/ 6092]\n",
      "loss: 1.177463  [ 5600/ 6092]\n",
      "524.0392735004425\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 1.117189  [    0/ 6092]\n",
      "loss: 1.140954  [ 1400/ 6092]\n",
      "loss: 1.132911  [ 2800/ 6092]\n",
      "loss: 1.139321  [ 4200/ 6092]\n",
      "loss: 1.126255  [ 5600/ 6092]\n",
      "524.2734612226486\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 1.147932  [    0/ 6092]\n",
      "loss: 1.122339  [ 1400/ 6092]\n",
      "loss: 1.137058  [ 2800/ 6092]\n",
      "loss: 1.123199  [ 4200/ 6092]\n",
      "loss: 1.117324  [ 5600/ 6092]\n",
      "523.8162763118744\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 1.109533  [    0/ 6092]\n",
      "loss: 1.128967  [ 1400/ 6092]\n",
      "loss: 1.136238  [ 2800/ 6092]\n",
      "loss: 1.127287  [ 4200/ 6092]\n",
      "loss: 1.153587  [ 5600/ 6092]\n",
      "524.5237292051315\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 1.153699  [    0/ 6092]\n",
      "loss: 1.116626  [ 1400/ 6092]\n",
      "loss: 1.106855  [ 2800/ 6092]\n",
      "loss: 1.106683  [ 4200/ 6092]\n",
      "loss: 1.118187  [ 5600/ 6092]\n",
      "522.9998457431793\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 1.174226  [    0/ 6092]\n",
      "loss: 1.104644  [ 1400/ 6092]\n",
      "loss: 1.116329  [ 2800/ 6092]\n",
      "loss: 1.136329  [ 4200/ 6092]\n",
      "loss: 1.140952  [ 5600/ 6092]\n",
      "523.3432725667953\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 1.111442  [    0/ 6092]\n",
      "loss: 1.133014  [ 1400/ 6092]\n",
      "loss: 1.120186  [ 2800/ 6092]\n",
      "loss: 1.118612  [ 4200/ 6092]\n",
      "loss: 1.128842  [ 5600/ 6092]\n",
      "522.7705245018005\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 1.133490  [    0/ 6092]\n",
      "loss: 1.138265  [ 1400/ 6092]\n",
      "loss: 1.128177  [ 2800/ 6092]\n",
      "loss: 1.112859  [ 4200/ 6092]\n",
      "loss: 1.167796  [ 5600/ 6092]\n",
      "523.0677460432053\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 1.151621  [    0/ 6092]\n",
      "loss: 1.114592  [ 1400/ 6092]\n",
      "loss: 1.101681  [ 2800/ 6092]\n",
      "loss: 1.138399  [ 4200/ 6092]\n",
      "loss: 1.114647  [ 5600/ 6092]\n",
      "523.3733240365982\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 1.159703  [    0/ 6092]\n",
      "loss: 1.144648  [ 1400/ 6092]\n",
      "loss: 1.145974  [ 2800/ 6092]\n",
      "loss: 1.120677  [ 4200/ 6092]\n",
      "loss: 1.113347  [ 5600/ 6092]\n",
      "523.6712449789047\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 1.112778  [    0/ 6092]\n",
      "loss: 1.112924  [ 1400/ 6092]\n",
      "loss: 1.122322  [ 2800/ 6092]\n",
      "loss: 1.124546  [ 4200/ 6092]\n",
      "loss: 1.132939  [ 5600/ 6092]\n",
      "522.275651216507\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 1.131839  [    0/ 6092]\n",
      "loss: 1.115107  [ 1400/ 6092]\n",
      "loss: 1.128198  [ 2800/ 6092]\n",
      "loss: 1.123387  [ 4200/ 6092]\n",
      "loss: 1.130742  [ 5600/ 6092]\n",
      "522.9274102449417\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 1.130711  [    0/ 6092]\n",
      "loss: 1.133853  [ 1400/ 6092]\n",
      "loss: 1.114942  [ 2800/ 6092]\n",
      "loss: 1.125075  [ 4200/ 6092]\n",
      "loss: 1.157030  [ 5600/ 6092]\n",
      "522.7128795385361\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 1.107972  [    0/ 6092]\n",
      "loss: 1.133372  [ 1400/ 6092]\n",
      "loss: 1.136654  [ 2800/ 6092]\n",
      "loss: 1.115850  [ 4200/ 6092]\n",
      "loss: 1.123282  [ 5600/ 6092]\n",
      "522.8504202365875\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 1.102804  [    0/ 6092]\n",
      "loss: 1.140140  [ 1400/ 6092]\n",
      "loss: 1.108379  [ 2800/ 6092]\n",
      "loss: 1.107168  [ 4200/ 6092]\n",
      "loss: 1.108273  [ 5600/ 6092]\n",
      "522.297661781311\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 1.128060  [    0/ 6092]\n",
      "loss: 1.132016  [ 1400/ 6092]\n",
      "loss: 1.123709  [ 2800/ 6092]\n",
      "loss: 1.159568  [ 4200/ 6092]\n",
      "loss: 1.102008  [ 5600/ 6092]\n",
      "522.2055932283401\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 1.125962  [    0/ 6092]\n",
      "loss: 1.115671  [ 1400/ 6092]\n",
      "loss: 1.111468  [ 2800/ 6092]\n",
      "loss: 1.152685  [ 4200/ 6092]\n",
      "loss: 1.150256  [ 5600/ 6092]\n",
      "521.0019221305847\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 1.127479  [    0/ 6092]\n",
      "loss: 1.094842  [ 1400/ 6092]\n",
      "loss: 1.111830  [ 2800/ 6092]\n",
      "loss: 1.110363  [ 4200/ 6092]\n",
      "loss: 1.103729  [ 5600/ 6092]\n",
      "521.3930015563965\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 1.102189  [    0/ 6092]\n",
      "loss: 1.102527  [ 1400/ 6092]\n",
      "loss: 1.143432  [ 2800/ 6092]\n",
      "loss: 1.111843  [ 4200/ 6092]\n",
      "loss: 1.104819  [ 5600/ 6092]\n",
      "520.8166936635971\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 1.139088  [    0/ 6092]\n",
      "loss: 1.125010  [ 1400/ 6092]\n",
      "loss: 1.117094  [ 2800/ 6092]\n",
      "loss: 1.106310  [ 4200/ 6092]\n",
      "loss: 1.112177  [ 5600/ 6092]\n",
      "521.0120587348938\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 1.093320  [    0/ 6092]\n",
      "loss: 1.109962  [ 1400/ 6092]\n",
      "loss: 1.131918  [ 2800/ 6092]\n",
      "loss: 1.126446  [ 4200/ 6092]\n",
      "loss: 1.098898  [ 5600/ 6092]\n",
      "522.1919798851013\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 1.113958  [    0/ 6092]\n",
      "loss: 1.119392  [ 1400/ 6092]\n",
      "loss: 1.117861  [ 2800/ 6092]\n",
      "loss: 1.144762  [ 4200/ 6092]\n",
      "loss: 1.095882  [ 5600/ 6092]\n",
      "522.0054086446762\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 1.104954  [    0/ 6092]\n",
      "loss: 1.102167  [ 1400/ 6092]\n",
      "loss: 1.100894  [ 2800/ 6092]\n",
      "loss: 1.135457  [ 4200/ 6092]\n",
      "loss: 1.106985  [ 5600/ 6092]\n",
      "520.3981965780258\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 1.127672  [    0/ 6092]\n",
      "loss: 1.126477  [ 1400/ 6092]\n",
      "loss: 1.120095  [ 2800/ 6092]\n",
      "loss: 1.111388  [ 4200/ 6092]\n",
      "loss: 1.111653  [ 5600/ 6092]\n",
      "520.3100609779358\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 1.126960  [    0/ 6092]\n",
      "loss: 1.090533  [ 1400/ 6092]\n",
      "loss: 1.102668  [ 2800/ 6092]\n",
      "loss: 1.134717  [ 4200/ 6092]\n",
      "loss: 1.182493  [ 5600/ 6092]\n",
      "520.6777414083481\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 1.122578  [    0/ 6092]\n",
      "loss: 1.120578  [ 1400/ 6092]\n",
      "loss: 1.125823  [ 2800/ 6092]\n",
      "loss: 1.130227  [ 4200/ 6092]\n",
      "loss: 1.096187  [ 5600/ 6092]\n",
      "520.0392136573792\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 1.110281  [    0/ 6092]\n",
      "loss: 1.117534  [ 1400/ 6092]\n",
      "loss: 1.133634  [ 2800/ 6092]\n",
      "loss: 1.111356  [ 4200/ 6092]\n",
      "loss: 1.127890  [ 5600/ 6092]\n",
      "521.4397364854813\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 1.118877  [    0/ 6092]\n",
      "loss: 1.094380  [ 1400/ 6092]\n",
      "loss: 1.113627  [ 2800/ 6092]\n",
      "loss: 1.136399  [ 4200/ 6092]\n",
      "loss: 1.153837  [ 5600/ 6092]\n",
      "520.7539596557617\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 1.110321  [    0/ 6092]\n",
      "loss: 1.116294  [ 1400/ 6092]\n",
      "loss: 1.131932  [ 2800/ 6092]\n",
      "loss: 1.147191  [ 4200/ 6092]\n",
      "loss: 1.104243  [ 5600/ 6092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m enc_trainer \u001B[38;5;241m=\u001B[39m sddt\u001B[38;5;241m.\u001B[39mSSClipTrainer(batch_size\u001B[38;5;241m=\u001B[39mbatch_size, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model_path\u001B[38;5;241m=\u001B[39mmodels_dir, save_model_every\u001B[38;5;241m=\u001B[39msave_every, lr\u001B[38;5;241m=\u001B[39mstart_lr, dataset\u001B[38;5;241m=\u001B[39mdata_set, device\u001B[38;5;241m=\u001B[39mdevice, model\u001B[38;5;241m=\u001B[39menc_model, is_encoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     16\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m---> 17\u001B[0m \u001B[43menc_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/clip/trainer_and_utils/ss_clip_trainer.py:52\u001B[0m, in \u001B[0;36mSSClipTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     50\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 52\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m epochloss \u001B[38;5;241m=\u001B[39m epochloss \u001B[38;5;241m+\u001B[39m [loss]\n\u001B[1;32m     54\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/clip/trainer_and_utils/ss_clip_trainer.py:98\u001B[0m, in \u001B[0;36mSSClipTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     97\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 98\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\u001B[39;00m\n\u001B[1;32m    100\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils/encoder\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundsream_models_and_utils.clip_like.encoder.ss_encoder_trainer as sset\n",
    "import network_models.clip.trainer_and_utils.ss_clip_trainer as sddt\n",
    "\n",
    "epochs =1001\n",
    "save_every = 200\n",
    "start_lr = 1e-5\n",
    "\n",
    "#enc_trainer = sset.SSEncoderTrainer(batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=enc_model)\n",
    "enc_trainer = sddt.SSClipTrainer(batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=enc_model, is_encoder=True)\n",
    "gc.collect()\n",
    "enc_trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
