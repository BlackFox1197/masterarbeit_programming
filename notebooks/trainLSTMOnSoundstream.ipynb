{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "SoundStream(\n  (encoder): Sequential(\n    (0): CausalConv1d(\n      (conv): Conv1d(1, 32, kernel_size=(7,), stride=(1,))\n    )\n    (1): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n      )\n    )\n    (2): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(64, 128, kernel_size=(8,), stride=(4,))\n      )\n    )\n    (3): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(128, 256, kernel_size=(10,), stride=(5,))\n      )\n    )\n    (4): Sequential(\n      (0): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): CausalConv1d(\n        (conv): Conv1d(256, 512, kernel_size=(16,), stride=(8,))\n      )\n    )\n    (5): CausalConv1d(\n      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n    )\n  )\n  (encoder_attn): LocalMHA(\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n    (attn_fn): LocalAttention(\n      (dropout): Dropout(p=0.0, inplace=False)\n      (rel_pos): SinusoidalEmbeddings()\n    )\n    (to_out): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (rq): ResidualVQ(\n    (layers): ModuleList(\n      (0): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (1): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (2): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (3): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (4): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (5): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (6): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n      (7): VectorQuantize(\n        (project_in): Identity()\n        (project_out): Identity()\n        (_codebook): EuclideanCodebook()\n      )\n    )\n  )\n  (decoder_attn): LocalMHA(\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n    (attn_fn): LocalAttention(\n      (dropout): Dropout(p=0.0, inplace=False)\n      (rel_pos): SinusoidalEmbeddings()\n    )\n    (to_out): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (decoder): Sequential(\n    (0): CausalConv1d(\n      (conv): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n    )\n    (1): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (2): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(256, 128, kernel_size=(10,), stride=(5,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (3): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (4): Sequential(\n      (0): CausalConvTranspose1d(\n        (conv): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,))\n      )\n      (1): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (2): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(3,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n      (3): Residual(\n        (fn): Sequential(\n          (0): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(9,))\n          )\n          (1): ELU(alpha=1.0)\n          (2): CausalConv1d(\n            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n          )\n          (3): ELU(alpha=1.0)\n        )\n      )\n    )\n    (5): CausalConv1d(\n      (conv): Conv1d(32, 1, kernel_size=(7,), stride=(1,))\n    )\n  )\n  (discriminators): ModuleList(\n    (0): MultiScaleDiscriminator(\n      (init_conv): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n      (conv_layers): ModuleList(\n        (0): Sequential(\n          (0): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (1): Sequential(\n          (0): Conv1d(64, 256, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (2): Sequential(\n          (0): Conv1d(256, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (3): Sequential(\n          (0): Conv1d(1024, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n      )\n      (final_conv): Sequential(\n        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,))\n        (1): LeakyReLU(negative_slope=0.1)\n        (2): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (1): MultiScaleDiscriminator(\n      (init_conv): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n      (conv_layers): ModuleList(\n        (0): Sequential(\n          (0): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (1): Sequential(\n          (0): Conv1d(64, 256, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (2): Sequential(\n          (0): Conv1d(256, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (3): Sequential(\n          (0): Conv1d(1024, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n      )\n      (final_conv): Sequential(\n        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,))\n        (1): LeakyReLU(negative_slope=0.1)\n        (2): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n      )\n    )\n    (2): MultiScaleDiscriminator(\n      (init_conv): Conv1d(1, 16, kernel_size=(7,), stride=(1,))\n      (conv_layers): ModuleList(\n        (0): Sequential(\n          (0): Conv1d(16, 64, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (1): Sequential(\n          (0): Conv1d(64, 256, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (2): Sequential(\n          (0): Conv1d(256, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n        (3): Sequential(\n          (0): Conv1d(1024, 1024, kernel_size=(8,), stride=(4,), padding=(4,), groups=4)\n          (1): LeakyReLU(negative_slope=0.1)\n        )\n      )\n      (final_conv): Sequential(\n        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,))\n        (1): LeakyReLU(negative_slope=0.1)\n        (2): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n      )\n    )\n  )\n  (stft_discriminator): STFTDiscriminator(\n    (init_conv): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n    (layers): ModuleList(\n      (0): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(32, 32, kernel_size=(3, 4), stride=(1, 2), padding=(1, 2))\n      )\n      (1): Sequential(\n        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n      )\n      (2): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(64, 128, kernel_size=(3, 4), stride=(1, 2), padding=(1, 2))\n      )\n      (3): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n      )\n      (4): Sequential(\n        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(128, 256, kernel_size=(3, 4), stride=(1, 2), padding=(1, 2))\n      )\n      (5): Sequential(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ModReLU()\n        (2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n      )\n    )\n    (final_conv): Conv2d(256, 1, kernel_size=(16, 1), stride=(1, 1))\n  )\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "soundstream.load(\"content/soundstream/results3/soundstream.9000.pt\")\n",
    "soundstream.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-----------------------------------------------------------] 1.3% ...generating encoding\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundstream_lstm/LSTM_dataset.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensors.append(torch.nn.functional.pad(torch.tensor(data).to(\"cuda\"), (0, 400 - data.shape[1], 0, 0)).detach())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...generating encoding\r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from importlib import reload\n",
    "import network_models.soundstream_lstm.LSTM_dataset as lds\n",
    "import torch.utils.data as data_utils\n",
    "#from network_models.soundstream_lstm.LSTM_dataset  import AudioEmotionTessDataset\n",
    "reload(lds)\n",
    "\n",
    "\n",
    "tess_dataset = lds.AudioEmotionTessDataset(\"../tess\")\n",
    "\n",
    "encoded_dataset = lds.AudioEmotionTessSoundStreamEncodedDataset(dataSet= tess_dataset, soundStream=soundstream)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 9.77 GiB total capacity; 3.74 GiB already allocated; 1.17 GiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m datacollator\u001B[38;5;241m.\u001B[39mSoundStreamDataCollator(\u001B[38;5;241m200\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# print(encoded_dataset.__getitem__(3).shape)\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43msm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmotionClassifierSevenEmos\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(encoded_dataset\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;241m3\u001B[39m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     18\u001B[0m tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mpad(tensor, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m-\u001B[39m tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    983\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    984\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 987\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:639\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 639\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    642\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    643\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    644\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    649\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    650\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:662\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    661\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 662\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    663\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    664\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:985\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m    983\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    984\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m--> 985\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 9.77 GiB total capacity; 3.74 GiB already allocated; 1.17 GiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_lstm\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundstream_lstm.small_model as sm\n",
    "importlib.reload(sm)\n",
    "\n",
    "\n",
    "import network_models.soundstream_lstm.soundstream_datacollator as datacollator\n",
    "importlib.reload(datacollator)\n",
    "data_collator = datacollator.SoundStreamDataCollator(200)\n",
    "\n",
    "# print(encoded_dataset.__getitem__(3).shape)\n",
    "model = sm.EmotionClassifierSevenEmos().to(\"cuda\")\n",
    "tensor = torch.tensor(encoded_dataset.__getitem__(3)[0])\n",
    "tensor = torch.nn.functional.pad(tensor, (0, 200 - tensor.shape[1], 0, 0))\n",
    "tensor = tensor.flatten()\n",
    "print(tensor[0].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from network_models.soundstream_lstm.soundstream_datacollator import SoundstreamModelTrainer\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "\n",
    "is_regression = False\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    trainSet = Subset(dataset, train_idx)\n",
    "    valSet = Subset(dataset, val_idx)\n",
    "    return trainSet, valSet\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=0)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "\n",
    "train_dataset, eval_dataset = train_val_dataset(encoded_dataset, val_split=0.25)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"content/soundStreamClassModelsEmo\",\n",
    "    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=5.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "trainer = SoundstreamModelTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "#to override the collate_fn in dataloader because we have different sizes\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def pad_inputs(batch):\n",
    "    input_batch = [torch.nn.functional.pad(torch.tensor(input_val).to(\"cuda\"),\n",
    "                        (0, 400 - input_val.shape[1], 0, 0))\n",
    "                       for input_val in batch]\n",
    "    return torch.stack((input_batch[0], input_batch[1]))\n",
    "\n",
    "def pad_labels(batch):\n",
    "    enc_list=[]\n",
    "    for x in batch:\n",
    "        a = np.zeros(7,)\n",
    "        a[x] = 1\n",
    "        enc_list.append(a)\n",
    "    input_labels = [torch.tensor(label, dtype=torch.float32).to(\"cuda\")\n",
    "                    for label in enc_list]\n",
    "\n",
    "    return torch.stack((input_labels[0], input_labels[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(train_dataset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "evalLoader = torch.utils.data.DataLoader(eval_dataset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "train_dataset[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 9.77 GiB total capacity; 3.74 GiB already allocated; 1.17 GiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), labels\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#print(loss)\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# print statistics\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 9.77 GiB total capacity; 3.74 GiB already allocated; 1.17 GiB free; 5.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainLoader, 0):\n",
    "        #get input values and labels from the batch of the dataloaded\n",
    "        # print(data)\n",
    "        inputs, labels = data[0].to(\"cuda\"), data[1].to(\"cuda\")\n",
    "\n",
    "        # labels = data[1]\n",
    "        #print(labels)\n",
    "        #pad inputs\n",
    "        # padded_inputs = pad_inputs(inputs)\n",
    "        padded_labels = pad_labels(labels)\n",
    "        #print(padded_inputs)\n",
    "        #print(padded_labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs.to(torch.float32), labels.to(torch.float32))\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # valid_loss = 0.0\n",
    "        #\n",
    "        # for i, data in enumerate(evalLoader, 0):\n",
    "        #     inputs= data[0]\n",
    "        #     labels = data[1]\n",
    "        #     padded_inputs = pad_inputs(inputs)\n",
    "        #     padded_labels = pad_labels(labels)\n",
    "        #\n",
    "        #     outputs = model(padded_inputs)\n",
    "        #     loss = criterion(outputs, padded_labels)\n",
    "        #     valid_loss = loss.item() * padded_inputs.size(0)\n",
    "        #\n",
    "        # print(valid_loss)\n",
    "\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
