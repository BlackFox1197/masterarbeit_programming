{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 14:55:05.581180: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-17 14:55:06.179692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:55:06.179746: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-17 14:55:06.179751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from network_models.soundsream_models_and_utils.mapping_down.ss_direct_downmapping import SS_Direct_Downmapping_Model\n",
    "from network_models.soundsream_models_and_utils.encoder.ss_encoder_downmapping import EncoderDownmapping\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/downmapping/0_0216/\"\n",
    "epochs =200\n",
    "save_every = 40\n",
    "start_lr = 1e-3\n",
    "gc.collect()\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=device)\n",
    "\n",
    "model = SS_Direct_Downmapping_Model(dropout=0.2, output=1024, start_dim=512 * 175).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.947770  [    0/ 6092]\n",
      "loss: 1.935009  [ 1400/ 6092]\n",
      "loss: 1.941308  [ 2800/ 6092]\n",
      "loss: 1.939478  [ 4200/ 6092]\n",
      "loss: 1.936070  [ 5600/ 6092]\n",
      "tensor([[0.1517, 0.1484, 0.1509, 0.1560, 0.1145, 0.1300, 0.1485],\n",
      "        [0.1250, 0.1775, 0.1302, 0.1239, 0.1553, 0.1414, 0.1467],\n",
      "        [0.1706, 0.1192, 0.1668, 0.1733, 0.1026, 0.1264, 0.1411],\n",
      "        [0.1736, 0.1208, 0.1636, 0.1717, 0.1032, 0.1251, 0.1419],\n",
      "        [0.1230, 0.1553, 0.1195, 0.1175, 0.1853, 0.1587, 0.1406],\n",
      "        [0.1284, 0.1371, 0.1289, 0.1289, 0.1895, 0.1464, 0.1408],\n",
      "        [0.1648, 0.1319, 0.1639, 0.1603, 0.1090, 0.1254, 0.1446]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.932549  [    0/ 6092]\n",
      "loss: 1.933104  [ 1400/ 6092]\n",
      "loss: 1.929704  [ 2800/ 6092]\n",
      "loss: 1.915962  [ 4200/ 6092]\n",
      "loss: 1.914993  [ 5600/ 6092]\n",
      "tensor([[0.1348, 0.1049, 0.2008, 0.1290, 0.1132, 0.1699, 0.1473],\n",
      "        [0.1289, 0.1743, 0.1299, 0.1327, 0.1393, 0.1640, 0.1311],\n",
      "        [0.1348, 0.1007, 0.2042, 0.1312, 0.1104, 0.1624, 0.1564],\n",
      "        [0.1414, 0.1588, 0.1249, 0.1421, 0.1532, 0.1339, 0.1457],\n",
      "        [0.1573, 0.1720, 0.0921, 0.1602, 0.1884, 0.0960, 0.1339],\n",
      "        [0.1500, 0.1453, 0.1359, 0.1559, 0.1446, 0.1339, 0.1343],\n",
      "        [0.1413, 0.1784, 0.1101, 0.1414, 0.1630, 0.1303, 0.1356]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.932438  [    0/ 6092]\n",
      "loss: 1.941607  [ 1400/ 6092]\n",
      "loss: 1.941603  [ 2800/ 6092]\n",
      "loss: 1.930814  [ 4200/ 6092]\n",
      "loss: 1.925408  [ 5600/ 6092]\n",
      "tensor([[0.1650, 0.1363, 0.1614, 0.1345, 0.1166, 0.1267, 0.1595],\n",
      "        [0.1029, 0.1910, 0.0993, 0.1382, 0.1503, 0.2017, 0.1166],\n",
      "        [0.1984, 0.0979, 0.2092, 0.1189, 0.0994, 0.0917, 0.1844],\n",
      "        [0.1342, 0.1484, 0.1299, 0.1685, 0.1589, 0.1471, 0.1131],\n",
      "        [0.0979, 0.1678, 0.0947, 0.1625, 0.1991, 0.1751, 0.1029],\n",
      "        [0.0964, 0.1858, 0.0962, 0.1477, 0.1605, 0.2085, 0.1050],\n",
      "        [0.1984, 0.0987, 0.2055, 0.1147, 0.1013, 0.0913, 0.1901]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.921391  [    0/ 6092]\n",
      "loss: 1.909735  [ 1400/ 6092]\n",
      "loss: 1.925533  [ 2800/ 6092]\n",
      "loss: 1.920508  [ 4200/ 6092]\n",
      "loss: 1.929065  [ 5600/ 6092]\n",
      "tensor([[0.1013, 0.1555, 0.1716, 0.1343, 0.1714, 0.1722, 0.0938],\n",
      "        [0.1277, 0.1346, 0.1726, 0.1300, 0.1524, 0.1654, 0.1173],\n",
      "        [0.1730, 0.1501, 0.1297, 0.1494, 0.1155, 0.1295, 0.1529],\n",
      "        [0.1981, 0.1332, 0.1088, 0.1483, 0.1014, 0.1106, 0.1996],\n",
      "        [0.0932, 0.1496, 0.1757, 0.1325, 0.1872, 0.1695, 0.0922],\n",
      "        [0.1480, 0.1228, 0.1640, 0.1314, 0.1450, 0.1522, 0.1366],\n",
      "        [0.1966, 0.1035, 0.1143, 0.1380, 0.1168, 0.1112, 0.2194]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.950853  [    0/ 6092]\n",
      "loss: 1.940311  [ 1400/ 6092]\n",
      "loss: 1.922107  [ 2800/ 6092]\n",
      "loss: 1.926304  [ 4200/ 6092]\n",
      "loss: 1.914780  [ 5600/ 6092]\n",
      "tensor([[0.1890, 0.1395, 0.1549, 0.1045, 0.1440, 0.1106, 0.1575],\n",
      "        [0.1010, 0.1456, 0.1439, 0.1670, 0.1561, 0.1894, 0.0971],\n",
      "        [0.1753, 0.1298, 0.1226, 0.1338, 0.1156, 0.1061, 0.2169],\n",
      "        [0.1596, 0.1335, 0.1198, 0.1508, 0.1146, 0.1115, 0.2104],\n",
      "        [0.1222, 0.1449, 0.1684, 0.1235, 0.1770, 0.1785, 0.0857],\n",
      "        [0.1008, 0.1490, 0.1463, 0.1639, 0.1573, 0.1897, 0.0930],\n",
      "        [0.1162, 0.1461, 0.1263, 0.1837, 0.1288, 0.1530, 0.1458]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.917571  [    0/ 6092]\n",
      "loss: 1.922548  [ 1400/ 6092]\n",
      "loss: 1.903714  [ 2800/ 6092]\n",
      "loss: 1.924939  [ 4200/ 6092]\n",
      "loss: 1.920421  [ 5600/ 6092]\n",
      "tensor([[0.1423, 0.1585, 0.1006, 0.1641, 0.1647, 0.1685, 0.1013],\n",
      "        [0.1378, 0.1196, 0.2165, 0.1095, 0.1074, 0.1031, 0.2060],\n",
      "        [0.1301, 0.1163, 0.2205, 0.1212, 0.1003, 0.0985, 0.2129],\n",
      "        [0.1220, 0.1157, 0.2023, 0.1384, 0.1086, 0.1075, 0.2055],\n",
      "        [0.1313, 0.1545, 0.0894, 0.1720, 0.1754, 0.1820, 0.0953],\n",
      "        [0.1307, 0.1206, 0.2084, 0.1222, 0.1080, 0.1060, 0.2041],\n",
      "        [0.1261, 0.1134, 0.2220, 0.1213, 0.1013, 0.0988, 0.2171]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.920115  [    0/ 6092]\n",
      "loss: 1.909970  [ 1400/ 6092]\n",
      "loss: 1.924098  [ 2800/ 6092]\n",
      "loss: 1.919549  [ 4200/ 6092]\n",
      "loss: 1.910600  [ 5600/ 6092]\n",
      "tensor([[0.1282, 0.0882, 0.1996, 0.1964, 0.1215, 0.1258, 0.1402],\n",
      "        [0.1516, 0.2102, 0.0888, 0.0934, 0.1673, 0.1552, 0.1335],\n",
      "        [0.1662, 0.1459, 0.1535, 0.1380, 0.1405, 0.1372, 0.1188],\n",
      "        [0.1179, 0.0895, 0.1944, 0.1971, 0.1197, 0.1290, 0.1524],\n",
      "        [0.1574, 0.2138, 0.0995, 0.0981, 0.1557, 0.1461, 0.1294],\n",
      "        [0.1601, 0.2161, 0.0958, 0.0948, 0.1591, 0.1486, 0.1256],\n",
      "        [0.1190, 0.1252, 0.1477, 0.1592, 0.1341, 0.1408, 0.1740]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0', dtype=torch.float64)\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.913604  [    0/ 6092]\n",
      "loss: 1.935641  [ 1400/ 6092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m enc_trainer \u001B[38;5;241m=\u001B[39m sset\u001B[38;5;241m.\u001B[39mSSDirectDMTrainer(batch_size\u001B[38;5;241m=\u001B[39mbatch_size, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model_path\u001B[38;5;241m=\u001B[39mmodels_dir, save_model_every\u001B[38;5;241m=\u001B[39msave_every, lr\u001B[38;5;241m=\u001B[39mstart_lr, dataset\u001B[38;5;241m=\u001B[39mdata_set, device\u001B[38;5;241m=\u001B[39mdevice, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m     12\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m---> 13\u001B[0m \u001B[43menc_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/mapping_down/ss_driect_dm_trainer.py:56\u001B[0m, in \u001B[0;36mSSDirectDMTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     54\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/mapping_down/ss_driect_dm_trainer.py:62\u001B[0m, in \u001B[0;36mSSDirectDMTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_loop\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataloader, model, loss_fn, optimizer):\n\u001B[1;32m     61\u001B[0m     size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, z) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m     63\u001B[0m         z \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     64\u001B[0m         X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m   1108\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1109\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1117\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1118\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1119\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1120\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1121\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1122\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1123\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1124\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1125\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/queues.py:122\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    120\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rlock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# unserialize the data after having released the lock\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ForkingPickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:100\u001B[0m, in \u001B[0;36mrebuild_tensor\u001B[0;34m(cls, storage, metadata)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrebuild_tensor\u001B[39m(\u001B[38;5;28mcls\u001B[39m, storage, metadata):\n\u001B[1;32m     99\u001B[0m     storage_offset, size, stride, requires_grad \u001B[38;5;241m=\u001B[39m metadata\n\u001B[0;32m--> 100\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rebuild_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mparameter\u001B[38;5;241m.\u001B[39mParameter:\n\u001B[1;32m    102\u001B[0m         \u001B[38;5;66;03m# we have to pass requires_grad into constructor, rather than set it as an\u001B[39;00m\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;66;03m# attribute later, because it's an important check for Integer Tensors to\u001B[39;00m\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;66;03m# have requires_grad=False (or else they raise an error)\u001B[39;00m\n\u001B[1;32m    105\u001B[0m         t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mparameter\u001B[38;5;241m.\u001B[39mParameter(t, requires_grad\u001B[38;5;241m=\u001B[39mrequires_grad)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_utils.py:144\u001B[0m, in \u001B[0;36m_rebuild_tensor\u001B[0;34m(storage, storage_offset, size, stride)\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masync\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# Note [Don't serialize hooks]\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# Since time immemorial, we have serialized the backward hooks associated with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, `storage` no longer needs to\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;66;03m# be a TypedStorage\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_rebuild_tensor\u001B[39m(storage, storage_offset, size, stride):\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;66;03m# first construct a tensor with the correct dtype/device\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([], dtype\u001B[38;5;241m=\u001B[39mstorage\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mstorage\u001B[38;5;241m.\u001B[39muntyped()\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mset_(storage\u001B[38;5;241m.\u001B[39muntyped(), storage_offset, size, stride)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils/encoder\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundsream_models_and_utils.mapping_down.ss_driect_dm_trainer as sset\n",
    "importlib.reload(sset)\n",
    "\n",
    "\n",
    "enc_trainer = sset.SSDirectDMTrainer(batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=model)\n",
    "gc.collect()\n",
    "enc_trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n"
     ]
    }
   ],
   "source": [
    "# from network_models.soundsream_models_and_utils.ss___util_class_batches_sampler import ClassBatchesSampler\n",
    "# np1 = data_set.encoded_dataset.encodedData[data_set.encoded_dataset.labelcolumn].to_numpy()\n",
    "# from torch.utils.data import DataLoader\n",
    "# #\n",
    "# # l = [np1[i][0] for i in range(len(np1))]\n",
    "# #\n",
    "# #\n",
    "# dl = DataLoader(data_set, batch_sampler=ClassBatchesSampler(np1, num_class_samples=2))\n",
    "# #\n",
    "# #\n",
    "# for batch, (X, z) in enumerate(dl):\n",
    "#\n",
    "#     print(torch.tensor_split(torch.argmax(z, dim=1), 2))\n",
    "    # z1, z2 = torch.tensor_split(z, 2)\n",
    "    # pred = model(X.cuda())\n",
    "    # pred1, pred2 = torch.tensor_split(pred, 2)\n",
    "    # dp = ((torch.matmul(pred1, pred2.T) / (pred1.norm(dim=-1, keepdim=True) * pred2.norm(dim=-1, keepdim=True)))+1)/2\n",
    "    #dp = torch.matmul(z1, z2.T) / (z1.norm(dim=-1, keepdim=True) * z2.norm(dim=-1, keepdim=True))\n",
    "    # print(dp)\n",
    "    # print(batch)\n",
    "\n",
    "    # print(X[0][0][0][170])\n",
    "\n",
    "    # if batch == 1:\n",
    "    #     break\n",
    "\n",
    "#(len(dl)/(7*2))-2 == 462"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
