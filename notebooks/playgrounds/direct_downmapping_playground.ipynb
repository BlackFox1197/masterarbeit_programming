{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:09:27.034335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 16:09:27.590347: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:09:27.590397: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:09:27.590401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_direct_downmapping import SS_Direct_Downmapping_Model\n",
    "import gc\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils/encoder\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "import network_models.clip.models.ss_encoder_downmapping as ed\n",
    "\n",
    "importlib.reload(ed)\n",
    "\n",
    "#encoder = Encoder(vocab_size=1024, embed_dim=512, n_heads=1, ff_dim=0, n_layers=2, dropout=0)\n",
    "encoder = ed.EncoderDownmapping(embed_dim=512, n_heads=4, ff_dim=2, n_layers=1, dropout=0, output=1024,\n",
    "                                max_seq_len=175).to(device)\n",
    "\n",
    "gc.collect()\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "data_set = ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings.pkl\",\n",
    "    device=\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "model = SS_Direct_Downmapping_Model(1024, 512*175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 2, 3, 0, 0, 5, 1, 0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "indices = [1, 155, 12, 2, 30, 60, 20, 90, 30, 40, 900]\n",
    "\n",
    "batch = torch.stack([ data_set[155][0][0], data_set[0][0][0], data_set[30][0][0], data_set[90][0][0]])\n",
    "batch2 = torch.stack([ data_set[1][0][0], data_set[80][0][0], data_set[2][0][0], data_set[1][0][0]])\n",
    "\n",
    "batch_b = torch.stack([data_set[i][0][0] for i in indices])\n",
    "batch_emo = [np.argmax(data_set[i][1]) for  i in indices]\n",
    "batch_emo_ohe = np.argmax([data_set[i][1] for i in indices], axis=1)\n",
    "\n",
    "print(batch_emo)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 1 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 1 1 0 0 1 0 0]\n",
      " [0 1 0 0 1 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 1 0 0 1 1 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.6315, 0.2639, 0.5309, 0.4868], grad_fn=<SumBackward1>)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = model(batch)\n",
    "x2 = model(batch2)\n",
    "\n",
    "x_new = model(batch_b)\n",
    "\n",
    "def cosine_matr(x):\n",
    "    mul1 = torch.matmul(x, x.T)\n",
    "    mul1 = mul1/(x.norm(dim=-1, keepdim=True) * x.norm(dim=-1, keepdim=True))\n",
    "    return mul1\n",
    "\n",
    "mul1 = torch.matmul(x1, x1.T)\n",
    "mul1 = mul1/(x1.norm(dim=-1, keepdim=True) * x1.norm(dim=-1, keepdim=True))\n",
    "#print(mul1)\n",
    "#print(torch.softmax(mul1, dim=1))\n",
    "\n",
    "#print(x1.norm(dim=-1, keepdim=True))\n",
    "\n",
    "#x1 = x1 / x1.norm(dim=-1, keepdim=True)\n",
    "x2 = x2 / x2.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#print(cosine_matr(x_new))\n",
    "print(np.equal.outer(batch_emo_ohe, batch_emo_ohe)*1)\n",
    "#   print((np.equal.outer(batch_emo, batch_emo)*2)-1)\n",
    "\n",
    "\n",
    "torch.cosine_similarity(x1, x2, dim=1)\n",
    "# print(x1.flip(dims=[0]))\n",
    "# print(x1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1.1257, -0.1272,  0.3292,  ..., -1.2398,  1.5788,  0.5068],\n         [-0.1415, -0.5052,  1.2873,  ..., -0.7325,  0.8649,  0.2101]],\n        grad_fn=<SplitBackward0>),\n tensor([[ 1.5992,  0.3381,  2.0955,  ...,  0.2265,  1.1394, -0.2271],\n         [ 0.8858,  0.3328,  1.2692,  ..., -0.4881,  1.2690,  1.1370]],\n        grad_fn=<SplitBackward0>))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.split(split_size=len(x1)//2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks_clip/content/datasets/soundstream_encoded/allEncodings_noInducednoStimuli_0_2_sec_v12_1_basic.pkl\", device=\"cuda\")\n",
    "# data_set= ss_encoded_dataset_full(\n",
    "#     csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/encoder_datasets/full_encodings_with_umap.pkl\", device=\"cuda\", umap=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 512, 10])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.encoded_dataset.encodedData[\"encoded\"].iloc[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "ds= data_set.encoded_dataset.encodedData[\"pca\"].to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npds = np.asarray([fp.numpy() for fp in ds])\n",
    "searchVec = np.asarray([0.972153, 0.208038, -0.180692, -0.192047])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "kdt = KDTree(npds, metric='euclidean')\n",
    "cv = kdt.query(np.atleast_2d(searchVec), k=1, return_distance=False)[0,0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "5505"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
