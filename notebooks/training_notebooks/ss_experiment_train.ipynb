{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 17:35:29 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-02-08 17:35:29.208113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 17:35:29.698812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 17:35:29.698863: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 17:35:29.698867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 6\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/nr2\"\n",
    "trials_per_model_type = 4\n",
    "epochs_per_model = 501\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.347403  [    0/ 5482]\n",
      "loss: 1.841355  [  600/ 5482]\n",
      "loss: 2.040296  [ 1200/ 5482]\n",
      "loss: 1.979437  [ 1800/ 5482]\n",
      "loss: 1.946964  [ 2400/ 5482]\n",
      "loss: 1.896968  [ 3000/ 5482]\n",
      "loss: 1.798804  [ 3600/ 5482]\n",
      "loss: 1.878263  [ 4200/ 5482]\n",
      "loss: 1.985994  [ 4800/ 5482]\n",
      "loss: 1.891570  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.198     0.000     0.000    99\n",
      " disgust     0.198     0.163     0.710    107\n",
      "    fear     0.198     0.000     0.000    80\n",
      "   happy     0.198     0.000     0.000    77\n",
      " neutral     0.198     0.000     0.000    95\n",
      "     sad     0.198     0.299     0.440    91\n",
      "surprise     0.198     0.455     0.082    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.198     0.131     0.176    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 19.8%, Avg loss: 1.919933 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.882594  [    0/ 5482]\n",
      "loss: 1.792002  [  600/ 5482]\n",
      "loss: 1.994157  [ 1200/ 5482]\n",
      "loss: 2.057290  [ 1800/ 5482]\n",
      "loss: 1.705474  [ 2400/ 5482]\n",
      "loss: 1.612664  [ 3000/ 5482]\n",
      "loss: 1.603868  [ 3600/ 5482]\n",
      "loss: 1.697765  [ 4200/ 5482]\n",
      "loss: 1.758464  [ 4800/ 5482]\n",
      "loss: 1.945075  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.298     0.000     0.000    99\n",
      " disgust     0.298     0.206     0.626    107\n",
      "    fear     0.298     0.000     0.000    80\n",
      "   happy     0.298     0.000     0.000    77\n",
      " neutral     0.298     0.476     0.516    95\n",
      "     sad     0.298     0.455     0.495    91\n",
      "surprise     0.298     0.253     0.344    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.298     0.198     0.283    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 29.8%, Avg loss: 1.776320 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.809200  [    0/ 5482]\n",
      "loss: 1.757795  [  600/ 5482]\n",
      "loss: 1.800181  [ 1200/ 5482]\n",
      "loss: 1.925928  [ 1800/ 5482]\n",
      "loss: 1.368535  [ 2400/ 5482]\n",
      "loss: 1.631429  [ 3000/ 5482]\n",
      "loss: 1.523561  [ 3600/ 5482]\n",
      "loss: 1.773596  [ 4200/ 5482]\n",
      "loss: 1.796401  [ 4800/ 5482]\n",
      "loss: 1.815476  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.362     0.000     0.000    99\n",
      " disgust     0.362     0.329     0.645    107\n",
      "    fear     0.362     0.176     0.075    80\n",
      "   happy     0.362     0.000     0.000    77\n",
      " neutral     0.362     0.671     0.516    95\n",
      "     sad     0.362     0.451     0.659    91\n",
      "surprise     0.362     0.231     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.362     0.266     0.357    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 1.722598 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.008870  [    0/ 5482]\n",
      "loss: 1.654145  [  600/ 5482]\n",
      "loss: 1.555395  [ 1200/ 5482]\n",
      "loss: 1.847351  [ 1800/ 5482]\n",
      "loss: 1.724511  [ 2400/ 5482]\n",
      "loss: 1.708445  [ 3000/ 5482]\n",
      "loss: 1.782477  [ 3600/ 5482]\n",
      "loss: 1.686339  [ 4200/ 5482]\n",
      "loss: 1.542020  [ 4800/ 5482]\n",
      "loss: 1.658682  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.407     0.000     0.000    99\n",
      " disgust     0.407     0.366     0.692    107\n",
      "    fear     0.407     0.406     0.350    80\n",
      "   happy     0.407     0.204     0.143    77\n",
      " neutral     0.407     0.550     0.632    95\n",
      "     sad     0.407     0.479     0.495    91\n",
      "surprise     0.407     0.366     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.407     0.339     0.400    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.675030 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.619801  [    0/ 5482]\n",
      "loss: 2.077918  [  600/ 5482]\n",
      "loss: 1.781791  [ 1200/ 5482]\n",
      "loss: 1.516718  [ 1800/ 5482]\n",
      "loss: 1.719288  [ 2400/ 5482]\n",
      "loss: 1.431926  [ 3000/ 5482]\n",
      "loss: 1.621939  [ 3600/ 5482]\n",
      "loss: 1.621237  [ 4200/ 5482]\n",
      "loss: 1.783067  [ 4800/ 5482]\n",
      "loss: 1.422934  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.387     0.000     0.000    99\n",
      " disgust     0.387     0.553     0.393    107\n",
      "    fear     0.387     0.116     0.062    80\n",
      "   happy     0.387     0.543     0.247    77\n",
      " neutral     0.387     0.537     0.695    95\n",
      "     sad     0.387     0.483     0.637    91\n",
      "surprise     0.387     0.216     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.387     0.350     0.398    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.639130 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.724225  [    0/ 5482]\n",
      "loss: 1.767203  [  600/ 5482]\n",
      "loss: 1.661923  [ 1200/ 5482]\n",
      "loss: 1.407940  [ 1800/ 5482]\n",
      "loss: 1.522352  [ 2400/ 5482]\n",
      "loss: 1.185884  [ 3000/ 5482]\n",
      "loss: 1.591454  [ 3600/ 5482]\n",
      "loss: 2.030441  [ 4200/ 5482]\n",
      "loss: 1.463096  [ 4800/ 5482]\n",
      "loss: 1.381057  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.469     0.000     0.000    99\n",
      " disgust     0.469     0.578     0.626    107\n",
      "    fear     0.469     0.438     0.487    80\n",
      "   happy     0.469     0.406     0.364    77\n",
      " neutral     0.469     0.472     0.705    95\n",
      "     sad     0.469     0.509     0.615    91\n",
      "surprise     0.469     0.345     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.469     0.393     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 1.587750 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.486955  [    0/ 5482]\n",
      "loss: 1.332410  [  600/ 5482]\n",
      "loss: 1.609191  [ 1200/ 5482]\n",
      "loss: 1.484472  [ 1800/ 5482]\n",
      "loss: 1.457036  [ 2400/ 5482]\n",
      "loss: 1.387564  [ 3000/ 5482]\n",
      "loss: 1.741995  [ 3600/ 5482]\n",
      "loss: 1.293246  [ 4200/ 5482]\n",
      "loss: 1.404730  [ 4800/ 5482]\n",
      "loss: 1.438844  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.000     0.000    99\n",
      " disgust     0.484     0.558     0.542    107\n",
      "    fear     0.484     0.473     0.550    80\n",
      "   happy     0.484     0.452     0.494    77\n",
      " neutral     0.484     0.520     0.684    95\n",
      "     sad     0.484     0.528     0.615    91\n",
      "surprise     0.484     0.347     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.411     0.492    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.547150 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.434960  [    0/ 5482]\n",
      "loss: 1.428402  [  600/ 5482]\n",
      "loss: 1.173600  [ 1200/ 5482]\n",
      "loss: 1.553119  [ 1800/ 5482]\n",
      "loss: 1.409025  [ 2400/ 5482]\n",
      "loss: 1.276195  [ 3000/ 5482]\n",
      "loss: 1.557977  [ 3600/ 5482]\n",
      "loss: 1.232932  [ 4200/ 5482]\n",
      "loss: 1.411746  [ 4800/ 5482]\n",
      "loss: 1.252110  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.833     0.051    99\n",
      " disgust     0.489     0.567     0.514    107\n",
      "    fear     0.489     0.446     0.512    80\n",
      "   happy     0.489     0.392     0.519    77\n",
      " neutral     0.489     0.567     0.716    95\n",
      "     sad     0.489     0.517     0.670    91\n",
      "surprise     0.489     0.373     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.528     0.492    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.525131 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.020012  [    0/ 5482]\n",
      "loss: 1.218827  [  600/ 5482]\n",
      "loss: 1.082524  [ 1200/ 5482]\n",
      "loss: 1.673024  [ 1800/ 5482]\n",
      "loss: 1.060054  [ 2400/ 5482]\n",
      "loss: 1.622274  [ 3000/ 5482]\n",
      "loss: 1.206018  [ 3600/ 5482]\n",
      "loss: 1.232670  [ 4200/ 5482]\n",
      "loss: 1.526824  [ 4800/ 5482]\n",
      "loss: 1.477261  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.497     0.900     0.091    99\n",
      " disgust     0.497     0.607     0.607    107\n",
      "    fear     0.497     0.506     0.537    80\n",
      "   happy     0.497     0.342     0.519    77\n",
      " neutral     0.497     0.632     0.632    95\n",
      "     sad     0.497     0.480     0.670    91\n",
      "surprise     0.497     0.362     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.497     0.547     0.495    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.505297 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.116753  [    0/ 5482]\n",
      "loss: 1.162900  [  600/ 5482]\n",
      "loss: 1.355589  [ 1200/ 5482]\n",
      "loss: 0.990044  [ 1800/ 5482]\n",
      "loss: 1.401699  [ 2400/ 5482]\n",
      "loss: 1.436458  [ 3000/ 5482]\n",
      "loss: 1.309682  [ 3600/ 5482]\n",
      "loss: 1.553729  [ 4200/ 5482]\n",
      "loss: 1.403735  [ 4800/ 5482]\n",
      "loss: 1.429563  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.875     0.212    99\n",
      " disgust     0.533     0.596     0.607    107\n",
      "    fear     0.533     0.505     0.575    80\n",
      "   happy     0.533     0.421     0.416    77\n",
      " neutral     0.533     0.619     0.737    95\n",
      "     sad     0.533     0.509     0.648    91\n",
      "surprise     0.533     0.395     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.560     0.531    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.448709 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.582244  [    0/ 5482]\n",
      "loss: 1.038920  [  600/ 5482]\n",
      "loss: 1.541192  [ 1200/ 5482]\n",
      "loss: 1.205208  [ 1800/ 5482]\n",
      "loss: 1.162065  [ 2400/ 5482]\n",
      "loss: 1.229644  [ 3000/ 5482]\n",
      "loss: 1.386273  [ 3600/ 5482]\n",
      "loss: 1.056838  [ 4200/ 5482]\n",
      "loss: 1.400277  [ 4800/ 5482]\n",
      "loss: 0.785579  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.528     0.690     0.293    99\n",
      " disgust     0.528     0.724     0.514    107\n",
      "    fear     0.528     0.469     0.562    80\n",
      "   happy     0.528     0.398     0.455    77\n",
      " neutral     0.528     0.684     0.684    95\n",
      "     sad     0.528     0.564     0.681    91\n",
      "surprise     0.528     0.301     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.528     0.547     0.528    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.472181 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.310557  [    0/ 5482]\n",
      "loss: 0.809140  [  600/ 5482]\n",
      "loss: 1.180704  [ 1200/ 5482]\n",
      "loss: 0.845799  [ 1800/ 5482]\n",
      "loss: 1.148483  [ 2400/ 5482]\n",
      "loss: 1.038503  [ 3000/ 5482]\n",
      "loss: 1.368078  [ 3600/ 5482]\n",
      "loss: 0.970724  [ 4200/ 5482]\n",
      "loss: 1.638841  [ 4800/ 5482]\n",
      "loss: 0.952699  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.714     0.253    99\n",
      " disgust     0.516     0.724     0.514    107\n",
      "    fear     0.516     0.422     0.575    80\n",
      "   happy     0.516     0.395     0.442    77\n",
      " neutral     0.516     0.741     0.632    95\n",
      "     sad     0.516     0.500     0.670    91\n",
      "surprise     0.516     0.337     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.548     0.520    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.467584 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.910275  [    0/ 5482]\n",
      "loss: 1.374829  [  600/ 5482]\n",
      "loss: 1.184609  [ 1200/ 5482]\n",
      "loss: 1.224218  [ 1800/ 5482]\n",
      "loss: 1.232825  [ 2400/ 5482]\n",
      "loss: 1.157893  [ 3000/ 5482]\n",
      "loss: 1.492552  [ 3600/ 5482]\n",
      "loss: 0.975628  [ 4200/ 5482]\n",
      "loss: 0.674329  [ 4800/ 5482]\n",
      "loss: 0.943333  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.543     0.720     0.364    99\n",
      " disgust     0.543     0.639     0.579    107\n",
      "    fear     0.543     0.457     0.537    80\n",
      "   happy     0.543     0.425     0.442    77\n",
      " neutral     0.543     0.673     0.758    95\n",
      "     sad     0.543     0.570     0.582    91\n",
      "surprise     0.543     0.348     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.543     0.548     0.539    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.458778 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.013539  [    0/ 5482]\n",
      "loss: 0.710548  [  600/ 5482]\n",
      "loss: 1.534929  [ 1200/ 5482]\n",
      "loss: 1.179322  [ 1800/ 5482]\n",
      "loss: 1.190881  [ 2400/ 5482]\n",
      "loss: 0.794212  [ 3000/ 5482]\n",
      "loss: 0.814476  [ 3600/ 5482]\n",
      "loss: 0.948476  [ 4200/ 5482]\n",
      "loss: 0.913677  [ 4800/ 5482]\n",
      "loss: 1.074302  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.538     0.651     0.414    99\n",
      " disgust     0.538     0.553     0.589    107\n",
      "    fear     0.538     0.525     0.525    80\n",
      "   happy     0.538     0.427     0.416    77\n",
      " neutral     0.538     0.584     0.695    95\n",
      "     sad     0.538     0.579     0.604    91\n",
      "surprise     0.538     0.414     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.538     0.533     0.531    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.466067 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.896228  [    0/ 5482]\n",
      "loss: 1.311755  [  600/ 5482]\n",
      "loss: 0.997855  [ 1200/ 5482]\n",
      "loss: 0.898281  [ 1800/ 5482]\n",
      "loss: 1.498968  [ 2400/ 5482]\n",
      "loss: 1.138389  [ 3000/ 5482]\n",
      "loss: 0.820249  [ 3600/ 5482]\n",
      "loss: 0.884455  [ 4200/ 5482]\n",
      "loss: 1.008647  [ 4800/ 5482]\n",
      "loss: 0.764727  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.536     0.662     0.475    99\n",
      " disgust     0.536     0.593     0.505    107\n",
      "    fear     0.536     0.514     0.463    80\n",
      "   happy     0.536     0.483     0.377    77\n",
      " neutral     0.536     0.534     0.747    95\n",
      "     sad     0.536     0.635     0.670    91\n",
      "surprise     0.536     0.322     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.536     0.535     0.528    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.450728 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.724341  [    0/ 5482]\n",
      "loss: 0.889986  [  600/ 5482]\n",
      "loss: 0.928530  [ 1200/ 5482]\n",
      "loss: 1.269450  [ 1800/ 5482]\n",
      "loss: 0.670602  [ 2400/ 5482]\n",
      "loss: 0.414294  [ 3000/ 5482]\n",
      "loss: 0.592571  [ 3600/ 5482]\n",
      "loss: 0.754167  [ 4200/ 5482]\n",
      "loss: 0.514795  [ 4800/ 5482]\n",
      "loss: 1.319775  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.531     0.606     0.434    99\n",
      " disgust     0.531     0.626     0.579    107\n",
      "    fear     0.531     0.468     0.463    80\n",
      "   happy     0.531     0.435     0.390    77\n",
      " neutral     0.531     0.657     0.684    95\n",
      "     sad     0.531     0.527     0.637    91\n",
      "surprise     0.531     0.349     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.531     0.524     0.523    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.465457 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.852691  [    0/ 5482]\n",
      "loss: 1.233482  [  600/ 5482]\n",
      "loss: 0.578538  [ 1200/ 5482]\n",
      "loss: 0.550479  [ 1800/ 5482]\n",
      "loss: 0.755218  [ 2400/ 5482]\n",
      "loss: 1.020159  [ 3000/ 5482]\n",
      "loss: 0.843381  [ 3600/ 5482]\n",
      "loss: 1.913181  [ 4200/ 5482]\n",
      "loss: 0.715680  [ 4800/ 5482]\n",
      "loss: 1.152890  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.554     0.600     0.455    99\n",
      " disgust     0.554     0.660     0.579    107\n",
      "    fear     0.554     0.489     0.562    80\n",
      "   happy     0.554     0.405     0.442    77\n",
      " neutral     0.554     0.660     0.737    95\n",
      "     sad     0.554     0.607     0.593    91\n",
      "surprise     0.554     0.400     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.554     0.546     0.547    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.453293 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.918617  [    0/ 5482]\n",
      "loss: 1.179705  [  600/ 5482]\n",
      "loss: 1.041049  [ 1200/ 5482]\n",
      "loss: 0.709326  [ 1800/ 5482]\n",
      "loss: 0.935082  [ 2400/ 5482]\n",
      "loss: 1.278592  [ 3000/ 5482]\n",
      "loss: 0.532236  [ 3600/ 5482]\n",
      "loss: 0.804314  [ 4200/ 5482]\n",
      "loss: 0.416002  [ 4800/ 5482]\n",
      "loss: 0.568770  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.509     0.545    99\n",
      " disgust     0.557     0.671     0.514    107\n",
      "    fear     0.557     0.467     0.613    80\n",
      "   happy     0.557     0.473     0.455    77\n",
      " neutral     0.557     0.778     0.663    95\n",
      "     sad     0.557     0.573     0.648    91\n",
      "surprise     0.557     0.424     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.556     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.440099 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.782819  [    0/ 5482]\n",
      "loss: 0.604873  [  600/ 5482]\n",
      "loss: 1.297656  [ 1200/ 5482]\n",
      "loss: 0.726613  [ 1800/ 5482]\n",
      "loss: 0.739497  [ 2400/ 5482]\n",
      "loss: 0.752421  [ 3000/ 5482]\n",
      "loss: 0.472686  [ 3600/ 5482]\n",
      "loss: 0.901352  [ 4200/ 5482]\n",
      "loss: 0.494577  [ 4800/ 5482]\n",
      "loss: 0.425005  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.627     0.525    99\n",
      " disgust     0.575     0.626     0.626    107\n",
      "    fear     0.575     0.594     0.512    80\n",
      "   happy     0.575     0.474     0.468    77\n",
      " neutral     0.575     0.667     0.758    95\n",
      "     sad     0.575     0.561     0.604    91\n",
      "surprise     0.575     0.406     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.565     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.442899 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.445891  [    0/ 5482]\n",
      "loss: 0.676750  [  600/ 5482]\n",
      "loss: 0.681745  [ 1200/ 5482]\n",
      "loss: 0.573640  [ 1800/ 5482]\n",
      "loss: 0.502510  [ 2400/ 5482]\n",
      "loss: 0.440524  [ 3000/ 5482]\n",
      "loss: 0.642025  [ 3600/ 5482]\n",
      "loss: 1.225297  [ 4200/ 5482]\n",
      "loss: 0.454595  [ 4800/ 5482]\n",
      "loss: 0.401148  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.561     0.533     0.566    99\n",
      " disgust     0.561     0.579     0.514    107\n",
      "    fear     0.561     0.544     0.537    80\n",
      "   happy     0.561     0.451     0.416    77\n",
      " neutral     0.561     0.651     0.726    95\n",
      "     sad     0.561     0.634     0.648    91\n",
      "surprise     0.561     0.459     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.561     0.550     0.552    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.445664 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.577275  [    0/ 5482]\n",
      "loss: 0.431546  [  600/ 5482]\n",
      "loss: 0.351013  [ 1200/ 5482]\n",
      "loss: 0.744921  [ 1800/ 5482]\n",
      "loss: 1.319293  [ 2400/ 5482]\n",
      "loss: 0.956130  [ 3000/ 5482]\n",
      "loss: 0.634208  [ 3600/ 5482]\n",
      "loss: 0.363381  [ 4200/ 5482]\n",
      "loss: 0.469714  [ 4800/ 5482]\n",
      "loss: 0.632662  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.559     0.560     0.475    99\n",
      " disgust     0.559     0.640     0.598    107\n",
      "    fear     0.559     0.462     0.525    80\n",
      "   happy     0.559     0.462     0.545    77\n",
      " neutral     0.559     0.736     0.674    95\n",
      "     sad     0.559     0.568     0.593    91\n",
      "surprise     0.559     0.452     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.559     0.554     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.499989 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.626509  [    0/ 5482]\n",
      "loss: 0.508591  [  600/ 5482]\n",
      "loss: 0.433246  [ 1200/ 5482]\n",
      "loss: 0.347982  [ 1800/ 5482]\n",
      "loss: 0.708049  [ 2400/ 5482]\n",
      "loss: 0.398311  [ 3000/ 5482]\n",
      "loss: 0.583542  [ 3600/ 5482]\n",
      "loss: 0.878463  [ 4200/ 5482]\n",
      "loss: 0.519758  [ 4800/ 5482]\n",
      "loss: 0.484854  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.570     0.582     0.535    99\n",
      " disgust     0.570     0.613     0.636    107\n",
      "    fear     0.570     0.489     0.550    80\n",
      "   happy     0.570     0.394     0.481    77\n",
      " neutral     0.570     0.800     0.716    95\n",
      "     sad     0.570     0.625     0.604    91\n",
      "surprise     0.570     0.451     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.570     0.565     0.557    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.450971 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.563377  [    0/ 5482]\n",
      "loss: 0.444925  [  600/ 5482]\n",
      "loss: 0.769497  [ 1200/ 5482]\n",
      "loss: 0.502137  [ 1800/ 5482]\n",
      "loss: 0.450436  [ 2400/ 5482]\n",
      "loss: 0.262196  [ 3000/ 5482]\n",
      "loss: 0.563790  [ 3600/ 5482]\n",
      "loss: 0.441354  [ 4200/ 5482]\n",
      "loss: 0.791821  [ 4800/ 5482]\n",
      "loss: 0.415309  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.548     0.551     0.495    99\n",
      " disgust     0.548     0.552     0.598    107\n",
      "    fear     0.548     0.458     0.550    80\n",
      "   happy     0.548     0.416     0.416    77\n",
      " neutral     0.548     0.744     0.642    95\n",
      "     sad     0.548     0.618     0.604    91\n",
      "surprise     0.548     0.475     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.548     0.545     0.540    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.553338 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.211245  [    0/ 5482]\n",
      "loss: 0.539143  [  600/ 5482]\n",
      "loss: 0.297172  [ 1200/ 5482]\n",
      "loss: 0.340890  [ 1800/ 5482]\n",
      "loss: 0.205613  [ 2400/ 5482]\n",
      "loss: 0.335546  [ 3000/ 5482]\n",
      "loss: 0.619928  [ 3600/ 5482]\n",
      "loss: 0.289259  [ 4200/ 5482]\n",
      "loss: 0.979045  [ 4800/ 5482]\n",
      "loss: 0.390044  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.561     0.528     0.566    99\n",
      " disgust     0.561     0.621     0.598    107\n",
      "    fear     0.561     0.455     0.562    80\n",
      "   happy     0.561     0.458     0.494    77\n",
      " neutral     0.561     0.723     0.632    95\n",
      "     sad     0.561     0.616     0.582    91\n",
      "surprise     0.561     0.520     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.561     0.560     0.551    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.500192 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.297355  [    0/ 5482]\n",
      "loss: 0.366263  [  600/ 5482]\n",
      "loss: 0.416808  [ 1200/ 5482]\n",
      "loss: 1.231360  [ 1800/ 5482]\n",
      "loss: 0.560461  [ 2400/ 5482]\n",
      "loss: 0.331945  [ 3000/ 5482]\n",
      "loss: 0.506564  [ 3600/ 5482]\n",
      "loss: 0.338329  [ 4200/ 5482]\n",
      "loss: 0.479304  [ 4800/ 5482]\n",
      "loss: 0.329823  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.527     0.586    99\n",
      " disgust     0.551     0.621     0.598    107\n",
      "    fear     0.551     0.444     0.500    80\n",
      "   happy     0.551     0.478     0.416    77\n",
      " neutral     0.551     0.649     0.663    95\n",
      "     sad     0.551     0.655     0.604    91\n",
      "surprise     0.551     0.407     0.393    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.540     0.537    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.533119 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.365727  [    0/ 5482]\n",
      "loss: 1.271043  [  600/ 5482]\n",
      "loss: 0.276456  [ 1200/ 5482]\n",
      "loss: 0.360503  [ 1800/ 5482]\n",
      "loss: 0.287626  [ 2400/ 5482]\n",
      "loss: 0.244688  [ 3000/ 5482]\n",
      "loss: 0.450865  [ 3600/ 5482]\n",
      "loss: 0.651098  [ 4200/ 5482]\n",
      "loss: 0.358089  [ 4800/ 5482]\n",
      "loss: 0.261206  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.544     0.626    99\n",
      " disgust     0.562     0.653     0.579    107\n",
      "    fear     0.562     0.558     0.600    80\n",
      "   happy     0.562     0.365     0.494    77\n",
      " neutral     0.562     0.720     0.621    95\n",
      "     sad     0.562     0.627     0.571    91\n",
      "surprise     0.562     0.478     0.361    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.563     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.540132 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.872023  [    0/ 5482]\n",
      "loss: 0.280883  [  600/ 5482]\n",
      "loss: 0.327392  [ 1200/ 5482]\n",
      "loss: 0.490906  [ 1800/ 5482]\n",
      "loss: 0.327284  [ 2400/ 5482]\n",
      "loss: 0.308798  [ 3000/ 5482]\n",
      "loss: 0.263468  [ 3600/ 5482]\n",
      "loss: 0.320000  [ 4200/ 5482]\n",
      "loss: 0.283941  [ 4800/ 5482]\n",
      "loss: 0.539897  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.570     0.647     0.556    99\n",
      " disgust     0.570     0.559     0.664    107\n",
      "    fear     0.570     0.517     0.575    80\n",
      "   happy     0.570     0.424     0.468    77\n",
      " neutral     0.570     0.787     0.663    95\n",
      "     sad     0.570     0.588     0.626    91\n",
      "surprise     0.570     0.426     0.328    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.570     0.564     0.554    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.589665 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.291100  [    0/ 5482]\n",
      "loss: 0.467032  [  600/ 5482]\n",
      "loss: 0.228782  [ 1200/ 5482]\n",
      "loss: 0.281468  [ 1800/ 5482]\n",
      "loss: 0.375733  [ 2400/ 5482]\n",
      "loss: 0.242930  [ 3000/ 5482]\n",
      "loss: 0.599530  [ 3600/ 5482]\n",
      "loss: 0.733679  [ 4200/ 5482]\n",
      "loss: 0.440579  [ 4800/ 5482]\n",
      "loss: 1.138065  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.546     0.510     0.535    99\n",
      " disgust     0.546     0.566     0.561    107\n",
      "    fear     0.546     0.494     0.537    80\n",
      "   happy     0.546     0.328     0.506    77\n",
      " neutral     0.546     0.808     0.663    95\n",
      "     sad     0.546     0.658     0.527    91\n",
      "surprise     0.546     0.628     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.546     0.570     0.539    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.630307 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.266413  [    0/ 5482]\n",
      "loss: 0.235407  [  600/ 5482]\n",
      "loss: 0.278860  [ 1200/ 5482]\n",
      "loss: 1.025697  [ 1800/ 5482]\n",
      "loss: 0.292804  [ 2400/ 5482]\n",
      "loss: 0.307502  [ 3000/ 5482]\n",
      "loss: 0.282055  [ 3600/ 5482]\n",
      "loss: 0.319028  [ 4200/ 5482]\n",
      "loss: 0.325293  [ 4800/ 5482]\n",
      "loss: 0.275064  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.528     0.576    99\n",
      " disgust     0.584     0.590     0.645    107\n",
      "    fear     0.584     0.571     0.550    80\n",
      "   happy     0.584     0.514     0.481    77\n",
      " neutral     0.584     0.761     0.705    95\n",
      "     sad     0.584     0.627     0.571    91\n",
      "surprise     0.584     0.462     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.579     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.579139 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.411095  [    0/ 5482]\n",
      "loss: 0.280351  [  600/ 5482]\n",
      "loss: 0.781766  [ 1200/ 5482]\n",
      "loss: 0.627631  [ 1800/ 5482]\n",
      "loss: 0.260685  [ 2400/ 5482]\n",
      "loss: 0.233329  [ 3000/ 5482]\n",
      "loss: 0.198893  [ 3600/ 5482]\n",
      "loss: 0.241542  [ 4200/ 5482]\n",
      "loss: 0.421293  [ 4800/ 5482]\n",
      "loss: 0.500940  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.527     0.485    99\n",
      " disgust     0.566     0.571     0.636    107\n",
      "    fear     0.566     0.532     0.512    80\n",
      "   happy     0.566     0.423     0.532    77\n",
      " neutral     0.566     0.760     0.768    95\n",
      "     sad     0.566     0.609     0.582    91\n",
      "surprise     0.566     0.488     0.344    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.559     0.551    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.643144 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.282066  [    0/ 5482]\n",
      "loss: 0.184320  [  600/ 5482]\n",
      "loss: 0.216059  [ 1200/ 5482]\n",
      "loss: 0.341804  [ 1800/ 5482]\n",
      "loss: 0.380228  [ 2400/ 5482]\n",
      "loss: 0.678638  [ 3000/ 5482]\n",
      "loss: 0.400667  [ 3600/ 5482]\n",
      "loss: 0.210159  [ 4200/ 5482]\n",
      "loss: 0.271784  [ 4800/ 5482]\n",
      "loss: 0.235993  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.504     0.596    99\n",
      " disgust     0.567     0.581     0.636    107\n",
      "    fear     0.567     0.512     0.537    80\n",
      "   happy     0.567     0.404     0.545    77\n",
      " neutral     0.567     0.765     0.684    95\n",
      "     sad     0.567     0.691     0.516    91\n",
      "surprise     0.567     0.629     0.361    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.584     0.554    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.653288 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.178264  [    0/ 5482]\n",
      "loss: 0.180613  [  600/ 5482]\n",
      "loss: 0.224463  [ 1200/ 5482]\n",
      "loss: 0.160306  [ 1800/ 5482]\n",
      "loss: 0.360490  [ 2400/ 5482]\n",
      "loss: 0.255276  [ 3000/ 5482]\n",
      "loss: 0.238091  [ 3600/ 5482]\n",
      "loss: 0.165465  [ 4200/ 5482]\n",
      "loss: 0.208906  [ 4800/ 5482]\n",
      "loss: 0.526923  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.559     0.515     0.515    99\n",
      " disgust     0.559     0.643     0.589    107\n",
      "    fear     0.559     0.488     0.525    80\n",
      "   happy     0.559     0.364     0.506    77\n",
      " neutral     0.559     0.767     0.726    95\n",
      "     sad     0.559     0.651     0.593    91\n",
      "surprise     0.559     0.489     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.559     0.560     0.547    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.706690 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.135145  [    0/ 5482]\n",
      "loss: 0.187993  [  600/ 5482]\n",
      "loss: 0.521860  [ 1200/ 5482]\n",
      "loss: 0.306494  [ 1800/ 5482]\n",
      "loss: 0.256016  [ 2400/ 5482]\n",
      "loss: 0.187738  [ 3000/ 5482]\n",
      "loss: 0.170751  [ 3600/ 5482]\n",
      "loss: 0.315812  [ 4200/ 5482]\n",
      "loss: 0.391967  [ 4800/ 5482]\n",
      "loss: 0.181053  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.598     0.624     0.586    99\n",
      " disgust     0.598     0.542     0.720    107\n",
      "    fear     0.598     0.679     0.475    80\n",
      "   happy     0.598     0.446     0.532    77\n",
      " neutral     0.598     0.758     0.726    95\n",
      "     sad     0.598     0.648     0.626    91\n",
      "surprise     0.598     0.521     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.598     0.602     0.582    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.583044 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.181986  [    0/ 5482]\n",
      "loss: 0.139379  [  600/ 5482]\n",
      "loss: 0.737763  [ 1200/ 5482]\n",
      "loss: 0.165169  [ 1800/ 5482]\n",
      "loss: 0.180654  [ 2400/ 5482]\n",
      "loss: 0.239572  [ 3000/ 5482]\n",
      "loss: 0.100727  [ 3600/ 5482]\n",
      "loss: 0.188049  [ 4200/ 5482]\n",
      "loss: 0.134961  [ 4800/ 5482]\n",
      "loss: 0.163432  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.505     0.545    99\n",
      " disgust     0.562     0.646     0.579    107\n",
      "    fear     0.562     0.475     0.588    80\n",
      "   happy     0.562     0.468     0.481    77\n",
      " neutral     0.562     0.784     0.611    95\n",
      "     sad     0.562     0.621     0.593    91\n",
      "surprise     0.562     0.456     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.565     0.558    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.729802 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.140123  [    0/ 5482]\n",
      "loss: 0.178532  [  600/ 5482]\n",
      "loss: 0.112138  [ 1200/ 5482]\n",
      "loss: 0.160805  [ 1800/ 5482]\n",
      "loss: 0.203721  [ 2400/ 5482]\n",
      "loss: 0.111547  [ 3000/ 5482]\n",
      "loss: 0.177663  [ 3600/ 5482]\n",
      "loss: 0.126751  [ 4200/ 5482]\n",
      "loss: 0.417841  [ 4800/ 5482]\n",
      "loss: 0.149755  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.576     0.535    99\n",
      " disgust     0.582     0.596     0.607    107\n",
      "    fear     0.582     0.554     0.575    80\n",
      "   happy     0.582     0.419     0.571    77\n",
      " neutral     0.582     0.720     0.705    95\n",
      "     sad     0.582     0.655     0.626    91\n",
      "surprise     0.582     0.561     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.583     0.571    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.704284 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.133105  [    0/ 5482]\n",
      "loss: 0.371046  [  600/ 5482]\n",
      "loss: 0.130473  [ 1200/ 5482]\n",
      "loss: 0.120779  [ 1800/ 5482]\n",
      "loss: 0.135181  [ 2400/ 5482]\n",
      "loss: 0.132335  [ 3000/ 5482]\n",
      "loss: 0.164684  [ 3600/ 5482]\n",
      "loss: 0.752625  [ 4200/ 5482]\n",
      "loss: 0.148265  [ 4800/ 5482]\n",
      "loss: 0.095793  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.465     0.535    99\n",
      " disgust     0.556     0.583     0.626    107\n",
      "    fear     0.556     0.488     0.500    80\n",
      "   happy     0.556     0.412     0.545    77\n",
      " neutral     0.556     0.848     0.705    95\n",
      "     sad     0.556     0.691     0.516    91\n",
      "surprise     0.556     0.460     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.564     0.544    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.795900 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.117963  [    0/ 5482]\n",
      "loss: 0.165550  [  600/ 5482]\n",
      "loss: 0.129858  [ 1200/ 5482]\n",
      "loss: 0.191391  [ 1800/ 5482]\n",
      "loss: 0.119250  [ 2400/ 5482]\n",
      "loss: 0.292572  [ 3000/ 5482]\n",
      "loss: 0.540207  [ 3600/ 5482]\n",
      "loss: 0.155417  [ 4200/ 5482]\n",
      "loss: 0.137781  [ 4800/ 5482]\n",
      "loss: 0.160929  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.591     0.525    99\n",
      " disgust     0.582     0.617     0.664    107\n",
      "    fear     0.582     0.545     0.525    80\n",
      "   happy     0.582     0.494     0.506    77\n",
      " neutral     0.582     0.684     0.684    95\n",
      "     sad     0.582     0.556     0.659    91\n",
      "surprise     0.582     0.542     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.576     0.570    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.771901 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.108187  [    0/ 5482]\n",
      "loss: 0.151378  [  600/ 5482]\n",
      "loss: 0.116007  [ 1200/ 5482]\n",
      "loss: 0.089980  [ 1800/ 5482]\n",
      "loss: 0.409821  [ 2400/ 5482]\n",
      "loss: 0.127840  [ 3000/ 5482]\n",
      "loss: 0.320047  [ 3600/ 5482]\n",
      "loss: 0.378349  [ 4200/ 5482]\n",
      "loss: 0.167908  [ 4800/ 5482]\n",
      "loss: 0.109532  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.667     0.525    99\n",
      " disgust     0.587     0.602     0.636    107\n",
      "    fear     0.587     0.560     0.525    80\n",
      "   happy     0.587     0.444     0.571    77\n",
      " neutral     0.587     0.691     0.705    95\n",
      "     sad     0.587     0.632     0.604    91\n",
      "surprise     0.587     0.492     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.584     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.750844 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.141868  [    0/ 5482]\n",
      "loss: 0.114230  [  600/ 5482]\n",
      "loss: 0.103463  [ 1200/ 5482]\n",
      "loss: 0.121948  [ 1800/ 5482]\n",
      "loss: 0.126692  [ 2400/ 5482]\n",
      "loss: 0.501908  [ 3000/ 5482]\n",
      "loss: 0.086937  [ 3600/ 5482]\n",
      "loss: 0.147726  [ 4200/ 5482]\n",
      "loss: 0.110443  [ 4800/ 5482]\n",
      "loss: 0.162086  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.615     0.566    99\n",
      " disgust     0.589     0.579     0.757    107\n",
      "    fear     0.589     0.549     0.562    80\n",
      "   happy     0.589     0.465     0.519    77\n",
      " neutral     0.589     0.788     0.705    95\n",
      "     sad     0.589     0.608     0.527    91\n",
      "surprise     0.589     0.468     0.361    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.582     0.571    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.771289 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.209441  [    0/ 5482]\n",
      "loss: 0.112772  [  600/ 5482]\n",
      "loss: 0.122555  [ 1200/ 5482]\n",
      "loss: 0.110902  [ 1800/ 5482]\n",
      "loss: 0.129395  [ 2400/ 5482]\n",
      "loss: 0.100447  [ 3000/ 5482]\n",
      "loss: 0.120308  [ 3600/ 5482]\n",
      "loss: 0.089790  [ 4200/ 5482]\n",
      "loss: 0.148543  [ 4800/ 5482]\n",
      "loss: 0.126912  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.564     0.556     0.556    99\n",
      " disgust     0.564     0.598     0.598    107\n",
      "    fear     0.564     0.480     0.600    80\n",
      "   happy     0.564     0.396     0.468    77\n",
      " neutral     0.564     0.780     0.674    95\n",
      "     sad     0.564     0.600     0.527    91\n",
      "surprise     0.564     0.569     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.564     0.568     0.557    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.814234 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.104433  [    0/ 5482]\n",
      "loss: 0.089190  [  600/ 5482]\n",
      "loss: 0.096643  [ 1200/ 5482]\n",
      "loss: 0.181274  [ 1800/ 5482]\n",
      "loss: 0.127805  [ 2400/ 5482]\n",
      "loss: 0.408580  [ 3000/ 5482]\n",
      "loss: 0.088691  [ 3600/ 5482]\n",
      "loss: 0.088366  [ 4200/ 5482]\n",
      "loss: 0.733866  [ 4800/ 5482]\n",
      "loss: 0.098163  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.516     0.667    99\n",
      " disgust     0.579     0.615     0.551    107\n",
      "    fear     0.579     0.634     0.562    80\n",
      "   happy     0.579     0.442     0.442    77\n",
      " neutral     0.579     0.663     0.726    95\n",
      "     sad     0.579     0.631     0.582    91\n",
      "surprise     0.579     0.540     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.577     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.839165 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.195666  [    0/ 5482]\n",
      "loss: 0.518501  [  600/ 5482]\n",
      "loss: 0.458511  [ 1200/ 5482]\n",
      "loss: 0.102719  [ 1800/ 5482]\n",
      "loss: 0.108345  [ 2400/ 5482]\n",
      "loss: 0.070945  [ 3000/ 5482]\n",
      "loss: 0.099672  [ 3600/ 5482]\n",
      "loss: 0.105171  [ 4200/ 5482]\n",
      "loss: 0.077399  [ 4800/ 5482]\n",
      "loss: 0.090516  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.496     0.586    99\n",
      " disgust     0.597     0.655     0.673    107\n",
      "    fear     0.597     0.558     0.537    80\n",
      "   happy     0.597     0.462     0.545    77\n",
      " neutral     0.597     0.740     0.779    95\n",
      "     sad     0.597     0.671     0.560    91\n",
      "surprise     0.597     0.615     0.393    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.600     0.582    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.732158 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.092809  [    0/ 5482]\n",
      "loss: 0.086674  [  600/ 5482]\n",
      "loss: 0.117556  [ 1200/ 5482]\n",
      "loss: 0.089676  [ 1800/ 5482]\n",
      "loss: 0.247063  [ 2400/ 5482]\n",
      "loss: 0.172394  [ 3000/ 5482]\n",
      "loss: 0.090086  [ 3600/ 5482]\n",
      "loss: 0.095235  [ 4200/ 5482]\n",
      "loss: 0.067631  [ 4800/ 5482]\n",
      "loss: 0.081543  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.591     0.556    99\n",
      " disgust     0.557     0.496     0.626    107\n",
      "    fear     0.557     0.580     0.500    80\n",
      "   happy     0.557     0.447     0.494    77\n",
      " neutral     0.557     0.708     0.663    95\n",
      "     sad     0.557     0.636     0.538    91\n",
      "surprise     0.557     0.452     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.559     0.548    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.964311 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.093962  [    0/ 5482]\n",
      "loss: 0.082763  [  600/ 5482]\n",
      "loss: 0.056179  [ 1200/ 5482]\n",
      "loss: 0.092869  [ 1800/ 5482]\n",
      "loss: 0.432728  [ 2400/ 5482]\n",
      "loss: 0.071703  [ 3000/ 5482]\n",
      "loss: 0.086181  [ 3600/ 5482]\n",
      "loss: 0.418347  [ 4200/ 5482]\n",
      "loss: 0.100376  [ 4800/ 5482]\n",
      "loss: 0.076097  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.675     0.525    99\n",
      " disgust     0.575     0.598     0.626    107\n",
      "    fear     0.575     0.529     0.575    80\n",
      "   happy     0.575     0.350     0.532    77\n",
      " neutral     0.575     0.821     0.674    95\n",
      "     sad     0.575     0.615     0.615    91\n",
      "surprise     0.575     0.521     0.410    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.587     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.856610 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.648660  [    0/ 5482]\n",
      "loss: 0.075845  [  600/ 5482]\n",
      "loss: 0.350245  [ 1200/ 5482]\n",
      "loss: 0.080161  [ 1800/ 5482]\n",
      "loss: 0.077493  [ 2400/ 5482]\n",
      "loss: 0.136505  [ 3000/ 5482]\n",
      "loss: 0.085521  [ 3600/ 5482]\n",
      "loss: 0.091035  [ 4200/ 5482]\n",
      "loss: 0.079480  [ 4800/ 5482]\n",
      "loss: 0.375432  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.564     0.576    99\n",
      " disgust     0.574     0.598     0.598    107\n",
      "    fear     0.574     0.475     0.588    80\n",
      "   happy     0.574     0.455     0.519    77\n",
      " neutral     0.574     0.787     0.621    95\n",
      "     sad     0.574     0.639     0.582    91\n",
      "surprise     0.574     0.526     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.578     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.959519 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.083228  [    0/ 5482]\n",
      "loss: 0.082417  [  600/ 5482]\n",
      "loss: 0.066361  [ 1200/ 5482]\n",
      "loss: 0.102422  [ 1800/ 5482]\n",
      "loss: 0.083130  [ 2400/ 5482]\n",
      "loss: 0.066084  [ 3000/ 5482]\n",
      "loss: 0.069597  [ 3600/ 5482]\n",
      "loss: 0.078807  [ 4200/ 5482]\n",
      "loss: 0.071796  [ 4800/ 5482]\n",
      "loss: 0.066282  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.544     0.566    99\n",
      " disgust     0.587     0.564     0.617    107\n",
      "    fear     0.587     0.690     0.500    80\n",
      "   happy     0.587     0.514     0.468    77\n",
      " neutral     0.587     0.690     0.726    95\n",
      "     sad     0.587     0.629     0.615    91\n",
      "surprise     0.587     0.479     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.587     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.840008 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.053390  [    0/ 5482]\n",
      "loss: 0.098741  [  600/ 5482]\n",
      "loss: 0.059448  [ 1200/ 5482]\n",
      "loss: 0.076321  [ 1800/ 5482]\n",
      "loss: 0.069494  [ 2400/ 5482]\n",
      "loss: 0.083113  [ 3000/ 5482]\n",
      "loss: 0.455964  [ 3600/ 5482]\n",
      "loss: 0.073288  [ 4200/ 5482]\n",
      "loss: 0.068926  [ 4800/ 5482]\n",
      "loss: 0.486635  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.605     0.580     0.586    99\n",
      " disgust     0.605     0.588     0.654    107\n",
      "    fear     0.605     0.667     0.500    80\n",
      "   happy     0.605     0.515     0.455    77\n",
      " neutral     0.605     0.696     0.821    95\n",
      "     sad     0.605     0.626     0.626    91\n",
      "surprise     0.605     0.517     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.605     0.598     0.593    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.825124 \n",
      "\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/nr2Run_Nr_0_conv/emo_reco_best_ep46_acc_60\"!  new accuracy: 60.5\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.682196  [    0/ 5482]\n",
      "loss: 0.284221  [  600/ 5482]\n",
      "loss: 0.065549  [ 1200/ 5482]\n",
      "loss: 0.158075  [ 1800/ 5482]\n",
      "loss: 0.080942  [ 2400/ 5482]\n",
      "loss: 0.077144  [ 3000/ 5482]\n",
      "loss: 0.056024  [ 3600/ 5482]\n",
      "loss: 0.751157  [ 4200/ 5482]\n",
      "loss: 0.059490  [ 4800/ 5482]\n",
      "loss: 0.131248  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.530     0.626    99\n",
      " disgust     0.575     0.616     0.570    107\n",
      "    fear     0.575     0.627     0.463    80\n",
      "   happy     0.575     0.532     0.429    77\n",
      " neutral     0.575     0.703     0.747    95\n",
      "     sad     0.575     0.640     0.626    91\n",
      "surprise     0.575     0.361     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.573     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 2.021667 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.086041  [    0/ 5482]\n",
      "loss: 0.091065  [  600/ 5482]\n",
      "loss: 0.058144  [ 1200/ 5482]\n",
      "loss: 0.088060  [ 1800/ 5482]\n",
      "loss: 0.065377  [ 2400/ 5482]\n",
      "loss: 0.063276  [ 3000/ 5482]\n",
      "loss: 0.058793  [ 3600/ 5482]\n",
      "loss: 0.068145  [ 4200/ 5482]\n",
      "loss: 0.060278  [ 4800/ 5482]\n",
      "loss: 0.051909  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.606     0.576    99\n",
      " disgust     0.584     0.540     0.626    107\n",
      "    fear     0.584     0.618     0.525    80\n",
      "   happy     0.584     0.523     0.442    77\n",
      " neutral     0.584     0.742     0.758    95\n",
      "     sad     0.584     0.566     0.615    91\n",
      "surprise     0.584     0.444     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.577     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.980426 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.073650  [    0/ 5482]\n",
      "loss: 0.049844  [  600/ 5482]\n",
      "loss: 0.054289  [ 1200/ 5482]\n",
      "loss: 0.086010  [ 1800/ 5482]\n",
      "loss: 0.073615  [ 2400/ 5482]\n",
      "loss: 0.551185  [ 3000/ 5482]\n",
      "loss: 0.091342  [ 3600/ 5482]\n",
      "loss: 0.563026  [ 4200/ 5482]\n",
      "loss: 0.054580  [ 4800/ 5482]\n",
      "loss: 0.063199  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.640     0.556    99\n",
      " disgust     0.592     0.644     0.626    107\n",
      "    fear     0.592     0.530     0.550    80\n",
      "   happy     0.592     0.435     0.519    77\n",
      " neutral     0.592     0.736     0.705    95\n",
      "     sad     0.592     0.655     0.626    91\n",
      "surprise     0.592     0.463     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.586     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.982760 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.114010  [    0/ 5482]\n",
      "loss: 0.061309  [  600/ 5482]\n",
      "loss: 0.293614  [ 1200/ 5482]\n",
      "loss: 0.062680  [ 1800/ 5482]\n",
      "loss: 0.052403  [ 2400/ 5482]\n",
      "loss: 0.056515  [ 3000/ 5482]\n",
      "loss: 0.055405  [ 3600/ 5482]\n",
      "loss: 0.063329  [ 4200/ 5482]\n",
      "loss: 0.050156  [ 4800/ 5482]\n",
      "loss: 0.065123  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.589     0.535    99\n",
      " disgust     0.592     0.617     0.617    107\n",
      "    fear     0.592     0.707     0.512    80\n",
      "   happy     0.592     0.416     0.545    77\n",
      " neutral     0.592     0.626     0.811    95\n",
      "     sad     0.592     0.643     0.593    91\n",
      "surprise     0.592     0.596     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.599     0.582    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.893360 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.055216  [    0/ 5482]\n",
      "loss: 0.058337  [  600/ 5482]\n",
      "loss: 0.053986  [ 1200/ 5482]\n",
      "loss: 0.271833  [ 1800/ 5482]\n",
      "loss: 0.084164  [ 2400/ 5482]\n",
      "loss: 0.047667  [ 3000/ 5482]\n",
      "loss: 0.061476  [ 3600/ 5482]\n",
      "loss: 0.063455  [ 4200/ 5482]\n",
      "loss: 0.064394  [ 4800/ 5482]\n",
      "loss: 0.081403  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.649     0.505    99\n",
      " disgust     0.595     0.537     0.738    107\n",
      "    fear     0.595     0.565     0.600    80\n",
      "   happy     0.595     0.453     0.442    77\n",
      " neutral     0.595     0.750     0.726    95\n",
      "     sad     0.595     0.620     0.626    91\n",
      "surprise     0.595     0.619     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.599     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.973256 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.059182  [    0/ 5482]\n",
      "loss: 0.052720  [  600/ 5482]\n",
      "loss: 0.065280  [ 1200/ 5482]\n",
      "loss: 0.050324  [ 1800/ 5482]\n",
      "loss: 0.056066  [ 2400/ 5482]\n",
      "loss: 0.072436  [ 3000/ 5482]\n",
      "loss: 0.045216  [ 3600/ 5482]\n",
      "loss: 0.065800  [ 4200/ 5482]\n",
      "loss: 0.049812  [ 4800/ 5482]\n",
      "loss: 0.060708  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.546     0.600     0.515    99\n",
      " disgust     0.546     0.598     0.542    107\n",
      "    fear     0.546     0.466     0.512    80\n",
      "   happy     0.546     0.432     0.532    77\n",
      " neutral     0.546     0.765     0.684    95\n",
      "     sad     0.546     0.543     0.549    91\n",
      "surprise     0.546     0.397     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.546     0.543     0.540    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 2.166898 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.052667  [    0/ 5482]\n",
      "loss: 0.087608  [  600/ 5482]\n",
      "loss: 0.037620  [ 1200/ 5482]\n",
      "loss: 0.046928  [ 1800/ 5482]\n",
      "loss: 0.060834  [ 2400/ 5482]\n",
      "loss: 0.048213  [ 3000/ 5482]\n",
      "loss: 0.052393  [ 3600/ 5482]\n",
      "loss: 0.050092  [ 4200/ 5482]\n",
      "loss: 0.882263  [ 4800/ 5482]\n",
      "loss: 0.046508  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.504     0.616    99\n",
      " disgust     0.575     0.608     0.579    107\n",
      "    fear     0.575     0.655     0.475    80\n",
      "   happy     0.575     0.452     0.494    77\n",
      " neutral     0.575     0.727     0.674    95\n",
      "     sad     0.575     0.651     0.615    91\n",
      "surprise     0.575     0.451     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.578     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.970356 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.053667  [    0/ 5482]\n",
      "loss: 0.135959  [  600/ 5482]\n",
      "loss: 0.044224  [ 1200/ 5482]\n",
      "loss: 0.608625  [ 1800/ 5482]\n",
      "loss: 0.093830  [ 2400/ 5482]\n",
      "loss: 0.040736  [ 3000/ 5482]\n",
      "loss: 0.043977  [ 3600/ 5482]\n",
      "loss: 0.045093  [ 4200/ 5482]\n",
      "loss: 0.064460  [ 4800/ 5482]\n",
      "loss: 0.049140  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.568     0.545    99\n",
      " disgust     0.584     0.577     0.598    107\n",
      "    fear     0.584     0.541     0.575    80\n",
      "   happy     0.584     0.409     0.494    77\n",
      " neutral     0.584     0.824     0.737    95\n",
      "     sad     0.584     0.591     0.604    91\n",
      "surprise     0.584     0.604     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.588     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 2.033620 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.047663  [    0/ 5482]\n",
      "loss: 0.040085  [  600/ 5482]\n",
      "loss: 0.061023  [ 1200/ 5482]\n",
      "loss: 0.038518  [ 1800/ 5482]\n",
      "loss: 0.048294  [ 2400/ 5482]\n",
      "loss: 0.034020  [ 3000/ 5482]\n",
      "loss: 0.039574  [ 3600/ 5482]\n",
      "loss: 0.040668  [ 4200/ 5482]\n",
      "loss: 0.043295  [ 4800/ 5482]\n",
      "loss: 0.657539  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.562     0.505    99\n",
      " disgust     0.572     0.698     0.626    107\n",
      "    fear     0.572     0.476     0.500    80\n",
      "   happy     0.572     0.516     0.429    77\n",
      " neutral     0.572     0.800     0.716    95\n",
      "     sad     0.572     0.545     0.659    91\n",
      "surprise     0.572     0.378     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.568     0.563    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.915102 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.074949  [    0/ 5482]\n",
      "loss: 0.040352  [  600/ 5482]\n",
      "loss: 0.045372  [ 1200/ 5482]\n",
      "loss: 0.141964  [ 1800/ 5482]\n",
      "loss: 0.043521  [ 2400/ 5482]\n",
      "loss: 0.248738  [ 3000/ 5482]\n",
      "loss: 0.034487  [ 3600/ 5482]\n",
      "loss: 0.224058  [ 4200/ 5482]\n",
      "loss: 0.093045  [ 4800/ 5482]\n",
      "loss: 0.059140  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.570     0.495    99\n",
      " disgust     0.566     0.621     0.598    107\n",
      "    fear     0.566     0.505     0.575    80\n",
      "   happy     0.566     0.415     0.506    77\n",
      " neutral     0.566     0.810     0.674    95\n",
      "     sad     0.566     0.525     0.582    91\n",
      "surprise     0.566     0.536     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.569     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.019975 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.041539  [    0/ 5482]\n",
      "loss: 0.046830  [  600/ 5482]\n",
      "loss: 0.036001  [ 1200/ 5482]\n",
      "loss: 0.041379  [ 1800/ 5482]\n",
      "loss: 0.045355  [ 2400/ 5482]\n",
      "loss: 0.041123  [ 3000/ 5482]\n",
      "loss: 0.042400  [ 3600/ 5482]\n",
      "loss: 0.033489  [ 4200/ 5482]\n",
      "loss: 0.040293  [ 4800/ 5482]\n",
      "loss: 0.034142  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.604     0.556    99\n",
      " disgust     0.582     0.653     0.598    107\n",
      "    fear     0.582     0.489     0.562    80\n",
      "   happy     0.582     0.449     0.455    77\n",
      " neutral     0.582     0.833     0.684    95\n",
      "     sad     0.582     0.566     0.659    91\n",
      "surprise     0.582     0.463     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.580     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 2.045060 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.068543  [    0/ 5482]\n",
      "loss: 0.038207  [  600/ 5482]\n",
      "loss: 0.040593  [ 1200/ 5482]\n",
      "loss: 0.038026  [ 1800/ 5482]\n",
      "loss: 0.382605  [ 2400/ 5482]\n",
      "loss: 0.333587  [ 3000/ 5482]\n",
      "loss: 0.091173  [ 3600/ 5482]\n",
      "loss: 0.063937  [ 4200/ 5482]\n",
      "loss: 0.040670  [ 4800/ 5482]\n",
      "loss: 0.041380  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.570     0.546     0.596    99\n",
      " disgust     0.570     0.564     0.617    107\n",
      "    fear     0.570     0.597     0.500    80\n",
      "   happy     0.570     0.432     0.532    77\n",
      " neutral     0.570     0.790     0.674    95\n",
      "     sad     0.570     0.583     0.615    91\n",
      "surprise     0.570     0.478     0.361    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.570     0.570     0.556    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 2.115286 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.066966  [    0/ 5482]\n",
      "loss: 0.029540  [  600/ 5482]\n",
      "loss: 0.046399  [ 1200/ 5482]\n",
      "loss: 0.036123  [ 1800/ 5482]\n",
      "loss: 0.040011  [ 2400/ 5482]\n",
      "loss: 0.041287  [ 3000/ 5482]\n",
      "loss: 0.034684  [ 3600/ 5482]\n",
      "loss: 0.034279  [ 4200/ 5482]\n",
      "loss: 0.030491  [ 4800/ 5482]\n",
      "loss: 0.043090  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.570     0.556     0.556    99\n",
      " disgust     0.570     0.637     0.607    107\n",
      "    fear     0.570     0.532     0.525    80\n",
      "   happy     0.570     0.500     0.455    77\n",
      " neutral     0.570     0.789     0.747    95\n",
      "     sad     0.570     0.509     0.593    91\n",
      "surprise     0.570     0.406     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.570     0.561     0.559    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 2.100331 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.039218  [    0/ 5482]\n",
      "loss: 0.042292  [  600/ 5482]\n",
      "loss: 0.034784  [ 1200/ 5482]\n",
      "loss: 0.398057  [ 1800/ 5482]\n",
      "loss: 0.043689  [ 2400/ 5482]\n",
      "loss: 0.032250  [ 3000/ 5482]\n",
      "loss: 0.035923  [ 3600/ 5482]\n",
      "loss: 0.032467  [ 4200/ 5482]\n",
      "loss: 0.043456  [ 4800/ 5482]\n",
      "loss: 0.035528  [ 5400/ 5482]\n",
      "8\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.647     0.556    99\n",
      " disgust     0.566     0.547     0.654    107\n",
      "    fear     0.566     0.526     0.512    80\n",
      "   happy     0.566     0.466     0.442    77\n",
      " neutral     0.566     0.750     0.726    95\n",
      "     sad     0.566     0.505     0.538    91\n",
      "surprise     0.566     0.474     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.559     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.137703 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.543898  [    0/ 5482]\n",
      "loss: 0.039765  [  600/ 5482]\n",
      "loss: 0.039631  [ 1200/ 5482]\n",
      "loss: 0.032272  [ 1800/ 5482]\n",
      "loss: 0.062525  [ 2400/ 5482]\n",
      "loss: 0.037924  [ 3000/ 5482]\n",
      "loss: 0.046032  [ 3600/ 5482]\n",
      "loss: 0.977787  [ 4200/ 5482]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
