{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_bottleneck_encoder import SSBottelneckLayer\n",
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/AUTO_encoder/Nr1\"\n",
    "safe_model_every = 50\n",
    "epochs = 1000\n",
    "save_highest_acc_min_acc = 0.5\n",
    "lr = 1e-2\n",
    "save_every = 50\n",
    "start_lr = 4e-3\n",
    "gc.collect()\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncs_betterSS_12_1_wrap_music.pkl\", device=device)\n",
    "trainDS, testDs = train_val_dataset(data_set, val_split=0.1, seed=100)\n",
    "\n",
    "torch.manual_seed(300)\n",
    "model = SSBottelneckLayer(num_cols = 175,bottleneck_size =  4, dropout=0.2)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.021327  [    0/ 6092]\n",
      "loss: 2.011011  [ 2000/ 6092]\n",
      "loss: 2.095205  [ 4000/ 6092]\n",
      "loss: 2.002723  [ 6000/ 6092]\n",
      "773.8284888813893\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.134087  [    0/ 6092]\n",
      "loss: 2.174909  [ 2000/ 6092]\n",
      "loss: 2.043833  [ 4000/ 6092]\n",
      "loss: 2.001592  [ 6000/ 6092]\n",
      "776.064561372002\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.043734  [    0/ 6092]\n",
      "loss: 1.959320  [ 2000/ 6092]\n",
      "loss: 1.968706  [ 4000/ 6092]\n",
      "loss: 2.134660  [ 6000/ 6092]\n",
      "775.880025225381\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.983407  [    0/ 6092]\n",
      "loss: 2.055547  [ 2000/ 6092]\n",
      "loss: 2.066809  [ 4000/ 6092]\n",
      "loss: 2.108790  [ 6000/ 6092]\n",
      "775.8658433606228\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.961752  [    0/ 6092]\n",
      "loss: 1.969554  [ 2000/ 6092]\n",
      "loss: 2.127558  [ 4000/ 6092]\n",
      "loss: 2.034909  [ 6000/ 6092]\n",
      "774.379191438357\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.994187  [    0/ 6092]\n",
      "loss: 2.130719  [ 2000/ 6092]\n",
      "loss: 2.124242  [ 4000/ 6092]\n",
      "loss: 2.039732  [ 6000/ 6092]\n",
      "773.0892895658811\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.068986  [    0/ 6092]\n",
      "loss: 2.017000  [ 2000/ 6092]\n",
      "loss: 2.024721  [ 4000/ 6092]\n",
      "loss: 2.010724  [ 6000/ 6092]\n",
      "774.4751219054064\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.000674  [    0/ 6092]\n",
      "loss: 2.107120  [ 2000/ 6092]\n",
      "loss: 2.041771  [ 4000/ 6092]\n",
      "loss: 2.153578  [ 6000/ 6092]\n",
      "773.9610007181764\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.034303  [    0/ 6092]\n",
      "loss: 2.016110  [ 2000/ 6092]\n",
      "loss: 1.990862  [ 4000/ 6092]\n",
      "loss: 1.955652  [ 6000/ 6092]\n",
      "772.816659361124\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.089063  [    0/ 6092]\n",
      "loss: 1.997950  [ 2000/ 6092]\n",
      "loss: 1.993853  [ 4000/ 6092]\n",
      "loss: 2.084758  [ 6000/ 6092]\n",
      "773.3404707287749\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.966523  [    0/ 6092]\n",
      "loss: 1.966250  [ 2000/ 6092]\n",
      "loss: 1.987161  [ 4000/ 6092]\n",
      "loss: 2.053017  [ 6000/ 6092]\n",
      "774.5943386132518\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.063604  [    0/ 6092]\n",
      "loss: 2.067502  [ 2000/ 6092]\n",
      "loss: 1.954569  [ 4000/ 6092]\n",
      "loss: 2.011787  [ 6000/ 6092]\n",
      "775.269915625453\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.073544  [    0/ 6092]\n",
      "loss: 1.962600  [ 2000/ 6092]\n",
      "loss: 2.019794  [ 4000/ 6092]\n",
      "loss: 2.047332  [ 6000/ 6092]\n",
      "773.3390503277382\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.970256  [    0/ 6092]\n",
      "loss: 2.082146  [ 2000/ 6092]\n",
      "loss: 2.005168  [ 4000/ 6092]\n",
      "loss: 2.019294  [ 6000/ 6092]\n",
      "774.9578017095724\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.005807  [    0/ 6092]\n",
      "loss: 2.048170  [ 2000/ 6092]\n",
      "loss: 1.996811  [ 4000/ 6092]\n",
      "loss: 2.029060  [ 6000/ 6092]\n",
      "773.7590914120277\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.999541  [    0/ 6092]\n",
      "loss: 2.055538  [ 2000/ 6092]\n",
      "loss: 2.018428  [ 4000/ 6092]\n",
      "loss: 2.087660  [ 6000/ 6092]\n",
      "772.5931506554285\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.065169  [    0/ 6092]\n",
      "loss: 2.022007  [ 2000/ 6092]\n",
      "loss: 2.080910  [ 4000/ 6092]\n",
      "loss: 2.087135  [ 6000/ 6092]\n",
      "774.7143281077346\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.100784  [    0/ 6092]\n",
      "loss: 2.002809  [ 2000/ 6092]\n",
      "loss: 2.023636  [ 4000/ 6092]\n",
      "loss: 2.009191  [ 6000/ 6092]\n",
      "773.9972295314074\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.016698  [    0/ 6092]\n",
      "loss: 1.982680  [ 2000/ 6092]\n",
      "loss: 2.108025  [ 4000/ 6092]\n",
      "loss: 2.059263  [ 6000/ 6092]\n",
      "774.3414152612289\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.016907  [    0/ 6092]\n",
      "loss: 2.024857  [ 2000/ 6092]\n",
      "loss: 2.022284  [ 4000/ 6092]\n",
      "loss: 2.048680  [ 6000/ 6092]\n",
      "774.9987120553851\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.047403  [    0/ 6092]\n",
      "loss: 2.162772  [ 2000/ 6092]\n",
      "loss: 2.001244  [ 4000/ 6092]\n",
      "loss: 2.051663  [ 6000/ 6092]\n",
      "775.7316776017348\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.991739  [    0/ 6092]\n",
      "loss: 2.003493  [ 2000/ 6092]\n",
      "loss: 2.022059  [ 4000/ 6092]\n",
      "loss: 2.009819  [ 6000/ 6092]\n",
      "774.4229136357704\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.040533  [    0/ 6092]\n",
      "loss: 2.043594  [ 2000/ 6092]\n",
      "loss: 2.033178  [ 4000/ 6092]\n",
      "loss: 1.973909  [ 6000/ 6092]\n",
      "774.643984089295\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.090507  [    0/ 6092]\n",
      "loss: 1.954909  [ 2000/ 6092]\n",
      "loss: 2.017414  [ 4000/ 6092]\n",
      "loss: 1.957569  [ 6000/ 6092]\n",
      "773.0031141390403\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.982803  [    0/ 6092]\n",
      "loss: 2.109790  [ 2000/ 6092]\n",
      "loss: 1.913545  [ 4000/ 6092]\n",
      "loss: 2.009191  [ 6000/ 6092]\n",
      "773.278761116167\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.010375  [    0/ 6092]\n",
      "loss: 2.025413  [ 2000/ 6092]\n",
      "loss: 2.022145  [ 4000/ 6092]\n",
      "loss: 2.084912  [ 6000/ 6092]\n",
      "774.1307661235332\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.025794  [    0/ 6092]\n",
      "loss: 2.031829  [ 2000/ 6092]\n",
      "loss: 2.067296  [ 4000/ 6092]\n",
      "loss: 1.996008  [ 6000/ 6092]\n",
      "774.5284598295888\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.026839  [    0/ 6092]\n",
      "loss: 2.008634  [ 2000/ 6092]\n",
      "loss: 2.119591  [ 4000/ 6092]\n",
      "loss: 2.035631  [ 6000/ 6092]\n",
      "777.2041160017252\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.134274  [    0/ 6092]\n",
      "loss: 2.020616  [ 2000/ 6092]\n",
      "loss: 2.112261  [ 4000/ 6092]\n",
      "loss: 1.999734  [ 6000/ 6092]\n",
      "773.4116419479251\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.953369  [    0/ 6092]\n",
      "loss: 1.993847  [ 2000/ 6092]\n",
      "loss: 2.087068  [ 4000/ 6092]\n",
      "loss: 1.937831  [ 6000/ 6092]\n",
      "774.1534893239537\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.028746  [    0/ 6092]\n",
      "loss: 1.980747  [ 2000/ 6092]\n",
      "loss: 2.052845  [ 4000/ 6092]\n",
      "loss: 1.975896  [ 6000/ 6092]\n",
      "775.4179574574033\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.083076  [    0/ 6092]\n",
      "loss: 2.133798  [ 2000/ 6092]\n",
      "loss: 1.996905  [ 4000/ 6092]\n",
      "loss: 2.041106  [ 6000/ 6092]\n",
      "774.2592197408279\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.991152  [    0/ 6092]\n",
      "loss: 1.976201  [ 2000/ 6092]\n",
      "loss: 2.063168  [ 4000/ 6092]\n",
      "loss: 2.034468  [ 6000/ 6092]\n",
      "775.4017699832717\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.000273  [    0/ 6092]\n",
      "loss: 2.027144  [ 2000/ 6092]\n",
      "loss: 2.048314  [ 4000/ 6092]\n",
      "loss: 2.035769  [ 6000/ 6092]\n",
      "774.7063915456334\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.001762  [    0/ 6092]\n",
      "loss: 1.948335  [ 2000/ 6092]\n",
      "loss: 2.082644  [ 4000/ 6092]\n",
      "loss: 1.972044  [ 6000/ 6092]\n",
      "775.6159145707885\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.962512  [    0/ 6092]\n",
      "loss: 2.025779  [ 2000/ 6092]\n",
      "loss: 2.037485  [ 4000/ 6092]\n",
      "loss: 2.039242  [ 6000/ 6092]\n",
      "774.1926728238662\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.009722  [    0/ 6092]\n",
      "loss: 1.988289  [ 2000/ 6092]\n",
      "loss: 1.971604  [ 4000/ 6092]\n",
      "loss: 2.007973  [ 6000/ 6092]\n",
      "773.525842996935\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.996559  [    0/ 6092]\n",
      "loss: 2.050026  [ 2000/ 6092]\n",
      "loss: 2.050824  [ 4000/ 6092]\n",
      "loss: 2.086356  [ 6000/ 6092]\n",
      "773.1998472909132\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.961497  [    0/ 6092]\n",
      "loss: 2.144814  [ 2000/ 6092]\n",
      "loss: 2.054087  [ 4000/ 6092]\n",
      "loss: 2.009637  [ 6000/ 6092]\n",
      "773.55695506682\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.034070  [    0/ 6092]\n",
      "loss: 2.019247  [ 2000/ 6092]\n",
      "loss: 2.065606  [ 4000/ 6092]\n",
      "loss: 2.020376  [ 6000/ 6092]\n",
      "773.355698277553\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.049147  [    0/ 6092]\n",
      "loss: 2.031299  [ 2000/ 6092]\n",
      "loss: 2.030828  [ 4000/ 6092]\n",
      "loss: 2.061832  [ 6000/ 6092]\n",
      "772.005133462449\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.025563  [    0/ 6092]\n",
      "loss: 2.068958  [ 2000/ 6092]\n",
      "loss: 2.144817  [ 4000/ 6092]\n",
      "loss: 1.998195  [ 6000/ 6092]\n",
      "774.8221969157457\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.961942  [    0/ 6092]\n",
      "loss: 1.973201  [ 2000/ 6092]\n",
      "loss: 1.970527  [ 4000/ 6092]\n",
      "loss: 2.007797  [ 6000/ 6092]\n",
      "774.9820054819187\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.962282  [    0/ 6092]\n",
      "loss: 2.002953  [ 2000/ 6092]\n",
      "loss: 2.045717  [ 4000/ 6092]\n",
      "loss: 2.086556  [ 6000/ 6092]\n",
      "773.3739847317338\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.000782  [    0/ 6092]\n",
      "loss: 2.114413  [ 2000/ 6092]\n",
      "loss: 1.987022  [ 4000/ 6092]\n",
      "loss: 2.010888  [ 6000/ 6092]\n",
      "774.5704033300281\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.140414  [    0/ 6092]\n",
      "loss: 2.086423  [ 2000/ 6092]\n",
      "loss: 1.952110  [ 4000/ 6092]\n",
      "loss: 2.092481  [ 6000/ 6092]\n",
      "774.8992581789693\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.117936  [    0/ 6092]\n",
      "loss: 2.145685  [ 2000/ 6092]\n",
      "loss: 2.012292  [ 4000/ 6092]\n",
      "loss: 2.023576  [ 6000/ 6092]\n",
      "774.2804985592762\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.020036  [    0/ 6092]\n",
      "loss: 2.014553  [ 2000/ 6092]\n",
      "loss: 1.996421  [ 4000/ 6092]\n",
      "loss: 2.175402  [ 6000/ 6092]\n",
      "775.1292113189896\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.071308  [    0/ 6092]\n",
      "loss: 1.901403  [ 2000/ 6092]\n",
      "loss: 1.977006  [ 4000/ 6092]\n",
      "loss: 2.056680  [ 6000/ 6092]\n",
      "773.5197738359371\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.076194  [    0/ 6092]\n",
      "loss: 2.101961  [ 2000/ 6092]\n",
      "loss: 2.063186  [ 4000/ 6092]\n",
      "loss: 1.994933  [ 6000/ 6092]\n",
      "774.1265386169156\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.960347  [    0/ 6092]\n",
      "loss: 1.983504  [ 2000/ 6092]\n",
      "loss: 2.042298  [ 4000/ 6092]\n",
      "loss: 2.114211  [ 6000/ 6092]\n",
      "774.5162538041671\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.008996  [    0/ 6092]\n",
      "loss: 1.999298  [ 2000/ 6092]\n",
      "loss: 1.991357  [ 4000/ 6092]\n",
      "loss: 2.101910  [ 6000/ 6092]\n",
      "774.7073508550724\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.900129  [    0/ 6092]\n",
      "loss: 2.017773  [ 2000/ 6092]\n",
      "loss: 2.035723  [ 4000/ 6092]\n",
      "loss: 2.080682  [ 6000/ 6092]\n",
      "774.0634416391453\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.034923  [    0/ 6092]\n",
      "loss: 2.186360  [ 2000/ 6092]\n",
      "loss: 1.999951  [ 4000/ 6092]\n",
      "loss: 2.044323  [ 6000/ 6092]\n",
      "775.275331457456\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.010571  [    0/ 6092]\n",
      "loss: 2.061733  [ 2000/ 6092]\n",
      "loss: 2.002220  [ 4000/ 6092]\n",
      "loss: 2.018635  [ 6000/ 6092]\n",
      "774.965185602506\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.954618  [    0/ 6092]\n",
      "loss: 2.076534  [ 2000/ 6092]\n",
      "loss: 2.020632  [ 4000/ 6092]\n",
      "loss: 2.021695  [ 6000/ 6092]\n",
      "772.9240543221434\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.065726  [    0/ 6092]\n",
      "loss: 1.999002  [ 2000/ 6092]\n",
      "loss: 2.029903  [ 4000/ 6092]\n",
      "loss: 2.008497  [ 6000/ 6092]\n",
      "774.6316417654356\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.993177  [    0/ 6092]\n",
      "loss: 1.916595  [ 2000/ 6092]\n",
      "loss: 2.024315  [ 4000/ 6092]\n",
      "loss: 2.062314  [ 6000/ 6092]\n",
      "774.9559569333991\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.972318  [    0/ 6092]\n",
      "loss: 2.028252  [ 2000/ 6092]\n",
      "loss: 2.025532  [ 4000/ 6092]\n",
      "loss: 1.960166  [ 6000/ 6092]\n",
      "774.7049084976315\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.997128  [    0/ 6092]\n",
      "loss: 2.019657  [ 2000/ 6092]\n",
      "loss: 2.135544  [ 4000/ 6092]\n",
      "loss: 2.016318  [ 6000/ 6092]\n",
      "774.1542522559563\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.062473  [    0/ 6092]\n",
      "loss: 2.028659  [ 2000/ 6092]\n",
      "loss: 2.009807  [ 4000/ 6092]\n",
      "loss: 2.058238  [ 6000/ 6092]\n",
      "774.272689409554\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.047752  [    0/ 6092]\n",
      "loss: 2.075364  [ 2000/ 6092]\n",
      "loss: 2.005924  [ 4000/ 6092]\n",
      "loss: 2.047414  [ 6000/ 6092]\n",
      "773.5490310688814\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.943622  [    0/ 6092]\n",
      "loss: 2.029734  [ 2000/ 6092]\n",
      "loss: 2.036814  [ 4000/ 6092]\n",
      "loss: 2.051710  [ 6000/ 6092]\n",
      "776.6890885084867\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.943369  [    0/ 6092]\n",
      "loss: 2.020807  [ 2000/ 6092]\n",
      "loss: 2.097622  [ 4000/ 6092]\n",
      "loss: 2.086197  [ 6000/ 6092]\n",
      "775.1561306963364\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.949253  [    0/ 6092]\n",
      "loss: 2.000822  [ 2000/ 6092]\n",
      "loss: 2.022192  [ 4000/ 6092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m      7\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SSAutoencoderTrainer(batch_size\u001B[38;5;241m=\u001B[39mbatch_size, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model_path\u001B[38;5;241m=\u001B[39mmodels_dir, save_model_every\u001B[38;5;241m=\u001B[39msave_every, lr\u001B[38;5;241m=\u001B[39mstart_lr, dataset\u001B[38;5;241m=\u001B[39mdata_set, device\u001B[38;5;241m=\u001B[39mdevice, model\u001B[38;5;241m=\u001B[39mmodel, loss_fn\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss())\n\u001B[0;32m----> 8\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/encoder/ss_trainer_bottleneck_encoder.py:54\u001B[0m, in \u001B[0;36mSSAutoencoderTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     52\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/encoder/ss_trainer_bottleneck_encoder.py:69\u001B[0m, in \u001B[0;36mSSAutoencoderTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m#X = X/X.norm(dim=2, keepdim=True)\u001B[39;00m\n\u001B[1;32m     68\u001B[0m X \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(X, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 69\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m pred \u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m/\u001B[39mpred\u001B[38;5;241m.\u001B[39mnorm(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     73\u001B[0m target_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(np\u001B[38;5;241m.\u001B[39midentity(\u001B[38;5;28mlen\u001B[39m(X)))\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/encoder/ss_bottleneck_encoder.py:23\u001B[0m, in \u001B[0;36mSSBottelneckLayer.forward\u001B[0;34m(self, x, return_code, flatten)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear1m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;241m512\u001B[39m \u001B[38;5;241m*\u001B[39m num_cols \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m512\u001B[39m \u001B[38;5;241m*\u001B[39m num_cols)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdroput \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mDropout(dropout)\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, return_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, flatten \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     24\u001B[0m     relu \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu\n\u001B[1;32m     25\u001B[0m     tanh \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_trainer_bottleneck_encoder import SSAutoencoderTrainer\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/AUTO_encoder/Nr1/1500_more/\"\n",
    "epochs = 1500\n",
    "safe_model_every = 100\n",
    "start_lr = 1e-6\n",
    "gc.collect()\n",
    "trainer = SSAutoencoderTrainer(batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=model, loss_fn=torch.nn.CrossEntropyLoss())\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
