{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 18:58:30.286051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 18:58:30.802525: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 18:58:30.802583: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 18:58:30.802588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_dims_class_model import SS_Enc_Class_Dims\n",
    "from network_models.soundsream_models_and_utils.ss_encoderbased_model import SSClipBasedModel\n",
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 12\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/from_dims/Nr1/\"\n",
    "safe_model_every = 50\n",
    "epochs = 1000\n",
    "save_highest_acc_min_acc = 0.5\n",
    "lr = 1e-6\n",
    "gc.collect()\n",
    "\n",
    "# data_set= ss_encoded_dataset_full(\n",
    "#     csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_clip_like.pkl\", device=\"cuda\")\n",
    "# trainDS, testDs = train_val_dataset(data_set, val_split=0.1, seed=100)\n",
    "#\n",
    "# torch.manual_seed(300)\n",
    "# model = SSClipBasedModel(dropout=0.3).to(device)\n",
    "\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_encoder.pkl\", device=\"cuda\")\n",
    "trainDS, testDs = train_val_dataset(data_set, val_split=0.1, seed=100)\n",
    "torch.manual_seed(300)\n",
    "model = SS_Enc_Class_Dims().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.714306  [    0/ 5482]\n",
      "loss: 1.751278  [ 1200/ 5482]\n",
      "loss: 1.696393  [ 2400/ 5482]\n",
      "loss: 1.828271  [ 3600/ 5482]\n",
      "loss: 1.834684  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.323     0.517     0.152    99\n",
      " disgust     0.323     0.440     0.374    107\n",
      "    fear     0.323     0.275     0.412    80\n",
      "   happy     0.323     0.269     0.234    77\n",
      " neutral     0.323     0.400     0.063    95\n",
      "     sad     0.323     0.327     0.582    91\n",
      "surprise     0.323     0.254     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.323     0.355     0.335    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 1.641371 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.309931  [    0/ 5482]\n",
      "loss: 1.900506  [ 1200/ 5482]\n",
      "loss: 1.745123  [ 2400/ 5482]\n",
      "loss: 1.828570  [ 3600/ 5482]\n",
      "loss: 1.675136  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.334     0.500     0.152    99\n",
      " disgust     0.334     0.475     0.439    107\n",
      "    fear     0.334     0.275     0.412    80\n",
      "   happy     0.334     0.265     0.234    77\n",
      " neutral     0.334     0.375     0.063    95\n",
      "     sad     0.334     0.340     0.582    91\n",
      "surprise     0.334     0.264     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.334     0.356     0.344    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.641116 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.819764  [    0/ 5482]\n",
      "loss: 1.914254  [ 1200/ 5482]\n",
      "loss: 1.504216  [ 2400/ 5482]\n",
      "loss: 1.763548  [ 3600/ 5482]\n",
      "loss: 1.531830  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.334     0.517     0.152    99\n",
      " disgust     0.334     0.475     0.449    107\n",
      "    fear     0.334     0.273     0.412    80\n",
      "   happy     0.334     0.261     0.234    77\n",
      " neutral     0.334     0.333     0.053    95\n",
      "     sad     0.334     0.342     0.582    91\n",
      "surprise     0.334     0.267     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.334     0.353     0.344    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.640799 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.679652  [    0/ 5482]\n",
      "loss: 1.797985  [ 1200/ 5482]\n",
      "loss: 1.388262  [ 2400/ 5482]\n",
      "loss: 1.504407  [ 3600/ 5482]\n",
      "loss: 1.770374  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.533     0.162    99\n",
      " disgust     0.336     0.475     0.449    107\n",
      "    fear     0.336     0.279     0.425    80\n",
      "   happy     0.336     0.261     0.234    77\n",
      " neutral     0.336     0.333     0.053    95\n",
      "     sad     0.336     0.342     0.582    91\n",
      "surprise     0.336     0.263     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.355     0.345    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.640456 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.551464  [    0/ 5482]\n",
      "loss: 1.721009  [ 1200/ 5482]\n",
      "loss: 1.392335  [ 2400/ 5482]\n",
      "loss: 1.295573  [ 3600/ 5482]\n",
      "loss: 1.630074  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.533     0.162    99\n",
      " disgust     0.338     0.475     0.449    107\n",
      "    fear     0.338     0.281     0.425    80\n",
      "   happy     0.338     0.261     0.234    77\n",
      " neutral     0.338     0.375     0.063    95\n",
      "     sad     0.338     0.340     0.582    91\n",
      "surprise     0.338     0.265     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.361     0.346    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.640221 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.570885  [    0/ 5482]\n",
      "loss: 1.739922  [ 1200/ 5482]\n",
      "loss: 1.696272  [ 2400/ 5482]\n",
      "loss: 1.732608  [ 3600/ 5482]\n",
      "loss: 1.287174  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.341     0.533     0.162    99\n",
      " disgust     0.341     0.480     0.458    107\n",
      "    fear     0.341     0.281     0.425    80\n",
      "   happy     0.341     0.261     0.234    77\n",
      " neutral     0.341     0.375     0.063    95\n",
      "     sad     0.341     0.346     0.593    91\n",
      "surprise     0.341     0.267     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.341     0.363     0.349    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.639871 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.634979  [    0/ 5482]\n",
      "loss: 1.879943  [ 1200/ 5482]\n",
      "loss: 1.641332  [ 2400/ 5482]\n",
      "loss: 1.552333  [ 3600/ 5482]\n",
      "loss: 1.463394  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.343     0.533     0.162    99\n",
      " disgust     0.343     0.486     0.477    107\n",
      "    fear     0.343     0.281     0.425    80\n",
      "   happy     0.343     0.257     0.234    77\n",
      " neutral     0.343     0.375     0.063    95\n",
      "     sad     0.343     0.349     0.582    91\n",
      "surprise     0.343     0.267     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.343     0.364     0.350    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.639716 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.748818  [    0/ 5482]\n",
      "loss: 1.378063  [ 1200/ 5482]\n",
      "loss: 1.894978  [ 2400/ 5482]\n",
      "loss: 1.574944  [ 3600/ 5482]\n",
      "loss: 1.858503  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.533     0.162    99\n",
      " disgust     0.344     0.485     0.467    107\n",
      "    fear     0.344     0.287     0.438    80\n",
      "   happy     0.344     0.257     0.234    77\n",
      " neutral     0.344     0.353     0.063    95\n",
      "     sad     0.344     0.351     0.593    91\n",
      "surprise     0.344     0.272     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.363     0.352    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.639449 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.929140  [    0/ 5482]\n",
      "loss: 1.448281  [ 1200/ 5482]\n",
      "loss: 1.568937  [ 2400/ 5482]\n",
      "loss: 1.611356  [ 3600/ 5482]\n",
      "loss: 1.611457  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.343     0.533     0.162    99\n",
      " disgust     0.343     0.480     0.458    107\n",
      "    fear     0.343     0.287     0.438    80\n",
      "   happy     0.343     0.257     0.234    77\n",
      " neutral     0.343     0.375     0.063    95\n",
      "     sad     0.343     0.346     0.593    91\n",
      "surprise     0.343     0.272     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.343     0.364     0.351    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.639190 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.690327  [    0/ 5482]\n",
      "loss: 1.712744  [ 1200/ 5482]\n",
      "loss: 1.870187  [ 2400/ 5482]\n",
      "loss: 2.006203  [ 3600/ 5482]\n",
      "loss: 1.850745  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.533     0.162    99\n",
      " disgust     0.344     0.476     0.458    107\n",
      "    fear     0.344     0.289     0.438    80\n",
      "   happy     0.344     0.257     0.234    77\n",
      " neutral     0.344     0.412     0.074    95\n",
      "     sad     0.344     0.346     0.593    91\n",
      "surprise     0.344     0.274     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.370     0.352    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.638855 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.549572  [    0/ 5482]\n",
      "loss: 1.667040  [ 1200/ 5482]\n",
      "loss: 1.537242  [ 2400/ 5482]\n",
      "loss: 1.861068  [ 3600/ 5482]\n",
      "loss: 1.777229  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.533     0.162    99\n",
      " disgust     0.344     0.476     0.458    107\n",
      "    fear     0.344     0.287     0.438    80\n",
      "   happy     0.344     0.257     0.234    77\n",
      " neutral     0.344     0.412     0.074    95\n",
      "     sad     0.344     0.346     0.593    91\n",
      "surprise     0.344     0.277     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.370     0.352    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.638660 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.572398  [    0/ 5482]\n",
      "loss: 1.683629  [ 1200/ 5482]\n",
      "loss: 1.872988  [ 2400/ 5482]\n",
      "loss: 1.546792  [ 3600/ 5482]\n",
      "loss: 2.191139  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.533     0.162    99\n",
      " disgust     0.344     0.476     0.458    107\n",
      "    fear     0.344     0.287     0.438    80\n",
      "   happy     0.344     0.254     0.234    77\n",
      " neutral     0.344     0.412     0.074    95\n",
      "     sad     0.344     0.346     0.593    91\n",
      "surprise     0.344     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.370     0.352    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.638355 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.866781  [    0/ 5482]\n",
      "loss: 1.757780  [ 1200/ 5482]\n",
      "loss: 1.632288  [ 2400/ 5482]\n",
      "loss: 1.350706  [ 3600/ 5482]\n",
      "loss: 1.452320  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.346     0.548     0.172    99\n",
      " disgust     0.346     0.476     0.458    107\n",
      "    fear     0.346     0.287     0.438    80\n",
      "   happy     0.346     0.257     0.234    77\n",
      " neutral     0.346     0.412     0.074    95\n",
      "     sad     0.346     0.346     0.593    91\n",
      "surprise     0.346     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.346     0.372     0.354    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 1.638026 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.330814  [    0/ 5482]\n",
      "loss: 1.458673  [ 1200/ 5482]\n",
      "loss: 1.497049  [ 2400/ 5482]\n",
      "loss: 1.457872  [ 3600/ 5482]\n",
      "loss: 1.419650  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.348     0.548     0.172    99\n",
      " disgust     0.348     0.476     0.458    107\n",
      "    fear     0.348     0.287     0.438    80\n",
      "   happy     0.348     0.257     0.234    77\n",
      " neutral     0.348     0.421     0.084    95\n",
      "     sad     0.348     0.351     0.593    91\n",
      "surprise     0.348     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.348     0.374     0.355    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.637765 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.557457  [    0/ 5482]\n",
      "loss: 1.696700  [ 1200/ 5482]\n",
      "loss: 1.478918  [ 2400/ 5482]\n",
      "loss: 1.626640  [ 3600/ 5482]\n",
      "loss: 1.852257  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.348     0.548     0.172    99\n",
      " disgust     0.348     0.476     0.458    107\n",
      "    fear     0.348     0.287     0.438    80\n",
      "   happy     0.348     0.257     0.234    77\n",
      " neutral     0.348     0.421     0.084    95\n",
      "     sad     0.348     0.351     0.593    91\n",
      "surprise     0.348     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.348     0.374     0.355    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.637523 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.941280  [    0/ 5482]\n",
      "loss: 1.868425  [ 1200/ 5482]\n",
      "loss: 1.647539  [ 2400/ 5482]\n",
      "loss: 1.731228  [ 3600/ 5482]\n",
      "loss: 1.524765  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.349     0.548     0.172    99\n",
      " disgust     0.349     0.476     0.458    107\n",
      "    fear     0.349     0.287     0.438    80\n",
      "   happy     0.349     0.257     0.234    77\n",
      " neutral     0.349     0.450     0.095    95\n",
      "     sad     0.349     0.353     0.593    91\n",
      "surprise     0.349     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.349     0.379     0.357    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 1.637204 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.583726  [    0/ 5482]\n",
      "loss: 1.636663  [ 1200/ 5482]\n",
      "loss: 1.774951  [ 2400/ 5482]\n",
      "loss: 1.662470  [ 3600/ 5482]\n",
      "loss: 1.773025  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.351     0.567     0.172    99\n",
      " disgust     0.351     0.481     0.467    107\n",
      "    fear     0.351     0.289     0.438    80\n",
      "   happy     0.351     0.254     0.234    77\n",
      " neutral     0.351     0.429     0.095    95\n",
      "     sad     0.351     0.355     0.593    91\n",
      "surprise     0.351     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.351     0.379     0.358    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 1.636875 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.684379  [    0/ 5482]\n",
      "loss: 1.432779  [ 1200/ 5482]\n",
      "loss: 1.925133  [ 2400/ 5482]\n",
      "loss: 1.732339  [ 3600/ 5482]\n",
      "loss: 1.728785  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.351     0.567     0.172    99\n",
      " disgust     0.351     0.481     0.467    107\n",
      "    fear     0.351     0.289     0.438    80\n",
      "   happy     0.351     0.254     0.234    77\n",
      " neutral     0.351     0.429     0.095    95\n",
      "     sad     0.351     0.355     0.593    91\n",
      "surprise     0.351     0.279     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.351     0.379     0.358    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 1.636644 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.676907  [    0/ 5482]\n",
      "loss: 1.637392  [ 1200/ 5482]\n",
      "loss: 1.689858  [ 2400/ 5482]\n",
      "loss: 1.647627  [ 3600/ 5482]\n",
      "loss: 1.523193  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.586     0.172    99\n",
      " disgust     0.352     0.471     0.458    107\n",
      "    fear     0.352     0.289     0.438    80\n",
      "   happy     0.352     0.257     0.234    77\n",
      " neutral     0.352     0.455     0.105    95\n",
      "     sad     0.352     0.355     0.593    91\n",
      "surprise     0.352     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.386     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.636325 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.614322  [    0/ 5482]\n",
      "loss: 1.690539  [ 1200/ 5482]\n",
      "loss: 1.673233  [ 2400/ 5482]\n",
      "loss: 1.634953  [ 3600/ 5482]\n",
      "loss: 1.840080  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.607     0.172    99\n",
      " disgust     0.356     0.476     0.467    107\n",
      "    fear     0.356     0.289     0.438    80\n",
      "   happy     0.356     0.257     0.234    77\n",
      " neutral     0.356     0.478     0.116    95\n",
      "     sad     0.356     0.358     0.593    91\n",
      "surprise     0.356     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.393     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.635980 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.657854  [    0/ 5482]\n",
      "loss: 1.435399  [ 1200/ 5482]\n",
      "loss: 1.509399  [ 2400/ 5482]\n",
      "loss: 1.986838  [ 3600/ 5482]\n",
      "loss: 1.591389  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.607     0.172    99\n",
      " disgust     0.356     0.471     0.458    107\n",
      "    fear     0.356     0.295     0.450    80\n",
      "   happy     0.356     0.257     0.234    77\n",
      " neutral     0.356     0.478     0.116    95\n",
      "     sad     0.356     0.355     0.593    91\n",
      "surprise     0.356     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.393     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.635664 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.564488  [    0/ 5482]\n",
      "loss: 1.277289  [ 1200/ 5482]\n",
      "loss: 1.801503  [ 2400/ 5482]\n",
      "loss: 1.932855  [ 3600/ 5482]\n",
      "loss: 2.034782  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.607     0.172    99\n",
      " disgust     0.356     0.471     0.458    107\n",
      "    fear     0.356     0.295     0.450    80\n",
      "   happy     0.356     0.257     0.234    77\n",
      " neutral     0.356     0.478     0.116    95\n",
      "     sad     0.356     0.355     0.593    91\n",
      "surprise     0.356     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.393     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.635309 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.527092  [    0/ 5482]\n",
      "loss: 1.777816  [ 1200/ 5482]\n",
      "loss: 1.689169  [ 2400/ 5482]\n",
      "loss: 1.787108  [ 3600/ 5482]\n",
      "loss: 1.500756  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.607     0.172    99\n",
      " disgust     0.357     0.476     0.467    107\n",
      "    fear     0.357     0.295     0.450    80\n",
      "   happy     0.357     0.257     0.234    77\n",
      " neutral     0.357     0.478     0.116    95\n",
      "     sad     0.357     0.358     0.593    91\n",
      "surprise     0.357     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.394     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.635002 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.061972  [    0/ 5482]\n",
      "loss: 1.460787  [ 1200/ 5482]\n",
      "loss: 1.978083  [ 2400/ 5482]\n",
      "loss: 1.608425  [ 3600/ 5482]\n",
      "loss: 1.643718  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.607     0.172    99\n",
      " disgust     0.357     0.476     0.467    107\n",
      "    fear     0.357     0.298     0.450    80\n",
      "   happy     0.357     0.254     0.234    77\n",
      " neutral     0.357     0.478     0.116    95\n",
      "     sad     0.357     0.355     0.593    91\n",
      "surprise     0.357     0.291     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.394     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.634718 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.969523  [    0/ 5482]\n",
      "loss: 1.466991  [ 1200/ 5482]\n",
      "loss: 1.752096  [ 2400/ 5482]\n",
      "loss: 1.576825  [ 3600/ 5482]\n",
      "loss: 1.565052  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.607     0.172    99\n",
      " disgust     0.357     0.476     0.467    107\n",
      "    fear     0.357     0.298     0.450    80\n",
      "   happy     0.357     0.254     0.234    77\n",
      " neutral     0.357     0.478     0.116    95\n",
      "     sad     0.357     0.355     0.593    91\n",
      "surprise     0.357     0.291     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.394     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.634398 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.415070  [    0/ 5482]\n",
      "loss: 2.101791  [ 1200/ 5482]\n",
      "loss: 1.578848  [ 2400/ 5482]\n",
      "loss: 1.835815  [ 3600/ 5482]\n",
      "loss: 1.594606  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.607     0.172    99\n",
      " disgust     0.357     0.476     0.467    107\n",
      "    fear     0.357     0.298     0.450    80\n",
      "   happy     0.357     0.257     0.234    77\n",
      " neutral     0.357     0.478     0.116    95\n",
      "     sad     0.357     0.355     0.593    91\n",
      "surprise     0.357     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.394     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.634006 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.594157  [    0/ 5482]\n",
      "loss: 1.596262  [ 1200/ 5482]\n",
      "loss: 1.648350  [ 2400/ 5482]\n",
      "loss: 1.384658  [ 3600/ 5482]\n",
      "loss: 1.743505  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.607     0.172    99\n",
      " disgust     0.357     0.476     0.467    107\n",
      "    fear     0.357     0.298     0.450    80\n",
      "   happy     0.357     0.254     0.234    77\n",
      " neutral     0.357     0.478     0.116    95\n",
      "     sad     0.357     0.355     0.593    91\n",
      "surprise     0.357     0.291     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.394     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.633789 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.837130  [    0/ 5482]\n",
      "loss: 1.535209  [ 1200/ 5482]\n",
      "loss: 1.594751  [ 2400/ 5482]\n",
      "loss: 1.853188  [ 3600/ 5482]\n",
      "loss: 1.455871  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.607     0.172    99\n",
      " disgust     0.357     0.476     0.467    107\n",
      "    fear     0.357     0.298     0.450    80\n",
      "   happy     0.357     0.254     0.234    77\n",
      " neutral     0.357     0.478     0.116    95\n",
      "     sad     0.357     0.355     0.593    91\n",
      "surprise     0.357     0.291     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.394     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.633375 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.811530  [    0/ 5482]\n",
      "loss: 1.477198  [ 1200/ 5482]\n",
      "loss: 1.802747  [ 2400/ 5482]\n",
      "loss: 1.401273  [ 3600/ 5482]\n",
      "loss: 1.948887  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.593     0.162    99\n",
      " disgust     0.354     0.471     0.458    107\n",
      "    fear     0.354     0.298     0.450    80\n",
      "   happy     0.354     0.257     0.234    77\n",
      " neutral     0.354     0.478     0.116    95\n",
      "     sad     0.354     0.355     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.391     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.633187 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.695557  [    0/ 5482]\n",
      "loss: 1.523220  [ 1200/ 5482]\n",
      "loss: 1.511012  [ 2400/ 5482]\n",
      "loss: 1.629263  [ 3600/ 5482]\n",
      "loss: 1.894329  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.593     0.162    99\n",
      " disgust     0.356     0.476     0.467    107\n",
      "    fear     0.356     0.292     0.438    80\n",
      "   happy     0.356     0.254     0.234    77\n",
      " neutral     0.356     0.478     0.116    95\n",
      "     sad     0.356     0.362     0.604    91\n",
      "surprise     0.356     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.391     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.632788 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.412448  [    0/ 5482]\n",
      "loss: 1.784812  [ 1200/ 5482]\n",
      "loss: 1.610574  [ 2400/ 5482]\n",
      "loss: 1.917807  [ 3600/ 5482]\n",
      "loss: 1.773714  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.593     0.162    99\n",
      " disgust     0.356     0.476     0.467    107\n",
      "    fear     0.356     0.292     0.438    80\n",
      "   happy     0.356     0.250     0.234    77\n",
      " neutral     0.356     0.478     0.116    95\n",
      "     sad     0.356     0.362     0.604    91\n",
      "surprise     0.356     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.391     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.632495 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.425609  [    0/ 5482]\n",
      "loss: 1.475043  [ 1200/ 5482]\n",
      "loss: 1.570929  [ 2400/ 5482]\n",
      "loss: 1.689581  [ 3600/ 5482]\n",
      "loss: 1.584430  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.571     0.162    99\n",
      " disgust     0.356     0.476     0.467    107\n",
      "    fear     0.356     0.294     0.438    80\n",
      "   happy     0.356     0.250     0.234    77\n",
      " neutral     0.356     0.478     0.116    95\n",
      "     sad     0.356     0.362     0.604    91\n",
      "surprise     0.356     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.389     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.632193 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.884492  [    0/ 5482]\n",
      "loss: 1.757440  [ 1200/ 5482]\n",
      "loss: 1.868456  [ 2400/ 5482]\n",
      "loss: 1.943785  [ 3600/ 5482]\n",
      "loss: 1.566865  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.571     0.162    99\n",
      " disgust     0.354     0.476     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.250     0.234    77\n",
      " neutral     0.354     0.458     0.116    95\n",
      "     sad     0.354     0.358     0.593    91\n",
      "surprise     0.354     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.385     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.631845 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.515048  [    0/ 5482]\n",
      "loss: 1.346729  [ 1200/ 5482]\n",
      "loss: 1.442859  [ 2400/ 5482]\n",
      "loss: 1.380257  [ 3600/ 5482]\n",
      "loss: 1.495886  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.571     0.162    99\n",
      " disgust     0.354     0.476     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.250     0.234    77\n",
      " neutral     0.354     0.458     0.116    95\n",
      "     sad     0.354     0.358     0.593    91\n",
      "surprise     0.354     0.288     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.385     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.631544 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.913372  [    0/ 5482]\n",
      "loss: 1.541270  [ 1200/ 5482]\n",
      "loss: 1.944461  [ 2400/ 5482]\n",
      "loss: 1.748220  [ 3600/ 5482]\n",
      "loss: 1.623232  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.571     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.294     0.438    80\n",
      "   happy     0.352     0.239     0.221    77\n",
      " neutral     0.352     0.458     0.116    95\n",
      "     sad     0.352     0.358     0.593    91\n",
      "surprise     0.352     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.383     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.631302 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.800571  [    0/ 5482]\n",
      "loss: 1.486198  [ 1200/ 5482]\n",
      "loss: 1.913790  [ 2400/ 5482]\n",
      "loss: 1.516973  [ 3600/ 5482]\n",
      "loss: 1.760232  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.571     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.294     0.438    80\n",
      "   happy     0.352     0.239     0.221    77\n",
      " neutral     0.352     0.458     0.116    95\n",
      "     sad     0.352     0.358     0.593    91\n",
      "surprise     0.352     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.383     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.630887 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.709392  [    0/ 5482]\n",
      "loss: 1.642346  [ 1200/ 5482]\n",
      "loss: 1.601463  [ 2400/ 5482]\n",
      "loss: 1.253941  [ 3600/ 5482]\n",
      "loss: 1.440202  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.571     0.162    99\n",
      " disgust     0.354     0.476     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.221    77\n",
      " neutral     0.354     0.480     0.126    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.387     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.630635 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.561373  [    0/ 5482]\n",
      "loss: 1.809296  [ 1200/ 5482]\n",
      "loss: 1.507987  [ 2400/ 5482]\n",
      "loss: 1.599590  [ 3600/ 5482]\n",
      "loss: 1.664050  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.552     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.243     0.221    77\n",
      " neutral     0.354     0.480     0.126    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.385     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.630372 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.519968  [    0/ 5482]\n",
      "loss: 1.581592  [ 1200/ 5482]\n",
      "loss: 2.016659  [ 2400/ 5482]\n",
      "loss: 1.444054  [ 3600/ 5482]\n",
      "loss: 1.268933  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.552     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.243     0.221    77\n",
      " neutral     0.354     0.480     0.126    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.385     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.630061 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.512592  [    0/ 5482]\n",
      "loss: 1.496708  [ 1200/ 5482]\n",
      "loss: 1.588999  [ 2400/ 5482]\n",
      "loss: 1.823237  [ 3600/ 5482]\n",
      "loss: 1.705997  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.533     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.250     0.221    77\n",
      " neutral     0.354     0.480     0.126    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.383     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.629652 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.649422  [    0/ 5482]\n",
      "loss: 1.993705  [ 1200/ 5482]\n",
      "loss: 1.424892  [ 2400/ 5482]\n",
      "loss: 1.613852  [ 3600/ 5482]\n",
      "loss: 1.457238  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.533     0.162    99\n",
      " disgust     0.356     0.481     0.467    107\n",
      "    fear     0.356     0.294     0.438    80\n",
      "   happy     0.356     0.250     0.221    77\n",
      " neutral     0.356     0.500     0.137    95\n",
      "     sad     0.356     0.362     0.593    91\n",
      "surprise     0.356     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.386     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.629333 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.797191  [    0/ 5482]\n",
      "loss: 1.632755  [ 1200/ 5482]\n",
      "loss: 1.791074  [ 2400/ 5482]\n",
      "loss: 1.700426  [ 3600/ 5482]\n",
      "loss: 1.744114  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.500     0.137    95\n",
      "     sad     0.354     0.362     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.382     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.629008 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.910808  [    0/ 5482]\n",
      "loss: 1.784087  [ 1200/ 5482]\n",
      "loss: 1.731923  [ 2400/ 5482]\n",
      "loss: 1.562140  [ 3600/ 5482]\n",
      "loss: 1.934432  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.500     0.137    95\n",
      "     sad     0.354     0.362     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.382     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.628726 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.719893  [    0/ 5482]\n",
      "loss: 1.769267  [ 1200/ 5482]\n",
      "loss: 1.503807  [ 2400/ 5482]\n",
      "loss: 1.928028  [ 3600/ 5482]\n",
      "loss: 1.351041  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.500     0.137    95\n",
      "     sad     0.354     0.362     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.382     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.628536 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.440662  [    0/ 5482]\n",
      "loss: 1.673324  [ 1200/ 5482]\n",
      "loss: 1.354699  [ 2400/ 5482]\n",
      "loss: 1.525520  [ 3600/ 5482]\n",
      "loss: 1.625799  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.500     0.137    95\n",
      "     sad     0.354     0.362     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.382     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.628187 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.782810  [    0/ 5482]\n",
      "loss: 1.598814  [ 1200/ 5482]\n",
      "loss: 1.569010  [ 2400/ 5482]\n",
      "loss: 1.602698  [ 3600/ 5482]\n",
      "loss: 2.027195  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.627877 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.127065  [    0/ 5482]\n",
      "loss: 1.589159  [ 1200/ 5482]\n",
      "loss: 1.715438  [ 2400/ 5482]\n",
      "loss: 1.937047  [ 3600/ 5482]\n",
      "loss: 1.775073  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.281     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.627583 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.774977  [    0/ 5482]\n",
      "loss: 1.832768  [ 1200/ 5482]\n",
      "loss: 1.625012  [ 2400/ 5482]\n",
      "loss: 1.733691  [ 3600/ 5482]\n",
      "loss: 1.449795  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.292     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.627230 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.536696  [    0/ 5482]\n",
      "loss: 1.529900  [ 1200/ 5482]\n",
      "loss: 1.504794  [ 2400/ 5482]\n",
      "loss: 1.446891  [ 3600/ 5482]\n",
      "loss: 1.643787  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.292     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.626961 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.266174  [    0/ 5482]\n",
      "loss: 1.518120  [ 1200/ 5482]\n",
      "loss: 1.708947  [ 2400/ 5482]\n",
      "loss: 1.510959  [ 3600/ 5482]\n",
      "loss: 1.793164  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.297     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.626713 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.720285  [    0/ 5482]\n",
      "loss: 1.696991  [ 1200/ 5482]\n",
      "loss: 1.589106  [ 2400/ 5482]\n",
      "loss: 1.407894  [ 3600/ 5482]\n",
      "loss: 1.655312  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.626376 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.495722  [    0/ 5482]\n",
      "loss: 1.752207  [ 1200/ 5482]\n",
      "loss: 1.722952  [ 2400/ 5482]\n",
      "loss: 1.880829  [ 3600/ 5482]\n",
      "loss: 1.724246  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.626008 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.790113  [    0/ 5482]\n",
      "loss: 1.390113  [ 1200/ 5482]\n",
      "loss: 1.628067  [ 2400/ 5482]\n",
      "loss: 1.699227  [ 3600/ 5482]\n",
      "loss: 1.502177  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.625696 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.786999  [    0/ 5482]\n",
      "loss: 1.493991  [ 1200/ 5482]\n",
      "loss: 1.912793  [ 2400/ 5482]\n",
      "loss: 1.542536  [ 3600/ 5482]\n",
      "loss: 1.811520  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.362     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.625617 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.492632  [    0/ 5482]\n",
      "loss: 1.834277  [ 1200/ 5482]\n",
      "loss: 1.778416  [ 2400/ 5482]\n",
      "loss: 1.792429  [ 3600/ 5482]\n",
      "loss: 1.775999  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.625111 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.026074  [    0/ 5482]\n",
      "loss: 1.739121  [ 1200/ 5482]\n",
      "loss: 1.654477  [ 2400/ 5482]\n",
      "loss: 1.649786  [ 3600/ 5482]\n",
      "loss: 1.666997  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.624849 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.655337  [    0/ 5482]\n",
      "loss: 1.465642  [ 1200/ 5482]\n",
      "loss: 1.544991  [ 2400/ 5482]\n",
      "loss: 1.750471  [ 3600/ 5482]\n",
      "loss: 1.837785  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.516     0.162    99\n",
      " disgust     0.352     0.481     0.467    107\n",
      "    fear     0.352     0.288     0.425    80\n",
      "   happy     0.352     0.239     0.208    77\n",
      " neutral     0.352     0.464     0.137    95\n",
      "     sad     0.352     0.362     0.593    91\n",
      "surprise     0.352     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.376     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.624681 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.794501  [    0/ 5482]\n",
      "loss: 1.488657  [ 1200/ 5482]\n",
      "loss: 1.646297  [ 2400/ 5482]\n",
      "loss: 1.820206  [ 3600/ 5482]\n",
      "loss: 1.377928  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.294     0.438    80\n",
      "   happy     0.354     0.239     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.380     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.624427 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.657624  [    0/ 5482]\n",
      "loss: 1.689462  [ 1200/ 5482]\n",
      "loss: 1.428030  [ 2400/ 5482]\n",
      "loss: 1.514105  [ 3600/ 5482]\n",
      "loss: 1.577490  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.516     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.288     0.425    80\n",
      "   happy     0.352     0.242     0.208    77\n",
      " neutral     0.352     0.464     0.137    95\n",
      "     sad     0.352     0.360     0.593    91\n",
      "surprise     0.352     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.376     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.623925 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.579395  [    0/ 5482]\n",
      "loss: 1.627711  [ 1200/ 5482]\n",
      "loss: 1.572299  [ 2400/ 5482]\n",
      "loss: 2.221160  [ 3600/ 5482]\n",
      "loss: 1.623540  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.516     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.288     0.425    80\n",
      "   happy     0.352     0.242     0.208    77\n",
      " neutral     0.352     0.464     0.137    95\n",
      "     sad     0.352     0.360     0.593    91\n",
      "surprise     0.352     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.376     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.623669 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.923928  [    0/ 5482]\n",
      "loss: 1.509384  [ 1200/ 5482]\n",
      "loss: 1.434660  [ 2400/ 5482]\n",
      "loss: 1.494099  [ 3600/ 5482]\n",
      "loss: 1.769691  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.500     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.288     0.425    80\n",
      "   happy     0.352     0.246     0.208    77\n",
      " neutral     0.352     0.464     0.137    95\n",
      "     sad     0.352     0.360     0.593    91\n",
      "surprise     0.352     0.286     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.374     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.623506 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.496672  [    0/ 5482]\n",
      "loss: 1.680920  [ 1200/ 5482]\n",
      "loss: 1.494685  [ 2400/ 5482]\n",
      "loss: 1.372416  [ 3600/ 5482]\n",
      "loss: 1.535297  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.516     0.162    99\n",
      " disgust     0.354     0.481     0.477    107\n",
      "    fear     0.354     0.288     0.425    80\n",
      "   happy     0.354     0.242     0.208    77\n",
      " neutral     0.354     0.481     0.137    95\n",
      "     sad     0.354     0.362     0.593    91\n",
      "surprise     0.354     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.379     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.623192 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.639437  [    0/ 5482]\n",
      "loss: 1.692326  [ 1200/ 5482]\n",
      "loss: 1.883582  [ 2400/ 5482]\n",
      "loss: 1.554268  [ 3600/ 5482]\n",
      "loss: 1.603146  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.500     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.288     0.425    80\n",
      "   happy     0.352     0.246     0.208    77\n",
      " neutral     0.352     0.481     0.137    95\n",
      "     sad     0.352     0.360     0.593    91\n",
      "surprise     0.352     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.376     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.622929 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.726935  [    0/ 5482]\n",
      "loss: 1.840460  [ 1200/ 5482]\n",
      "loss: 1.636376  [ 2400/ 5482]\n",
      "loss: 1.783233  [ 3600/ 5482]\n",
      "loss: 1.406186  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.500     0.162    99\n",
      " disgust     0.352     0.476     0.467    107\n",
      "    fear     0.352     0.288     0.425    80\n",
      "   happy     0.352     0.246     0.208    77\n",
      " neutral     0.352     0.481     0.137    95\n",
      "     sad     0.352     0.360     0.593    91\n",
      "surprise     0.352     0.283     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.376     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.622611 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.777152  [    0/ 5482]\n",
      "loss: 1.962425  [ 1200/ 5482]\n",
      "loss: 1.703911  [ 2400/ 5482]\n",
      "loss: 1.594140  [ 3600/ 5482]\n",
      "loss: 1.400701  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.485     0.162    99\n",
      " disgust     0.354     0.476     0.467    107\n",
      "    fear     0.354     0.288     0.425    80\n",
      "   happy     0.354     0.254     0.208    77\n",
      " neutral     0.354     0.464     0.137    95\n",
      "     sad     0.354     0.360     0.593    91\n",
      "surprise     0.354     0.292     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.622382 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.525689  [    0/ 5482]\n",
      "loss: 1.429219  [ 1200/ 5482]\n",
      "loss: 1.563801  [ 2400/ 5482]\n",
      "loss: 2.106768  [ 3600/ 5482]\n",
      "loss: 1.629350  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.500     0.162    99\n",
      " disgust     0.354     0.476     0.467    107\n",
      "    fear     0.354     0.288     0.425    80\n",
      "   happy     0.354     0.250     0.208    77\n",
      " neutral     0.354     0.448     0.137    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.622025 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.640054  [    0/ 5482]\n",
      "loss: 1.652198  [ 1200/ 5482]\n",
      "loss: 1.805601  [ 2400/ 5482]\n",
      "loss: 1.617014  [ 3600/ 5482]\n",
      "loss: 1.421484  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.485     0.162    99\n",
      " disgust     0.356     0.481     0.477    107\n",
      "    fear     0.356     0.288     0.425    80\n",
      "   happy     0.356     0.254     0.208    77\n",
      " neutral     0.356     0.448     0.137    95\n",
      "     sad     0.356     0.367     0.593    91\n",
      "surprise     0.356     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.373     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.621714 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.727843  [    0/ 5482]\n",
      "loss: 1.447065  [ 1200/ 5482]\n",
      "loss: 1.693779  [ 2400/ 5482]\n",
      "loss: 2.107135  [ 3600/ 5482]\n",
      "loss: 1.386312  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.485     0.162    99\n",
      " disgust     0.356     0.476     0.467    107\n",
      "    fear     0.356     0.291     0.425    80\n",
      "   happy     0.356     0.254     0.208    77\n",
      " neutral     0.356     0.467     0.147    95\n",
      "     sad     0.356     0.365     0.593    91\n",
      "surprise     0.356     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.375     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.621397 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.493989  [    0/ 5482]\n",
      "loss: 1.844289  [ 1200/ 5482]\n",
      "loss: 1.461836  [ 2400/ 5482]\n",
      "loss: 1.752414  [ 3600/ 5482]\n",
      "loss: 1.544668  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.485     0.162    99\n",
      " disgust     0.356     0.481     0.467    107\n",
      "    fear     0.356     0.291     0.425    80\n",
      "   happy     0.356     0.250     0.208    77\n",
      " neutral     0.356     0.467     0.147    95\n",
      "     sad     0.356     0.365     0.593    91\n",
      "surprise     0.356     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.375     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.621226 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.695843  [    0/ 5482]\n",
      "loss: 1.607699  [ 1200/ 5482]\n",
      "loss: 1.662359  [ 2400/ 5482]\n",
      "loss: 1.672006  [ 3600/ 5482]\n",
      "loss: 1.990953  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.485     0.162    99\n",
      " disgust     0.354     0.476     0.467    107\n",
      "    fear     0.354     0.284     0.412    80\n",
      "   happy     0.354     0.250     0.208    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.620900 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.774632  [    0/ 5482]\n",
      "loss: 1.721191  [ 1200/ 5482]\n",
      "loss: 1.371892  [ 2400/ 5482]\n",
      "loss: 1.865172  [ 3600/ 5482]\n",
      "loss: 1.633571  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.485     0.162    99\n",
      " disgust     0.356     0.481     0.477    107\n",
      "    fear     0.356     0.284     0.412    80\n",
      "   happy     0.356     0.250     0.208    77\n",
      " neutral     0.356     0.467     0.147    95\n",
      "     sad     0.356     0.367     0.593    91\n",
      "surprise     0.356     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.375     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.620559 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.584368  [    0/ 5482]\n",
      "loss: 1.855758  [ 1200/ 5482]\n",
      "loss: 1.743469  [ 2400/ 5482]\n",
      "loss: 1.635158  [ 3600/ 5482]\n",
      "loss: 1.826961  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.485     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.284     0.412    80\n",
      "   happy     0.354     0.246     0.208    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.620452 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.556623  [    0/ 5482]\n",
      "loss: 1.763553  [ 1200/ 5482]\n",
      "loss: 1.669729  [ 2400/ 5482]\n",
      "loss: 1.917523  [ 3600/ 5482]\n",
      "loss: 1.741553  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.485     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.284     0.412    80\n",
      "   happy     0.354     0.246     0.208    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.620197 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.821682  [    0/ 5482]\n",
      "loss: 2.062841  [ 1200/ 5482]\n",
      "loss: 1.609857  [ 2400/ 5482]\n",
      "loss: 1.961731  [ 3600/ 5482]\n",
      "loss: 1.940272  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.485     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.284     0.412    80\n",
      "   happy     0.354     0.246     0.208    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.365     0.593    91\n",
      "surprise     0.354     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.619891 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.401943  [    0/ 5482]\n",
      "loss: 1.577830  [ 1200/ 5482]\n",
      "loss: 1.647140  [ 2400/ 5482]\n",
      "loss: 1.707156  [ 3600/ 5482]\n",
      "loss: 1.691055  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.485     0.162    99\n",
      " disgust     0.354     0.481     0.467    107\n",
      "    fear     0.354     0.284     0.412    80\n",
      "   happy     0.354     0.242     0.208    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.367     0.593    91\n",
      "surprise     0.354     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.619609 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.695208  [    0/ 5482]\n",
      "loss: 1.589504  [ 1200/ 5482]\n",
      "loss: 1.566391  [ 2400/ 5482]\n",
      "loss: 1.669050  [ 3600/ 5482]\n",
      "loss: 1.166993  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.500     0.172    99\n",
      " disgust     0.356     0.481     0.467    107\n",
      "    fear     0.356     0.284     0.412    80\n",
      "   happy     0.356     0.246     0.208    77\n",
      " neutral     0.356     0.467     0.147    95\n",
      "     sad     0.356     0.367     0.593    91\n",
      "surprise     0.356     0.289     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.376     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.619373 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.573591  [    0/ 5482]\n",
      "loss: 1.396251  [ 1200/ 5482]\n",
      "loss: 1.495351  [ 2400/ 5482]\n",
      "loss: 1.761161  [ 3600/ 5482]\n",
      "loss: 1.740632  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.500     0.172    99\n",
      " disgust     0.354     0.485     0.467    107\n",
      "    fear     0.354     0.282     0.412    80\n",
      "   happy     0.354     0.234     0.195    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.367     0.593    91\n",
      "surprise     0.354     0.287     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.375     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.619102 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.537821  [    0/ 5482]\n",
      "loss: 1.601707  [ 1200/ 5482]\n",
      "loss: 1.586435  [ 2400/ 5482]\n",
      "loss: 1.293453  [ 3600/ 5482]\n",
      "loss: 1.729399  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.486     0.172    99\n",
      " disgust     0.354     0.490     0.467    107\n",
      "    fear     0.354     0.282     0.412    80\n",
      "   happy     0.354     0.238     0.195    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.367     0.593    91\n",
      "surprise     0.354     0.284     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.374     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.618877 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.589721  [    0/ 5482]\n",
      "loss: 1.973997  [ 1200/ 5482]\n",
      "loss: 1.399832  [ 2400/ 5482]\n",
      "loss: 1.602879  [ 3600/ 5482]\n",
      "loss: 2.103228  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.354     0.500     0.172    99\n",
      " disgust     0.354     0.485     0.467    107\n",
      "    fear     0.354     0.282     0.412    80\n",
      "   happy     0.354     0.242     0.195    77\n",
      " neutral     0.354     0.467     0.147    95\n",
      "     sad     0.354     0.367     0.593    91\n",
      "surprise     0.354     0.282     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.354     0.375     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.618604 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.421033  [    0/ 5482]\n",
      "loss: 1.576466  [ 1200/ 5482]\n",
      "loss: 1.512862  [ 2400/ 5482]\n",
      "loss: 1.502635  [ 3600/ 5482]\n",
      "loss: 1.540367  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.500     0.182    99\n",
      " disgust     0.356     0.490     0.467    107\n",
      "    fear     0.356     0.282     0.412    80\n",
      "   happy     0.356     0.250     0.195    77\n",
      " neutral     0.356     0.467     0.147    95\n",
      "     sad     0.356     0.367     0.593    91\n",
      "surprise     0.356     0.280     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.377     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.618367 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.761617  [    0/ 5482]\n",
      "loss: 1.995946  [ 1200/ 5482]\n",
      "loss: 1.721846  [ 2400/ 5482]\n",
      "loss: 1.512733  [ 3600/ 5482]\n",
      "loss: 1.444976  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.500     0.182    99\n",
      " disgust     0.356     0.490     0.467    107\n",
      "    fear     0.356     0.282     0.412    80\n",
      "   happy     0.356     0.250     0.195    77\n",
      " neutral     0.356     0.467     0.147    95\n",
      "     sad     0.356     0.367     0.593    91\n",
      "surprise     0.356     0.280     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.377     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.618197 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils/encoder\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundsream_models_and_utils.ss_trainer_gen_models as sset\n",
    "importlib.reload(sset)\n",
    "\n",
    "lr = 1e-4\n",
    "trainer = sset.SSGenModelTrainer(lr=lr, num_epochs=epochs, model=model, train_dataset=trainDS,\n",
    "                            eval_dataset=testDs,\n",
    "                            device=device, labelList=data_set.encoded_dataset.label_list,\n",
    "                            batch_size=batch_size,\n",
    "                            save_model_every=safe_model_every,\n",
    "                            save_highest_acc_min_acc=save_highest_acc_min_acc,\n",
    "                            model_path = models_dir)\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
