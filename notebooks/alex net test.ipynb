{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.6\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "import torchvision as torchvision\n",
    "#from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions defined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndef scale_minmax(X, min=0.0, max=1.0):\\n    X_std = (X - X.min()) / (X.max() - X.min())\\n    X_scaled = X_std * (max - min) + min\\n    return X_scaled\\n\\ndef spectrogram_image(y, sr, out, hop_length, n_mels):\\n    # use log-melspectrogram\\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\\n                                            n_fft=hop_length*2, hop_length=hop_length)\\n    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\\n\\n    # min-max scale to fit inside 8-bit range\\n    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\\n    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\\n    img = 255-img # invert. make black==more energy\\n\\n    # save as PNG\\n    #plt.axis(\\'off\\')\\n    plt.imsave(\"spec.png\",img)\\n    plt.imshow(img)\\n\\ndef createSpectrogramm(path, label):\\n    data, sr = librosa.load(path)\\n    fourier = librosa.stft(data)\\n    fourierdb = librosa.amplitude_to_db(abs(fourier))\\n    librosa.display.specshow(fourierdb, sr=sr, x_axis=\\'time\\', y_axis=\\'hz\\')\\n\\ndef createMelSpectrogramm(path, label):\\n    data, sr = librosa.load(path, sr=22050, offset=0, duration=1)\\n    melSpec = librosa.feature.melspectrogram(data, sr, n_mels=192, n_fft=1024, hop_length=260)\\n\\n    # plt.figure(figsize=(10, 4))\\n    # plt.title(label, size=20)\\n    librosa.display.specshow(melSpec, sr=sr, x_axis=\\'time\\', y_axis=\\'hz\\')\\n    # plt.colorbar()\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def load_custom_dataset():\n",
    "    paths = []\n",
    "    testpaths = []\n",
    "    testlabels = []\n",
    "    terminator = 'D:/Uni/19.Master/Daten/terminator.wav'\n",
    "    print(sys.executable)\n",
    "    labels = []\n",
    "    # for dirname, _, filenames in os.walk('Daten/TESS Toronto emotional speech set data'):\n",
    "    # D:\\Uni\\19.Master\\DATEN\n",
    "    for dirname, _, filenames in os.walk('../tess'):\n",
    "        for filename in filenames:\n",
    "            label = filename.split('_')[-1]\n",
    "            label = label.split('.')[0]\n",
    "            if (label != 'neutral'):\n",
    "                labels.append(label.lower())\n",
    "                paths.append(os.path.join(dirname, filename))\n",
    "    for dirname, _, filenames in os.walk('../stimuli_intensit√§tsmorphs'):\n",
    "        for filename in filenames:\n",
    "\n",
    "            intens = filename.split('_')[-2]\n",
    "            emot = filename.split('_')[1]\n",
    "            label = emot\n",
    "            match label:\n",
    "                case 'ang':\n",
    "                    label = 'angry'\n",
    "                case 'dis':\n",
    "                    label = 'disgust'\n",
    "                case 'fea':\n",
    "                    label = 'fear'\n",
    "                case 'hap':\n",
    "                    label = 'happy'\n",
    "                case 'sad':\n",
    "                    label = 'sad'\n",
    "                case 'sur':\n",
    "                    label = 'ps'\n",
    "            if (emot != 'ple'):\n",
    "                testpaths.append(os.path.join(dirname, filename))\n",
    "                testlabels.append(label.lower())\n",
    "    com_labels = testlabels + labels\n",
    "    com_paths = testpaths + paths\n",
    "    print(testlabels)\n",
    "    print(testpaths)\n",
    "    print('Dataset is loaded')\n",
    "    return paths, labels, testpaths, testlabels\n",
    "\"\"\"\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def spectrogram_image(y, sr, out, hop_length, n_mels):\n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
    "                                            n_fft=hop_length*2, hop_length=hop_length)\n",
    "    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\n",
    "    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "\n",
    "    # save as PNG\n",
    "    #plt.axis('off')\n",
    "    plt.imsave(\"spec.png\",img)\n",
    "    plt.imshow(img)\n",
    "\n",
    "def createSpectrogramm(path, label):\n",
    "    data, sr = librosa.load(path)\n",
    "    fourier = librosa.stft(data)\n",
    "    fourierdb = librosa.amplitude_to_db(abs(fourier))\n",
    "    librosa.display.specshow(fourierdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "\n",
    "def createMelSpectrogramm(path, label):\n",
    "    data, sr = librosa.load(path, sr=22050, offset=0, duration=1)\n",
    "    melSpec = librosa.feature.melspectrogram(data, sr, n_mels=192, n_fft=1024, hop_length=260)\n",
    "\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # plt.title(label, size=20)\n",
    "    librosa.display.specshow(melSpec, sr=sr, x_axis='time', y_axis='hz')\n",
    "    # plt.colorbar()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\Scripts\\python.exe\n",
      "['angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps']\n",
      "['../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_75_70dB.wav']\n",
      "Dataset is loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  path  label\n0    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_10...  angry\n1    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_25...  angry\n2    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_50...  angry\n3    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_75...  angry\n4    ../stimuli_intensit√§tsmorphs\\nf01_ang_w02_o_10...  angry\n..                                                 ...    ...\n763  ../stimuli_intensit√§tsmorphs\\nm04_sur_w03_o_75...     ps\n764  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_10...     ps\n765  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_25...     ps\n766  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_50...     ps\n767  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_75...     ps\n\n[768 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_10...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_25...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_50...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_75...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w02_o_10...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w03_o_75...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_10...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_25...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_50...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_75...</td>\n      <td>ps</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpaths, trainlabels, testpaths, testlabels = load_custom_dataset()\n",
    "\n",
    "###create dataframes for training and testing###\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF[\"path\"] = trainpaths\n",
    "trainDF[\"label\"] = trainlabels\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF[\"path\"] = testpaths\n",
    "testDF[\"label\"] = testlabels\n",
    "\n",
    "\n",
    "testDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\npath = trainDF[\"speech\"][0]\\n# settings\\nhop_length = 302 # number of samples per time-step in spectrogram\\nn_mels = 192 # number of bins in spectrogram. Height of image\\ntime_steps = 192 # number of time-steps. Width of image\\n\\n# load audio. Using example from librosa\\ny, sr = librosa.load(path, offset=0.1, duration=1, sr=99100)\\nout = \\'out.png\\'\\n\\n# extract a fixed length window\\nstart_sample = 0 # starting at beginning\\nlength_samples = time_steps*hop_length\\nwindow = y[start_sample:start_sample+length_samples]\\n\\n# convert to PNG\\nspectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\\n\\n#train_spectros = trainDF[\"speech\"].apply(lambda x: spectrogram_image(x))\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = trainDF[\"speech\"][0]\n",
    "# settings\n",
    "hop_length = 302 # number of samples per time-step in spectrogram\n",
    "n_mels = 192 # number of bins in spectrogram. Height of image\n",
    "time_steps = 192 # number of time-steps. Width of image\n",
    "\n",
    "# load audio. Using example from librosa\n",
    "y, sr = librosa.load(path, offset=0.1, duration=1, sr=99100)\n",
    "out = 'out.png'\n",
    "\n",
    "# extract a fixed length window\n",
    "start_sample = 0 # starting at beginning\n",
    "length_samples = time_steps*hop_length\n",
    "window = y[start_sample:start_sample+length_samples]\n",
    "\n",
    "# convert to PNG\n",
    "spectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "#train_spectros = trainDF[\"speech\"].apply(lambda x: spectrogram_image(x))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\\n'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchvision.models import AlexNet_Weights\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\"\"\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# load dummy dataset and read soundfiles\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\n# tokenize\\ninput_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\\n\\n# retrieve logits\\nlogits = model(input_values).logits\\n\\n# take argmax and decode\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\\n'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import wer\n",
    "import torch\n",
    "\"\"\"\n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ny = librosa.load(sr=16000, path=testDF[\"speech\"][0])\\n\\n\\ninput_values = processor(y[0], return_tensors=\"pt\",  sampling_rate=16000).input_values  # Batch size 1\\n\\n# retrieve logits\\nlogits = model(input_values).logits\\n\\n# take argmax and decode\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "y = librosa.load(sr=16000, path=testDF[\"speech\"][0])\n",
    "\n",
    "\n",
    "input_values = processor(y[0], return_tensors=\"pt\",  sampling_rate=16000).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ntrain_audios=[]\\nfor x in testDF[\"speech\"]:\\n    audio=librosa.load(path=x, sr=16000)\\n    train_audios.append(audio[0])\\ntrain_audios\\n'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_audios=[]\n",
    "for x in testDF[\"speech\"]:\n",
    "    audio=librosa.load(path=x, sr=16000)\n",
    "    train_audios.append(audio[0])\n",
    "train_audios\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "save_path=\"/masterarbeit_programming/notebooks/content/data\"\n",
    "train_df, test_df=train_test_split(trainDF, test_size=0.2, random_state=101, stratify=trainDF[\"label\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 2)\n",
      "(480, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0ed2ef8784c640c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/tonib/.cache/huggingface/datasets/csv/default-0ed2ef8784c640c8/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd6af461bfce4b6daede3304d93c80b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f65a3f5b0e744e5ae5be2f14cee52cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "325ce853678440f090c333a5e4f23f12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/tonib/.cache/huggingface/datasets/csv/default-0ed2ef8784c640c8/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63fa150315ea4e89b8f2f7c864cb1526"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'label'],\n",
      "    num_rows: 1920\n",
      "})\n",
      "Dataset({\n",
      "    features: ['path', 'label'],\n",
      "    num_rows: 480\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading the created dataset using datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/masterarbeit_programming/notebooks/content/data/train.csv\",\n",
    "    \"validation\": \"/masterarbeit_programming/notebooks/content/data/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "train_df = dataset[\"train\"]\n",
    "test_df = dataset[\"validation\"]\n",
    "\n",
    "print(train_df)\n",
    "print(test_df)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "# specifiy input and output column\n",
    "input_colum=\"path\"\n",
    "output_column=\"label\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Classes: ['angry', 'disgust', 'fear', 'happy', 'ps', 'sad']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#distinguish labels\n",
    "class_list = train_df.unique(output_column)\n",
    "class_list.sort()\n",
    "num_class = len(class_list)\n",
    "\n",
    "print(f\"{num_class} Classes: {class_list}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "\n",
    "model_name_path =  \"facebook/wav2vec2-base-960h\"\n",
    "pooling_mode = \"mean\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angry\",\n",
      "    \"1\": \"disgust\",\n",
      "    \"2\": \"fear\",\n",
      "    \"3\": \"happy\",\n",
      "    \"4\": \"ps\",\n",
      "    \"5\": \"sad\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"angry\": 0,\n",
      "    \"disgust\": 1,\n",
      "    \"fear\": 2,\n",
      "    \"happy\": 3,\n",
      "    \"ps\": 4,\n",
      "    \"sad\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_path,\n",
    "    num_labels=num_class,\n",
    "    label2id={label: i for i, label in enumerate(class_list)},\n",
    "    id2label={i: label for i, label in enumerate(class_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\"\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\vocab.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\tokenizer_config.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_path)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"Target sample rate: {target_sampling_rate}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import soundfile\n",
    "\n",
    "def speech_file_to_array(speech_path):\n",
    "    speech_array, sampling_rate = torchaudio.load(speech_path)\n",
    "    resampler = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def speech_file_to_array_librosa(speech_path):\n",
    "    speech_array, sampling_rate = librosa.load(speech_path)\n",
    "    resampler = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=target_sampling_rate)\n",
    "    return speech_array\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list=[]\n",
    "    speech_list = [ speech_file_to_array_librosa(speech_path) for speech_path in examples[input_colum]]\n",
    "    target_list = [ label_to_id(label, class_list) for label in examples[output_column]]\n",
    "\n",
    "    result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    result[\"labels\"] = list(target_list)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "### resample\n",
    "#from datasets import Audio\n",
    "#train_df = train_df.cast_column(\"path\", Audio(sampling_rate=target_sampling_rate))\n",
    "\n",
    "#test_df = test_df.cast_column(\"path\", Audio(sampling_rate=target_sampling_rate))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/192 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e2f37ec50d0405b910caf04823e8415"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\feature_extraction_utils.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/48 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce1ad818aae7432ea4ea3a579e484029"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = train_df.map(\n",
    "    preprocess_function,\n",
    "    batch_size=10,\n",
    "    batched=True,\n",
    "    # num_proc=4\n",
    ")\n",
    "test_df = test_df.map(\n",
    "    preprocess_function,\n",
    "    batch_size=10,\n",
    "    batched=True,\n",
    "    # num_proc=4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "{'path': '../tess\\\\OAF_Fear\\\\OAF_choice_fear.wav',\n 'label': 'fear',\n 'input_values': [0.005347009282559156,\n  -7.528260175604373e-05,\n  0.00019006124057341367,\n  0.0011511975899338722,\n  -7.297880802070722e-05,\n  -0.002332984237000346,\n  -0.0027273274026811123,\n  -0.0007437366875819862,\n  -0.0034132860600948334,\n  -0.005155489780008793,\n  -0.0022231556940823793,\n  -0.003020818345248699,\n  -0.007137116510421038,\n  -0.007239627186208963,\n  -0.007200169377028942,\n  -0.005426994990557432,\n  -0.007078962400555611,\n  -0.012079843319952488,\n  -0.009139073081314564,\n  -0.01059677917510271,\n  -0.01161156129091978,\n  -0.008673600852489471,\n  -0.009095572866499424,\n  -0.00934215635061264,\n  -0.01163135189563036,\n  -0.01184934563934803,\n  -0.013085247948765755,\n  -0.013547476381063461,\n  -0.012033736333251,\n  -0.011728492565453053,\n  -0.012789004482328892,\n  -0.0141847999766469,\n  -0.012567385099828243,\n  -0.014910061843693256,\n  -0.01439056545495987,\n  -0.016309164464473724,\n  -0.018531007692217827,\n  -0.018635498359799385,\n  -0.020213710144162178,\n  -0.017976028844714165,\n  -0.022646835073828697,\n  -0.021242275834083557,\n  -0.01951524056494236,\n  -0.02127530798316002,\n  -0.02151043526828289,\n  -0.023723144084215164,\n  -0.023958932608366013,\n  -0.023451928049325943,\n  -0.022668614983558655,\n  -0.021707816049456596,\n  -0.024089746177196503,\n  -0.026851145550608635,\n  -0.023667337372899055,\n  -0.02607182040810585,\n  -0.025598889216780663,\n  -0.027882706373929977,\n  -0.029352456331253052,\n  -0.027608059346675873,\n  -0.030620915815234184,\n  -0.02830434776842594,\n  -0.027278302237391472,\n  -0.02826009877026081,\n  -0.0301347766071558,\n  -0.028584128245711327,\n  -0.029199955984950066,\n  -0.03141048178076744,\n  -0.03279034420847893,\n  -0.03573068603873253,\n  -0.031382862478494644,\n  -0.02975788153707981,\n  -0.033312197774648666,\n  -0.03236908093094826,\n  -0.0329926535487175,\n  -0.03237873688340187,\n  -0.03313979133963585,\n  -0.03642917051911354,\n  -0.034545231610536575,\n  -0.03134406358003616,\n  -0.03364281728863716,\n  -0.03649059310555458,\n  -0.0340711772441864,\n  -0.03650952875614166,\n  -0.03597807139158249,\n  -0.03697953000664711,\n  -0.03975094109773636,\n  -0.03903840854763985,\n  -0.037748757749795914,\n  -0.03865578770637512,\n  -0.041217319667339325,\n  -0.037296924740076065,\n  -0.042197320610284805,\n  -0.041074346750974655,\n  -0.03978899493813515,\n  -0.04285493120551109,\n  -0.04031037911772728,\n  -0.04151604697108269,\n  -0.039792876690626144,\n  -0.04418560117483139,\n  -0.047724660485982895,\n  -0.04390135779976845,\n  -0.042669627815485,\n  -0.04469012841582298,\n  -0.04402008280158043,\n  -0.04712050408124924,\n  -0.04721155762672424,\n  -0.04230722039937973,\n  -0.04461022838950157,\n  -0.04847142472863197,\n  -0.045544032007455826,\n  -0.04404142498970032,\n  -0.050150804221630096,\n  -0.04922415688633919,\n  -0.05046914145350456,\n  -0.05181894451379776,\n  -0.05056541785597801,\n  -0.049793541431427,\n  -0.05004620552062988,\n  -0.05288445204496384,\n  -0.055363427847623825,\n  -0.05676973983645439,\n  -0.055086586624383926,\n  -0.05878915265202522,\n  -0.056677624583244324,\n  -0.05456624552607536,\n  -0.05753297358751297,\n  -0.05738149210810661,\n  -0.05912036448717117,\n  -0.0584263876080513,\n  -0.05769548565149307,\n  -0.061248235404491425,\n  -0.06056543439626694,\n  -0.05677906796336174,\n  -0.06169453263282776,\n  -0.06542026251554489,\n  -0.06304341554641724,\n  -0.06215743348002434,\n  -0.06318928301334381,\n  -0.06414484232664108,\n  -0.06153558939695358,\n  -0.06406714767217636,\n  -0.06631997227668762,\n  -0.06065190210938454,\n  -0.0638326108455658,\n  -0.06855978816747665,\n  -0.06565103679895401,\n  -0.06594006717205048,\n  -0.06798005104064941,\n  -0.06819800287485123,\n  -0.07070647925138474,\n  -0.0688636302947998,\n  -0.06796720623970032,\n  -0.07214673608541489,\n  -0.06735453754663467,\n  -0.06689336150884628,\n  -0.06673049181699753,\n  -0.06664057821035385,\n  -0.06977763772010803,\n  -0.06865425407886505,\n  -0.07169222086668015,\n  -0.07081956416368484,\n  -0.07009257376194,\n  -0.07127097994089127,\n  -0.07142983376979828,\n  -0.07025903463363647,\n  -0.07200880348682404,\n  -0.07336340844631195,\n  -0.06999937444925308,\n  -0.07115687429904938,\n  -0.06982484459877014,\n  -0.07486323267221451,\n  -0.0745893269777298,\n  -0.06885895878076553,\n  -0.07431479543447495,\n  -0.07318353652954102,\n  -0.07337356358766556,\n  -0.07658766955137253,\n  -0.07520008832216263,\n  -0.07730913907289505,\n  -0.07863728702068329,\n  -0.07477553188800812,\n  -0.0778944119811058,\n  -0.07788123935461044,\n  -0.0747329443693161,\n  -0.07819579541683197,\n  -0.07474648207426071,\n  -0.07496875524520874,\n  -0.08018936961889267,\n  -0.08105842024087906,\n  -0.07731121778488159,\n  -0.0767759308218956,\n  -0.07818810641765594,\n  -0.07759695500135422,\n  -0.07899449020624161,\n  -0.07851375639438629,\n  -0.07774323970079422,\n  -0.07795147597789764,\n  -0.07940756529569626,\n  -0.07741690427064896,\n  -0.07726385444402695,\n  -0.07892841845750809,\n  -0.07904801517724991,\n  -0.0800020843744278,\n  -0.07711268216371536,\n  -0.07834825664758682,\n  -0.07885212451219559,\n  -0.077033631503582,\n  -0.0793093591928482,\n  -0.08103495091199875,\n  -0.08103751391172409,\n  -0.079989954829216,\n  -0.08051768690347672,\n  -0.07805933058261871,\n  -0.07911088317632675,\n  -0.07723028212785721,\n  -0.07238370925188065,\n  -0.07889531552791595,\n  -0.07848048955202103,\n  -0.07889188081026077,\n  -0.0808689296245575,\n  -0.07800407707691193,\n  -0.07987599074840546,\n  -0.07981257140636444,\n  -0.08274756371974945,\n  -0.08051107078790665,\n  -0.07750420272350311,\n  -0.0797305554151535,\n  -0.07702989131212234,\n  -0.07746684551239014,\n  -0.07911309599876404,\n  -0.07881229370832443,\n  -0.08104008436203003,\n  -0.07931685447692871,\n  -0.07982827723026276,\n  -0.08279173821210861,\n  -0.08189412951469421,\n  -0.08110731840133667,\n  -0.07778101414442062,\n  -0.07803061604499817,\n  -0.077072374522686,\n  -0.07783068716526031,\n  -0.08006034791469574,\n  -0.0789889544248581,\n  -0.07844851166009903,\n  -0.08055377006530762,\n  -0.08278621733188629,\n  -0.07965653389692307,\n  -0.08079494535923004,\n  -0.07772213965654373,\n  -0.07741241157054901,\n  -0.08132597804069519,\n  -0.07931321859359741,\n  -0.07963370531797409,\n  -0.07924196869134903,\n  -0.08113276213407516,\n  -0.08069863170385361,\n  -0.08123618364334106,\n  -0.08305077254772186,\n  -0.08014698326587677,\n  -0.0821366235613823,\n  -0.08326736837625504,\n  -0.0832265317440033,\n  -0.08138125389814377,\n  -0.08080773055553436,\n  -0.08248887956142426,\n  -0.08274264633655548,\n  -0.08364302664995193,\n  -0.08173301815986633,\n  -0.08441212028265,\n  -0.08382465690374374,\n  -0.08140034973621368,\n  -0.08343060314655304,\n  -0.08080048114061356,\n  -0.07990509271621704,\n  -0.08468078821897507,\n  -0.08499284088611603,\n  -0.08368627727031708,\n  -0.08743955940008163,\n  -0.08229381591081619,\n  -0.08271826058626175,\n  -0.0879507064819336,\n  -0.08439541608095169,\n  -0.0848015546798706,\n  -0.08433312177658081,\n  -0.08523108065128326,\n  -0.08572616428136826,\n  -0.08353248983621597,\n  -0.08362272381782532,\n  -0.08730310201644897,\n  -0.08392981439828873,\n  -0.08284198492765427,\n  -0.08777020126581192,\n  -0.07942961901426315,\n  -0.07971478998661041,\n  -0.08706632256507874,\n  -0.08594062924385071,\n  -0.08419309556484222,\n  -0.08585995435714722,\n  -0.08491311222314835,\n  -0.08333370834589005,\n  -0.0824904516339302,\n  -0.0792730525135994,\n  -0.08342865109443665,\n  -0.08328726887702942,\n  -0.08187979459762573,\n  -0.08451970666646957,\n  -0.08176764845848083,\n  -0.08043356239795685,\n  -0.0825326144695282,\n  -0.08490120619535446,\n  -0.08586295694112778,\n  -0.08421161025762558,\n  -0.0831807479262352,\n  -0.08537857979536057,\n  -0.0824754610657692,\n  -0.08329185843467712,\n  -0.08425945788621902,\n  -0.07992446422576904,\n  -0.08205185830593109,\n  -0.08231018483638763,\n  -0.0808461382985115,\n  -0.08066181093454361,\n  -0.08175937086343765,\n  -0.08156611770391464,\n  -0.08106662333011627,\n  -0.07863624393939972,\n  -0.07889463007450104,\n  -0.08087018877267838,\n  -0.07785387337207794,\n  -0.0763995423913002,\n  -0.07879342138767242,\n  -0.08119016885757446,\n  -0.07698888331651688,\n  -0.07937256991863251,\n  -0.07743290066719055,\n  -0.07502318173646927,\n  -0.07843188941478729,\n  -0.07628156989812851,\n  -0.07780531793832779,\n  -0.07457435131072998,\n  -0.07489616423845291,\n  -0.07195285707712173,\n  -0.07114548236131668,\n  -0.07588484138250351,\n  -0.07168718427419662,\n  -0.07300793379545212,\n  -0.07312541455030441,\n  -0.07268442213535309,\n  -0.06867191940546036,\n  -0.0696197897195816,\n  -0.07108746469020844,\n  -0.07145831733942032,\n  -0.07210114598274231,\n  -0.06894776970148087,\n  -0.07252737134695053,\n  -0.07001198828220367,\n  -0.07153265923261642,\n  -0.06788859516382217,\n  -0.0636264830827713,\n  -0.06531824916601181,\n  -0.06761662662029266,\n  -0.06778336316347122,\n  -0.06470552831888199,\n  -0.06529895216226578,\n  -0.06579495221376419,\n  -0.06435208767652512,\n  -0.0654681846499443,\n  -0.06641267985105515,\n  -0.06035752594470978,\n  -0.06491363793611526,\n  -0.06484920531511307,\n  -0.0640273466706276,\n  -0.0662236139178276,\n  -0.06342140585184097,\n  -0.06418784707784653,\n  -0.06201217696070671,\n  -0.0604887381196022,\n  -0.059983450919389725,\n  -0.06322063505649567,\n  -0.06431177258491516,\n  -0.06160224974155426,\n  -0.06295410543680191,\n  -0.060731541365385056,\n  -0.06111924350261688,\n  -0.06211327388882637,\n  -0.060814328491687775,\n  -0.0634407103061676,\n  -0.06210717558860779,\n  -0.06331215053796768,\n  -0.06223742291331291,\n  -0.06145661696791649,\n  -0.06166013702750206,\n  -0.058122120797634125,\n  -0.05891736224293709,\n  -0.058702751994132996,\n  -0.06043757125735283,\n  -0.06195132061839104,\n  -0.057707589119672775,\n  -0.06041509285569191,\n  -0.06313750147819519,\n  -0.0577840618789196,\n  -0.05930265411734581,\n  -0.059377241879701614,\n  -0.06135951355099678,\n  -0.0651642382144928,\n  -0.059393830597400665,\n  -0.0603150837123394,\n  -0.06101495027542114,\n  -0.059084270149469376,\n  -0.05942492559552193,\n  -0.060461100190877914,\n  -0.06248123571276665,\n  -0.058971647173166275,\n  -0.060455985367298126,\n  -0.06109006330370903,\n  -0.05802295729517937,\n  -0.06002609059214592,\n  -0.06227763742208481,\n  -0.06097733974456787,\n  -0.061354879289865494,\n  -0.05881865695118904,\n  -0.0591985285282135,\n  -0.06411407142877579,\n  -0.05943533033132553,\n  -0.05874626338481903,\n  -0.057490915060043335,\n  -0.05570077896118164,\n  -0.05867185443639755,\n  -0.0595364011824131,\n  -0.05924176797270775,\n  -0.05968031287193298,\n  -0.05863278731703758,\n  -0.05515439808368683,\n  -0.05643797665834427,\n  -0.05662462115287781,\n  -0.056088171899318695,\n  -0.05683194473385811,\n  -0.0567193329334259,\n  -0.061193108558654785,\n  -0.058817118406295776,\n  -0.054278574883937836,\n  -0.055899728089571,\n  -0.05506320297718048,\n  -0.054883163422346115,\n  -0.05436404421925545,\n  -0.053326837718486786,\n  -0.05236108973622322,\n  -0.05426615849137306,\n  -0.051270678639411926,\n  -0.05046791583299637,\n  -0.0550282821059227,\n  -0.053334616124629974,\n  -0.052976805716753006,\n  -0.05199027061462402,\n  -0.05104643851518631,\n  -0.05131854861974716,\n  -0.052151601761579514,\n  -0.05196046829223633,\n  -0.050528209656476974,\n  -0.050548553466796875,\n  -0.04821477085351944,\n  -0.049051254987716675,\n  -0.05459287390112877,\n  -0.051694028079509735,\n  -0.048559267073869705,\n  -0.04819313809275627,\n  -0.04718161001801491,\n  -0.049141477793455124,\n  -0.04558735713362694,\n  -0.04738693684339523,\n  -0.048971183598041534,\n  -0.045295123010873795,\n  -0.04945957660675049,\n  -0.04697008058428764,\n  -0.044868484139442444,\n  -0.045729998499155045,\n  -0.04224357381463051,\n  -0.044441498816013336,\n  -0.044031258672475815,\n  -0.044238340109586716,\n  -0.04462176933884621,\n  -0.04530055448412895,\n  -0.04483950510621071,\n  -0.04153318703174591,\n  -0.043665703386068344,\n  -0.04296954721212387,\n  -0.04005492851138115,\n  -0.04086035490036011,\n  -0.04213801398873329,\n  -0.04113627225160599,\n  -0.0421745628118515,\n  -0.03968951106071472,\n  -0.04077481850981712,\n  -0.04345910623669624,\n  -0.03779594600200653,\n  -0.04000552371144295,\n  -0.04259072244167328,\n  -0.04065724089741707,\n  -0.040590181946754456,\n  -0.03915678709745407,\n  -0.03974676504731178,\n  -0.04307302460074425,\n  -0.04207070544362068,\n  -0.039292287081480026,\n  -0.041861869394779205,\n  -0.03991269692778587,\n  -0.03867635130882263,\n  -0.040651097893714905,\n  -0.038790781050920486,\n  -0.03718656301498413,\n  -0.03665771707892418,\n  -0.039510831236839294,\n  -0.03896588087081909,\n  -0.037628427147865295,\n  -0.03640038147568703,\n  -0.03526483476161957,\n  -0.03817791864275932,\n  -0.039845842868089676,\n  -0.038388870656490326,\n  -0.03855818882584572,\n  -0.0404561311006546,\n  -0.03995541110634804,\n  -0.04134411737322807,\n  -0.039146535098552704,\n  -0.03973104804754257,\n  -0.03855552896857262,\n  -0.036983828991651535,\n  -0.03955667465925217,\n  -0.036868005990982056,\n  -0.04094361141324043,\n  -0.04174822196364403,\n  -0.03940305486321449,\n  -0.03932573273777962,\n  -0.03669553995132446,\n  -0.03763536363840103,\n  -0.03866250440478325,\n  -0.038363017141819,\n  -0.040454670786857605,\n  -0.043830931186676025,\n  -0.04275844991207123,\n  -0.041265521198511124,\n  -0.0405745767056942,\n  -0.04122120887041092,\n  -0.04179368540644646,\n  -0.039800968021154404,\n  -0.045382339507341385,\n  -0.04276266321539879,\n  -0.03960438072681427,\n  -0.044738274067640305,\n  -0.04084031656384468,\n  -0.041419923305511475,\n  -0.04261656478047371,\n  -0.04398616775870323,\n  -0.047024231404066086,\n  -0.04633825272321701,\n  -0.045571524649858475,\n  -0.04663892462849617,\n  -0.04484965279698372,\n  -0.04358058050274849,\n  -0.0461360327899456,\n  -0.043348655104637146,\n  -0.047608524560928345,\n  -0.04831906408071518,\n  -0.04496612027287483,\n  -0.04734017699956894,\n  -0.046685006469488144,\n  -0.045702021569013596,\n  -0.048875633627176285,\n  -0.05109647661447525,\n  -0.049034345895051956,\n  -0.04739445820450783,\n  -0.04661022871732712,\n  -0.048337120562791824,\n  -0.04669103026390076,\n  -0.04918816685676575,\n  -0.05020806938409805,\n  -0.046697523444890976,\n  -0.04793420061469078,\n  -0.050526440143585205,\n  -0.048011261969804764,\n  -0.045312512665987015,\n  -0.049339067190885544,\n  -0.04567256569862366,\n  -0.04727577418088913,\n  -0.049042437225580215,\n  -0.05077086016535759,\n  -0.051384054124355316,\n  -0.048990923911333084,\n  -0.053943898528814316,\n  -0.048462774604558945,\n  -0.046571534126996994,\n  -0.04906853288412094,\n  -0.049630049616098404,\n  -0.04972485452890396,\n  -0.05118422582745552,\n  -0.05254664272069931,\n  -0.04943358153104782,\n  -0.05122276023030281,\n  -0.05133316293358803,\n  -0.05084416642785072,\n  -0.05242956429719925,\n  -0.05272166430950165,\n  -0.05012883245944977,\n  -0.050456006079912186,\n  -0.050908252596855164,\n  -0.048694416880607605,\n  -0.05433584749698639,\n  -0.05334227532148361,\n  -0.05263705924153328,\n  -0.05227552726864815,\n  -0.052157528698444366,\n  -0.0564647875726223,\n  -0.0504138357937336,\n  -0.050378937274217606,\n  -0.05251713842153549,\n  -0.05376763641834259,\n  -0.0556882843375206,\n  -0.054260723292827606,\n  -0.05393040552735329,\n  -0.05270010232925415,\n  -0.05279197171330452,\n  -0.05573036149144173,\n  -0.05299264192581177,\n  -0.05177714303135872,\n  -0.0520939826965332,\n  -0.05182570964097977,\n  -0.054933954030275345,\n  -0.05148715525865555,\n  -0.052894651889801025,\n  -0.05266096442937851,\n  -0.0556432344019413,\n  -0.05391470342874527,\n  -0.05237176641821861,\n  -0.055887963622808456,\n  -0.05322230979800224,\n  -0.058995407074689865,\n  -0.05623602122068405,\n  -0.05571072921156883,\n  -0.059005267918109894,\n  -0.05666426196694374,\n  -0.05646692216396332,\n  -0.05811107158660889,\n  -0.05782785266637802,\n  -0.05701049789786339,\n  -0.059402015060186386,\n  -0.05841577425599098,\n  -0.05826707184314728,\n  -0.057885851711034775,\n  -0.057539939880371094,\n  -0.059123966842889786,\n  -0.058149248361587524,\n  -0.05898473411798477,\n  -0.06320104002952576,\n  -0.06586592644453049,\n  -0.061445996165275574,\n  -0.06250432878732681,\n  -0.06351377069950104,\n  -0.059984419494867325,\n  -0.06327557563781738,\n  -0.06386027485132217,\n  -0.06481128185987473,\n  -0.06344936043024063,\n  -0.06194884702563286,\n  -0.06376469880342484,\n  -0.06572110950946808,\n  -0.06230849772691727,\n  -0.06304003298282623,\n  -0.06586243212223053,\n  -0.06432375311851501,\n  -0.06521400809288025,\n  -0.06745269149541855,\n  -0.06856068223714828,\n  -0.06432878226041794,\n  -0.06657755374908447,\n  -0.06681294739246368,\n  -0.06958901137113571,\n  -0.0695718303322792,\n  -0.06825441867113113,\n  -0.07074861228466034,\n  -0.06645489484071732,\n  -0.06596089899539948,\n  -0.06628323346376419,\n  -0.06702860444784164,\n  -0.06849109381437302,\n  -0.06959285587072372,\n  -0.06798727810382843,\n  -0.06894484162330627,\n  -0.06956494599580765,\n  -0.06590793281793594,\n  -0.06707990914583206,\n  -0.0712999552488327,\n  -0.0721835047006607,\n  -0.07050199061632156,\n  -0.0721903070807457,\n  -0.07162921875715256,\n  -0.07015980780124664,\n  -0.07078881561756134,\n  -0.07087825983762741,\n  -0.07279231399297714,\n  -0.07117202877998352,\n  -0.07169058173894882,\n  -0.07050040364265442,\n  -0.07093007117509842,\n  -0.07366511970758438,\n  -0.07077237218618393,\n  -0.07172010093927383,\n  -0.06986086070537567,\n  -0.07269071787595749,\n  -0.07460734248161316,\n  -0.07535767555236816,\n  -0.0756111666560173,\n  -0.07295820116996765,\n  -0.07600568979978561,\n  -0.07547981292009354,\n  -0.07633725553750992,\n  -0.07730607688426971,\n  -0.0780324637889862,\n  -0.07704830914735794,\n  -0.07373525202274323,\n  -0.07478152960538864,\n  -0.07824863493442535,\n  -0.0771404579281807,\n  -0.07515187561511993,\n  -0.077002614736557,\n  -0.07895367592573166,\n  -0.07864031195640564,\n  -0.07754600048065186,\n  -0.08040116727352142,\n  -0.079766646027565,\n  -0.07778047770261765,\n  -0.07980597019195557,\n  -0.08091087639331818,\n  -0.07778031378984451,\n  -0.07819347828626633,\n  -0.08088485151529312,\n  -0.07959090173244476,\n  -0.08062149584293365,\n  -0.07969281077384949,\n  -0.08068089932203293,\n  -0.08268801867961884,\n  -0.07968100905418396,\n  -0.08186465501785278,\n  -0.0871683806180954,\n  -0.08435668796300888,\n  -0.08083360642194748,\n  -0.08308843523263931,\n  -0.08296345919370651,\n  -0.0807228684425354,\n  -0.08100510388612747,\n  -0.08283986151218414,\n  -0.08367551863193512,\n  -0.08441830426454544,\n  -0.08178815990686417,\n  -0.08329836279153824,\n  -0.08599238097667694,\n  -0.08496062457561493,\n  -0.08560732752084732,\n  -0.08662677556276321,\n  -0.08865223824977875,\n  -0.08556918054819107,\n  -0.08724895119667053,\n  -0.08935092389583588,\n  -0.08890864998102188,\n  -0.08914642035961151,\n  -0.08792652934789658,\n  -0.088559091091156,\n  -0.08798126876354218,\n  -0.08720161020755768,\n  -0.08800680190324783,\n  -0.08935476094484329,\n  -0.08791723847389221,\n  -0.08997883647680283,\n  -0.09070775657892227,\n  -0.08954306691884995,\n  -0.08851918578147888,\n  -0.09128763526678085,\n  -0.0935346782207489,\n  -0.08849217742681503,\n  -0.09194004535675049,\n  -0.0927196592092514,\n  -0.09011226147413254,\n  -0.09410814940929413,\n  -0.0916483998298645,\n  -0.09163147211074829,\n  -0.09308148175477982,\n  -0.09126436710357666,\n  -0.09491804242134094,\n  -0.09701031446456909,\n  -0.0959646925330162,\n  -0.09675639867782593,\n  -0.09710176289081573,\n  -0.09629372507333755,\n  -0.09864846616983414,\n  -0.09618016332387924,\n  -0.09699937701225281,\n  -0.10046179592609406,\n  -0.10011933743953705,\n  -0.09911485016345978,\n  -0.09495488554239273,\n  -0.10038629174232483,\n  -0.10114385187625885,\n  -0.09659510850906372,\n  -0.09883343428373337,\n  -0.10247854143381119,\n  -0.10362307727336884,\n  -0.10062649846076965,\n  -0.10364478081464767,\n  -0.10155104845762253,\n  -0.09749993681907654,\n  -0.10196875780820847,\n  -0.10128920525312424,\n  -0.1028820052742958,\n  -0.10587092489004135,\n  -0.10315405577421188,\n  -0.10508905351161957,\n  -0.10577071458101273,\n  -0.10419078171253204,\n  -0.10655268281698227,\n  -0.1091822013258934,\n  -0.10670701414346695,\n  -0.10712394118309021,\n  -0.10896102339029312,\n  -0.10767152160406113,\n  -0.11105307191610336,\n  -0.1090097576379776,\n  -0.10716640949249268,\n  -0.10763150453567505,\n  -0.10888227075338364,\n  -0.11176301538944244,\n  -0.11332084238529205,\n  -0.11203699558973312,\n  -0.11130564659833908,\n  -0.11357295513153076,\n  -0.11060778796672821,\n  -0.11399554461240768,\n  -0.11346250772476196,\n  -0.1130150705575943,\n  -0.11602823436260223,\n  -0.1121060848236084,\n  -0.11367560923099518,\n  -0.11278972029685974,\n  -0.11496912688016891,\n  -0.11781271547079086,\n  -0.1168590560555458,\n  -0.11733373999595642,\n  -0.11766257137060165,\n  -0.11965127289295197,\n  -0.12135063111782074,\n  -0.11966410279273987,\n  -0.11804382503032684,\n  -0.11939934641122818,\n  -0.11588486284017563,\n  -0.11794370412826538,\n  -0.12040579319000244,\n  -0.12020272761583328,\n  -0.11993180215358734,\n  -0.12122828513383865,\n  -0.12271536141633987,\n  -0.12115879356861115,\n  -0.12197341024875641,\n  -0.12142272293567657,\n  -0.1239556148648262,\n  -0.12395133078098297,\n  -0.12676604092121124,\n  -0.12380558252334595,\n  -0.1216006726026535,\n  -0.12597712874412537,\n  -0.12366270273923874,\n  -0.12549029290676117,\n  -0.12655429542064667,\n  -0.12563441693782806,\n  -0.1268165111541748,\n  -0.12747369706630707,\n  -0.1277841031551361,\n  -0.12673738598823547,\n  -0.1265036016702652,\n  -0.1296227127313614,\n  -0.1256474107503891,\n  -0.12451067566871643,\n  -0.12936857342720032,\n  -0.13098609447479248,\n  -0.12931190431118011,\n  -0.12718290090560913,\n  -0.12452501058578491,\n  -0.12812791764736176,\n  -0.13383172452449799,\n  -0.12706150114536285,\n  -0.12734535336494446,\n  -0.13058175146579742,\n  -0.1307530254125595,\n  -0.12748722732067108,\n  -0.1307217925786972,\n  -0.1309807151556015,\n  -0.12802651524543762,\n  -0.13370029628276825,\n  -0.13206635415554047,\n  -0.13083729147911072,\n  -0.1288071870803833,\n  -0.13333027064800262,\n  -0.13352294266223907,\n  -0.13105209171772003,\n  -0.1310829371213913,\n  -0.13299015164375305,\n  -0.13479793071746826,\n  -0.12974518537521362,\n  -0.13108006119728088,\n  -0.1330575942993164,\n  -0.13572590053081512,\n  -0.1355431228876114,\n  -0.13402387499809265,\n  -0.13406270742416382,\n  -0.13663047552108765,\n  -0.13761278986930847,\n  -0.13582280278205872,\n  -0.1381060779094696,\n  -0.1405322253704071,\n  -0.14116564393043518,\n  -0.13849948346614838,\n  -0.13700631260871887,\n  -0.14012393355369568,\n  -0.13890698552131653,\n  -0.14058658480644226,\n  -0.1410498172044754,\n  -0.14061668515205383,\n  -0.14653630554676056,\n  -0.13752013444900513,\n  -0.13960854709148407,\n  -0.1424749195575714,\n  -0.13818596303462982,\n  -0.14175528287887573,\n  -0.14036139845848083,\n  -0.14233547449111938,\n  -0.1400918811559677,\n  -0.14170172810554504,\n  -0.14126041531562805,\n  -0.1393938958644867,\n  -0.1430346518754959,\n  -0.14193172752857208,\n  -0.14326760172843933,\n  -0.1434599757194519,\n  -0.14161020517349243,\n  -0.145217165350914,\n  -0.1449873000383377,\n  -0.14606085419654846,\n  -0.14570283889770508,\n  -0.14119194447994232,\n  -0.14240750670433044,\n  -0.14307811856269836,\n  -0.14794522523880005,\n  -0.14459577202796936,\n  -0.14461760222911835,\n  -0.14907214045524597,\n  -0.14529232680797577,\n  -0.14680631458759308,\n  -0.14740397036075592,\n  -0.1500559002161026,\n  -0.15277670323848724,\n  -0.15411676466464996,\n  -0.15547916293144226,\n  -0.15461131930351257,\n  -0.15342403948307037,\n  -0.1508660614490509,\n  -0.15351763367652893,\n  -0.15512430667877197,\n  -0.15687361359596252,\n  -0.15558497607707977,\n  -0.15257352590560913,\n  -0.1567288041114807,\n  -0.15426182746887207,\n  -0.15551331639289856,\n  -0.15566283464431763,\n  -0.1534697711467743,\n  -0.15808573365211487,\n  -0.1560550332069397,\n  -0.1551084816455841,\n  -0.16087059676647186,\n  -0.15982912480831146,\n  -0.15411344170570374,\n  -0.15129810571670532,\n  -0.15257301926612854,\n  -0.15284410119056702,\n  -0.15284855663776398,\n  -0.15627890825271606,\n  -0.1552513986825943,\n  -0.16299636662006378,\n  -0.16238220036029816,\n  -0.15717653930187225,\n  -0.16262872517108917,\n  -0.15985238552093506,\n  -0.16054309904575348,\n  -0.15993821620941162,\n  -0.160519078373909,\n  -0.16162937879562378,\n  -0.15952615439891815,\n  -0.16157427430152893,\n  -0.15966446697711945,\n  -0.1605106145143509,\n  -0.16097761690616608,\n  -0.15987101197242737,\n  -0.16173967719078064,\n  -0.16340754926204681,\n  ...],\n 'labels': 2}"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "Casting the dataset:   0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14a3545bb02b47c699fb85fdd2f0df7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d26094b60ed44f489fe4b5a48764d79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from datasets import ClassLabel\n",
    "\n",
    "### label 2 id\n",
    "\n",
    "#train_df = train_df.cast_column(output_column, ClassLabel(num_classes=num_class,names=class_list))\n",
    "\n",
    "#test_df = test_df.cast_column(output_column, ClassLabel(num_classes=num_class,names=class_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "{'path': {'path': '../tess\\\\OAF_Fear\\\\OAF_choice_fear.wav',\n  'array': array([-2.0995614e-05, -1.2846249e-04, -9.7319069e-05, ...,\n          1.7393984e-04,  1.4581185e-04,  0.0000000e+00], dtype=float32),\n  'sampling_rate': 16000},\n 'label': 2}"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "is_regression = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = numpy.squeeze(preds) if is_regression else numpy.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(numpy.float32).mean().item()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'wav2vec2.masked_spec_embed', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_path,\n",
    "    config=config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "#print(is_apex_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [175], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mapex\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mapex\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m amp\n\u001B[1;32m----> 5\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/masterarbeit_programming/notebooks/content\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\u001B[39;49;00m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msteps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfp16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<string>:104\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout)\u001B[0m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\training_args.py:1122\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1113\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim \u001B[38;5;241m=\u001B[39m OptimizerNames\u001B[38;5;241m.\u001B[39mADAFACTOR\n\u001B[0;32m   1115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1117\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1120\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n\u001B[0;32m   1121\u001B[0m ):\n\u001B[1;32m-> 1122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1123\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1124\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1125\u001B[0m     )\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1133\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[0;32m   1134\u001B[0m ):\n\u001B[0;32m   1135\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1137\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--bf16_full_eval`) can only be used on CUDA or CPU devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1138\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import apex\n",
    "from apex import amp\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/masterarbeit_programming/notebooks/content\",\n",
    "    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "#install apex\n",
    "#!pip install -v --no-cache-dir   ../apex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\n",
      "Cuda version 11.8.1\n",
      "Numba version: 0.56.4\n",
      "Numpy version: 1.23.4\n",
      "Torch version:  1.13.0+cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": "<module 'apex' from 'X:\\\\masterarbeit_programming\\\\venv\\\\lib\\\\site-packages\\\\apex\\\\__init__.py'>"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###import\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import cuda\n",
    "import sys\n",
    "import numba\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Cuda version\", cuda.__version__)\n",
    "print(\"Numba version:\", numba.__version__)\n",
    "print(\"Numpy version:\", numpy.__version__)\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "apex\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        #if self.amp.use_amp:\n",
    "        if self.use_apex:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        #if self.use_amp:\n",
    "        if self.use_apex:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        #elif self.use_apex:\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=test_df,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: path. If path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 90766470\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/240 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [224], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:1501\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1498\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1500\u001B[0m )\n\u001B[1;32m-> 1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1503\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:1749\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1747\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[0;32m   1748\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1749\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1751\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1752\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   1753\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[0;32m   1754\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   1755\u001B[0m ):\n\u001B[0;32m   1756\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   1757\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "Cell \u001B[1;32mIn [222], line 49\u001B[0m, in \u001B[0;36mCTCTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m     47\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 49\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     52\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:2540\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[0;32m   2538\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2539\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 2540\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m   2541\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[0;32m   2542\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[0;32m   2543\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [161], line 72\u001B[0m, in \u001B[0;36mWav2Vec2ForSpeechClassification.forward\u001B[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     64\u001B[0m         input_values,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     69\u001B[0m         labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     70\u001B[0m ):\n\u001B[0;32m     71\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m---> 72\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwav2vec2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     80\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerged_strategy(hidden_states, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooling_mode)\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1311\u001B[0m, in \u001B[0;36mWav2Vec2Model.forward\u001B[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1306\u001B[0m hidden_states, extract_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_projection(extract_features)\n\u001B[0;32m   1307\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mask_hidden_states(\n\u001B[0;32m   1308\u001B[0m     hidden_states, mask_time_indices\u001B[38;5;241m=\u001B[39mmask_time_indices, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask\n\u001B[0;32m   1309\u001B[0m )\n\u001B[1;32m-> 1311\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1314\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1315\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1317\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1319\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madapter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:794\u001B[0m, in \u001B[0;36mWav2Vec2Encoder.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    788\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    789\u001B[0m             create_custom_forward(layer),\n\u001B[0;32m    790\u001B[0m             hidden_states,\n\u001B[0;32m    791\u001B[0m             attention_mask,\n\u001B[0;32m    792\u001B[0m         )\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 794\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    797\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    799\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m skip_the_layer:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:682\u001B[0m, in \u001B[0;36mWav2Vec2EncoderLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions)\u001B[0m\n\u001B[0;32m    679\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m attn_residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[0;32m    681\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(hidden_states)\n\u001B[1;32m--> 682\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    683\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_layer_norm(hidden_states)\n\u001B[0;32m    685\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:654\u001B[0m, in \u001B[0;36mWav2Vec2FeedForward.forward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    651\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_act_fn(hidden_states)\n\u001B[0;32m    652\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_dropout(hidden_states)\n\u001B[1;32m--> 654\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_dense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    655\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_dropout(hidden_states)\n\u001B[0;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
