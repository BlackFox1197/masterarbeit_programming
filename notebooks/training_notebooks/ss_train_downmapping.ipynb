{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 14:04:20.102866: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 14:04:20.607049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 14:04:20.607107: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 14:04:20.607111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_direct_downmapping_with_conv import SS_Direct_Downmapping_Conv_Model\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_direct_downmapping import SS_Direct_Downmapping_Model\n",
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_encoder_downmapping import EncoderDownmapping\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/downmapping/0_0218/\"\n",
    "epochs =1500\n",
    "save_every = 40\n",
    "start_lr = 6e-7\n",
    "gc.collect()\n",
    "seed = 300\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=device)\n",
    "\n",
    "#model = SS_Direct_Downmapping_Model(dropout=0.2, output=1024, start_dim=512 * 175).to(device)\n",
    "#model = SS_Direct_Downmapping_Conv_Model(dropout=0.2, output=1024, x_size=512, y_size=175).to(device)\n",
    "torch.manual_seed(seed)\n",
    "model = EncoderDownmapping(embed_dim=512, n_heads=4, ff_dim=2, n_layers=1, dropout=0.2, output=1024, max_seq_len=175).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.955763  [    0/ 6092]\n",
      "loss: 1.952858  [ 1400/ 6092]\n",
      "loss: 2.002033  [ 2800/ 6092]\n",
      "loss: 1.998015  [ 4200/ 6092]\n",
      "loss: 1.999245  [ 5600/ 6092]\n",
      "916.7464684247971\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.976887  [    0/ 6092]\n",
      "loss: 1.990719  [ 1400/ 6092]\n",
      "loss: 1.935518  [ 2800/ 6092]\n",
      "loss: 1.983413  [ 4200/ 6092]\n",
      "loss: 2.007831  [ 5600/ 6092]\n",
      "914.7998729944229\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.948315  [    0/ 6092]\n",
      "loss: 1.929753  [ 1400/ 6092]\n",
      "loss: 1.998237  [ 2800/ 6092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m enc_trainer \u001B[38;5;241m=\u001B[39m sset\u001B[38;5;241m.\u001B[39mSSDirectDMTrainer(is_encoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model_path\u001B[38;5;241m=\u001B[39mmodels_dir, save_model_every\u001B[38;5;241m=\u001B[39msave_every, lr\u001B[38;5;241m=\u001B[39mstart_lr, dataset\u001B[38;5;241m=\u001B[39mdata_set, device\u001B[38;5;241m=\u001B[39mdevice, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m     13\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m---> 14\u001B[0m \u001B[43menc_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# 200 +500 + 500 +500+500\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/mapping_down/ss_driect_dm_trainer.py:58\u001B[0m, in \u001B[0;36mSSDirectDMTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/clip_like/mapping_down/ss_driect_dm_trainer.py:65\u001B[0m, in \u001B[0;36mSSDirectDMTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     63\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, z) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m---> 65\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[43mz\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(X) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils/encoder\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_driect_dm_trainer as sset\n",
    "importlib.reload(sset)\n",
    "\n",
    "epochs =1500\n",
    "start_lr = 4e-6\n",
    "enc_trainer = sset.SSDirectDMTrainer(is_encoder=True, batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=model)\n",
    "gc.collect()\n",
    "enc_trainer.train() # 200 +500 + 500 +500+500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4, 5, 6]), tensor([0, 1, 2, 3, 4, 5, 6]))\n",
      "tensor(0.0257, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "tensor([[ 0.0257,  0.0682,  0.1137,  0.1003,  0.0861,  0.0434,  0.1270],\n",
      "        [ 0.0364,  0.1691,  0.0499,  0.0520,  0.0445,  0.0215,  0.0879],\n",
      "        [ 0.0471,  0.2021,  0.1437,  0.1197,  0.0319,  0.0232,  0.0524],\n",
      "        [-0.0349,  0.1412, -0.0580,  0.1619,  0.0691,  0.0561,  0.1994],\n",
      "        [ 0.1065,  0.1545,  0.1924,  0.1211,  0.1037,  0.0426,  0.1208],\n",
      "        [ 0.1051,  0.1988,  0.1682,  0.2198,  0.1852,  0.0045,  0.1342],\n",
      "        [ 0.0185,  0.0735,  0.0420,  0.0426,  0.0786,  0.0026,  0.5665]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor(0.5645, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from network_models.soundsream_models_and_utils.ss___util_class_batches_sampler import ClassBatchesSampler\n",
    "# np1 = data_set.encoded_dataset.encodedData[data_set.encoded_dataset.labelcolumn].to_numpy()\n",
    "# from torch.utils.data import DataLoader\n",
    "# #\n",
    "# # l = [np1[i][0] for i in range(len(np1))]\n",
    "# #\n",
    "# #\n",
    "# dl = DataLoader(data_set, batch_sampler=ClassBatchesSampler(np1, num_class_samples=2, shuffle=False))\n",
    "# #\n",
    "# #\n",
    "# for batch, (X, z) in enumerate(dl):\n",
    "#\n",
    "#     print(torch.tensor_split(torch.argmax(z, dim=1), 2))\n",
    "#     z1, z2 = torch.tensor_split(z, 2)\n",
    "#     pred = model(X.cuda())\n",
    "#     pred1, pred2 = torch.tensor_split(pred, 2)\n",
    "#     dp = torch.matmul(pred1, pred2.t())\n",
    "#     #dp = pred1 @ pred2.t()\n",
    "#\n",
    "#\n",
    "#     print(torch.dot(pred1[0],pred2[0]))\n",
    "#     # print(torch.dot(pred[1],pred2[0]))\n",
    "#     # print(torch.dot(pred[2],pred2[0]))\n",
    "#     # print(torch.dot(pred[3],pred2[0]))\n",
    "#     # print(torch.dot(pred[4],pred2[0]))\n",
    "#     # print(torch.dot(pred[5],pred2[0]))\n",
    "#     # print(torch.dot(pred[7],pred[0]))\n",
    "#     #dp = torch.matmul(z1, z2.T) / (z1.norm(dim=-1, keepdim=True) * z2.norm(dim=-1, keepdim=True))\n",
    "#     #print(dp)\n",
    "#     print(dp)\n",
    "#     print(torch.sum(dp[0, :]))\n",
    "#\n",
    "#     #print(X[0][0][0][170])\n",
    "#\n",
    "#     if batch == 0:\n",
    "#         break\n",
    "#\n",
    "# (len(dl)/(7*2))-2 == 462"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
