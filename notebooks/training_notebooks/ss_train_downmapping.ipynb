{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 12:57:10.632614: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:57:11.114950: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:57:11.115000: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:57:11.115004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_direct_downmapping_with_conv import SS_Direct_Downmapping_Conv_Model\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_direct_downmapping import SS_Direct_Downmapping_Model\n",
    "from network_models.soundsream_models_and_utils.clip_like.encoder.ss_encoder_downmapping import EncoderDownmapping\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/downmapping/0_0216/\"\n",
    "epochs =200\n",
    "save_every = 40\n",
    "start_lr = 6e-6\n",
    "gc.collect()\n",
    "\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=device)\n",
    "\n",
    "#model = SS_Direct_Downmapping_Model(dropout=0.2, output=1024, start_dim=512 * 175).to(device)\n",
    "#model = SS_Direct_Downmapping_Conv_Model(dropout=0.2, output=1024, x_size=512, y_size=175).to(device)\n",
    "model = EncoderDownmapping(embed_dim=512, n_heads=4, ff_dim=2, n_layers=1, dropout=0.2, output=1024, max_seq_len=175).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165429  [ 4200/ 6092]\n",
      "loss: 1.165593  [ 5600/ 6092]\n",
      "552.0717619657516\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.308279  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.235397  [ 2800/ 6092]\n",
      "loss: 1.181350  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.816523194313\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165434  [ 1400/ 6092]\n",
      "loss: 1.191912  [ 2800/ 6092]\n",
      "loss: 1.178546  [ 4200/ 6092]\n",
      "loss: 1.165581  [ 5600/ 6092]\n",
      "552.5810016393661\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.165459  [    0/ 6092]\n",
      "loss: 1.165426  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.184098  [ 4200/ 6092]\n",
      "loss: 1.166292  [ 5600/ 6092]\n",
      "551.4457051753998\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.165428  [    0/ 6092]\n",
      "loss: 1.217754  [ 1400/ 6092]\n",
      "loss: 1.165446  [ 2800/ 6092]\n",
      "loss: 1.167743  [ 4200/ 6092]\n",
      "loss: 1.165456  [ 5600/ 6092]\n",
      "552.1298453807831\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.165532  [    0/ 6092]\n",
      "loss: 1.183225  [ 1400/ 6092]\n",
      "loss: 1.165441  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.9669992923737\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.165473  [    0/ 6092]\n",
      "loss: 1.165434  [ 1400/ 6092]\n",
      "loss: 1.165474  [ 2800/ 6092]\n",
      "loss: 1.175141  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "553.1059411764145\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.165439  [    0/ 6092]\n",
      "loss: 1.165699  [ 1400/ 6092]\n",
      "loss: 1.173184  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.166201  [ 5600/ 6092]\n",
      "553.350482583046\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.165817  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.165432  [ 4200/ 6092]\n",
      "loss: 1.304986  [ 5600/ 6092]\n",
      "551.3499763011932\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.165437  [    0/ 6092]\n",
      "loss: 1.165433  [ 1400/ 6092]\n",
      "loss: 1.173379  [ 2800/ 6092]\n",
      "loss: 1.180361  [ 4200/ 6092]\n",
      "loss: 1.307036  [ 5600/ 6092]\n",
      "552.4323126077652\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.176132  [    0/ 6092]\n",
      "loss: 1.169157  [ 1400/ 6092]\n",
      "loss: 1.165467  [ 2800/ 6092]\n",
      "loss: 1.237184  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.5925667285919\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.316694  [    0/ 6092]\n",
      "loss: 1.165438  [ 1400/ 6092]\n",
      "loss: 1.307775  [ 2800/ 6092]\n",
      "loss: 1.233892  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.5480521917343\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.190808  [    0/ 6092]\n",
      "loss: 1.166100  [ 1400/ 6092]\n",
      "loss: 1.378279  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.166363  [ 5600/ 6092]\n",
      "552.5933382511139\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.165440  [    0/ 6092]\n",
      "loss: 1.165513  [ 1400/ 6092]\n",
      "loss: 1.165639  [ 2800/ 6092]\n",
      "loss: 1.165730  [ 4200/ 6092]\n",
      "loss: 1.166741  [ 5600/ 6092]\n",
      "552.3362600803375\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.229060  [    0/ 6092]\n",
      "loss: 1.165771  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165430  [ 4200/ 6092]\n",
      "loss: 1.217917  [ 5600/ 6092]\n",
      "552.3673791885376\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.166294  [    0/ 6092]\n",
      "loss: 1.309146  [ 1400/ 6092]\n",
      "loss: 1.220866  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.166038  [ 5600/ 6092]\n",
      "552.489933013916\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.288855  [    0/ 6092]\n",
      "loss: 1.165680  [ 1400/ 6092]\n",
      "loss: 1.165485  [ 2800/ 6092]\n",
      "loss: 1.209397  [ 4200/ 6092]\n",
      "loss: 1.224301  [ 5600/ 6092]\n",
      "553.7329156398773\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165492  [ 1400/ 6092]\n",
      "loss: 1.172462  [ 2800/ 6092]\n",
      "loss: 1.165516  [ 4200/ 6092]\n",
      "loss: 1.165539  [ 5600/ 6092]\n",
      "552.034760594368\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.245423  [    0/ 6092]\n",
      "loss: 1.165426  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.241505  [ 4200/ 6092]\n",
      "loss: 1.165440  [ 5600/ 6092]\n",
      "553.0497498512268\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.234750  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.360048  [ 2800/ 6092]\n",
      "loss: 1.165427  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "552.3449400663376\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.165429  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.166060  [ 2800/ 6092]\n",
      "loss: 1.165565  [ 4200/ 6092]\n",
      "loss: 1.165885  [ 5600/ 6092]\n",
      "552.0350635051727\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165539  [ 4200/ 6092]\n",
      "loss: 1.229339  [ 5600/ 6092]\n",
      "553.7474867105484\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.344581  [    0/ 6092]\n",
      "loss: 1.165821  [ 1400/ 6092]\n",
      "loss: 1.170426  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.173983  [ 5600/ 6092]\n",
      "552.2088468074799\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.186766  [    0/ 6092]\n",
      "loss: 1.165444  [ 1400/ 6092]\n",
      "loss: 1.166826  [ 2800/ 6092]\n",
      "loss: 1.165433  [ 4200/ 6092]\n",
      "loss: 1.165641  [ 5600/ 6092]\n",
      "552.4936470985413\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.165593  [    0/ 6092]\n",
      "loss: 1.282918  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.184116  [ 5600/ 6092]\n",
      "553.3359738588333\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.165820  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165433  [ 2800/ 6092]\n",
      "loss: 1.165429  [ 4200/ 6092]\n",
      "loss: 1.168940  [ 5600/ 6092]\n",
      "553.4165933132172\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165855  [ 2800/ 6092]\n",
      "loss: 1.308288  [ 4200/ 6092]\n",
      "loss: 1.239999  [ 5600/ 6092]\n",
      "552.1880050897598\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.228829  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.168988  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165561  [ 5600/ 6092]\n",
      "552.609169960022\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.165812  [    0/ 6092]\n",
      "loss: 1.165433  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165724  [ 4200/ 6092]\n",
      "loss: 1.165425  [ 5600/ 6092]\n",
      "552.9237328767776\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165734  [ 1400/ 6092]\n",
      "loss: 1.254830  [ 2800/ 6092]\n",
      "loss: 1.166239  [ 4200/ 6092]\n",
      "loss: 1.307499  [ 5600/ 6092]\n",
      "552.7645676136017\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165476  [ 1400/ 6092]\n",
      "loss: 1.166130  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.307491  [ 5600/ 6092]\n",
      "552.6139920949936\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165469  [ 1400/ 6092]\n",
      "loss: 1.166552  [ 2800/ 6092]\n",
      "loss: 1.187556  [ 4200/ 6092]\n",
      "loss: 1.165515  [ 5600/ 6092]\n",
      "552.6222124099731\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.304892  [    0/ 6092]\n",
      "loss: 1.167321  [ 1400/ 6092]\n",
      "loss: 1.165560  [ 2800/ 6092]\n",
      "loss: 1.165478  [ 4200/ 6092]\n",
      "loss: 1.170958  [ 5600/ 6092]\n",
      "553.1205654144287\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.307551  [    0/ 6092]\n",
      "loss: 1.229607  [ 1400/ 6092]\n",
      "loss: 1.166137  [ 2800/ 6092]\n",
      "loss: 1.282604  [ 4200/ 6092]\n",
      "loss: 1.165530  [ 5600/ 6092]\n",
      "553.1568598747253\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.308430  [    0/ 6092]\n",
      "loss: 1.230044  [ 1400/ 6092]\n",
      "loss: 1.166229  [ 2800/ 6092]\n",
      "loss: 1.165486  [ 4200/ 6092]\n",
      "loss: 1.165725  [ 5600/ 6092]\n",
      "551.9502065181732\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.166190  [    0/ 6092]\n",
      "loss: 1.237993  [ 1400/ 6092]\n",
      "loss: 1.167766  [ 2800/ 6092]\n",
      "loss: 1.165718  [ 4200/ 6092]\n",
      "loss: 1.167291  [ 5600/ 6092]\n",
      "552.4582353830338\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.168331  [    0/ 6092]\n",
      "loss: 1.305820  [ 1400/ 6092]\n",
      "loss: 1.308254  [ 2800/ 6092]\n",
      "loss: 1.165531  [ 4200/ 6092]\n",
      "loss: 1.165441  [ 5600/ 6092]\n",
      "552.5024991035461\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.255234  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.308279  [ 4200/ 6092]\n",
      "loss: 1.186872  [ 5600/ 6092]\n",
      "554.0014497041702\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.181516  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.169270  [ 5600/ 6092]\n",
      "552.7480175495148\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.166846  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.251761  [ 5600/ 6092]\n",
      "552.4545325040817\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165448  [ 1400/ 6092]\n",
      "loss: 1.168859  [ 2800/ 6092]\n",
      "loss: 1.168096  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "552.1787278652191\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165578  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165453  [ 4200/ 6092]\n",
      "loss: 1.165997  [ 5600/ 6092]\n",
      "552.9352054595947\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.172311  [ 1400/ 6092]\n",
      "loss: 1.165601  [ 2800/ 6092]\n",
      "loss: 1.174273  [ 4200/ 6092]\n",
      "loss: 1.165444  [ 5600/ 6092]\n",
      "552.876801609993\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.165568  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165433  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.172322  [ 5600/ 6092]\n",
      "552.3274624347687\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.165437  [    0/ 6092]\n",
      "loss: 1.165431  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165601  [ 4200/ 6092]\n",
      "loss: 1.165492  [ 5600/ 6092]\n",
      "552.1179491281509\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.165430  [    0/ 6092]\n",
      "loss: 1.231939  [ 1400/ 6092]\n",
      "loss: 1.165449  [ 2800/ 6092]\n",
      "loss: 1.165461  [ 4200/ 6092]\n",
      "loss: 1.165800  [ 5600/ 6092]\n",
      "552.6323473453522\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.165430  [    0/ 6092]\n",
      "loss: 1.299742  [ 1400/ 6092]\n",
      "loss: 1.166704  [ 2800/ 6092]\n",
      "loss: 1.177726  [ 4200/ 6092]\n",
      "loss: 1.168264  [ 5600/ 6092]\n",
      "552.4859839677811\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.170755  [    0/ 6092]\n",
      "loss: 1.167051  [ 1400/ 6092]\n",
      "loss: 1.166126  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.287588  [ 5600/ 6092]\n",
      "552.1830619573593\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.308284  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.240755  [ 4200/ 6092]\n",
      "loss: 1.165468  [ 5600/ 6092]\n",
      "552.7067792415619\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.199688  [    0/ 6092]\n",
      "loss: 1.165493  [ 1400/ 6092]\n",
      "loss: 1.307183  [ 2800/ 6092]\n",
      "loss: 1.214942  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "552.2725433111191\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.165644  [    0/ 6092]\n",
      "loss: 1.224703  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.167526  [ 4200/ 6092]\n",
      "loss: 1.232143  [ 5600/ 6092]\n",
      "552.2983683347702\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.166931  [    0/ 6092]\n",
      "loss: 1.166061  [ 1400/ 6092]\n",
      "loss: 1.195283  [ 2800/ 6092]\n",
      "loss: 1.237054  [ 4200/ 6092]\n",
      "loss: 1.246405  [ 5600/ 6092]\n",
      "552.0917618274689\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.303317  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.973809838295\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.174788  [    0/ 6092]\n",
      "loss: 1.307678  [ 1400/ 6092]\n",
      "loss: 1.236074  [ 2800/ 6092]\n",
      "loss: 1.306959  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.032249212265\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165531  [ 1400/ 6092]\n",
      "loss: 1.165440  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165492  [ 5600/ 6092]\n",
      "551.924647808075\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.309699  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.0055717229843\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.178898  [    0/ 6092]\n",
      "loss: 1.165458  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165440  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "552.6001822948456\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.189493  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.5208148956299\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.165443  [    0/ 6092]\n",
      "loss: 1.305077  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "550.7769947052002\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.286479  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.6633133888245\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.165432  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.167554  [ 2800/ 6092]\n",
      "loss: 1.165633  [ 4200/ 6092]\n",
      "loss: 1.168412  [ 5600/ 6092]\n",
      "552.7775970697403\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.166079  [    0/ 6092]\n",
      "loss: 1.180393  [ 1400/ 6092]\n",
      "loss: 1.165892  [ 2800/ 6092]\n",
      "loss: 1.233211  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.5036811828613\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.166126  [    0/ 6092]\n",
      "loss: 1.165450  [ 1400/ 6092]\n",
      "loss: 1.165457  [ 2800/ 6092]\n",
      "loss: 1.170386  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.7083089351654\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.165557  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.204353  [ 2800/ 6092]\n",
      "loss: 1.185928  [ 4200/ 6092]\n",
      "loss: 1.165529  [ 5600/ 6092]\n",
      "551.3801217079163\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.165702  [    0/ 6092]\n",
      "loss: 1.167424  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.167284  [ 4200/ 6092]\n",
      "loss: 1.165620  [ 5600/ 6092]\n",
      "551.1855834722519\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.179920  [ 5600/ 6092]\n",
      "551.45137155056\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.165866  [    0/ 6092]\n",
      "loss: 1.165622  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.301193  [ 4200/ 6092]\n",
      "loss: 1.193180  [ 5600/ 6092]\n",
      "552.0381140708923\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.444542  [    0/ 6092]\n",
      "loss: 1.165943  [ 1400/ 6092]\n",
      "loss: 1.236483  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.0372341871262\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.165534  [    0/ 6092]\n",
      "loss: 1.165431  [ 1400/ 6092]\n",
      "loss: 1.219093  [ 2800/ 6092]\n",
      "loss: 1.165436  [ 4200/ 6092]\n",
      "loss: 1.167595  [ 5600/ 6092]\n",
      "552.4182721376419\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.289725  [    0/ 6092]\n",
      "loss: 1.308113  [ 1400/ 6092]\n",
      "loss: 1.165697  [ 2800/ 6092]\n",
      "loss: 1.165444  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.8260496854782\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.219103  [    0/ 6092]\n",
      "loss: 1.165438  [ 1400/ 6092]\n",
      "loss: 1.165481  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.2069420814514\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.166093  [ 1400/ 6092]\n",
      "loss: 1.181826  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.308876  [ 5600/ 6092]\n",
      "551.7571934461594\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.189166  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.168882  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.0093891620636\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.165427  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.242081  [ 2800/ 6092]\n",
      "loss: 1.171059  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.5948497056961\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.311229  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.185633  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.4266927242279\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.267556  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.165439  [ 2800/ 6092]\n",
      "loss: 1.170640  [ 4200/ 6092]\n",
      "loss: 1.209743  [ 5600/ 6092]\n",
      "551.8463453054428\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.165432  [    0/ 6092]\n",
      "loss: 1.165425  [ 1400/ 6092]\n",
      "loss: 1.167757  [ 2800/ 6092]\n",
      "loss: 1.165456  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.8924993276596\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.165432  [    0/ 6092]\n",
      "loss: 1.399931  [ 1400/ 6092]\n",
      "loss: 1.309614  [ 2800/ 6092]\n",
      "loss: 1.169969  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.9739739894867\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.166353  [    0/ 6092]\n",
      "loss: 1.165433  [ 1400/ 6092]\n",
      "loss: 1.166426  [ 2800/ 6092]\n",
      "loss: 1.308106  [ 4200/ 6092]\n",
      "loss: 1.442725  [ 5600/ 6092]\n",
      "552.2971743345261\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.165996  [    0/ 6092]\n",
      "loss: 1.165425  [ 1400/ 6092]\n",
      "loss: 1.165528  [ 2800/ 6092]\n",
      "loss: 1.235449  [ 4200/ 6092]\n",
      "loss: 1.303158  [ 5600/ 6092]\n",
      "552.3958567380905\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.165430  [    0/ 6092]\n",
      "loss: 1.165426  [ 1400/ 6092]\n",
      "loss: 1.165469  [ 2800/ 6092]\n",
      "loss: 1.165706  [ 4200/ 6092]\n",
      "loss: 1.303946  [ 5600/ 6092]\n",
      "551.3843685388565\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.168452  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.237194  [ 5600/ 6092]\n",
      "551.6619853973389\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.308270  [ 2800/ 6092]\n",
      "loss: 1.165427  [ 4200/ 6092]\n",
      "loss: 1.200766  [ 5600/ 6092]\n",
      "552.785546541214\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.165671  [    0/ 6092]\n",
      "loss: 1.165541  [ 1400/ 6092]\n",
      "loss: 1.165469  [ 2800/ 6092]\n",
      "loss: 1.242039  [ 4200/ 6092]\n",
      "loss: 1.165584  [ 5600/ 6092]\n",
      "551.6513023376465\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.170040  [ 1400/ 6092]\n",
      "loss: 1.165431  [ 2800/ 6092]\n",
      "loss: 1.166179  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.0879665613174\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.165427  [    0/ 6092]\n",
      "loss: 1.165444  [ 1400/ 6092]\n",
      "loss: 1.227993  [ 2800/ 6092]\n",
      "loss: 1.176986  [ 4200/ 6092]\n",
      "loss: 1.231106  [ 5600/ 6092]\n",
      "552.2302397489548\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.165443  [    0/ 6092]\n",
      "loss: 1.308276  [ 1400/ 6092]\n",
      "loss: 1.165901  [ 2800/ 6092]\n",
      "loss: 1.500212  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.2110332250595\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.194824  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.375680  [ 4200/ 6092]\n",
      "loss: 1.235970  [ 5600/ 6092]\n",
      "552.1350821256638\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165534  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.228736  [ 4200/ 6092]\n",
      "loss: 1.165639  [ 5600/ 6092]\n",
      "552.9203014373779\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.305559  [    0/ 6092]\n",
      "loss: 1.167141  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.165428  [ 5600/ 6092]\n",
      "553.2426596879959\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.167605  [    0/ 6092]\n",
      "loss: 1.165434  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165726  [ 4200/ 6092]\n",
      "loss: 1.168959  [ 5600/ 6092]\n",
      "551.8028087615967\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.200392  [ 1400/ 6092]\n",
      "loss: 1.166197  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.442303  [ 5600/ 6092]\n",
      "552.7513161897659\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.301446  [    0/ 6092]\n",
      "loss: 1.165428  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.0478862524033\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165450  [ 5600/ 6092]\n",
      "551.7237805128098\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.283867  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165644  [ 4200/ 6092]\n",
      "loss: 1.167148  [ 5600/ 6092]\n",
      "552.5455878973007\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.166083  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.184064  [ 4200/ 6092]\n",
      "loss: 1.170980  [ 5600/ 6092]\n",
      "551.6821628808975\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.226670  [ 1400/ 6092]\n",
      "loss: 1.201589  [ 2800/ 6092]\n",
      "loss: 1.168130  [ 4200/ 6092]\n",
      "loss: 1.165516  [ 5600/ 6092]\n",
      "551.145966053009\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.274776  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.231955  [ 5600/ 6092]\n",
      "552.1609991788864\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.305206  [    0/ 6092]\n",
      "loss: 1.165431  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165481  [ 4200/ 6092]\n",
      "loss: 1.165495  [ 5600/ 6092]\n",
      "550.6096097230911\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.165436  [    0/ 6092]\n",
      "loss: 1.308645  [ 1400/ 6092]\n",
      "loss: 1.165429  [ 2800/ 6092]\n",
      "loss: 1.166655  [ 4200/ 6092]\n",
      "loss: 1.165591  [ 5600/ 6092]\n",
      "551.5451719760895\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.165433  [    0/ 6092]\n",
      "loss: 1.170545  [ 1400/ 6092]\n",
      "loss: 1.308419  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.166653  [ 5600/ 6092]\n",
      "551.4041351079941\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 1.165499  [    0/ 6092]\n",
      "loss: 1.167247  [ 1400/ 6092]\n",
      "loss: 1.165931  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.3545142412186\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 1.307101  [    0/ 6092]\n",
      "loss: 1.198495  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.236871  [ 4200/ 6092]\n",
      "loss: 1.171155  [ 5600/ 6092]\n",
      "550.8069583177567\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.165430  [ 1400/ 6092]\n",
      "loss: 1.176746  [ 2800/ 6092]\n",
      "loss: 1.165565  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.065315246582\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165524  [ 1400/ 6092]\n",
      "loss: 1.165435  [ 2800/ 6092]\n",
      "loss: 1.236848  [ 4200/ 6092]\n",
      "loss: 1.351117  [ 5600/ 6092]\n",
      "551.7027356624603\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 1.165432  [    0/ 6092]\n",
      "loss: 1.165427  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.448741  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.5782291889191\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 1.379464  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.307441  [ 5600/ 6092]\n",
      "551.8209549188614\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.384468  [ 4200/ 6092]\n",
      "loss: 1.165451  [ 5600/ 6092]\n",
      "551.934628367424\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 1.165466  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.165592  [ 5600/ 6092]\n",
      "551.343426823616\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.304904  [    0/ 6092]\n",
      "loss: 1.166275  [ 1400/ 6092]\n",
      "loss: 1.165471  [ 2800/ 6092]\n",
      "loss: 1.308427  [ 4200/ 6092]\n",
      "loss: 1.165617  [ 5600/ 6092]\n",
      "551.1828466653824\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 1.165906  [    0/ 6092]\n",
      "loss: 1.165697  [ 1400/ 6092]\n",
      "loss: 1.166927  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "552.3436478376389\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165517  [ 1400/ 6092]\n",
      "loss: 1.165538  [ 2800/ 6092]\n",
      "loss: 1.165437  [ 4200/ 6092]\n",
      "loss: 1.170588  [ 5600/ 6092]\n",
      "551.9711689949036\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.166762  [ 1400/ 6092]\n",
      "loss: 1.165437  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.174039  [ 5600/ 6092]\n",
      "550.9831845760345\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 1.166915  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.308455  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.0264070034027\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 1.167979  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.166186  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.236139  [ 5600/ 6092]\n",
      "551.0509895086288\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.301553  [ 1400/ 6092]\n",
      "loss: 1.308363  [ 2800/ 6092]\n",
      "loss: 1.168825  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.9917416572571\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 1.165432  [    0/ 6092]\n",
      "loss: 1.177812  [ 1400/ 6092]\n",
      "loss: 1.168289  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.6561294794083\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 1.165743  [    0/ 6092]\n",
      "loss: 1.211938  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.305038  [ 5600/ 6092]\n",
      "552.034882068634\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165487  [ 1400/ 6092]\n",
      "loss: 1.306445  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165425  [ 5600/ 6092]\n",
      "551.3590587377548\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 1.165430  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165426  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165431  [ 5600/ 6092]\n",
      "551.6585861444473\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 1.236559  [    0/ 6092]\n",
      "loss: 1.224831  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.184055  [ 5600/ 6092]\n",
      "552.5226848125458\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165438  [ 1400/ 6092]\n",
      "loss: 1.165461  [ 2800/ 6092]\n",
      "loss: 1.165446  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.8271675109863\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165579  [ 2800/ 6092]\n",
      "loss: 1.305423  [ 4200/ 6092]\n",
      "loss: 1.167895  [ 5600/ 6092]\n",
      "550.8281108140945\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 1.165716  [    0/ 6092]\n",
      "loss: 1.165667  [ 1400/ 6092]\n",
      "loss: 1.168630  [ 2800/ 6092]\n",
      "loss: 1.165426  [ 4200/ 6092]\n",
      "loss: 1.304947  [ 5600/ 6092]\n",
      "551.0450359582901\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 1.165427  [    0/ 6092]\n",
      "loss: 1.169575  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.232134  [ 5600/ 6092]\n",
      "552.0104341506958\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.165468  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165432  [ 5600/ 6092]\n",
      "550.2978415489197\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 1.165693  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.308279  [ 2800/ 6092]\n",
      "loss: 1.165506  [ 4200/ 6092]\n",
      "loss: 1.165570  [ 5600/ 6092]\n",
      "552.5468225479126\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.4800579547882\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.207626  [ 1400/ 6092]\n",
      "loss: 1.231068  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.3445761203766\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 1.237122  [    0/ 6092]\n",
      "loss: 1.200399  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.169728  [ 4200/ 6092]\n",
      "loss: 1.176442  [ 5600/ 6092]\n",
      "550.850461602211\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 1.165427  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.304558  [ 5600/ 6092]\n",
      "550.1750193834305\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 1.230157  [    0/ 6092]\n",
      "loss: 1.165510  [ 1400/ 6092]\n",
      "loss: 1.166738  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.304792  [ 5600/ 6092]\n",
      "551.0402168035507\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.207694  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.173631  [ 5600/ 6092]\n",
      "551.1367070674896\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.7152416706085\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.173076  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165428  [ 5600/ 6092]\n",
      "551.9323772192001\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.236175  [ 1400/ 6092]\n",
      "loss: 1.165470  [ 2800/ 6092]\n",
      "loss: 1.165440  [ 4200/ 6092]\n",
      "loss: 1.165431  [ 5600/ 6092]\n",
      "551.3672643899918\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 1.168642  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.0788145065308\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 1.166399  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.305911  [ 2800/ 6092]\n",
      "loss: 1.165454  [ 4200/ 6092]\n",
      "loss: 1.165809  [ 5600/ 6092]\n",
      "551.4820725917816\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 1.165540  [    0/ 6092]\n",
      "loss: 1.290591  [ 1400/ 6092]\n",
      "loss: 1.166030  [ 2800/ 6092]\n",
      "loss: 1.179454  [ 4200/ 6092]\n",
      "loss: 1.165427  [ 5600/ 6092]\n",
      "551.881716966629\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 1.165444  [    0/ 6092]\n",
      "loss: 1.378089  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.1316202878952\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 1.231341  [    0/ 6092]\n",
      "loss: 1.308054  [ 1400/ 6092]\n",
      "loss: 1.289417  [ 2800/ 6092]\n",
      "loss: 1.165435  [ 4200/ 6092]\n",
      "loss: 1.165499  [ 5600/ 6092]\n",
      "552.0255888700485\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 1.306336  [    0/ 6092]\n",
      "loss: 1.165430  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165509  [ 5600/ 6092]\n",
      "552.3204628229141\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 1.165477  [    0/ 6092]\n",
      "loss: 1.165851  [ 1400/ 6092]\n",
      "loss: 1.307426  [ 2800/ 6092]\n",
      "loss: 1.307525  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.1896457672119\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 1.165514  [    0/ 6092]\n",
      "loss: 1.233107  [ 1400/ 6092]\n",
      "loss: 1.165448  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.7688302993774\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.166435  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.232885  [ 4200/ 6092]\n",
      "loss: 1.165433  [ 5600/ 6092]\n",
      "551.4516971111298\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165588  [ 1400/ 6092]\n",
      "loss: 1.165429  [ 2800/ 6092]\n",
      "loss: 1.165475  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "552.3341549634933\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.170915  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.2546844482422\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165513  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.176477  [ 4200/ 6092]\n",
      "loss: 1.234849  [ 5600/ 6092]\n",
      "552.4836381673813\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 1.236083  [    0/ 6092]\n",
      "loss: 1.176257  [ 1400/ 6092]\n",
      "loss: 1.165660  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.5567544698715\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 1.306282  [    0/ 6092]\n",
      "loss: 1.165484  [ 1400/ 6092]\n",
      "loss: 1.183238  [ 2800/ 6092]\n",
      "loss: 1.165654  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "551.1923617124557\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 1.165588  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.214929  [ 5600/ 6092]\n",
      "551.718407034874\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 1.165446  [    0/ 6092]\n",
      "loss: 1.236110  [ 1400/ 6092]\n",
      "loss: 1.165611  [ 2800/ 6092]\n",
      "loss: 1.165586  [ 4200/ 6092]\n",
      "loss: 1.178608  [ 5600/ 6092]\n",
      "551.9559412002563\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 1.192771  [    0/ 6092]\n",
      "loss: 1.165426  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.304195  [ 5600/ 6092]\n",
      "551.0508478879929\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.237374  [ 1400/ 6092]\n",
      "loss: 1.165558  [ 2800/ 6092]\n",
      "loss: 1.221189  [ 4200/ 6092]\n",
      "loss: 1.209748  [ 5600/ 6092]\n",
      "551.6418644189835\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 1.251886  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.304381  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165431  [ 5600/ 6092]\n",
      "551.1792943477631\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 1.211960  [    0/ 6092]\n",
      "loss: 1.165482  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.304902  [ 4200/ 6092]\n",
      "loss: 1.308262  [ 5600/ 6092]\n",
      "551.6756466627121\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165864  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.6148827075958\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 1.238090  [    0/ 6092]\n",
      "loss: 1.166030  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.166423  [ 5600/ 6092]\n",
      "550.8460508584976\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 1.165439  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165493  [ 2800/ 6092]\n",
      "loss: 1.305366  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.7988005876541\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 1.307827  [    0/ 6092]\n",
      "loss: 1.229143  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165524  [ 5600/ 6092]\n",
      "551.6549737453461\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 1.246897  [    0/ 6092]\n",
      "loss: 1.166260  [ 1400/ 6092]\n",
      "loss: 1.166123  [ 2800/ 6092]\n",
      "loss: 1.165538  [ 4200/ 6092]\n",
      "loss: 1.165729  [ 5600/ 6092]\n",
      "550.8403701782227\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 1.167241  [    0/ 6092]\n",
      "loss: 1.165530  [ 1400/ 6092]\n",
      "loss: 1.307336  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.166240  [ 5600/ 6092]\n",
      "550.8701281547546\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.177767  [ 2800/ 6092]\n",
      "loss: 1.165449  [ 4200/ 6092]\n",
      "loss: 1.167656  [ 5600/ 6092]\n",
      "551.3467193841934\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.190297  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.302361  [ 5600/ 6092]\n",
      "551.4233155250549\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165426  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165474  [ 5600/ 6092]\n",
      "550.8736528158188\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.165425  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165813  [ 4200/ 6092]\n",
      "loss: 1.165818  [ 5600/ 6092]\n",
      "550.9712800979614\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 1.165791  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.301996  [ 4200/ 6092]\n",
      "loss: 1.302339  [ 5600/ 6092]\n",
      "551.4121062755585\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 1.165461  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165440  [ 2800/ 6092]\n",
      "loss: 1.307932  [ 4200/ 6092]\n",
      "loss: 1.165443  [ 5600/ 6092]\n",
      "551.3355188369751\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.233778  [ 1400/ 6092]\n",
      "loss: 1.165949  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.165700  [ 5600/ 6092]\n",
      "550.8870457410812\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.166047  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.166960  [ 4200/ 6092]\n",
      "loss: 1.231709  [ 5600/ 6092]\n",
      "551.0070662498474\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 1.165478  [    0/ 6092]\n",
      "loss: 1.165656  [ 1400/ 6092]\n",
      "loss: 1.165434  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.166413  [ 5600/ 6092]\n",
      "551.2882572412491\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 1.167928  [    0/ 6092]\n",
      "loss: 1.165431  [ 1400/ 6092]\n",
      "loss: 1.308296  [ 2800/ 6092]\n",
      "loss: 1.165789  [ 4200/ 6092]\n",
      "loss: 1.165845  [ 5600/ 6092]\n",
      "551.3334439992905\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165879  [ 1400/ 6092]\n",
      "loss: 1.303844  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165438  [ 5600/ 6092]\n",
      "550.7900497913361\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 1.166333  [    0/ 6092]\n",
      "loss: 1.210592  [ 1400/ 6092]\n",
      "loss: 1.306875  [ 2800/ 6092]\n",
      "loss: 1.166059  [ 4200/ 6092]\n",
      "loss: 1.255065  [ 5600/ 6092]\n",
      "551.3743650913239\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165430  [ 1400/ 6092]\n",
      "loss: 1.304090  [ 2800/ 6092]\n",
      "loss: 1.165490  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.7778652906418\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165435  [ 1400/ 6092]\n",
      "loss: 1.169349  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.251827  [ 5600/ 6092]\n",
      "550.541767001152\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 1.230296  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.178854  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.196818  [ 5600/ 6092]\n",
      "551.904501914978\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 1.308298  [    0/ 6092]\n",
      "loss: 1.169980  [ 1400/ 6092]\n",
      "loss: 1.165471  [ 2800/ 6092]\n",
      "loss: 1.307442  [ 4200/ 6092]\n",
      "loss: 1.165539  [ 5600/ 6092]\n",
      "550.600553393364\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.204476  [ 1400/ 6092]\n",
      "loss: 1.166438  [ 2800/ 6092]\n",
      "loss: 1.168243  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.5901781320572\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.167797  [ 1400/ 6092]\n",
      "loss: 1.166316  [ 2800/ 6092]\n",
      "loss: 1.165430  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "549.4919229745865\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 1.165438  [    0/ 6092]\n",
      "loss: 1.165429  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.305400  [ 4200/ 6092]\n",
      "loss: 1.166138  [ 5600/ 6092]\n",
      "550.1039842367172\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.165443  [ 4200/ 6092]\n",
      "loss: 1.172225  [ 5600/ 6092]\n",
      "551.2614114284515\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 1.165427  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.312895  [ 2800/ 6092]\n",
      "loss: 1.165441  [ 4200/ 6092]\n",
      "loss: 1.165427  [ 5600/ 6092]\n",
      "551.4198124408722\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 1.193182  [    0/ 6092]\n",
      "loss: 1.234169  [ 1400/ 6092]\n",
      "loss: 1.165487  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.1092799901962\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 1.167457  [    0/ 6092]\n",
      "loss: 1.165429  [ 1400/ 6092]\n",
      "loss: 1.166084  [ 2800/ 6092]\n",
      "loss: 1.308292  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.6763982772827\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 1.201754  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165456  [ 5600/ 6092]\n",
      "550.8192697763443\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.172274  [ 1400/ 6092]\n",
      "loss: 1.166036  [ 2800/ 6092]\n",
      "loss: 1.166928  [ 4200/ 6092]\n",
      "loss: 1.172769  [ 5600/ 6092]\n",
      "551.3085309267044\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.306423  [ 1400/ 6092]\n",
      "loss: 1.165527  [ 2800/ 6092]\n",
      "loss: 1.165802  [ 4200/ 6092]\n",
      "loss: 1.260829  [ 5600/ 6092]\n",
      "550.7347888946533\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 1.185012  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.178875  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.9611921310425\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 1.174334  [    0/ 6092]\n",
      "loss: 1.165559  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165450  [ 4200/ 6092]\n",
      "loss: 1.239974  [ 5600/ 6092]\n",
      "550.5856578350067\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.230278  [ 1400/ 6092]\n",
      "loss: 1.165547  [ 2800/ 6092]\n",
      "loss: 1.173579  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.0062503814697\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 1.165439  [    0/ 6092]\n",
      "loss: 1.192288  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165441  [ 4200/ 6092]\n",
      "loss: 1.165464  [ 5600/ 6092]\n",
      "550.6961559057236\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 1.168963  [    0/ 6092]\n",
      "loss: 1.306288  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165451  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.1740680932999\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 1.304949  [    0/ 6092]\n",
      "loss: 1.175631  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165439  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "551.14814722538\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 1.165430  [    0/ 6092]\n",
      "loss: 1.308210  [ 1400/ 6092]\n",
      "loss: 1.165443  [ 2800/ 6092]\n",
      "loss: 1.166683  [ 4200/ 6092]\n",
      "loss: 1.165425  [ 5600/ 6092]\n",
      "550.1540253162384\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.167262  [ 2800/ 6092]\n",
      "loss: 1.165452  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.3801791667938\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 1.305123  [    0/ 6092]\n",
      "loss: 1.169919  [ 1400/ 6092]\n",
      "loss: 1.168222  [ 2800/ 6092]\n",
      "loss: 1.165444  [ 4200/ 6092]\n",
      "loss: 1.172008  [ 5600/ 6092]\n",
      "550.3397362232208\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165684  [ 1400/ 6092]\n",
      "loss: 1.175561  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.1093881130219\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 1.165682  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165514  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.9827491044998\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 1.187394  [    0/ 6092]\n",
      "loss: 1.165429  [ 1400/ 6092]\n",
      "loss: 1.325047  [ 2800/ 6092]\n",
      "loss: 1.165427  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.2430728673935\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 1.165429  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.178438  [ 5600/ 6092]\n",
      "550.3452831506729\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165456  [ 1400/ 6092]\n",
      "loss: 1.170778  [ 2800/ 6092]\n",
      "loss: 1.165554  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.4543178081512\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 1.318297  [    0/ 6092]\n",
      "loss: 1.185498  [ 1400/ 6092]\n",
      "loss: 1.492505  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.8288514614105\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.166517  [ 4200/ 6092]\n",
      "loss: 1.302818  [ 5600/ 6092]\n",
      "550.8436402082443\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165659  [ 2800/ 6092]\n",
      "loss: 1.305108  [ 4200/ 6092]\n",
      "loss: 1.305250  [ 5600/ 6092]\n",
      "551.0220296382904\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.169549  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.167344  [ 5600/ 6092]\n",
      "550.683411359787\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.447920  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165457  [ 4200/ 6092]\n",
      "loss: 1.165468  [ 5600/ 6092]\n",
      "550.6103994846344\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.166047  [ 1400/ 6092]\n",
      "loss: 1.165849  [ 2800/ 6092]\n",
      "loss: 1.166732  [ 4200/ 6092]\n",
      "loss: 1.165433  [ 5600/ 6092]\n",
      "550.3435944318771\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 1.165473  [    0/ 6092]\n",
      "loss: 1.167552  [ 1400/ 6092]\n",
      "loss: 1.165477  [ 2800/ 6092]\n",
      "loss: 1.167360  [ 4200/ 6092]\n",
      "loss: 1.165431  [ 5600/ 6092]\n",
      "550.0100190639496\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.197179  [ 1400/ 6092]\n",
      "loss: 1.165690  [ 2800/ 6092]\n",
      "loss: 1.308307  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.9180129766464\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 1.303991  [    0/ 6092]\n",
      "loss: 1.224190  [ 1400/ 6092]\n",
      "loss: 1.166099  [ 2800/ 6092]\n",
      "loss: 1.165569  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.2510985136032\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.312407  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.307584  [ 5600/ 6092]\n",
      "549.827963590622\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165849  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.191056  [ 5600/ 6092]\n",
      "550.380254149437\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.169307  [ 1400/ 6092]\n",
      "loss: 1.165430  [ 2800/ 6092]\n",
      "loss: 1.192737  [ 4200/ 6092]\n",
      "loss: 1.165426  [ 5600/ 6092]\n",
      "550.3516714572906\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165459  [ 1400/ 6092]\n",
      "loss: 1.307761  [ 2800/ 6092]\n",
      "loss: 1.168486  [ 4200/ 6092]\n",
      "loss: 1.165450  [ 5600/ 6092]\n",
      "549.6614397764206\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 1.165638  [    0/ 6092]\n",
      "loss: 1.205173  [ 1400/ 6092]\n",
      "loss: 1.196601  [ 2800/ 6092]\n",
      "loss: 1.169813  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.7423356771469\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 1.307523  [    0/ 6092]\n",
      "loss: 1.166214  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165516  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.1663358211517\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 1.165609  [    0/ 6092]\n",
      "loss: 1.165621  [ 1400/ 6092]\n",
      "loss: 1.165426  [ 2800/ 6092]\n",
      "loss: 1.210562  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "549.6602157354355\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 1.169050  [    0/ 6092]\n",
      "loss: 1.294655  [ 1400/ 6092]\n",
      "loss: 1.307259  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.165552  [ 5600/ 6092]\n",
      "550.5483901500702\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 1.306651  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.165427  [ 5600/ 6092]\n",
      "551.2352995872498\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 1.165430  [    0/ 6092]\n",
      "loss: 1.165700  [ 1400/ 6092]\n",
      "loss: 1.165616  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.263999  [ 5600/ 6092]\n",
      "550.7784087657928\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165519  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.4128044843674\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 1.165487  [    0/ 6092]\n",
      "loss: 1.165437  [ 1400/ 6092]\n",
      "loss: 1.165474  [ 2800/ 6092]\n",
      "loss: 1.165999  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.2343013286591\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 1.167276  [    0/ 6092]\n",
      "loss: 1.304247  [ 1400/ 6092]\n",
      "loss: 1.165696  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165784  [ 5600/ 6092]\n",
      "550.0211132764816\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165473  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165459  [ 4200/ 6092]\n",
      "loss: 1.235977  [ 5600/ 6092]\n",
      "550.2828440666199\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 1.165454  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165636  [ 4200/ 6092]\n",
      "loss: 1.165464  [ 5600/ 6092]\n",
      "550.9366985559464\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 1.234335  [    0/ 6092]\n",
      "loss: 1.165461  [ 1400/ 6092]\n",
      "loss: 1.308199  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.236364  [ 5600/ 6092]\n",
      "549.1760876178741\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 1.165446  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165429  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165427  [ 5600/ 6092]\n",
      "550.4009145498276\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165516  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.168260  [ 5600/ 6092]\n",
      "550.6157698631287\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165430  [ 2800/ 6092]\n",
      "loss: 1.165437  [ 4200/ 6092]\n",
      "loss: 1.165426  [ 5600/ 6092]\n",
      "550.5149745941162\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 1.165460  [    0/ 6092]\n",
      "loss: 1.222840  [ 1400/ 6092]\n",
      "loss: 1.172535  [ 2800/ 6092]\n",
      "loss: 1.169296  [ 4200/ 6092]\n",
      "loss: 1.165425  [ 5600/ 6092]\n",
      "551.1753543615341\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.311165  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.4244015216827\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 1.236960  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.236932  [ 2800/ 6092]\n",
      "loss: 1.166883  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "550.1541875600815\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 1.165429  [    0/ 6092]\n",
      "loss: 1.221020  [ 1400/ 6092]\n",
      "loss: 1.187953  [ 2800/ 6092]\n",
      "loss: 1.213270  [ 4200/ 6092]\n",
      "loss: 1.165437  [ 5600/ 6092]\n",
      "549.5971477031708\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.172302  [ 1400/ 6092]\n",
      "loss: 1.193377  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165431  [ 5600/ 6092]\n",
      "550.1691160202026\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.166403  [ 2800/ 6092]\n",
      "loss: 1.165429  [ 4200/ 6092]\n",
      "loss: 1.165551  [ 5600/ 6092]\n",
      "550.0519134998322\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 1.184778  [    0/ 6092]\n",
      "loss: 1.209045  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.167400  [ 4200/ 6092]\n",
      "loss: 1.165433  [ 5600/ 6092]\n",
      "549.1443520784378\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 1.165428  [    0/ 6092]\n",
      "loss: 1.165526  [ 1400/ 6092]\n",
      "loss: 1.165578  [ 2800/ 6092]\n",
      "loss: 1.171231  [ 4200/ 6092]\n",
      "loss: 1.165462  [ 5600/ 6092]\n",
      "551.3902217149734\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.171754  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165940  [ 5600/ 6092]\n",
      "549.7843172550201\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 1.226683  [    0/ 6092]\n",
      "loss: 1.165866  [ 1400/ 6092]\n",
      "loss: 1.290994  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.178851  [ 5600/ 6092]\n",
      "550.0672277212143\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165446  [ 2800/ 6092]\n",
      "loss: 1.170362  [ 4200/ 6092]\n",
      "loss: 1.165495  [ 5600/ 6092]\n",
      "549.9697774648666\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.239607  [ 5600/ 6092]\n",
      "550.9055185317993\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.236810  [ 2800/ 6092]\n",
      "loss: 1.165631  [ 4200/ 6092]\n",
      "loss: 1.165466  [ 5600/ 6092]\n",
      "550.3998755216599\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.300664  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "549.6382204294205\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165432  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165484  [ 5600/ 6092]\n",
      "550.2391574382782\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 1.165440  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.166047  [ 5600/ 6092]\n",
      "549.7504241466522\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 1.165431  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165919  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.6509275436401\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.165626  [ 1400/ 6092]\n",
      "loss: 1.236754  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.307668  [ 5600/ 6092]\n",
      "549.7008836269379\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165833  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.310016  [ 4200/ 6092]\n",
      "loss: 1.307483  [ 5600/ 6092]\n",
      "549.9203395843506\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 1.165774  [    0/ 6092]\n",
      "loss: 1.165527  [ 1400/ 6092]\n",
      "loss: 1.165733  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165794  [ 5600/ 6092]\n",
      "550.4861748218536\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 1.171141  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165456  [ 2800/ 6092]\n",
      "loss: 1.182232  [ 4200/ 6092]\n",
      "loss: 1.447402  [ 5600/ 6092]\n",
      "549.7369070053101\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.165456  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165442  [ 4200/ 6092]\n",
      "loss: 1.236737  [ 5600/ 6092]\n",
      "549.359870672226\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165485  [ 2800/ 6092]\n",
      "loss: 1.165435  [ 4200/ 6092]\n",
      "loss: 1.165578  [ 5600/ 6092]\n",
      "550.0597002506256\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 1.167395  [    0/ 6092]\n",
      "loss: 1.237316  [ 1400/ 6092]\n",
      "loss: 1.165431  [ 2800/ 6092]\n",
      "loss: 1.165438  [ 4200/ 6092]\n",
      "loss: 1.165446  [ 5600/ 6092]\n",
      "549.4433460235596\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165921  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165808  [ 5600/ 6092]\n",
      "550.6748404502869\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165473  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.166217  [ 5600/ 6092]\n",
      "549.8662203550339\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.187015  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.166102  [ 4200/ 6092]\n",
      "loss: 1.165436  [ 5600/ 6092]\n",
      "549.9520832300186\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 1.168766  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165992  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.6506298780441\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.169007  [ 1400/ 6092]\n",
      "loss: 1.374109  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165439  [ 5600/ 6092]\n",
      "550.11390376091\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.165480  [ 1400/ 6092]\n",
      "loss: 1.165852  [ 2800/ 6092]\n",
      "loss: 1.165427  [ 4200/ 6092]\n",
      "loss: 1.165435  [ 5600/ 6092]\n",
      "549.1838866472244\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.167786  [ 5600/ 6092]\n",
      "548.9628255367279\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 1.165991  [    0/ 6092]\n",
      "loss: 1.165428  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165625  [ 4200/ 6092]\n",
      "loss: 1.165428  [ 5600/ 6092]\n",
      "550.3449614048004\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.324675  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165431  [ 4200/ 6092]\n",
      "loss: 1.305402  [ 5600/ 6092]\n",
      "549.724796295166\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.246189  [ 1400/ 6092]\n",
      "loss: 1.165429  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165476  [ 5600/ 6092]\n",
      "549.5052934885025\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 1.165450  [    0/ 6092]\n",
      "loss: 1.178781  [ 1400/ 6092]\n",
      "loss: 1.166064  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.180017  [ 5600/ 6092]\n",
      "549.7670937776566\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.177673  [ 2800/ 6092]\n",
      "loss: 1.307768  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.751064658165\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.236289  [ 1400/ 6092]\n",
      "loss: 1.244424  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.7322630882263\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 1.167067  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165430  [ 2800/ 6092]\n",
      "loss: 1.211984  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "550.0837663412094\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 1.168588  [    0/ 6092]\n",
      "loss: 1.166812  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.188815  [ 4200/ 6092]\n",
      "loss: 1.203095  [ 5600/ 6092]\n",
      "549.5170930624008\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 1.170581  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.445486  [ 2800/ 6092]\n",
      "loss: 1.449121  [ 4200/ 6092]\n",
      "loss: 1.304870  [ 5600/ 6092]\n",
      "549.0416054725647\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 1.165638  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165449  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.0558415651321\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 1.166206  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165447  [ 2800/ 6092]\n",
      "loss: 1.165488  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.8406058549881\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 1.307805  [    0/ 6092]\n",
      "loss: 1.233633  [ 1400/ 6092]\n",
      "loss: 1.215802  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.167932  [ 5600/ 6092]\n",
      "549.754102230072\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 1.165597  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165589  [ 2800/ 6092]\n",
      "loss: 1.236835  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.1439226865768\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.184229  [ 2800/ 6092]\n",
      "loss: 1.233641  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "549.8788778781891\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 1.166189  [    0/ 6092]\n",
      "loss: 1.234742  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.182557  [ 4200/ 6092]\n",
      "loss: 1.165672  [ 5600/ 6092]\n",
      "549.7957201004028\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 1.165431  [    0/ 6092]\n",
      "loss: 1.452110  [ 1400/ 6092]\n",
      "loss: 1.165442  [ 2800/ 6092]\n",
      "loss: 1.165588  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.0382516384125\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.166105  [ 1400/ 6092]\n",
      "loss: 1.170103  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "550.2581702470779\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 1.165914  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.304510  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.165435  [ 5600/ 6092]\n",
      "549.9721825122833\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165600  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.165875  [ 4200/ 6092]\n",
      "loss: 1.165504  [ 5600/ 6092]\n",
      "550.8930631875992\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165518  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.191220  [ 4200/ 6092]\n",
      "loss: 1.165524  [ 5600/ 6092]\n",
      "549.1735755205154\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 1.166002  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165969  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "549.5507216453552\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.294547  [ 2800/ 6092]\n",
      "loss: 1.165426  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.7146148681641\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.308254  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.0124545097351\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 1.165468  [    0/ 6092]\n",
      "loss: 1.165465  [ 1400/ 6092]\n",
      "loss: 1.165643  [ 2800/ 6092]\n",
      "loss: 1.166256  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.298580288887\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.303850  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "550.0323668718338\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 1.165488  [    0/ 6092]\n",
      "loss: 1.166046  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.301781  [ 4200/ 6092]\n",
      "loss: 1.303878  [ 5600/ 6092]\n",
      "549.8270061016083\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 1.302405  [    0/ 6092]\n",
      "loss: 1.215382  [ 1400/ 6092]\n",
      "loss: 1.165437  [ 2800/ 6092]\n",
      "loss: 1.165436  [ 4200/ 6092]\n",
      "loss: 1.166913  [ 5600/ 6092]\n",
      "548.99873316288\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165426  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.251270  [ 5600/ 6092]\n",
      "550.211505651474\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 1.165687  [    0/ 6092]\n",
      "loss: 1.165434  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.304813  [ 5600/ 6092]\n",
      "549.2915661334991\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.308253  [ 2800/ 6092]\n",
      "loss: 1.165426  [ 4200/ 6092]\n",
      "loss: 1.306787  [ 5600/ 6092]\n",
      "549.5238646268845\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.309104  [ 5600/ 6092]\n",
      "549.7166956663132\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 1.304680  [    0/ 6092]\n",
      "loss: 1.182146  [ 1400/ 6092]\n",
      "loss: 1.165495  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "549.2554374933243\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 1.243118  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.178833  [ 2800/ 6092]\n",
      "loss: 1.238604  [ 4200/ 6092]\n",
      "loss: 1.165426  [ 5600/ 6092]\n",
      "549.2714173793793\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165506  [ 1400/ 6092]\n",
      "loss: 1.166452  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165454  [ 5600/ 6092]\n",
      "550.0784496068954\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 1.167650  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.165426  [ 4200/ 6092]\n",
      "loss: 1.166184  [ 5600/ 6092]\n",
      "548.7416626214981\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.304353  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.9700078964233\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 1.165460  [    0/ 6092]\n",
      "loss: 1.165427  [ 1400/ 6092]\n",
      "loss: 1.165565  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.5550618171692\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 1.307693  [    0/ 6092]\n",
      "loss: 1.212555  [ 1400/ 6092]\n",
      "loss: 1.307830  [ 2800/ 6092]\n",
      "loss: 1.220890  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.6250110864639\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 1.165672  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.308272  [ 2800/ 6092]\n",
      "loss: 1.217149  [ 4200/ 6092]\n",
      "loss: 1.168921  [ 5600/ 6092]\n",
      "549.3720288276672\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.308266  [ 1400/ 6092]\n",
      "loss: 1.205383  [ 2800/ 6092]\n",
      "loss: 1.166976  [ 4200/ 6092]\n",
      "loss: 1.307011  [ 5600/ 6092]\n",
      "549.8543875217438\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 1.307422  [    0/ 6092]\n",
      "loss: 1.165432  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.445620  [ 4200/ 6092]\n",
      "loss: 1.166327  [ 5600/ 6092]\n",
      "549.9462394714355\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 1.165445  [    0/ 6092]\n",
      "loss: 1.165470  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165462  [ 4200/ 6092]\n",
      "loss: 1.307485  [ 5600/ 6092]\n",
      "549.7764245271683\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 1.206016  [    0/ 6092]\n",
      "loss: 1.166793  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.165656  [ 4200/ 6092]\n",
      "loss: 1.165432  [ 5600/ 6092]\n",
      "550.379154086113\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165609  [ 1400/ 6092]\n",
      "loss: 1.165457  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.242745757103\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.168228  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165425  [ 5600/ 6092]\n",
      "550.2843971252441\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.166191  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.165944  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.4567860364914\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165432  [ 1400/ 6092]\n",
      "loss: 1.306141  [ 2800/ 6092]\n",
      "loss: 1.165430  [ 4200/ 6092]\n",
      "loss: 1.165439  [ 5600/ 6092]\n",
      "549.441483259201\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 1.165472  [    0/ 6092]\n",
      "loss: 1.165439  [ 1400/ 6092]\n",
      "loss: 1.165440  [ 2800/ 6092]\n",
      "loss: 1.166850  [ 4200/ 6092]\n",
      "loss: 1.306489  [ 5600/ 6092]\n",
      "549.7283353805542\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165790  [ 4200/ 6092]\n",
      "loss: 1.182317  [ 5600/ 6092]\n",
      "549.7843898534775\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 1.166756  [    0/ 6092]\n",
      "loss: 1.166359  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.165436  [ 4200/ 6092]\n",
      "loss: 1.185141  [ 5600/ 6092]\n",
      "549.165367603302\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 1.184409  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165442  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.8143491744995\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 1.238855  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.167903  [ 5600/ 6092]\n",
      "549.0451233386993\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.306046  [ 1400/ 6092]\n",
      "loss: 1.165528  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.308224  [ 5600/ 6092]\n",
      "549.7825934886932\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.305934  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165502  [ 5600/ 6092]\n",
      "549.6160759925842\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.303667  [ 4200/ 6092]\n",
      "loss: 1.165565  [ 5600/ 6092]\n",
      "549.6392252445221\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165590  [ 1400/ 6092]\n",
      "loss: 1.165792  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.3793263435364\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 1.168165  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.308341  [ 4200/ 6092]\n",
      "loss: 1.303631  [ 5600/ 6092]\n",
      "549.2076381444931\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 1.166350  [    0/ 6092]\n",
      "loss: 1.236798  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165452  [ 5600/ 6092]\n",
      "550.1546775102615\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.5724346637726\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.166634  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165758  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "549.5230242013931\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.227541  [ 1400/ 6092]\n",
      "loss: 1.304728  [ 2800/ 6092]\n",
      "loss: 1.165457  [ 4200/ 6092]\n",
      "loss: 1.165463  [ 5600/ 6092]\n",
      "549.4972296953201\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 1.201686  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.167211  [ 2800/ 6092]\n",
      "loss: 1.165434  [ 4200/ 6092]\n",
      "loss: 1.165426  [ 5600/ 6092]\n",
      "548.8462469577789\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 1.170466  [    0/ 6092]\n",
      "loss: 1.165431  [ 1400/ 6092]\n",
      "loss: 1.167815  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.165452  [ 5600/ 6092]\n",
      "549.3349366188049\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 1.448120  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165438  [ 2800/ 6092]\n",
      "loss: 1.165464  [ 4200/ 6092]\n",
      "loss: 1.165425  [ 5600/ 6092]\n",
      "549.7131235599518\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.176050  [ 1400/ 6092]\n",
      "loss: 1.183768  [ 2800/ 6092]\n",
      "loss: 1.219713  [ 4200/ 6092]\n",
      "loss: 1.165428  [ 5600/ 6092]\n",
      "549.3979966640472\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165971  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165427  [ 4200/ 6092]\n",
      "loss: 1.166574  [ 5600/ 6092]\n",
      "549.3337278366089\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 1.165588  [    0/ 6092]\n",
      "loss: 1.235049  [ 1400/ 6092]\n",
      "loss: 1.165480  [ 2800/ 6092]\n",
      "loss: 1.202207  [ 4200/ 6092]\n",
      "loss: 1.165839  [ 5600/ 6092]\n",
      "549.140457868576\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.308277  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165721  [ 4200/ 6092]\n",
      "loss: 1.166137  [ 5600/ 6092]\n",
      "549.8306022882462\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 1.169151  [    0/ 6092]\n",
      "loss: 1.203879  [ 1400/ 6092]\n",
      "loss: 1.165448  [ 2800/ 6092]\n",
      "loss: 1.165482  [ 4200/ 6092]\n",
      "loss: 1.167240  [ 5600/ 6092]\n",
      "549.3924828767776\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.240840  [ 1400/ 6092]\n",
      "loss: 1.165873  [ 2800/ 6092]\n",
      "loss: 1.199916  [ 4200/ 6092]\n",
      "loss: 1.165426  [ 5600/ 6092]\n",
      "549.6959900856018\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 1.179743  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165755  [ 2800/ 6092]\n",
      "loss: 1.165449  [ 4200/ 6092]\n",
      "loss: 1.165427  [ 5600/ 6092]\n",
      "548.340167760849\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 1.165731  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165426  [ 5600/ 6092]\n",
      "549.6362234354019\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165461  [ 1400/ 6092]\n",
      "loss: 1.167124  [ 2800/ 6092]\n",
      "loss: 1.184437  [ 4200/ 6092]\n",
      "loss: 1.167264  [ 5600/ 6092]\n",
      "549.06034553051\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 1.165429  [    0/ 6092]\n",
      "loss: 1.307165  [ 1400/ 6092]\n",
      "loss: 1.165428  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.231477  [ 5600/ 6092]\n",
      "548.782636642456\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.307785  [ 1400/ 6092]\n",
      "loss: 1.165453  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.165671  [ 5600/ 6092]\n",
      "550.2289687395096\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 1.182977  [    0/ 6092]\n",
      "loss: 1.275385  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165712  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "549.2773451805115\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165657  [ 2800/ 6092]\n",
      "loss: 1.283530  [ 4200/ 6092]\n",
      "loss: 1.165458  [ 5600/ 6092]\n",
      "549.840434551239\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.166092  [ 2800/ 6092]\n",
      "loss: 1.271775  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.1974704265594\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 1.228418  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165450  [ 2800/ 6092]\n",
      "loss: 1.165431  [ 4200/ 6092]\n",
      "loss: 1.165434  [ 5600/ 6092]\n",
      "548.7234427928925\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165966  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.179044  [ 5600/ 6092]\n",
      "548.9184402227402\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 1.166374  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165500  [ 2800/ 6092]\n",
      "loss: 1.165440  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "548.3016856908798\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 1.165459  [    0/ 6092]\n",
      "loss: 1.302383  [ 1400/ 6092]\n",
      "loss: 1.168066  [ 2800/ 6092]\n",
      "loss: 1.166752  [ 4200/ 6092]\n",
      "loss: 1.165688  [ 5600/ 6092]\n",
      "548.4513912200928\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 1.165431  [    0/ 6092]\n",
      "loss: 1.165424  [ 1400/ 6092]\n",
      "loss: 1.165429  [ 2800/ 6092]\n",
      "loss: 1.165550  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.3943001031876\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165468  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "548.9036048650742\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 1.168404  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165424  [ 2800/ 6092]\n",
      "loss: 1.183274  [ 4200/ 6092]\n",
      "loss: 1.173963  [ 5600/ 6092]\n",
      "549.2948098182678\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165432  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.238063  [ 5600/ 6092]\n",
      "548.913348197937\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 1.309306  [    0/ 6092]\n",
      "loss: 1.167811  [ 1400/ 6092]\n",
      "loss: 1.173864  [ 2800/ 6092]\n",
      "loss: 1.165430  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "548.8900747299194\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 1.308091  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165455  [ 4200/ 6092]\n",
      "loss: 1.165433  [ 5600/ 6092]\n",
      "548.3351930379868\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 1.167198  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165543  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165582  [ 5600/ 6092]\n",
      "548.8790900707245\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 1.207497  [    0/ 6092]\n",
      "loss: 1.165506  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165424  [ 4200/ 6092]\n",
      "loss: 1.165424  [ 5600/ 6092]\n",
      "547.83536028862\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165425  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "548.3114234209061\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165915  [ 1400/ 6092]\n",
      "loss: 1.244241  [ 2800/ 6092]\n",
      "loss: 1.165458  [ 4200/ 6092]\n",
      "loss: 1.166127  [ 5600/ 6092]\n",
      "549.606073141098\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 1.234804  [    0/ 6092]\n",
      "loss: 1.165580  [ 1400/ 6092]\n",
      "loss: 1.303954  [ 2800/ 6092]\n",
      "loss: 1.166011  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "548.9990785121918\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 1.167551  [    0/ 6092]\n",
      "loss: 1.166380  [ 1400/ 6092]\n",
      "loss: 1.166294  [ 2800/ 6092]\n",
      "loss: 1.195309  [ 4200/ 6092]\n",
      "loss: 1.165544  [ 5600/ 6092]\n",
      "549.1488703489304\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 1.185768  [    0/ 6092]\n",
      "loss: 1.165538  [ 1400/ 6092]\n",
      "loss: 1.165427  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.165439  [ 5600/ 6092]\n",
      "548.2595961093903\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 1.236420  [    0/ 6092]\n",
      "loss: 1.377999  [ 1400/ 6092]\n",
      "loss: 1.165444  [ 2800/ 6092]\n",
      "loss: 1.197446  [ 4200/ 6092]\n",
      "loss: 1.165655  [ 5600/ 6092]\n",
      "548.038388967514\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165423  [ 1400/ 6092]\n",
      "loss: 1.165976  [ 2800/ 6092]\n",
      "loss: 1.165459  [ 4200/ 6092]\n",
      "loss: 1.526511  [ 5600/ 6092]\n",
      "548.81856071949\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 1.171370  [    0/ 6092]\n",
      "loss: 1.165436  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.304803  [ 5600/ 6092]\n",
      "549.2763226032257\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.165890  [ 1400/ 6092]\n",
      "loss: 1.165423  [ 2800/ 6092]\n",
      "loss: 1.165452  [ 4200/ 6092]\n",
      "loss: 1.234115  [ 5600/ 6092]\n",
      "548.7470499277115\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 1.165426  [    0/ 6092]\n",
      "loss: 1.166085  [ 1400/ 6092]\n",
      "loss: 1.170464  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.188823  [ 5600/ 6092]\n",
      "548.8403387069702\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 1.184673  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.306783  [ 2800/ 6092]\n",
      "loss: 1.308124  [ 4200/ 6092]\n",
      "loss: 1.309433  [ 5600/ 6092]\n",
      "548.7430334091187\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 1.165429  [    0/ 6092]\n",
      "loss: 1.165547  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165425  [ 4200/ 6092]\n",
      "loss: 1.207667  [ 5600/ 6092]\n",
      "549.0893411636353\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 1.180145  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.199534  [ 2800/ 6092]\n",
      "loss: 1.304439  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "548.9099460840225\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.165717  [ 1400/ 6092]\n",
      "loss: 1.167655  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.192344  [ 5600/ 6092]\n",
      "548.7808282375336\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 1.165423  [    0/ 6092]\n",
      "loss: 1.303093  [ 1400/ 6092]\n",
      "loss: 1.170768  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.165569  [ 5600/ 6092]\n",
      "549.2777652740479\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.165666  [ 1400/ 6092]\n",
      "loss: 1.165451  [ 2800/ 6092]\n",
      "loss: 1.165492  [ 4200/ 6092]\n",
      "loss: 1.165423  [ 5600/ 6092]\n",
      "548.6567981243134\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 1.165422  [    0/ 6092]\n",
      "loss: 1.304833  [ 1400/ 6092]\n",
      "loss: 1.165431  [ 2800/ 6092]\n",
      "loss: 1.165428  [ 4200/ 6092]\n",
      "loss: 1.449500  [ 5600/ 6092]\n",
      "549.1147819757462\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 1.218024  [    0/ 6092]\n",
      "loss: 1.165624  [ 1400/ 6092]\n",
      "loss: 1.165431  [ 2800/ 6092]\n",
      "loss: 1.165443  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "548.5415959358215\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 1.165425  [    0/ 6092]\n",
      "loss: 1.165427  [ 1400/ 6092]\n",
      "loss: 1.165436  [ 2800/ 6092]\n",
      "loss: 1.165441  [ 4200/ 6092]\n",
      "loss: 1.169657  [ 5600/ 6092]\n",
      "548.5107407569885\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 1.165424  [    0/ 6092]\n",
      "loss: 1.172734  [ 1400/ 6092]\n",
      "loss: 1.167621  [ 2800/ 6092]\n",
      "loss: 1.165423  [ 4200/ 6092]\n",
      "loss: 1.165422  [ 5600/ 6092]\n",
      "549.3121787309647\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 1.165434  [    0/ 6092]\n",
      "loss: 1.165422  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165422  [ 4200/ 6092]\n",
      "loss: 1.166080  [ 5600/ 6092]\n",
      "548.734464764595\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 1.165679  [    0/ 6092]\n",
      "loss: 1.165894  [ 1400/ 6092]\n",
      "loss: 1.165422  [ 2800/ 6092]\n",
      "loss: 1.165441  [ 4200/ 6092]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m enc_trainer \u001B[38;5;241m=\u001B[39m sset\u001B[38;5;241m.\u001B[39mSSDirectDMTrainer(is_encoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model_path\u001B[38;5;241m=\u001B[39mmodels_dir, save_model_every\u001B[38;5;241m=\u001B[39msave_every, lr\u001B[38;5;241m=\u001B[39mstart_lr, dataset\u001B[38;5;241m=\u001B[39mdata_set, device\u001B[38;5;241m=\u001B[39mdevice, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m     12\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m---> 13\u001B[0m \u001B[43menc_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# 200 +500 + 500 +500\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/mapping_down/ss_driect_dm_trainer.py:58\u001B[0m, in \u001B[0;36mSSDirectDMTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/mapping_down/ss_driect_dm_trainer.py:94\u001B[0m, in \u001B[0;36mSSDirectDMTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     93\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 94\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\u001B[39;00m\n\u001B[1;32m     96\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils/encoder\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import network_models.soundsream_models_and_utils.clip_like.mapping_down.ss_driect_dm_trainer as sset\n",
    "importlib.reload(sset)\n",
    "\n",
    "epochs =500\n",
    "enc_trainer = sset.SSDirectDMTrainer(is_encoder=True, batch_size=batch_size, num_epochs=epochs, model_path=models_dir, save_model_every=save_every, lr=start_lr, dataset=data_set, device=device, model=model)\n",
    "gc.collect()\n",
    "enc_trainer.train() # 200 +500 + 500 +500+500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------------------] 0.0% ...generating encoding\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnetwork_models\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msoundsream_models_and_utils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mss_encoded_dataset\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mssed\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mssed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mss_encoded_dataset_full\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirectory_cafe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/ckwdani/Music/emotionDatasets/converted_mono/cafe\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirectory_tess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/ckwdani/Music/emotionDatasets/converted_mono/tess\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirectory_ravdess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/ckwdani/Music/emotionDatasets/converted_mono/RAVDESS Audio_Speech_Actors_01-24\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirectory_mesd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/ckwdani/Music/emotionDatasets/converted_mono/mesd\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#sound_stream_path=\"../notebooks/content/soundstream/vers0.7.4/01_Soundstream_7_x_new_libri_full/5_3250/soundstream.3250.pt\")\u001B[39;49;00m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#sound_stream_path=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/vers0.7.4/01_Soundstream_7_x_new_libri_full/currentSelection/soundstream.72500.pt\")\u001B[39;49;00m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43msound_stream_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/verision0.12.1/10_10000_1e-4_bs6_gae8_dml320-32/soundstream.8000.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclip_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/downmapping/0_0216/encoder_360.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# trained for 1500 epochs\u001B[39;49;00m\n\u001B[1;32m     21\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m dataset\u001B[38;5;241m.\u001B[39msaveEncoding(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../notebooks/content/data/allEncodings_clip_like.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_encoded_dataset.py:47\u001B[0m, in \u001B[0;36mss_encoded_dataset_full.__init__\u001B[0;34m(self, sound_stream_path, directory_tess, directory_ravdess, directory_cafe, directory_mesd, seconds, sr, device, one_hot_encoded, csvPath, clip_path)\u001B[0m\n\u001B[1;32m     42\u001B[0m paths_dataset \u001B[38;5;241m=\u001B[39m CombinedEmoDataSet_7_emos(\n\u001B[1;32m     43\u001B[0m     directory_tess\u001B[38;5;241m=\u001B[39mdirectory_tess, directory_cafe\u001B[38;5;241m=\u001B[39mdirectory_cafe, directory_ravdess\u001B[38;5;241m=\u001B[39mdirectory_ravdess,\n\u001B[1;32m     44\u001B[0m     directory_mesd\u001B[38;5;241m=\u001B[39mdirectory_mesd, device\u001B[38;5;241m=\u001B[39mdevice, transFormAudio\u001B[38;5;241m=\u001B[39mcollateToSeconds(seconds, sr, const_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m     45\u001B[0m num_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(paths_dataset\u001B[38;5;241m.\u001B[39mlabel_list)\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoded_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mss_encoded_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpaths_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msound_stream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msoundstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mone_hot_encoded\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mone_hot_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcsvPath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcsvPath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_encoded_dataset.py:103\u001B[0m, in \u001B[0;36mss_encoded_dataset.__init__\u001B[0;34m(self, data_set, clip, sound_stream, num_labels, one_hot_encoded, device, csvPath)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(csvPath \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataSet\u001B[38;5;241m.\u001B[39mlabel_list\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodedData: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encodeWithSoundStream\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodedData \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloadEncoding(csvPath)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_encoded_dataset.py:156\u001B[0m, in \u001B[0;36mss_encoded_dataset._encodeWithSoundStream\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    153\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoundStream(sample[\u001B[38;5;241m0\u001B[39m], return_encoded \u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 156\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241m.\u001B[39mclip(data)\n\u001B[1;32m    158\u001B[0m tensors\u001B[38;5;241m.\u001B[39mappend(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu())\n\u001B[1;32m    159\u001B[0m emotions\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memoToId(sample[\u001B[38;5;241m1\u001B[39m]))\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_encoded_dataset.py:156\u001B[0m, in \u001B[0;36mss_encoded_dataset._encodeWithSoundStream\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    153\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoundStream(sample[\u001B[38;5;241m0\u001B[39m], return_encoded \u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 156\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241m.\u001B[39mclip(data)\n\u001B[1;32m    158\u001B[0m tensors\u001B[38;5;241m.\u001B[39mappend(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu())\n\u001B[1;32m    159\u001B[0m emotions\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memoToId(sample[\u001B[38;5;241m1\u001B[39m]))\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/222.4345.23/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:747\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 747\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/222.4345.23/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/222.4345.23/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/222.4345.23/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_models_and_utils\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "sys.path.insert(0, \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject\")\n",
    "import network_models.soundsream_models_and_utils.ss_encoded_dataset as ssed\n",
    "\n",
    "dataset = ssed.ss_encoded_dataset_full(\n",
    "    directory_cafe=\"/home/ckwdani/Music/emotionDatasets/converted_mono/cafe\",\n",
    "    directory_tess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\",\n",
    "    directory_ravdess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/RAVDESS Audio_Speech_Actors_01-24\",\n",
    "    directory_mesd=\"/home/ckwdani/Music/emotionDatasets/converted_mono/mesd\",\n",
    "    device=\"cuda\",\n",
    "    #sound_stream_path=\"../notebooks/content/soundstream/vers0.7.4/01_Soundstream_7_x_new_libri_full/5_3250/soundstream.3250.pt\")\n",
    "    #sound_stream_path=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/vers0.7.4/01_Soundstream_7_x_new_libri_full/currentSelection/soundstream.72500.pt\")\n",
    "    sound_stream_path=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/verision0.12.1/10_10000_1e-4_bs6_gae8_dml320-32/soundstream.8000.pt\",\n",
    "    clip_path=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/downmapping/0_0216/encoder_360.pth\" # trained for 1500 epochs\n",
    ")\n",
    "dataset.saveEncoding(\"../notebooks/content/data/allEncodings_clip_like.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3, 4, 5, 6]), tensor([0, 1, 2, 3, 4, 5, 6]))\n",
      "tensor(0.0257, device='cuda:0', grad_fn=<DotBackward0>)\n",
      "tensor([[ 0.0257,  0.0682,  0.1137,  0.1003,  0.0861,  0.0434,  0.1270],\n",
      "        [ 0.0364,  0.1691,  0.0499,  0.0520,  0.0445,  0.0215,  0.0879],\n",
      "        [ 0.0471,  0.2021,  0.1437,  0.1197,  0.0319,  0.0232,  0.0524],\n",
      "        [-0.0349,  0.1412, -0.0580,  0.1619,  0.0691,  0.0561,  0.1994],\n",
      "        [ 0.1065,  0.1545,  0.1924,  0.1211,  0.1037,  0.0426,  0.1208],\n",
      "        [ 0.1051,  0.1988,  0.1682,  0.2198,  0.1852,  0.0045,  0.1342],\n",
      "        [ 0.0185,  0.0735,  0.0420,  0.0426,  0.0786,  0.0026,  0.5665]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor(0.5645, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from network_models.soundsream_models_and_utils.ss___util_class_batches_sampler import ClassBatchesSampler\n",
    "# np1 = data_set.encoded_dataset.encodedData[data_set.encoded_dataset.labelcolumn].to_numpy()\n",
    "# from torch.utils.data import DataLoader\n",
    "# #\n",
    "# # l = [np1[i][0] for i in range(len(np1))]\n",
    "# #\n",
    "# #\n",
    "# dl = DataLoader(data_set, batch_sampler=ClassBatchesSampler(np1, num_class_samples=2, shuffle=False))\n",
    "# #\n",
    "# #\n",
    "# for batch, (X, z) in enumerate(dl):\n",
    "#\n",
    "#     print(torch.tensor_split(torch.argmax(z, dim=1), 2))\n",
    "#     z1, z2 = torch.tensor_split(z, 2)\n",
    "#     pred = model(X.cuda())\n",
    "#     pred1, pred2 = torch.tensor_split(pred, 2)\n",
    "#     dp = torch.matmul(pred1, pred2.t())\n",
    "#     #dp = pred1 @ pred2.t()\n",
    "#\n",
    "#\n",
    "#     print(torch.dot(pred1[0],pred2[0]))\n",
    "#     # print(torch.dot(pred[1],pred2[0]))\n",
    "#     # print(torch.dot(pred[2],pred2[0]))\n",
    "#     # print(torch.dot(pred[3],pred2[0]))\n",
    "#     # print(torch.dot(pred[4],pred2[0]))\n",
    "#     # print(torch.dot(pred[5],pred2[0]))\n",
    "#     # print(torch.dot(pred[7],pred[0]))\n",
    "#     #dp = torch.matmul(z1, z2.T) / (z1.norm(dim=-1, keepdim=True) * z2.norm(dim=-1, keepdim=True))\n",
    "#     #print(dp)\n",
    "#     print(dp)\n",
    "#     print(torch.sum(dp[0, :]))\n",
    "#\n",
    "#     #print(X[0][0][0][170])\n",
    "#\n",
    "#     if batch == 0:\n",
    "#         break\n",
    "#\n",
    "# (len(dl)/(7*2))-2 == 462"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
