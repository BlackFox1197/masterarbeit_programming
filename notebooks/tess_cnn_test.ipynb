{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy\n",
    "import tensorflow\n",
    "import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import warnings\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, Flatten, Activation, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###SEED###\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    paths = []\n",
    "    testpaths = []\n",
    "    testlabels = []\n",
    "    terminator = 'D:/Uni/19.Master/Daten/terminator.wav'\n",
    "    print(sys.executable)\n",
    "    labels = []\n",
    "    # for dirname, _, filenames in os.walk('Daten/TESS Toronto emotional speech set data'):\n",
    "    # D:\\Uni\\19.Master\\DATEN\n",
    "    for dirname, _, filenames in os.walk('/content/drive/My Drive/Colab Notebooks/data'):\n",
    "        for filename in filenames:\n",
    "            label = filename.split('_')[-1]\n",
    "            label = label.split('.')[0]\n",
    "            if (label != 'neutral'):\n",
    "                labels.append(label.lower())\n",
    "                paths.append(os.path.join(dirname, filename))\n",
    "    for dirname, _, filenames in os.walk('D:/Uni/19.Master/Daten/Stimuli_Intensit√§tsmorphs'):\n",
    "        for filename in filenames:\n",
    "\n",
    "            intens = filename.split('_')[-2]\n",
    "            emot = filename.split('_')[1]\n",
    "            label = emot\n",
    "            match label:\n",
    "                case 'ang':\n",
    "                    label = 'angry'\n",
    "                case 'dis':\n",
    "                    label = 'disgust'\n",
    "                case 'fea':\n",
    "                    label = 'fear'\n",
    "                case 'hap':\n",
    "                    label = 'happy'\n",
    "                case 'sad':\n",
    "                    label = 'sad'\n",
    "                case 'sur':\n",
    "                    label = 'ps'\n",
    "            if (emot != 'ple'):\n",
    "                testpaths.append(os.path.join(dirname, filename))\n",
    "                testlabels.append(label.lower())\n",
    "    com_labels = testlabels + labels\n",
    "    com_paths = testpaths + paths\n",
    "    print(testlabels)\n",
    "    print(testpaths)\n",
    "    print('Dataset is loaded')\n",
    "    return paths, labels, testpaths, testlabels\n",
    "\n",
    "\n",
    "def waveplot(data, sr, emotion):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(emotion, size=20)\n",
    "    librosa.display.waveshow(data, sr=sr)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def spectrogramm(data, sr, emotion):\n",
    "    fourier = librosa.stft(data)\n",
    "    fourierdb = librosa.amplitude_to_db(abs(fourier))\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(emotion, size=20)\n",
    "    librosa.display.specshow(fourierdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def extract_mfcc(filename, layer):\n",
    "    y, sr = librosa.load(filename, duration=3, offset=0.5)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=layer).T, axis=0)\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def create_new_model(layer):\n",
    "    model = Sequential([\n",
    "        LSTM(123, return_sequences=False, input_shape=(layer, 1)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_new_cnnmodelsea(layer):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(256, 5, padding='same',\n",
    "                     input_shape=(layer, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(128, 5, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=(8)))\n",
    "    model.add(Conv1D(128, 5, padding='same', ))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Conv1D(128, 5,padding='same',))\n",
    "    # model.add(Activation('relu'))\n",
    "    # model.add(Conv1D(128, 5,padding='same',))\n",
    "    # model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(128, 5, padding='same', ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_new_cnnmodel(layer):\n",
    "    # Build sequential CNN\n",
    "    CNN_model = Sequential()\n",
    "\n",
    "    # Build first layer\n",
    "    CNN_model.add(Conv1D(16, 5, padding='same',\n",
    "                         input_shape=(layer, 1), activation='relu'))\n",
    "\n",
    "    # Build second layer\n",
    "    CNN_model.add(Conv1D(32, 5, padding='same', activation='relu'))\n",
    "\n",
    "    # Build third layer\n",
    "    #CNN_model.add(Conv1D(64, 5, padding='same', activation='relu'))\n",
    "\n",
    "    # Build forth layer\n",
    "    CNN_model.add(Conv1D(128, 5, padding='same', activation='relu'))\n",
    "\n",
    "    # Add dropout\n",
    "    CNN_model.add(Dropout(0.1))\n",
    "\n",
    "    # Flatten\n",
    "    CNN_model.add(Flatten())\n",
    "\n",
    "    CNN_model.add(Dense(128, activation='relu'))\n",
    "    CNN_model.add(Dropout(0.1))\n",
    "    CNN_model.add(Dense(64, activation='relu'))\n",
    "    CNN_model.add(Dense(6, activation='softmax'))\n",
    "    CNN_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    CNN_model.summary()\n",
    "    return CNN_model\n",
    "\n",
    "\n",
    "def extract_mfcc_verbose(layer, filename, offset, duration):\n",
    "    y, sr = librosa.load(filename, duration=duration, offset=offset)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=layer).T, axis=0)\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def plot_accuracy(history, len):\n",
    "    epochs = list(range(len))\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    plt.plot(epochs, acc, label='train accuracy')\n",
    "    plt.plot(epochs, val_acc, label='val accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(history, len):\n",
    "    epochs = list(range(len))\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.plot(epochs, loss, label='train loss')\n",
    "    plt.plot(epochs, val_loss, label='val loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalize_data(datatrain, datatest):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(datatrain)\n",
    "    data_scaled_train = scaler.transform(datatrain)\n",
    "    data_scaled_test = scaler.transform(datatest)\n",
    "    return data_scaled_train, data_scaled_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer = 40\n",
    "trainpaths, trainlabels, testpaths, testlabels = load_dataset()\n",
    "\n",
    "\n",
    "###create dataframes for training and testing###\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF[\"speech\"] = trainpaths\n",
    "trainDF[\"label\"] = trainlabels\n",
    "testDF = pd.DataFrame()\n",
    "testDF[\"speech\"] = testpaths\n",
    "testDF[\"label\"] = testlabels\n",
    "\n",
    "### extracting features mfccs ###\n",
    "#train_mfccs = trainDF[\"speech\"].apply(lambda x: extract_mfcc_verbose(x, 0, 0.8))\n",
    "#test_mfccs = testDF[\"speech\"].apply(lambda x: extract_mfcc_verbose(x, 0, 0.8))\n",
    "\n",
    "#train_mfccs.to_csv(\"train_mfccs.csv\")\n",
    "#test_mfccs.to_csv(\"test_mfccs.csv\")\n",
    "\n",
    "### make model ###\n",
    "\n",
    "model = create_new_cnnmodel(layer=layer)\n",
    "\n",
    "# train_shaped = [x for x in train_mfccs]\n",
    "#train_shaped = np.array(train_shaped)\n",
    "\n",
    "#np.save(\"train_shaped_mfccs.npy\",train_shaped)\n",
    "train_shaped = np.load(\"train_shaped_mfccs.npy\")\n",
    "\n",
    "\n",
    "#test_shaped = [x for x in test_mfccs]\n",
    "#test_shaped = np.array(test_shaped)\n",
    "\n",
    "#np.save(\"test_shaped_mfccs.npy\", test_shaped)\n",
    "test_shaped = np.load(\"test_shaped_mfccs.npy\")\n",
    "\n",
    "### normalizing ###\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_shaped)\n",
    "data_scaled_train = scaler.transform(train_shaped)\n",
    "data_scaled_test = scaler.transform(test_shaped)\n",
    "\n",
    "train_shaped = np.expand_dims(data_scaled_train, -1)\n",
    "test_shaped = np.expand_dims(data_scaled_test, -1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "y = enc.fit_transform(trainDF[['label']])\n",
    "y = y.toarray()\n",
    "\n",
    "y_test2 = enc.fit_transform(testDF[['label']])\n",
    "y_test2 = y_test2.toarray()\n",
    "\n",
    "###split train and test data###\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_shaped, y, test_size=0.2, random_state=1)\n",
    "\n",
    "### training model ###\n",
    "epochs = 25\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=50, shuffle=True,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "### save the model ###\n",
    "# filename = \"Completed Cnn model.joblib\"\n",
    "# joblib.dump(model, filename)\n",
    "\n",
    "### plotting accuracy and loss ###\n",
    "plot_accuracy(history, epochs)\n",
    "plot_loss(history, epochs)\n",
    "\n",
    "predicted_test = model.predict(test_shaped)\n",
    "\n",
    "testres = pd.DataFrame()\n",
    "pred_emotions_for_test = []\n",
    "i = -1\n",
    "for val in predicted_test:\n",
    "    i = i + 1\n",
    "    maxima = max(val)\n",
    "    match np.argmin(val):\n",
    "        case 0:\n",
    "            pred_emotions_for_test.append(f'angy {maxima} original: {testlabels[i]}')\n",
    "        case 1:\n",
    "            pred_emotions_for_test.append(f'disgust{maxima} original: {testlabels[i]}')\n",
    "        case 2:\n",
    "            pred_emotions_for_test.append(f'fear{maxima} original: {testlabels[i]}')\n",
    "        case 3:\n",
    "            pred_emotions_for_test.append(f'happy{maxima} original: {testlabels[i]}')\n",
    "        case 4:\n",
    "            pred_emotions_for_test.append(f'neutral{maxima} original: {testlabels[i]}')\n",
    "        case 5:\n",
    "            pred_emotions_for_test.append(f'pleas_surprise{maxima} original: {testlabels[i]}')\n",
    "        case 6:\n",
    "            pred_emotions_for_test.append(f'sad{maxima} original: {testlabels[i]}')\n",
    "# testlabels\n",
    "pred_right = []\n",
    "pred_wrong = []\n",
    "for val in pred_emotions_for_test:\n",
    "    if val.split(' ')[0][0] == val.split(' ')[-1][0]:\n",
    "        pred_right.append(1)\n",
    "    else:\n",
    "        pred_wrong.append(1)\n",
    "print(f\"right+{len(pred_right)}\")\n",
    "print(f\"wrong+{len(pred_wrong)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
