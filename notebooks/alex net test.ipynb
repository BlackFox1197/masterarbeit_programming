{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.6\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "import torchvision as torchvision\n",
    "#from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions defined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndef scale_minmax(X, min=0.0, max=1.0):\\n    X_std = (X - X.min()) / (X.max() - X.min())\\n    X_scaled = X_std * (max - min) + min\\n    return X_scaled\\n\\ndef spectrogram_image(y, sr, out, hop_length, n_mels):\\n    # use log-melspectrogram\\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\\n                                            n_fft=hop_length*2, hop_length=hop_length)\\n    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\\n\\n    # min-max scale to fit inside 8-bit range\\n    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\\n    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\\n    img = 255-img # invert. make black==more energy\\n\\n    # save as PNG\\n    #plt.axis(\\'off\\')\\n    plt.imsave(\"spec.png\",img)\\n    plt.imshow(img)\\n\\ndef createSpectrogramm(path, label):\\n    data, sr = librosa.load(path)\\n    fourier = librosa.stft(data)\\n    fourierdb = librosa.amplitude_to_db(abs(fourier))\\n    librosa.display.specshow(fourierdb, sr=sr, x_axis=\\'time\\', y_axis=\\'hz\\')\\n\\ndef createMelSpectrogramm(path, label):\\n    data, sr = librosa.load(path, sr=22050, offset=0, duration=1)\\n    melSpec = librosa.feature.melspectrogram(data, sr, n_mels=192, n_fft=1024, hop_length=260)\\n\\n    # plt.figure(figsize=(10, 4))\\n    # plt.title(label, size=20)\\n    librosa.display.specshow(melSpec, sr=sr, x_axis=\\'time\\', y_axis=\\'hz\\')\\n    # plt.colorbar()\\n'"
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def load_custom_dataset():\n",
    "    paths = []\n",
    "    testpaths = []\n",
    "    testlabels = []\n",
    "    terminator = 'D:/Uni/19.Master/Daten/terminator.wav'\n",
    "    print(sys.executable)\n",
    "    labels = []\n",
    "    # for dirname, _, filenames in os.walk('Daten/TESS Toronto emotional speech set data'):\n",
    "    # D:\\Uni\\19.Master\\DATEN\n",
    "    for dirname, _, filenames in os.walk('../tess'):\n",
    "        for filename in filenames:\n",
    "            label = filename.split('_')[-1]\n",
    "            label = label.split('.')[0]\n",
    "            if (label != 'neutral'):\n",
    "                labels.append(label.lower())\n",
    "                paths.append(os.path.join(dirname, filename))\n",
    "    for dirname, _, filenames in os.walk('../stimuli_intensit√§tsmorphs'):\n",
    "        for filename in filenames:\n",
    "\n",
    "            intens = filename.split('_')[-2]\n",
    "            emot = filename.split('_')[1]\n",
    "            label = emot\n",
    "            match label:\n",
    "                case 'ang':\n",
    "                    label = 'angry'\n",
    "                case 'dis':\n",
    "                    label = 'disgust'\n",
    "                case 'fea':\n",
    "                    label = 'fear'\n",
    "                case 'hap':\n",
    "                    label = 'happy'\n",
    "                case 'sad':\n",
    "                    label = 'sad'\n",
    "                case 'sur':\n",
    "                    label = 'ps'\n",
    "            if (emot != 'ple'):\n",
    "                testpaths.append(os.path.join(dirname, filename))\n",
    "                testlabels.append(label.lower())\n",
    "    com_labels = testlabels + labels\n",
    "    com_paths = testpaths + paths\n",
    "    print(testlabels)\n",
    "    print(testpaths)\n",
    "    print('Dataset is loaded')\n",
    "    return paths, labels, testpaths, testlabels\n",
    "\"\"\"\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def spectrogram_image(y, sr, out, hop_length, n_mels):\n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
    "                                            n_fft=hop_length*2, hop_length=hop_length)\n",
    "    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\n",
    "    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "\n",
    "    # save as PNG\n",
    "    #plt.axis('off')\n",
    "    plt.imsave(\"spec.png\",img)\n",
    "    plt.imshow(img)\n",
    "\n",
    "def createSpectrogramm(path, label):\n",
    "    data, sr = librosa.load(path)\n",
    "    fourier = librosa.stft(data)\n",
    "    fourierdb = librosa.amplitude_to_db(abs(fourier))\n",
    "    librosa.display.specshow(fourierdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "\n",
    "def createMelSpectrogramm(path, label):\n",
    "    data, sr = librosa.load(path, sr=22050, offset=0, duration=1)\n",
    "    melSpec = librosa.feature.melspectrogram(data, sr, n_mels=192, n_fft=1024, hop_length=260)\n",
    "\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # plt.title(label, size=20)\n",
    "    librosa.display.specshow(melSpec, sr=sr, x_axis='time', y_axis='hz')\n",
    "    # plt.colorbar()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\Scripts\\python.exe\n",
      "['angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps']\n",
      "['../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf01_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf02_sur_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_fea_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf03_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_dis_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_fea_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nf04_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_fea_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sad_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm01_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm02_sur_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm03_sur_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_ang_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_dis_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_fea_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w01_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_hap_w05_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w02_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w03_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sad_w05_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w01_c_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w02_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w03_o_75_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_100_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_25_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_50_70dB.wav', '../stimuli_intensit√§tsmorphs\\\\nm04_sur_w05_o_75_70dB.wav']\n",
      "Dataset is loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                speech  label\n0    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_10...  angry\n1    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_25...  angry\n2    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_50...  angry\n3    ../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_75...  angry\n4    ../stimuli_intensit√§tsmorphs\\nf01_ang_w02_o_10...  angry\n..                                                 ...    ...\n763  ../stimuli_intensit√§tsmorphs\\nm04_sur_w03_o_75...     ps\n764  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_10...     ps\n765  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_25...     ps\n766  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_50...     ps\n767  ../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_75...     ps\n\n[768 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speech</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_10...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_25...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_50...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w01_o_75...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../stimuli_intensit√§tsmorphs\\nf01_ang_w02_o_10...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w03_o_75...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_10...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_25...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_50...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>../stimuli_intensit√§tsmorphs\\nm04_sur_w05_o_75...</td>\n      <td>ps</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows √ó 2 columns</p>\n</div>"
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpaths, trainlabels, testpaths, testlabels = load_custom_dataset()\n",
    "\n",
    "###create dataframes for training and testing###\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF[\"speech\"] = trainpaths\n",
    "trainDF[\"label\"] = trainlabels\n",
    "testDF = pd.DataFrame()\n",
    "testDF[\"speech\"] = testpaths\n",
    "testDF[\"label\"] = testlabels\n",
    "\n",
    "testDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\npath = trainDF[\"speech\"][0]\\n# settings\\nhop_length = 302 # number of samples per time-step in spectrogram\\nn_mels = 192 # number of bins in spectrogram. Height of image\\ntime_steps = 192 # number of time-steps. Width of image\\n\\n# load audio. Using example from librosa\\ny, sr = librosa.load(path, offset=0.1, duration=1, sr=99100)\\nout = \\'out.png\\'\\n\\n# extract a fixed length window\\nstart_sample = 0 # starting at beginning\\nlength_samples = time_steps*hop_length\\nwindow = y[start_sample:start_sample+length_samples]\\n\\n# convert to PNG\\nspectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\\n\\n#train_spectros = trainDF[\"speech\"].apply(lambda x: spectrogram_image(x))\\n'"
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = trainDF[\"speech\"][0]\n",
    "# settings\n",
    "hop_length = 302 # number of samples per time-step in spectrogram\n",
    "n_mels = 192 # number of bins in spectrogram. Height of image\n",
    "time_steps = 192 # number of time-steps. Width of image\n",
    "\n",
    "# load audio. Using example from librosa\n",
    "y, sr = librosa.load(path, offset=0.1, duration=1, sr=99100)\n",
    "out = 'out.png'\n",
    "\n",
    "# extract a fixed length window\n",
    "start_sample = 0 # starting at beginning\n",
    "length_samples = time_steps*hop_length\n",
    "window = y[start_sample:start_sample+length_samples]\n",
    "\n",
    "# convert to PNG\n",
    "spectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "#train_spectros = trainDF[\"speech\"].apply(lambda x: spectrogram_image(x))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\\n'"
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchvision.models import AlexNet_Weights\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\"\"\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# load dummy dataset and read soundfiles\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\n# tokenize\\ninput_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\\n\\n# retrieve logits\\nlogits = model(input_values).logits\\n\\n# take argmax and decode\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\\n'"
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import wer\n",
    "import torch\n",
    "\"\"\"\n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ny = librosa.load(sr=16000, path=testDF[\"speech\"][0])\\n\\n\\ninput_values = processor(y[0], return_tensors=\"pt\",  sampling_rate=16000).input_values  # Batch size 1\\n\\n# retrieve logits\\nlogits = model(input_values).logits\\n\\n# take argmax and decode\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\\n'"
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "y = librosa.load(sr=16000, path=testDF[\"speech\"][0])\n",
    "\n",
    "\n",
    "input_values = processor(y[0], return_tensors=\"pt\",  sampling_rate=16000).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ntrain_audios=[]\\nfor x in testDF[\"speech\"]:\\n    audio=librosa.load(path=x, sr=16000)\\n    train_audios.append(audio[0])\\ntrain_audios\\n'"
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_audios=[]\n",
    "for x in testDF[\"speech\"]:\n",
    "    audio=librosa.load(path=x, sr=16000)\n",
    "    train_audios.append(audio[0])\n",
    "train_audios\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "save_path=\"/masterarbeit_programming/notebooks/content/data\"\n",
    "train_df, test_df=train_test_split(trainDF, test_size=0.2, random_state=101, stratify=trainDF[\"label\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 2)\n",
      "(480, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7bdecb5de06ff1de\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/tonib/.cache/huggingface/datasets/csv/default-7bdecb5de06ff1de/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd078c4337fc4befab0ad6b580d22bea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dde1e853f03646079618ee9fde5853cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10789f3bf2ac44449c803e45f46728b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/tonib/.cache/huggingface/datasets/csv/default-7bdecb5de06ff1de/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc43f10efa4d4e29aa742bc9bf59a1d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['speech', 'label'],\n",
      "    num_rows: 1920\n",
      "})\n",
      "Dataset({\n",
      "    features: ['speech', 'label'],\n",
      "    num_rows: 480\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading the created dataset using datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/masterarbeit_programming/notebooks/content/data/train.csv\",\n",
    "    \"validation\": \"/masterarbeit_programming/notebooks/content/data/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "train_df = dataset[\"train\"]\n",
    "test_df = dataset[\"validation\"]\n",
    "\n",
    "print(train_df)\n",
    "print(test_df)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "outputs": [],
   "source": [
    "# specifiy input and output column\n",
    "input_colum=\"speech\"\n",
    "output_column=\"label\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Classes: ['angry', 'disgust', 'fear', 'happy', 'ps', 'sad']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#distinguish labels\n",
    "class_list = train_df.unique(output_column)\n",
    "class_list.sort()\n",
    "num_class = len(class_list)\n",
    "\n",
    "print(f\"{num_class} Classes: {class_list}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "\n",
    "model_name_path =  \"facebook/wav2vec2-base-960h\"\n",
    "pooling_mode = \"mean\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angry\",\n",
      "    \"1\": \"disgust\",\n",
      "    \"2\": \"fear\",\n",
      "    \"3\": \"happy\",\n",
      "    \"4\": \"ps\",\n",
      "    \"5\": \"sad\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"angry\": 0,\n",
      "    \"disgust\": 1,\n",
      "    \"fear\": 2,\n",
      "    \"happy\": 3,\n",
      "    \"ps\": 4,\n",
      "    \"sad\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_path,\n",
    "    num_labels=num_class,\n",
    "    label2id={label: i for i, label in enumerate(class_list)},\n",
    "    id2label={i: label for i, label in enumerate(class_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\"\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\vocab.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\tokenizer_config.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_path)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"Target sample rate: {target_sampling_rate}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import soundfile\n",
    "\n",
    "def speech_file_to_array(speech_path):\n",
    "    speech_array, sampling_rate = librosa.load(speech_path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list=[]\n",
    "    for example in examples:\n",
    "        speech_list.append(speech_file_to_array(examples[\"speech\"]))\n",
    "    #speech_list = [ speech_file_to_array(speech_path) for speech_path in examples[\"speech\"]]\n",
    "    target_list = [ label_to_id(label, class_list) for label in examples[\"label\"]]\n",
    "\n",
    "    result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    result[\"labels\"] = list(target_list)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [],
   "source": [
    "### resample\n",
    "from datasets import Audio\n",
    "\n",
    "train_df = train_df.cast_column(input_colum, Audio(sampling_rate=target_sampling_rate))\n",
    "\n",
    "test_df = test_df.cast_column(input_colum, Audio(sampling_rate=target_sampling_rate))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [
    {
     "data": {
      "text/plain": "Casting the dataset:   0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1d08bc1e00c4d6fa663092a80112745"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2de78e90d8c4425293bc639df94ca21c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "### label 2 id\n",
    "\n",
    "train_df = train_df.cast_column(output_column, ClassLabel(num_classes=num_class,names=class_list))\n",
    "\n",
    "test_df = test_df.cast_column(output_column, ClassLabel(num_classes=num_class,names=class_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [
    {
     "data": {
      "text/plain": "{'speech': {'path': '../tess\\\\OAF_Fear\\\\OAF_choice_fear.wav',\n  'array': array([-2.0995614e-05, -1.2846249e-04, -9.7319069e-05, ...,\n          1.7393984e-04,  1.4581185e-04,  0.0000000e+00], dtype=float32),\n  'sampling_rate': 16000},\n 'label': 2}"
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [],
   "source": [
    "is_regression = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = numpy.squeeze(preds) if is_regression else numpy.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(numpy.float32).mean().item()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_path,\n",
    "    config=config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_apex_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/masterarbeit_programming/notebooks/content\",\n",
    "    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=False,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#install apex\n",
    "#!pip install -v --no-cache-dir   ../apex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apex.parallel'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mX:\\masterarbeit_programming\\apex\\apex\\parallel\\__init__.py:15\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msyncbn\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimized_sync_batchnorm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SyncBatchNorm\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'syncbn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [423], line 16\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     11\u001B[0m     Trainer,\n\u001B[0;32m     12\u001B[0m     is_apex_available,\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_apex_available():\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mapex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mamp\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mamp\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.6\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     19\u001B[0m     _is_native_amp_available \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\apex\\apex\\__init__.py:12\u001B[0m\n\u001B[0;32m      8\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mamp\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfp16_utils\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizers\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnormalization\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parallel\n\u001B[0;32m     13\u001B[0m     __all__\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m amp\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\apex\\apex\\parallel\\__init__.py:18\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimized_sync_batchnorm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SyncBatchNorm\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msync_batchnorm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SyncBatchNorm\n\u001B[0;32m     19\u001B[0m     SyncBatchNorm\u001B[38;5;241m.\u001B[39msyncbn_import_error \u001B[38;5;241m=\u001B[39m err\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_syncbn_model\u001B[39m(module, process_group\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, channel_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\apex\\apex\\parallel\\sync_batchnorm.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatchnorm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _BatchNorm\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msync_batchnorm_kernel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SyncBatchnormFunction\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mapex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparallel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReduceOp\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSyncBatchNorm\u001B[39;00m(_BatchNorm):\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\apex\\apex\\parallel\\sync_batchnorm_kernel.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunction\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Function\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mapex\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparallel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ReduceOp\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSyncBatchnormFunction\u001B[39;00m(Function):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(ctx, \u001B[38;5;28minput\u001B[39m, weight, bias, running_mean, running_variance, eps, process_group, world_size):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'apex.parallel'"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "import apex\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    import apex.apex.amp as amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        #if self.amp.use_amp:\n",
    "        if self.use_cuda_amp:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        #if self.use_amp:\n",
    "        if self.use_cuda_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        #elif self.use_apex:\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "outputs": [],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=test_df,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: speech. If speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 90766470\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_values'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [425], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:1501\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1498\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1500\u001B[0m )\n\u001B[1;32m-> 1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1503\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:1723\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1720\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_rng_state(resume_from_checkpoint)\n\u001B[0;32m   1722\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1723\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1724\u001B[0m \n\u001B[0;32m   1725\u001B[0m     \u001B[38;5;66;03m# Skip past any already trained steps if resuming training\u001B[39;00m\n\u001B[0;32m   1726\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m steps_trained_in_current_epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1727\u001B[0m         steps_trained_in_current_epoch \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [413], line 43\u001B[0m, in \u001B[0;36mDataCollatorCTCWithPadding.__call__\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[Dict[\u001B[38;5;28mstr\u001B[39m, Union[List[\u001B[38;5;28mint\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor]]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m---> 43\u001B[0m     input_features \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_values\u001B[39m\u001B[38;5;124m\"\u001B[39m]} \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[0;32m     44\u001B[0m     label_features \u001B[38;5;241m=\u001B[39m [feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[0;32m     46\u001B[0m     d_type \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(label_features[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfloat\n",
      "Cell \u001B[1;32mIn [413], line 43\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[Dict[\u001B[38;5;28mstr\u001B[39m, Union[List[\u001B[38;5;28mint\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor]]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m---> 43\u001B[0m     input_features \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mfeature\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_values\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m} \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[0;32m     44\u001B[0m     label_features \u001B[38;5;241m=\u001B[39m [feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[0;32m     46\u001B[0m     d_type \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(label_features[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfloat\n",
      "\u001B[1;31mKeyError\u001B[0m: 'input_values'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
