{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 20:34:19.045569: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 20:34:19.655842: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 20:34:19.655898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 20:34:19.655902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 12\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8\"\n",
    "trials_per_model_type = 2\n",
    "epochs_per_model = 1000\n",
    "save_highest_acc_min_acc = 0.5\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncs_betterSS_12_1_relwrap_music.pkl\", device=\"cuda\")\n",
    "    #csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient, save_highest_acc_min_acc=save_highest_acc_min_acc, seed=385)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.883365  [    0/ 5482]\n",
      "loss: 1.883826  [ 1200/ 5482]\n",
      "loss: 1.903466  [ 2400/ 5482]\n",
      "loss: 1.888235  [ 3600/ 5482]\n",
      "loss: 1.964282  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.162     0.161     0.828    99\n",
      " disgust     0.162     1.000     0.019    107\n",
      "    fear     0.162     0.000     0.000    80\n",
      "   happy     0.162     0.000     0.000    77\n",
      " neutral     0.162     0.000     0.000    95\n",
      "     sad     0.162     0.000     0.000    91\n",
      "surprise     0.162     0.153     0.246    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.162     0.188     0.156    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 1.922542 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.798424  [    0/ 5482]\n",
      "loss: 1.880164  [ 1200/ 5482]\n",
      "loss: 1.854943  [ 2400/ 5482]\n",
      "loss: 2.004438  [ 3600/ 5482]\n",
      "loss: 1.888358  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.126     0.102     0.152    99\n",
      " disgust     0.126     0.750     0.028    107\n",
      "    fear     0.126     0.000     0.000    80\n",
      "   happy     0.126     0.000     0.000    77\n",
      " neutral     0.126     0.000     0.000    95\n",
      "     sad     0.126     0.000     0.000    91\n",
      "surprise     0.126     0.129     0.967    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.126     0.140     0.164    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 12.6%, Avg loss: 1.950374 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.968428  [    0/ 5482]\n",
      "loss: 1.860971  [ 1200/ 5482]\n",
      "loss: 2.081162  [ 2400/ 5482]\n",
      "loss: 1.844173  [ 3600/ 5482]\n",
      "loss: 1.950983  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.198     0.108     0.202    99\n",
      " disgust     0.198     0.523     0.421    107\n",
      "    fear     0.198     0.000     0.000    80\n",
      "   happy     0.198     0.000     0.000    77\n",
      " neutral     0.198     0.000     0.000    95\n",
      "     sad     0.198     0.000     0.000    91\n",
      "surprise     0.198     0.165     0.918    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.198     0.114     0.220    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 19.8%, Avg loss: 1.883968 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.903593  [    0/ 5482]\n",
      "loss: 1.878072  [ 1200/ 5482]\n",
      "loss: 1.898826  [ 2400/ 5482]\n",
      "loss: 1.793244  [ 3600/ 5482]\n",
      "loss: 1.815027  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.249     0.244     0.212    99\n",
      " disgust     0.249     0.346     0.776    107\n",
      "    fear     0.249     0.000     0.000    80\n",
      "   happy     0.249     0.000     0.000    77\n",
      " neutral     0.249     0.000     0.000    95\n",
      "     sad     0.249     0.189     0.110    91\n",
      "surprise     0.249     0.165     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.249     0.135     0.246    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 1.836966 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.864553  [    0/ 5482]\n",
      "loss: 1.964372  [ 1200/ 5482]\n",
      "loss: 1.766939  [ 2400/ 5482]\n",
      "loss: 1.737582  [ 3600/ 5482]\n",
      "loss: 1.761125  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.231     0.107     0.061    99\n",
      " disgust     0.231     0.389     0.673    107\n",
      "    fear     0.231     0.000     0.000    80\n",
      "   happy     0.231     0.000     0.000    77\n",
      " neutral     0.231     0.000     0.000    95\n",
      "     sad     0.231     0.176     0.132    91\n",
      "surprise     0.231     0.169     0.836    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.231     0.120     0.243    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.1%, Avg loss: 1.801482 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.876819  [    0/ 5482]\n",
      "loss: 1.942924  [ 1200/ 5482]\n",
      "loss: 1.821044  [ 2400/ 5482]\n",
      "loss: 1.756236  [ 3600/ 5482]\n",
      "loss: 1.691324  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.218     0.068     0.030    99\n",
      " disgust     0.218     0.495     0.467    107\n",
      "    fear     0.218     0.000     0.000    80\n",
      "   happy     0.218     0.000     0.000    77\n",
      " neutral     0.218     0.000     0.000    95\n",
      "     sad     0.218     0.250     0.242    91\n",
      "surprise     0.218     0.154     0.951    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.218     0.138     0.241    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.8%, Avg loss: 1.818887 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.777799  [    0/ 5482]\n",
      "loss: 1.599256  [ 1200/ 5482]\n",
      "loss: 1.729631  [ 2400/ 5482]\n",
      "loss: 1.681479  [ 3600/ 5482]\n",
      "loss: 1.743096  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.285     0.057     0.020    99\n",
      " disgust     0.285     0.923     0.449    107\n",
      "    fear     0.285     0.000     0.000    80\n",
      "   happy     0.285     0.231     0.390    77\n",
      " neutral     0.285     0.000     0.000    95\n",
      "     sad     0.285     0.322     0.615    91\n",
      "surprise     0.285     0.180     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.285     0.245     0.300    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 1.782578 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.808226  [    0/ 5482]\n",
      "loss: 1.787992  [ 1200/ 5482]\n",
      "loss: 1.679954  [ 2400/ 5482]\n",
      "loss: 1.692943  [ 3600/ 5482]\n",
      "loss: 1.674835  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.287     0.000     0.000    99\n",
      " disgust     0.287     0.755     0.346    107\n",
      "    fear     0.287     0.182     0.025    80\n",
      "   happy     0.287     0.237     0.714    77\n",
      " neutral     0.287     0.000     0.000    95\n",
      "     sad     0.287     0.286     0.725    91\n",
      "surprise     0.287     0.192     0.246    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.287     0.236     0.294    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 1.723029 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.737713  [    0/ 5482]\n",
      "loss: 1.634683  [ 1200/ 5482]\n",
      "loss: 1.945733  [ 2400/ 5482]\n",
      "loss: 1.671557  [ 3600/ 5482]\n",
      "loss: 1.682684  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.233     0.000     0.000    99\n",
      " disgust     0.233     0.000     0.000    107\n",
      "    fear     0.233     0.091     0.037    80\n",
      "   happy     0.233     0.229     0.844    77\n",
      " neutral     0.233     0.000     0.000    95\n",
      "     sad     0.233     0.258     0.791    91\n",
      "surprise     0.233     0.143     0.033    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.233     0.103     0.244    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 1.694308 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.680914  [    0/ 5482]\n",
      "loss: 1.619167  [ 1200/ 5482]\n",
      "loss: 1.759757  [ 2400/ 5482]\n",
      "loss: 1.600490  [ 3600/ 5482]\n",
      "loss: 1.480640  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.231     0.000     0.000    99\n",
      " disgust     0.231     0.000     0.000    107\n",
      "    fear     0.231     0.088     0.037    80\n",
      "   happy     0.231     0.225     0.870    77\n",
      " neutral     0.231     0.000     0.000    95\n",
      "     sad     0.231     0.257     0.780    91\n",
      "surprise     0.231     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.231     0.081     0.241    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.1%, Avg loss: 1.678101 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.967414  [    0/ 5482]\n",
      "loss: 1.602939  [ 1200/ 5482]\n",
      "loss: 1.553648  [ 2400/ 5482]\n",
      "loss: 1.671994  [ 3600/ 5482]\n",
      "loss: 1.726207  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.234     0.667     0.020    99\n",
      " disgust     0.234     0.000     0.000    107\n",
      "    fear     0.234     0.167     0.087    80\n",
      "   happy     0.234     0.223     0.675    77\n",
      " neutral     0.234     0.000     0.000    95\n",
      "     sad     0.234     0.248     0.901    91\n",
      "surprise     0.234     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.234     0.186     0.241    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.4%, Avg loss: 1.651835 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.634146  [    0/ 5482]\n",
      "loss: 1.769247  [ 1200/ 5482]\n",
      "loss: 1.772739  [ 2400/ 5482]\n",
      "loss: 1.660247  [ 3600/ 5482]\n",
      "loss: 1.568589  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.262     0.000     0.000    99\n",
      " disgust     0.262     0.857     0.168    107\n",
      "    fear     0.262     0.250     0.100    80\n",
      "   happy     0.262     0.212     0.870    77\n",
      " neutral     0.262     0.000     0.000    95\n",
      "     sad     0.262     0.291     0.736    91\n",
      "surprise     0.262     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.262     0.230     0.268    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 1.678605 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.577912  [    0/ 5482]\n",
      "loss: 1.511295  [ 1200/ 5482]\n",
      "loss: 1.413804  [ 2400/ 5482]\n",
      "loss: 1.590491  [ 3600/ 5482]\n",
      "loss: 1.490935  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.248     0.000     0.000    99\n",
      " disgust     0.248     0.500     0.019    107\n",
      "    fear     0.248     0.250     0.113    80\n",
      "   happy     0.248     0.229     0.870    77\n",
      " neutral     0.248     0.000     0.000    95\n",
      "     sad     0.248     0.264     0.791    91\n",
      "surprise     0.248     0.333     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.248     0.225     0.258    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.8%, Avg loss: 1.626535 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.638303  [    0/ 5482]\n",
      "loss: 1.555488  [ 1200/ 5482]\n",
      "loss: 1.477153  [ 2400/ 5482]\n",
      "loss: 1.536317  [ 3600/ 5482]\n",
      "loss: 1.663261  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.272     0.000     0.000    99\n",
      " disgust     0.272     0.778     0.131    107\n",
      "    fear     0.272     0.293     0.275    80\n",
      "   happy     0.272     0.233     0.766    77\n",
      " neutral     0.272     0.000     0.000    95\n",
      "     sad     0.272     0.283     0.780    91\n",
      "surprise     0.272     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.272     0.227     0.279    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 1.626787 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.652368  [    0/ 5482]\n",
      "loss: 1.482783  [ 1200/ 5482]\n",
      "loss: 1.609172  [ 2400/ 5482]\n",
      "loss: 1.469486  [ 3600/ 5482]\n",
      "loss: 1.441056  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.266     0.000     0.000    99\n",
      " disgust     0.266     0.500     0.009    107\n",
      "    fear     0.266     0.361     0.325    80\n",
      "   happy     0.266     0.236     0.688    77\n",
      " neutral     0.266     0.000     0.000    95\n",
      "     sad     0.266     0.264     0.901    91\n",
      "surprise     0.266     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.266     0.194     0.275    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.598923 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.312177  [    0/ 5482]\n",
      "loss: 1.404198  [ 1200/ 5482]\n",
      "loss: 1.666692  [ 2400/ 5482]\n",
      "loss: 1.488702  [ 3600/ 5482]\n",
      "loss: 1.669702  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.282     0.000     0.000    99\n",
      " disgust     0.282     0.692     0.084    107\n",
      "    fear     0.282     0.337     0.350    80\n",
      "   happy     0.282     0.241     0.818    77\n",
      " neutral     0.282     0.000     0.000    95\n",
      "     sad     0.282     0.288     0.791    91\n",
      "surprise     0.282     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.282     0.223     0.292    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 1.612289 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.548638  [    0/ 5482]\n",
      "loss: 1.441727  [ 1200/ 5482]\n",
      "loss: 1.512679  [ 2400/ 5482]\n",
      "loss: 1.789917  [ 3600/ 5482]\n",
      "loss: 1.863491  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.280     0.000     0.000    99\n",
      " disgust     0.280     0.643     0.084    107\n",
      "    fear     0.280     0.403     0.362    80\n",
      "   happy     0.280     0.241     0.805    77\n",
      " neutral     0.280     0.000     0.000    95\n",
      "     sad     0.280     0.270     0.780    91\n",
      "surprise     0.280     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.280     0.222     0.290    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.0%, Avg loss: 1.590392 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.706168  [    0/ 5482]\n",
      "loss: 1.533213  [ 1200/ 5482]\n",
      "loss: 1.449561  [ 2400/ 5482]\n",
      "loss: 1.684272  [ 3600/ 5482]\n",
      "loss: 1.843550  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.302     0.000     0.000    99\n",
      " disgust     0.302     0.636     0.131    107\n",
      "    fear     0.302     0.366     0.562    80\n",
      "   happy     0.302     0.251     0.740    77\n",
      " neutral     0.302     0.000     0.000    95\n",
      "     sad     0.302     0.291     0.747    91\n",
      "surprise     0.302     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.302     0.221     0.312    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 1.593587 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.716559  [    0/ 5482]\n",
      "loss: 1.583038  [ 1200/ 5482]\n",
      "loss: 1.596398  [ 2400/ 5482]\n",
      "loss: 1.435005  [ 3600/ 5482]\n",
      "loss: 1.580148  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.307     1.000     0.010    99\n",
      " disgust     0.307     0.404     0.178    107\n",
      "    fear     0.307     0.383     0.575    80\n",
      "   happy     0.307     0.265     0.636    77\n",
      " neutral     0.307     0.000     0.000    95\n",
      "     sad     0.307     0.283     0.791    91\n",
      "surprise     0.307     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.307     0.334     0.313    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.7%, Avg loss: 1.567495 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.427626  [    0/ 5482]\n",
      "loss: 1.508984  [ 1200/ 5482]\n",
      "loss: 1.289132  [ 2400/ 5482]\n",
      "loss: 1.477386  [ 3600/ 5482]\n",
      "loss: 1.482860  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.311     0.250     0.020    99\n",
      " disgust     0.311     0.442     0.215    107\n",
      "    fear     0.311     0.431     0.588    80\n",
      "   happy     0.311     0.256     0.714    77\n",
      " neutral     0.311     0.000     0.000    95\n",
      "     sad     0.311     0.278     0.681    91\n",
      "surprise     0.311     0.333     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.311     0.284     0.319    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.1%, Avg loss: 1.567704 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.343327  [    0/ 5482]\n",
      "loss: 1.288555  [ 1200/ 5482]\n",
      "loss: 1.357273  [ 2400/ 5482]\n",
      "loss: 1.435110  [ 3600/ 5482]\n",
      "loss: 1.518406  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.318     0.455     0.051    99\n",
      " disgust     0.318     0.438     0.196    107\n",
      "    fear     0.318     0.407     0.600    80\n",
      "   happy     0.318     0.267     0.662    77\n",
      " neutral     0.318     0.000     0.000    95\n",
      "     sad     0.318     0.282     0.747    91\n",
      "surprise     0.318     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.318     0.407     0.325    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 1.543934 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.604384  [    0/ 5482]\n",
      "loss: 1.580795  [ 1200/ 5482]\n",
      "loss: 1.564026  [ 2400/ 5482]\n",
      "loss: 1.284005  [ 3600/ 5482]\n",
      "loss: 1.344281  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.331     0.500     0.091    99\n",
      " disgust     0.331     0.500     0.224    107\n",
      "    fear     0.331     0.382     0.625    80\n",
      "   happy     0.331     0.275     0.649    77\n",
      " neutral     0.331     0.000     0.000    95\n",
      "     sad     0.331     0.296     0.747    91\n",
      "surprise     0.331     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.331     0.422     0.336    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.547925 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.289083  [    0/ 5482]\n",
      "loss: 1.397849  [ 1200/ 5482]\n",
      "loss: 1.304517  [ 2400/ 5482]\n",
      "loss: 1.401920  [ 3600/ 5482]\n",
      "loss: 1.636657  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.380     0.678     0.596    99\n",
      " disgust     0.380     0.365     0.215    107\n",
      "    fear     0.380     0.465     0.588    80\n",
      "   happy     0.380     0.298     0.506    77\n",
      " neutral     0.380     0.000     0.000    95\n",
      "     sad     0.380     0.288     0.703    91\n",
      "surprise     0.380     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.380     0.299     0.373    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.531242 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.446860  [    0/ 5482]\n",
      "loss: 1.735242  [ 1200/ 5482]\n",
      "loss: 1.246905  [ 2400/ 5482]\n",
      "loss: 1.285654  [ 3600/ 5482]\n",
      "loss: 1.252721  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.364     0.620     0.576    99\n",
      " disgust     0.364     0.405     0.159    107\n",
      "    fear     0.364     0.411     0.637    80\n",
      "   happy     0.364     0.286     0.416    77\n",
      " neutral     0.364     0.000     0.000    95\n",
      "     sad     0.364     0.272     0.714    91\n",
      "surprise     0.364     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.364     0.285     0.357    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 1.524739 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.253972  [    0/ 5482]\n",
      "loss: 1.511192  [ 1200/ 5482]\n",
      "loss: 1.538427  [ 2400/ 5482]\n",
      "loss: 1.585796  [ 3600/ 5482]\n",
      "loss: 1.319704  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.392     0.606     0.606    99\n",
      " disgust     0.392     0.588     0.280    107\n",
      "    fear     0.392     0.419     0.675    80\n",
      "   happy     0.392     0.275     0.390    77\n",
      " neutral     0.392     0.000     0.000    95\n",
      "     sad     0.392     0.294     0.714    91\n",
      "surprise     0.392     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.392     0.312     0.381    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.502487 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.374437  [    0/ 5482]\n",
      "loss: 1.255808  [ 1200/ 5482]\n",
      "loss: 1.381186  [ 2400/ 5482]\n",
      "loss: 1.447066  [ 3600/ 5482]\n",
      "loss: 1.288304  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.410     0.586     0.687    99\n",
      " disgust     0.410     0.451     0.346    107\n",
      "    fear     0.410     0.475     0.713    80\n",
      "   happy     0.410     0.286     0.234    77\n",
      " neutral     0.410     0.000     0.000    95\n",
      "     sad     0.410     0.305     0.747    91\n",
      "surprise     0.410     0.333     0.033    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.410     0.348     0.394    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.496932 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.500713  [    0/ 5482]\n",
      "loss: 1.402096  [ 1200/ 5482]\n",
      "loss: 1.178534  [ 2400/ 5482]\n",
      "loss: 1.691098  [ 3600/ 5482]\n",
      "loss: 1.578256  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.566     0.606    99\n",
      " disgust     0.390     0.450     0.252    107\n",
      "    fear     0.390     0.396     0.812    80\n",
      "   happy     0.390     0.315     0.221    77\n",
      " neutral     0.390     0.000     0.000    95\n",
      "     sad     0.390     0.305     0.758    91\n",
      "surprise     0.390     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.290     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.489874 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.319029  [    0/ 5482]\n",
      "loss: 1.288043  [ 1200/ 5482]\n",
      "loss: 1.291203  [ 2400/ 5482]\n",
      "loss: 1.231988  [ 3600/ 5482]\n",
      "loss: 1.252025  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.403     0.635     0.545    99\n",
      " disgust     0.403     0.472     0.318    107\n",
      "    fear     0.403     0.384     0.825    80\n",
      "   happy     0.403     0.418     0.364    77\n",
      " neutral     0.403     0.000     0.000    95\n",
      "     sad     0.403     0.302     0.703    91\n",
      "surprise     0.403     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.403     0.316     0.394    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.482050 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.247955  [    0/ 5482]\n",
      "loss: 1.323054  [ 1200/ 5482]\n",
      "loss: 1.521744  [ 2400/ 5482]\n",
      "loss: 1.395585  [ 3600/ 5482]\n",
      "loss: 1.153751  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.398     0.589     0.667    99\n",
      " disgust     0.398     0.450     0.252    107\n",
      "    fear     0.398     0.404     0.762    80\n",
      "   happy     0.398     0.339     0.260    77\n",
      " neutral     0.398     0.000     0.000    95\n",
      "     sad     0.398     0.304     0.758    91\n",
      "surprise     0.398     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.398     0.298     0.386    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 1.468440 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.296128  [    0/ 5482]\n",
      "loss: 1.205189  [ 1200/ 5482]\n",
      "loss: 1.436411  [ 2400/ 5482]\n",
      "loss: 1.327264  [ 3600/ 5482]\n",
      "loss: 1.657398  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.403     0.515     0.697    99\n",
      " disgust     0.403     0.533     0.299    107\n",
      "    fear     0.403     0.412     0.700    80\n",
      "   happy     0.403     0.309     0.221    77\n",
      " neutral     0.403     0.000     0.000    95\n",
      "     sad     0.403     0.312     0.758    91\n",
      "surprise     0.403     0.750     0.049    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.403     0.404     0.389    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.457370 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.215167  [    0/ 5482]\n",
      "loss: 1.225760  [ 1200/ 5482]\n",
      "loss: 1.491723  [ 2400/ 5482]\n",
      "loss: 1.382894  [ 3600/ 5482]\n",
      "loss: 1.285949  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.534     0.717    99\n",
      " disgust     0.390     0.508     0.280    107\n",
      "    fear     0.390     0.391     0.738    80\n",
      "   happy     0.390     0.273     0.195    77\n",
      " neutral     0.390     0.000     0.000    95\n",
      "     sad     0.390     0.299     0.692    91\n",
      "surprise     0.390     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.286     0.375    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.442580 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.061870  [    0/ 5482]\n",
      "loss: 1.243688  [ 1200/ 5482]\n",
      "loss: 1.144204  [ 2400/ 5482]\n",
      "loss: 1.278203  [ 3600/ 5482]\n",
      "loss: 1.240494  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.382     0.569     0.667    99\n",
      " disgust     0.382     0.385     0.187    107\n",
      "    fear     0.382     0.407     0.713    80\n",
      "   happy     0.382     0.375     0.195    77\n",
      " neutral     0.382     0.000     0.000    95\n",
      "     sad     0.382     0.286     0.824    91\n",
      "surprise     0.382     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.382     0.289     0.369    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 1.460971 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.434679  [    0/ 5482]\n",
      "loss: 1.137306  [ 1200/ 5482]\n",
      "loss: 1.221845  [ 2400/ 5482]\n",
      "loss: 1.128322  [ 3600/ 5482]\n",
      "loss: 1.147516  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.415     0.521     0.758    99\n",
      " disgust     0.415     0.571     0.336    107\n",
      "    fear     0.415     0.407     0.762    80\n",
      "   happy     0.415     0.340     0.208    77\n",
      " neutral     0.415     0.000     0.000    95\n",
      "     sad     0.415     0.320     0.714    91\n",
      "surprise     0.415     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.415     0.309     0.397    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.446341 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.304326  [    0/ 5482]\n",
      "loss: 1.229777  [ 1200/ 5482]\n",
      "loss: 1.137133  [ 2400/ 5482]\n",
      "loss: 1.260186  [ 3600/ 5482]\n",
      "loss: 1.289206  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.410     0.436     0.788    99\n",
      " disgust     0.410     0.630     0.477    107\n",
      "    fear     0.410     0.400     0.625    80\n",
      "   happy     0.410     0.326     0.182    77\n",
      " neutral     0.410     0.000     0.000    95\n",
      "     sad     0.410     0.327     0.615    91\n",
      "surprise     0.410     0.091     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.410     0.316     0.386    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.464273 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.289763  [    0/ 5482]\n",
      "loss: 1.098811  [ 1200/ 5482]\n",
      "loss: 1.397076  [ 2400/ 5482]\n",
      "loss: 1.196648  [ 3600/ 5482]\n",
      "loss: 1.123780  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.397     0.504     0.667    99\n",
      " disgust     0.397     0.554     0.336    107\n",
      "    fear     0.397     0.373     0.775    80\n",
      "   happy     0.397     0.367     0.234    77\n",
      " neutral     0.397     0.000     0.000    95\n",
      "     sad     0.397     0.312     0.659    91\n",
      "surprise     0.397     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.397     0.302     0.382    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.476115 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.062459  [    0/ 5482]\n",
      "loss: 1.271294  [ 1200/ 5482]\n",
      "loss: 1.236661  [ 2400/ 5482]\n",
      "loss: 1.080576  [ 3600/ 5482]\n",
      "loss: 1.061263  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.426     0.538     0.778    99\n",
      " disgust     0.426     0.551     0.402    107\n",
      "    fear     0.426     0.401     0.738    80\n",
      "   happy     0.426     0.414     0.156    77\n",
      " neutral     0.426     0.000     0.000    95\n",
      "     sad     0.426     0.322     0.714    91\n",
      "surprise     0.426     0.364     0.066    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.426     0.370     0.408    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 1.426049 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.110949  [    0/ 5482]\n",
      "loss: 1.346896  [ 1200/ 5482]\n",
      "loss: 1.211942  [ 2400/ 5482]\n",
      "loss: 1.381445  [ 3600/ 5482]\n",
      "loss: 1.197203  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.408     0.549     0.626    99\n",
      " disgust     0.408     0.530     0.327    107\n",
      "    fear     0.408     0.394     0.787    80\n",
      "   happy     0.408     0.391     0.325    77\n",
      " neutral     0.408     0.000     0.000    95\n",
      "     sad     0.408     0.310     0.681    91\n",
      "surprise     0.408     0.286     0.033    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.408     0.351     0.397    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 1.448336 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.038649  [    0/ 5482]\n",
      "loss: 1.179412  [ 1200/ 5482]\n",
      "loss: 1.026501  [ 2400/ 5482]\n",
      "loss: 1.117840  [ 3600/ 5482]\n",
      "loss: 0.965586  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.462     0.607     0.687    99\n",
      " disgust     0.462     0.579     0.514    107\n",
      "    fear     0.462     0.430     0.650    80\n",
      "   happy     0.462     0.521     0.481    77\n",
      " neutral     0.462     0.000     0.000    95\n",
      "     sad     0.462     0.326     0.692    91\n",
      "surprise     0.462     0.389     0.115    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.462     0.407     0.448    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 1.402852 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.205775  [    0/ 5482]\n",
      "loss: 1.004124  [ 1200/ 5482]\n",
      "loss: 1.289746  [ 2400/ 5482]\n",
      "loss: 1.333271  [ 3600/ 5482]\n",
      "loss: 1.099015  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.444     0.633     0.626    99\n",
      " disgust     0.444     0.479     0.318    107\n",
      "    fear     0.444     0.431     0.662    80\n",
      "   happy     0.444     0.538     0.649    77\n",
      " neutral     0.444     0.000     0.000    95\n",
      "     sad     0.444     0.316     0.725    91\n",
      "surprise     0.444     0.375     0.098    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.444     0.396     0.440    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.416885 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.750129  [    0/ 5482]\n",
      "loss: 1.177602  [ 1200/ 5482]\n",
      "loss: 1.362512  [ 2400/ 5482]\n",
      "loss: 0.971191  [ 3600/ 5482]\n",
      "loss: 1.055300  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.413     0.637     0.515    99\n",
      " disgust     0.413     0.500     0.252    107\n",
      "    fear     0.413     0.416     0.650    80\n",
      "   happy     0.413     0.432     0.701    77\n",
      " neutral     0.413     0.000     0.000    95\n",
      "     sad     0.413     0.300     0.703    91\n",
      "surprise     0.413     0.308     0.066    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.413     0.371     0.413    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.410441 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.218173  [    0/ 5482]\n",
      "loss: 1.078058  [ 1200/ 5482]\n",
      "loss: 1.035086  [ 2400/ 5482]\n",
      "loss: 0.860373  [ 3600/ 5482]\n",
      "loss: 1.127939  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.480     0.662     0.535    99\n",
      " disgust     0.480     0.675     0.505    107\n",
      "    fear     0.480     0.446     0.625    80\n",
      "   happy     0.480     0.438     0.740    77\n",
      " neutral     0.480     0.000     0.000    95\n",
      "     sad     0.480     0.360     0.703    91\n",
      "surprise     0.480     0.500     0.246    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.480     0.440     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.396022 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.045828  [    0/ 5482]\n",
      "loss: 1.011791  [ 1200/ 5482]\n",
      "loss: 1.079901  [ 2400/ 5482]\n",
      "loss: 1.229806  [ 3600/ 5482]\n",
      "loss: 1.121103  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.464     0.765     0.525    99\n",
      " disgust     0.464     0.621     0.505    107\n",
      "    fear     0.464     0.389     0.725    80\n",
      "   happy     0.464     0.528     0.610    77\n",
      " neutral     0.464     0.000     0.000    95\n",
      "     sad     0.464     0.344     0.681    91\n",
      "surprise     0.464     0.270     0.164    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.464     0.417     0.459    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.373780 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.176221  [    0/ 5482]\n",
      "loss: 1.070699  [ 1200/ 5482]\n",
      "loss: 1.026950  [ 2400/ 5482]\n",
      "loss: 0.949947  [ 3600/ 5482]\n",
      "loss: 0.999142  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.682     0.606    99\n",
      " disgust     0.492     0.646     0.477    107\n",
      "    fear     0.492     0.500     0.650    80\n",
      "   happy     0.492     0.500     0.649    77\n",
      " neutral     0.492     0.000     0.000    95\n",
      "     sad     0.492     0.332     0.703    91\n",
      "surprise     0.492     0.500     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.451     0.495    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.353564 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.800175  [    0/ 5482]\n",
      "loss: 0.982468  [ 1200/ 5482]\n",
      "loss: 0.684739  [ 2400/ 5482]\n",
      "loss: 1.335713  [ 3600/ 5482]\n",
      "loss: 1.224098  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.602     0.657    99\n",
      " disgust     0.502     0.602     0.467    107\n",
      "    fear     0.502     0.500     0.613    80\n",
      "   happy     0.502     0.505     0.649    77\n",
      " neutral     0.502     0.000     0.000    95\n",
      "     sad     0.502     0.346     0.681    91\n",
      "surprise     0.502     0.698     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.465     0.508    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.371341 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep43_acc_50.md \n",
      "\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep43_acc_50\"!  new accuracy: 50.2\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.915600  [    0/ 5482]\n",
      "loss: 0.959991  [ 1200/ 5482]\n",
      "loss: 1.044804  [ 2400/ 5482]\n",
      "loss: 1.048941  [ 3600/ 5482]\n",
      "loss: 1.066601  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.653     0.667    99\n",
      " disgust     0.513     0.547     0.542    107\n",
      "    fear     0.513     0.573     0.588    80\n",
      "   happy     0.513     0.545     0.623    77\n",
      " neutral     0.513     0.000     0.000    95\n",
      "     sad     0.513     0.339     0.681    91\n",
      "surprise     0.513     0.640     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.471     0.518    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.372827 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep44_acc_51.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep43_acc_50\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep44_acc_51\"! Old accuracy: 50.2, new accuracy: 51.3\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.865931  [    0/ 5482]\n",
      "loss: 0.984610  [ 1200/ 5482]\n",
      "loss: 0.772034  [ 2400/ 5482]\n",
      "loss: 1.144180  [ 3600/ 5482]\n",
      "loss: 1.041465  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.688     0.646    99\n",
      " disgust     0.516     0.621     0.505    107\n",
      "    fear     0.516     0.528     0.588    80\n",
      "   happy     0.516     0.500     0.662    77\n",
      " neutral     0.516     0.000     0.000    95\n",
      "     sad     0.516     0.346     0.725    91\n",
      "surprise     0.516     0.688     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.481     0.524    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.338978 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep45_acc_52.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep44_acc_51\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep45_acc_52\"! Old accuracy: 51.3, new accuracy: 51.6\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.913563  [    0/ 5482]\n",
      "loss: 0.804709  [ 1200/ 5482]\n",
      "loss: 0.844964  [ 2400/ 5482]\n",
      "loss: 1.047555  [ 3600/ 5482]\n",
      "loss: 0.911005  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.521     0.651     0.717    99\n",
      " disgust     0.521     0.617     0.542    107\n",
      "    fear     0.521     0.566     0.588    80\n",
      "   happy     0.521     0.557     0.571    77\n",
      " neutral     0.521     0.000     0.000    95\n",
      "     sad     0.521     0.335     0.736    91\n",
      "surprise     0.521     0.689     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.521     0.488     0.523    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.367056 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep46_acc_52.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep45_acc_52\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep46_acc_52\"! Old accuracy: 51.6, new accuracy: 52.1\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.739049  [    0/ 5482]\n",
      "loss: 1.085125  [ 1200/ 5482]\n",
      "loss: 0.819591  [ 2400/ 5482]\n",
      "loss: 0.906240  [ 3600/ 5482]\n",
      "loss: 0.729447  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.687     0.687    99\n",
      " disgust     0.551     0.681     0.579    107\n",
      "    fear     0.551     0.518     0.713    80\n",
      "   happy     0.551     0.582     0.597    77\n",
      " neutral     0.551     0.000     0.000    95\n",
      "     sad     0.551     0.366     0.703    91\n",
      "surprise     0.551     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.504     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.331717 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep47_acc_55.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep46_acc_52\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep47_acc_55\"! Old accuracy: 52.1, new accuracy: 55.1\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.818225  [    0/ 5482]\n",
      "loss: 1.081393  [ 1200/ 5482]\n",
      "loss: 0.909244  [ 2400/ 5482]\n",
      "loss: 0.964067  [ 3600/ 5482]\n",
      "loss: 0.580338  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.539     0.706     0.606    99\n",
      " disgust     0.539     0.687     0.636    107\n",
      "    fear     0.539     0.519     0.700    80\n",
      "   happy     0.539     0.564     0.571    77\n",
      " neutral     0.539     0.000     0.000    95\n",
      "     sad     0.539     0.356     0.692    91\n",
      "surprise     0.539     0.603     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.539     0.491     0.547    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.344779 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.088322  [    0/ 5482]\n",
      "loss: 0.870831  [ 1200/ 5482]\n",
      "loss: 1.380110  [ 2400/ 5482]\n",
      "loss: 0.892413  [ 3600/ 5482]\n",
      "loss: 0.723888  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.548     0.733     0.636    99\n",
      " disgust     0.548     0.663     0.607    107\n",
      "    fear     0.548     0.500     0.738    80\n",
      "   happy     0.548     0.638     0.571    77\n",
      " neutral     0.548     0.000     0.000    95\n",
      "     sad     0.548     0.350     0.692    91\n",
      "surprise     0.548     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.548     0.509     0.557    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.323828 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.875413  [    0/ 5482]\n",
      "loss: 0.727963  [ 1200/ 5482]\n",
      "loss: 0.749388  [ 2400/ 5482]\n",
      "loss: 0.970350  [ 3600/ 5482]\n",
      "loss: 1.046838  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.528     0.651     0.717    99\n",
      " disgust     0.528     0.639     0.579    107\n",
      "    fear     0.528     0.584     0.562    80\n",
      "   happy     0.528     0.506     0.571    77\n",
      " neutral     0.528     0.000     0.000    95\n",
      "     sad     0.528     0.346     0.714    91\n",
      "surprise     0.528     0.673     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.528     0.486     0.531    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.333282 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.938528  [    0/ 5482]\n",
      "loss: 0.794380  [ 1200/ 5482]\n",
      "loss: 0.824420  [ 2400/ 5482]\n",
      "loss: 0.969559  [ 3600/ 5482]\n",
      "loss: 0.781779  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.546     0.736     0.677    99\n",
      " disgust     0.546     0.687     0.636    107\n",
      "    fear     0.546     0.528     0.588    80\n",
      "   happy     0.546     0.515     0.662    77\n",
      " neutral     0.546     0.000     0.000    95\n",
      "     sad     0.546     0.363     0.714    91\n",
      "surprise     0.546     0.660     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.546     0.499     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.346097 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.915012  [    0/ 5482]\n",
      "loss: 0.743579  [ 1200/ 5482]\n",
      "loss: 0.761717  [ 2400/ 5482]\n",
      "loss: 1.102060  [ 3600/ 5482]\n",
      "loss: 0.710042  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.546     0.636     0.687    99\n",
      " disgust     0.546     0.756     0.636    107\n",
      "    fear     0.546     0.511     0.575    80\n",
      "   happy     0.546     0.510     0.662    77\n",
      " neutral     0.546     0.000     0.000    95\n",
      "     sad     0.546     0.380     0.659    91\n",
      "surprise     0.546     0.615     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.546     0.487     0.554    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.369562 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.909421  [    0/ 5482]\n",
      "loss: 0.663216  [ 1200/ 5482]\n",
      "loss: 0.735618  [ 2400/ 5482]\n",
      "loss: 0.663294  [ 3600/ 5482]\n",
      "loss: 0.834158  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.710     0.667    99\n",
      " disgust     0.557     0.721     0.701    107\n",
      "    fear     0.557     0.500     0.588    80\n",
      "   happy     0.557     0.542     0.584    77\n",
      " neutral     0.557     0.000     0.000    95\n",
      "     sad     0.557     0.390     0.758    91\n",
      "surprise     0.557     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.501     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.354082 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep53_acc_56.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep47_acc_55\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep53_acc_56\"! Old accuracy: 55.1, new accuracy: 55.7\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.932442  [    0/ 5482]\n",
      "loss: 0.708475  [ 1200/ 5482]\n",
      "loss: 0.552901  [ 2400/ 5482]\n",
      "loss: 0.890806  [ 3600/ 5482]\n",
      "loss: 0.644962  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.559     0.610     0.727    99\n",
      " disgust     0.559     0.670     0.682    107\n",
      "    fear     0.559     0.613     0.575    80\n",
      "   happy     0.559     0.595     0.610    77\n",
      " neutral     0.559     0.000     0.000    95\n",
      "     sad     0.559     0.379     0.736    91\n",
      "surprise     0.559     0.692     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.559     0.508     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.316051 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep54_acc_56.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep53_acc_56\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep54_acc_56\"! Old accuracy: 55.7, new accuracy: 55.9\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.718774  [    0/ 5482]\n",
      "loss: 1.045642  [ 1200/ 5482]\n",
      "loss: 0.735518  [ 2400/ 5482]\n",
      "loss: 0.735032  [ 3600/ 5482]\n",
      "loss: 0.513554  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.627     0.747    99\n",
      " disgust     0.556     0.699     0.673    107\n",
      "    fear     0.556     0.550     0.550    80\n",
      "   happy     0.556     0.547     0.675    77\n",
      " neutral     0.556     0.000     0.000    95\n",
      "     sad     0.556     0.371     0.648    91\n",
      "surprise     0.556     0.691     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.498     0.560    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.369317 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.899699  [    0/ 5482]\n",
      "loss: 0.983938  [ 1200/ 5482]\n",
      "loss: 0.761253  [ 2400/ 5482]\n",
      "loss: 0.613662  [ 3600/ 5482]\n",
      "loss: 0.881512  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.566     0.603     0.768    99\n",
      " disgust     0.566     0.597     0.720    107\n",
      "    fear     0.566     0.737     0.525    80\n",
      "   happy     0.566     0.677     0.571    77\n",
      " neutral     0.566     0.000     0.000    95\n",
      "     sad     0.566     0.383     0.769    91\n",
      "surprise     0.566     0.735     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.566     0.533     0.563    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.344418 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep56_acc_57.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep54_acc_56\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep56_acc_57\"! Old accuracy: 55.9, new accuracy: 56.6\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.530591  [    0/ 5482]\n",
      "loss: 0.776641  [ 1200/ 5482]\n",
      "loss: 0.728134  [ 2400/ 5482]\n",
      "loss: 0.540523  [ 3600/ 5482]\n",
      "loss: 0.562864  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.756     0.657    99\n",
      " disgust     0.582     0.681     0.720    107\n",
      "    fear     0.582     0.604     0.688    80\n",
      "   happy     0.582     0.615     0.623    77\n",
      " neutral     0.582     0.000     0.000    95\n",
      "     sad     0.582     0.389     0.747    91\n",
      "surprise     0.582     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.530     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.333729 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep57_acc_58.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep56_acc_57\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep57_acc_58\"! Old accuracy: 56.6, new accuracy: 58.2\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.835979  [    0/ 5482]\n",
      "loss: 0.643270  [ 1200/ 5482]\n",
      "loss: 0.696781  [ 2400/ 5482]\n",
      "loss: 0.753467  [ 3600/ 5482]\n",
      "loss: 0.592451  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.623     0.717    99\n",
      " disgust     0.551     0.698     0.626    107\n",
      "    fear     0.551     0.618     0.588    80\n",
      "   happy     0.551     0.526     0.649    77\n",
      " neutral     0.551     0.500     0.011    95\n",
      "     sad     0.551     0.360     0.692    91\n",
      "surprise     0.551     0.712     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.577     0.556    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.389717 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.635272  [    0/ 5482]\n",
      "loss: 0.759442  [ 1200/ 5482]\n",
      "loss: 0.772670  [ 2400/ 5482]\n",
      "loss: 0.826383  [ 3600/ 5482]\n",
      "loss: 0.412219  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.553     0.737    99\n",
      " disgust     0.551     0.689     0.664    107\n",
      "    fear     0.551     0.589     0.537    80\n",
      "   happy     0.551     0.536     0.584    77\n",
      " neutral     0.551     0.500     0.011    95\n",
      "     sad     0.551     0.411     0.714    91\n",
      "surprise     0.551     0.655     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.562     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.357530 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.694785  [    0/ 5482]\n",
      "loss: 0.819321  [ 1200/ 5482]\n",
      "loss: 0.763655  [ 2400/ 5482]\n",
      "loss: 0.683043  [ 3600/ 5482]\n",
      "loss: 0.895621  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.629     0.737    99\n",
      " disgust     0.556     0.814     0.654    107\n",
      "    fear     0.556     0.537     0.550    80\n",
      "   happy     0.556     0.554     0.597    77\n",
      " neutral     0.556     0.250     0.011    95\n",
      "     sad     0.556     0.358     0.736    91\n",
      "surprise     0.556     0.731     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.553     0.558    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.374044 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.474151  [    0/ 5482]\n",
      "loss: 0.413072  [ 1200/ 5482]\n",
      "loss: 0.665602  [ 2400/ 5482]\n",
      "loss: 0.859461  [ 3600/ 5482]\n",
      "loss: 0.815349  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.546     0.579     0.707    99\n",
      " disgust     0.546     0.737     0.654    107\n",
      "    fear     0.546     0.594     0.512    80\n",
      "   happy     0.546     0.490     0.636    77\n",
      " neutral     0.546     0.333     0.011    95\n",
      "     sad     0.546     0.390     0.703    91\n",
      "surprise     0.546     0.655     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.546     0.540     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.421665 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.337340  [    0/ 5482]\n",
      "loss: 0.492264  [ 1200/ 5482]\n",
      "loss: 0.629159  [ 2400/ 5482]\n",
      "loss: 0.416717  [ 3600/ 5482]\n",
      "loss: 0.869834  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.624     0.687    99\n",
      " disgust     0.562     0.694     0.720    107\n",
      "    fear     0.562     0.573     0.588    80\n",
      "   happy     0.562     0.652     0.584    77\n",
      " neutral     0.562     0.500     0.011    95\n",
      "     sad     0.562     0.387     0.736    91\n",
      "surprise     0.562     0.594     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.575     0.564    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.349335 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.696881  [    0/ 5482]\n",
      "loss: 0.555505  [ 1200/ 5482]\n",
      "loss: 0.523539  [ 2400/ 5482]\n",
      "loss: 0.592200  [ 3600/ 5482]\n",
      "loss: 0.463303  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.670     0.737    99\n",
      " disgust     0.562     0.725     0.738    107\n",
      "    fear     0.562     0.547     0.512    80\n",
      "   happy     0.562     0.551     0.636    77\n",
      " neutral     0.562     0.000     0.000    95\n",
      "     sad     0.562     0.386     0.703    91\n",
      "surprise     0.562     0.617     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.499     0.562    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.385146 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.547452  [    0/ 5482]\n",
      "loss: 1.161467  [ 1200/ 5482]\n",
      "loss: 0.514250  [ 2400/ 5482]\n",
      "loss: 0.837837  [ 3600/ 5482]\n",
      "loss: 0.425817  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.570     0.640     0.737    99\n",
      " disgust     0.570     0.720     0.720    107\n",
      "    fear     0.570     0.577     0.562    80\n",
      "   happy     0.570     0.658     0.623    77\n",
      " neutral     0.570     0.333     0.011    95\n",
      "     sad     0.570     0.396     0.714    91\n",
      "surprise     0.570     0.549     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.570     0.553     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.366647 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.552998  [    0/ 5482]\n",
      "loss: 0.800942  [ 1200/ 5482]\n",
      "loss: 1.231715  [ 2400/ 5482]\n",
      "loss: 0.668280  [ 3600/ 5482]\n",
      "loss: 0.855257  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.609     0.707    99\n",
      " disgust     0.562     0.737     0.682    107\n",
      "    fear     0.562     0.652     0.562    80\n",
      "   happy     0.562     0.527     0.623    77\n",
      " neutral     0.562     0.250     0.021    95\n",
      "     sad     0.562     0.405     0.747    91\n",
      "surprise     0.562     0.617     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.542     0.564    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.389185 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.602064  [    0/ 5482]\n",
      "loss: 0.483192  [ 1200/ 5482]\n",
      "loss: 0.766723  [ 2400/ 5482]\n",
      "loss: 0.498058  [ 3600/ 5482]\n",
      "loss: 0.531944  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.676     0.717    99\n",
      " disgust     0.580     0.732     0.766    107\n",
      "    fear     0.580     0.605     0.613    80\n",
      "   happy     0.580     0.568     0.597    77\n",
      " neutral     0.580     0.500     0.021    95\n",
      "     sad     0.580     0.405     0.725    91\n",
      "surprise     0.580     0.594     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.583     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.353539 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.912548  [    0/ 5482]\n",
      "loss: 0.615525  [ 1200/ 5482]\n",
      "loss: 0.557980  [ 2400/ 5482]\n",
      "loss: 0.843241  [ 3600/ 5482]\n",
      "loss: 0.565155  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.705     0.677    99\n",
      " disgust     0.582     0.710     0.710    107\n",
      "    fear     0.582     0.671     0.613    80\n",
      "   happy     0.582     0.597     0.597    77\n",
      " neutral     0.582     0.429     0.032    95\n",
      "     sad     0.582     0.387     0.813    91\n",
      "surprise     0.582     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.595     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.383866 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.381642  [    0/ 5482]\n",
      "loss: 0.524699  [ 1200/ 5482]\n",
      "loss: 0.444254  [ 2400/ 5482]\n",
      "loss: 0.396624  [ 3600/ 5482]\n",
      "loss: 0.549946  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.614     0.707    99\n",
      " disgust     0.572     0.779     0.692    107\n",
      "    fear     0.572     0.622     0.575    80\n",
      "   happy     0.572     0.515     0.662    77\n",
      " neutral     0.572     0.667     0.021    95\n",
      "     sad     0.572     0.409     0.736    91\n",
      "surprise     0.572     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.606     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.428065 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.528136  [    0/ 5482]\n",
      "loss: 0.487176  [ 1200/ 5482]\n",
      "loss: 0.247335  [ 2400/ 5482]\n",
      "loss: 0.577193  [ 3600/ 5482]\n",
      "loss: 0.295524  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.693     0.707    99\n",
      " disgust     0.582     0.767     0.738    107\n",
      "    fear     0.582     0.605     0.613    80\n",
      "   happy     0.582     0.547     0.675    77\n",
      " neutral     0.582     0.333     0.011    95\n",
      "     sad     0.582     0.400     0.725    91\n",
      "surprise     0.582     0.613     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.566     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.380294 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.939869  [    0/ 5482]\n",
      "loss: 0.369014  [ 1200/ 5482]\n",
      "loss: 0.657247  [ 2400/ 5482]\n",
      "loss: 0.497217  [ 3600/ 5482]\n",
      "loss: 0.655762  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.670     0.758    99\n",
      " disgust     0.577     0.757     0.757    107\n",
      "    fear     0.577     0.622     0.575    80\n",
      "   happy     0.577     0.500     0.662    77\n",
      " neutral     0.577     0.500     0.011    95\n",
      "     sad     0.577     0.388     0.703    91\n",
      "surprise     0.577     0.708     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.592     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.410686 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.565596  [    0/ 5482]\n",
      "loss: 0.348629  [ 1200/ 5482]\n",
      "loss: 0.531464  [ 2400/ 5482]\n",
      "loss: 0.506836  [ 3600/ 5482]\n",
      "loss: 0.346552  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.723     0.737    99\n",
      " disgust     0.572     0.695     0.766    107\n",
      "    fear     0.572     0.600     0.562    80\n",
      "   happy     0.572     0.549     0.649    77\n",
      " neutral     0.572     0.000     0.000    95\n",
      "     sad     0.572     0.377     0.637    91\n",
      "surprise     0.572     0.586     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.504     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.349787 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.461565  [    0/ 5482]\n",
      "loss: 0.466186  [ 1200/ 5482]\n",
      "loss: 0.438455  [ 2400/ 5482]\n",
      "loss: 0.548945  [ 3600/ 5482]\n",
      "loss: 0.376392  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.636     0.758    99\n",
      " disgust     0.575     0.764     0.757    107\n",
      "    fear     0.575     0.618     0.588    80\n",
      "   happy     0.575     0.581     0.649    77\n",
      " neutral     0.575     0.000     0.000    95\n",
      "     sad     0.575     0.370     0.659    91\n",
      "surprise     0.575     0.623     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.513     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.336423 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.577272  [    0/ 5482]\n",
      "loss: 0.400873  [ 1200/ 5482]\n",
      "loss: 0.356743  [ 2400/ 5482]\n",
      "loss: 0.537977  [ 3600/ 5482]\n",
      "loss: 0.421967  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.676     0.758    99\n",
      " disgust     0.585     0.669     0.813    107\n",
      "    fear     0.585     0.643     0.562    80\n",
      "   happy     0.585     0.613     0.597    77\n",
      " neutral     0.585     0.200     0.011    95\n",
      "     sad     0.585     0.398     0.703    91\n",
      "surprise     0.585     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.553     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.340074 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep73_acc_59.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep57_acc_58\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep73_acc_59\"! Old accuracy: 58.2, new accuracy: 58.5\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.471190  [    0/ 5482]\n",
      "loss: 0.412135  [ 1200/ 5482]\n",
      "loss: 0.388378  [ 2400/ 5482]\n",
      "loss: 0.324399  [ 3600/ 5482]\n",
      "loss: 0.411777  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.658     0.737    99\n",
      " disgust     0.587     0.746     0.794    107\n",
      "    fear     0.587     0.632     0.600    80\n",
      "   happy     0.587     0.593     0.662    77\n",
      " neutral     0.587     0.000     0.000    95\n",
      "     sad     0.587     0.382     0.692    91\n",
      "surprise     0.587     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.525     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.375286 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep74_acc_59.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep73_acc_59\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep74_acc_59\"! Old accuracy: 58.5, new accuracy: 58.7\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.344148  [    0/ 5482]\n",
      "loss: 0.428259  [ 1200/ 5482]\n",
      "loss: 0.405944  [ 2400/ 5482]\n",
      "loss: 0.435459  [ 3600/ 5482]\n",
      "loss: 0.323798  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.647     0.758    99\n",
      " disgust     0.584     0.755     0.776    107\n",
      "    fear     0.584     0.592     0.562    80\n",
      "   happy     0.584     0.557     0.636    77\n",
      " neutral     0.584     0.625     0.053    95\n",
      "     sad     0.584     0.405     0.659    91\n",
      "surprise     0.584     0.609     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.599     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.387600 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.342227  [    0/ 5482]\n",
      "loss: 0.288726  [ 1200/ 5482]\n",
      "loss: 0.321858  [ 2400/ 5482]\n",
      "loss: 0.266397  [ 3600/ 5482]\n",
      "loss: 0.403001  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.622     0.747    99\n",
      " disgust     0.562     0.738     0.738    107\n",
      "    fear     0.562     0.656     0.525    80\n",
      "   happy     0.562     0.512     0.571    77\n",
      " neutral     0.562     0.500     0.032    95\n",
      "     sad     0.562     0.386     0.725    91\n",
      "surprise     0.562     0.614     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.575     0.559    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.448073 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.339263  [    0/ 5482]\n",
      "loss: 0.187203  [ 1200/ 5482]\n",
      "loss: 0.427934  [ 2400/ 5482]\n",
      "loss: 0.695856  [ 3600/ 5482]\n",
      "loss: 0.406673  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.617     0.747    99\n",
      " disgust     0.572     0.639     0.794    107\n",
      "    fear     0.572     0.687     0.575    80\n",
      "   happy     0.572     0.577     0.584    77\n",
      " neutral     0.572     0.400     0.021    95\n",
      "     sad     0.572     0.397     0.681    91\n",
      "surprise     0.572     0.686     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.572     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.375165 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.502377  [    0/ 5482]\n",
      "loss: 0.319700  [ 1200/ 5482]\n",
      "loss: 0.630926  [ 2400/ 5482]\n",
      "loss: 0.375645  [ 3600/ 5482]\n",
      "loss: 0.297967  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.637     0.727    99\n",
      " disgust     0.572     0.737     0.785    107\n",
      "    fear     0.572     0.642     0.537    80\n",
      "   happy     0.572     0.529     0.597    77\n",
      " neutral     0.572     0.800     0.042    95\n",
      "     sad     0.572     0.383     0.703    91\n",
      "surprise     0.572     0.632     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.623     0.569    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.446510 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.282608  [    0/ 5482]\n",
      "loss: 0.367153  [ 1200/ 5482]\n",
      "loss: 0.370426  [ 2400/ 5482]\n",
      "loss: 0.342791  [ 3600/ 5482]\n",
      "loss: 0.276696  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.561     0.723     0.687    99\n",
      " disgust     0.561     0.701     0.701    107\n",
      "    fear     0.561     0.558     0.537    80\n",
      "   happy     0.561     0.546     0.688    77\n",
      " neutral     0.561     0.500     0.011    95\n",
      "     sad     0.561     0.401     0.626    91\n",
      "surprise     0.561     0.495     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.561     0.561     0.570    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.578973 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.287186  [    0/ 5482]\n",
      "loss: 0.225379  [ 1200/ 5482]\n",
      "loss: 0.231274  [ 2400/ 5482]\n",
      "loss: 0.519121  [ 3600/ 5482]\n",
      "loss: 0.188621  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.680     0.687    99\n",
      " disgust     0.572     0.701     0.766    107\n",
      "    fear     0.572     0.729     0.537    80\n",
      "   happy     0.572     0.529     0.584    77\n",
      " neutral     0.572     0.286     0.021    95\n",
      "     sad     0.572     0.404     0.736    91\n",
      "surprise     0.572     0.553     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.554     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.461798 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.634608  [    0/ 5482]\n",
      "loss: 0.391636  [ 1200/ 5482]\n",
      "loss: 0.249013  [ 2400/ 5482]\n",
      "loss: 0.677039  [ 3600/ 5482]\n",
      "loss: 0.421288  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.624     0.737    99\n",
      " disgust     0.572     0.677     0.785    107\n",
      "    fear     0.572     0.698     0.550    80\n",
      "   happy     0.572     0.560     0.610    77\n",
      " neutral     0.572     0.000     0.000    95\n",
      "     sad     0.572     0.404     0.692    91\n",
      "surprise     0.572     0.585     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.507     0.571    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.460777 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.162757  [    0/ 5482]\n",
      "loss: 0.439265  [ 1200/ 5482]\n",
      "loss: 0.614928  [ 2400/ 5482]\n",
      "loss: 0.358587  [ 3600/ 5482]\n",
      "loss: 0.503920  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.733     0.667    99\n",
      " disgust     0.592     0.708     0.794    107\n",
      "    fear     0.592     0.635     0.675    80\n",
      "   happy     0.592     0.630     0.597    77\n",
      " neutral     0.592     0.250     0.011    95\n",
      "     sad     0.592     0.393     0.725    91\n",
      "surprise     0.592     0.614     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.566     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.415682 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep82_acc_59.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep74_acc_59\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep82_acc_59\"! Old accuracy: 58.7, new accuracy: 59.2\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.508784  [    0/ 5482]\n",
      "loss: 0.467122  [ 1200/ 5482]\n",
      "loss: 0.293822  [ 2400/ 5482]\n",
      "loss: 0.218936  [ 3600/ 5482]\n",
      "loss: 0.251582  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.706     0.727    99\n",
      " disgust     0.592     0.729     0.804    107\n",
      "    fear     0.592     0.653     0.613    80\n",
      "   happy     0.592     0.570     0.636    77\n",
      " neutral     0.592     0.200     0.011    95\n",
      "     sad     0.592     0.402     0.725    91\n",
      "surprise     0.592     0.633     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.556     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.410871 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.274551  [    0/ 5482]\n",
      "loss: 0.256962  [ 1200/ 5482]\n",
      "loss: 0.399678  [ 2400/ 5482]\n",
      "loss: 0.857502  [ 3600/ 5482]\n",
      "loss: 0.379070  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.689     0.717    99\n",
      " disgust     0.595     0.731     0.813    107\n",
      "    fear     0.595     0.658     0.625    80\n",
      "   happy     0.595     0.570     0.636    77\n",
      " neutral     0.595     0.500     0.011    95\n",
      "     sad     0.595     0.392     0.714    91\n",
      "surprise     0.595     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.604     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.467869 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep84_acc_60.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep82_acc_59\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep84_acc_60\"! Old accuracy: 59.2, new accuracy: 59.5\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.343511  [    0/ 5482]\n",
      "loss: 0.178708  [ 1200/ 5482]\n",
      "loss: 0.252817  [ 2400/ 5482]\n",
      "loss: 0.273484  [ 3600/ 5482]\n",
      "loss: 0.275968  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.702     0.667    99\n",
      " disgust     0.585     0.688     0.804    107\n",
      "    fear     0.585     0.633     0.625    80\n",
      "   happy     0.585     0.636     0.636    77\n",
      " neutral     0.585     0.333     0.011    95\n",
      "     sad     0.585     0.388     0.703    91\n",
      "surprise     0.585     0.612     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.570     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.412533 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.189702  [    0/ 5482]\n",
      "loss: 0.290367  [ 1200/ 5482]\n",
      "loss: 0.353846  [ 2400/ 5482]\n",
      "loss: 0.338928  [ 3600/ 5482]\n",
      "loss: 0.981317  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.680     0.707    99\n",
      " disgust     0.582     0.689     0.785    107\n",
      "    fear     0.582     0.645     0.613    80\n",
      "   happy     0.582     0.608     0.584    77\n",
      " neutral     0.582     0.333     0.021    95\n",
      "     sad     0.582     0.389     0.670    91\n",
      "surprise     0.582     0.611     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.565     0.586    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.425444 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.710561  [    0/ 5482]\n",
      "loss: 0.331823  [ 1200/ 5482]\n",
      "loss: 0.546600  [ 2400/ 5482]\n",
      "loss: 0.180679  [ 3600/ 5482]\n",
      "loss: 0.145443  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.677     0.677    99\n",
      " disgust     0.584     0.718     0.785    107\n",
      "    fear     0.584     0.600     0.600    80\n",
      "   happy     0.584     0.590     0.636    77\n",
      " neutral     0.584     0.200     0.011    95\n",
      "     sad     0.584     0.405     0.725    91\n",
      "surprise     0.584     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.549     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.426749 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.491535  [    0/ 5482]\n",
      "loss: 0.225770  [ 1200/ 5482]\n",
      "loss: 0.201328  [ 2400/ 5482]\n",
      "loss: 0.229175  [ 3600/ 5482]\n",
      "loss: 0.222472  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.636     0.687    99\n",
      " disgust     0.593     0.726     0.766    107\n",
      "    fear     0.593     0.684     0.650    80\n",
      "   happy     0.593     0.605     0.675    77\n",
      " neutral     0.593     0.200     0.011    95\n",
      "     sad     0.593     0.399     0.714    91\n",
      "surprise     0.593     0.700     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.564     0.599    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.453882 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.325453  [    0/ 5482]\n",
      "loss: 0.277722  [ 1200/ 5482]\n",
      "loss: 0.225691  [ 2400/ 5482]\n",
      "loss: 0.459949  [ 3600/ 5482]\n",
      "loss: 0.453380  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.733     0.636    99\n",
      " disgust     0.592     0.757     0.813    107\n",
      "    fear     0.592     0.598     0.613    80\n",
      "   happy     0.592     0.593     0.662    77\n",
      " neutral     0.592     0.200     0.011    95\n",
      "     sad     0.592     0.402     0.747    91\n",
      "surprise     0.592     0.627     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.558     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.419153 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.174646  [    0/ 5482]\n",
      "loss: 0.670551  [ 1200/ 5482]\n",
      "loss: 0.278662  [ 2400/ 5482]\n",
      "loss: 0.320189  [ 3600/ 5482]\n",
      "loss: 0.484081  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.644     0.657    99\n",
      " disgust     0.574     0.699     0.804    107\n",
      "    fear     0.574     0.685     0.625    80\n",
      "   happy     0.574     0.543     0.649    77\n",
      " neutral     0.574     0.000     0.000    95\n",
      "     sad     0.574     0.394     0.692    91\n",
      "surprise     0.574     0.643     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.515     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.465842 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.115807  [    0/ 5482]\n",
      "loss: 0.196767  [ 1200/ 5482]\n",
      "loss: 0.638565  [ 2400/ 5482]\n",
      "loss: 0.128628  [ 3600/ 5482]\n",
      "loss: 0.351733  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.680     0.687    99\n",
      " disgust     0.582     0.672     0.785    107\n",
      "    fear     0.582     0.719     0.575    80\n",
      "   happy     0.582     0.570     0.636    77\n",
      " neutral     0.582     0.222     0.021    95\n",
      "     sad     0.582     0.403     0.703    91\n",
      "surprise     0.582     0.627     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.556     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.501879 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.305621  [    0/ 5482]\n",
      "loss: 0.353727  [ 1200/ 5482]\n",
      "loss: 0.221198  [ 2400/ 5482]\n",
      "loss: 0.282772  [ 3600/ 5482]\n",
      "loss: 0.206177  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.670     0.677    99\n",
      " disgust     0.590     0.679     0.832    107\n",
      "    fear     0.590     0.696     0.600    80\n",
      "   happy     0.590     0.628     0.636    77\n",
      " neutral     0.590     0.250     0.021    95\n",
      "     sad     0.590     0.410     0.725    91\n",
      "surprise     0.590     0.619     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.565     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.480391 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.239638  [    0/ 5482]\n",
      "loss: 0.199647  [ 1200/ 5482]\n",
      "loss: 0.238924  [ 2400/ 5482]\n",
      "loss: 0.331532  [ 3600/ 5482]\n",
      "loss: 0.316871  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.625     0.657    99\n",
      " disgust     0.574     0.739     0.766    107\n",
      "    fear     0.574     0.630     0.575    80\n",
      "   happy     0.574     0.563     0.636    77\n",
      " neutral     0.574     0.667     0.042    95\n",
      "     sad     0.574     0.404     0.692    91\n",
      "surprise     0.574     0.562     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.598     0.577    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.557709 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.347032  [    0/ 5482]\n",
      "loss: 0.072430  [ 1200/ 5482]\n",
      "loss: 0.462402  [ 2400/ 5482]\n",
      "loss: 0.279434  [ 3600/ 5482]\n",
      "loss: 0.111233  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.733     0.636    99\n",
      " disgust     0.587     0.779     0.822    107\n",
      "    fear     0.587     0.580     0.637    80\n",
      "   happy     0.587     0.590     0.636    77\n",
      " neutral     0.587     0.000     0.000    95\n",
      "     sad     0.587     0.384     0.692    91\n",
      "surprise     0.587     0.587     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.522     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.458878 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.257738  [    0/ 5482]\n",
      "loss: 0.299940  [ 1200/ 5482]\n",
      "loss: 0.510047  [ 2400/ 5482]\n",
      "loss: 0.177231  [ 3600/ 5482]\n",
      "loss: 0.317262  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.703     0.717    99\n",
      " disgust     0.600     0.748     0.832    107\n",
      "    fear     0.600     0.681     0.588    80\n",
      "   happy     0.600     0.653     0.610    77\n",
      " neutral     0.600     0.182     0.021    95\n",
      "     sad     0.600     0.410     0.725    91\n",
      "surprise     0.600     0.571     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.564     0.602    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.397203 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep95_acc_60.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep84_acc_60\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep95_acc_60\"! Old accuracy: 59.5, new accuracy: 60.0\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.347457  [    0/ 5482]\n",
      "loss: 0.521174  [ 1200/ 5482]\n",
      "loss: 0.191436  [ 2400/ 5482]\n",
      "loss: 0.369207  [ 3600/ 5482]\n",
      "loss: 0.346807  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.607     0.745     0.707    99\n",
      " disgust     0.607     0.738     0.841    107\n",
      "    fear     0.607     0.684     0.675    80\n",
      "   happy     0.607     0.648     0.597    77\n",
      " neutral     0.607     1.000     0.021    95\n",
      "     sad     0.607     0.384     0.725    91\n",
      "surprise     0.607     0.600     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.607     0.685     0.608    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.409714 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep96_acc_61.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep95_acc_60\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep96_acc_61\"! Old accuracy: 60.0, new accuracy: 60.7\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.305613  [    0/ 5482]\n",
      "loss: 0.267528  [ 1200/ 5482]\n",
      "loss: 0.229627  [ 2400/ 5482]\n",
      "loss: 0.415989  [ 3600/ 5482]\n",
      "loss: 0.385406  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.633     0.768    99\n",
      " disgust     0.593     0.752     0.794    107\n",
      "    fear     0.593     0.723     0.588    80\n",
      "   happy     0.593     0.554     0.662    77\n",
      " neutral     0.593     0.500     0.032    95\n",
      "     sad     0.593     0.398     0.703    91\n",
      "surprise     0.593     0.679     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.606     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.559797 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.518979  [    0/ 5482]\n",
      "loss: 0.689233  [ 1200/ 5482]\n",
      "loss: 0.271905  [ 2400/ 5482]\n",
      "loss: 0.381788  [ 3600/ 5482]\n",
      "loss: 0.335048  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.713     0.677    99\n",
      " disgust     0.602     0.746     0.822    107\n",
      "    fear     0.602     0.642     0.650    80\n",
      "   happy     0.602     0.622     0.662    77\n",
      " neutral     0.602     0.000     0.000    95\n",
      "     sad     0.602     0.398     0.725    91\n",
      "surprise     0.602     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.537     0.606    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.431488 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.208094  [    0/ 5482]\n",
      "loss: 0.198724  [ 1200/ 5482]\n",
      "loss: 0.342934  [ 2400/ 5482]\n",
      "loss: 0.387573  [ 3600/ 5482]\n",
      "loss: 0.138902  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.733     0.636    99\n",
      " disgust     0.592     0.739     0.822    107\n",
      "    fear     0.592     0.647     0.688    80\n",
      "   happy     0.592     0.620     0.636    77\n",
      " neutral     0.592     0.400     0.021    95\n",
      "     sad     0.592     0.365     0.681    91\n",
      "surprise     0.592     0.636     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.591     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.486352 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.167641  [    0/ 5482]\n",
      "loss: 0.291798  [ 1200/ 5482]\n",
      "loss: 0.110090  [ 2400/ 5482]\n",
      "loss: 0.331165  [ 3600/ 5482]\n",
      "loss: 0.209485  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.670     0.697    99\n",
      " disgust     0.590     0.728     0.850    107\n",
      "    fear     0.590     0.667     0.625    80\n",
      "   happy     0.590     0.628     0.636    77\n",
      " neutral     0.590     0.222     0.021    95\n",
      "     sad     0.590     0.385     0.659    91\n",
      "surprise     0.590     0.609     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.558     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.478412 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.135799  [    0/ 5482]\n",
      "loss: 0.055194  [ 1200/ 5482]\n",
      "loss: 0.303739  [ 2400/ 5482]\n",
      "loss: 0.103175  [ 3600/ 5482]\n",
      "loss: 0.180781  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.761     0.677    99\n",
      " disgust     0.597     0.756     0.841    107\n",
      "    fear     0.597     0.593     0.600    80\n",
      "   happy     0.597     0.605     0.675    77\n",
      " neutral     0.597     0.250     0.011    95\n",
      "     sad     0.597     0.398     0.747    91\n",
      "surprise     0.597     0.623     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.569     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.531243 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.207829  [    0/ 5482]\n",
      "loss: 0.130959  [ 1200/ 5482]\n",
      "loss: 0.215840  [ 2400/ 5482]\n",
      "loss: 0.263661  [ 3600/ 5482]\n",
      "loss: 0.316969  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.710     0.667    99\n",
      " disgust     0.593     0.789     0.804    107\n",
      "    fear     0.593     0.633     0.625    80\n",
      "   happy     0.593     0.589     0.688    77\n",
      " neutral     0.593     0.250     0.021    95\n",
      "     sad     0.593     0.386     0.703    91\n",
      "surprise     0.593     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.570     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.557035 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.154583  [    0/ 5482]\n",
      "loss: 0.121151  [ 1200/ 5482]\n",
      "loss: 0.312935  [ 2400/ 5482]\n",
      "loss: 0.257594  [ 3600/ 5482]\n",
      "loss: 0.041508  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.642     0.687    99\n",
      " disgust     0.580     0.715     0.822    107\n",
      "    fear     0.580     0.667     0.625    80\n",
      "   happy     0.580     0.608     0.584    77\n",
      " neutral     0.580     0.125     0.011    95\n",
      "     sad     0.580     0.387     0.692    91\n",
      "surprise     0.580     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.540     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.485393 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.059232  [    0/ 5482]\n",
      "loss: 0.230409  [ 1200/ 5482]\n",
      "loss: 0.351128  [ 2400/ 5482]\n",
      "loss: 0.155766  [ 3600/ 5482]\n",
      "loss: 0.503434  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.702     0.667    99\n",
      " disgust     0.582     0.759     0.766    107\n",
      "    fear     0.582     0.607     0.675    80\n",
      "   happy     0.582     0.585     0.623    77\n",
      " neutral     0.582     0.000     0.000    95\n",
      "     sad     0.582     0.381     0.703    91\n",
      "surprise     0.582     0.621     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.522     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.612813 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.110826  [    0/ 5482]\n",
      "loss: 0.289176  [ 1200/ 5482]\n",
      "loss: 0.097772  [ 2400/ 5482]\n",
      "loss: 0.298074  [ 3600/ 5482]\n",
      "loss: 0.349854  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.674     0.626    99\n",
      " disgust     0.582     0.754     0.804    107\n",
      "    fear     0.582     0.632     0.688    80\n",
      "   happy     0.582     0.592     0.584    77\n",
      " neutral     0.582     0.667     0.021    95\n",
      "     sad     0.582     0.381     0.703    91\n",
      "surprise     0.582     0.586     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.612     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.623407 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.162762  [    0/ 5482]\n",
      "loss: 0.442206  [ 1200/ 5482]\n",
      "loss: 0.206746  [ 2400/ 5482]\n",
      "loss: 0.347684  [ 3600/ 5482]\n",
      "loss: 0.129979  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.651     0.717    99\n",
      " disgust     0.579     0.725     0.813    107\n",
      "    fear     0.579     0.694     0.625    80\n",
      "   happy     0.579     0.536     0.584    77\n",
      " neutral     0.579     0.500     0.011    95\n",
      "     sad     0.579     0.373     0.681    91\n",
      "surprise     0.579     0.649     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.590     0.577    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.625545 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.258217  [    0/ 5482]\n",
      "loss: 0.283911  [ 1200/ 5482]\n",
      "loss: 0.268146  [ 2400/ 5482]\n",
      "loss: 0.272984  [ 3600/ 5482]\n",
      "loss: 0.239696  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.736     0.677    99\n",
      " disgust     0.590     0.790     0.776    107\n",
      "    fear     0.590     0.640     0.688    80\n",
      "   happy     0.590     0.566     0.610    77\n",
      " neutral     0.590     0.250     0.011    95\n",
      "     sad     0.590     0.371     0.725    91\n",
      "surprise     0.590     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.572     0.594    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.635532 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.300612  [    0/ 5482]\n",
      "loss: 0.307737  [ 1200/ 5482]\n",
      "loss: 0.198485  [ 2400/ 5482]\n",
      "loss: 0.374831  [ 3600/ 5482]\n",
      "loss: 0.564641  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.654     0.707    99\n",
      " disgust     0.587     0.725     0.813    107\n",
      "    fear     0.587     0.654     0.637    80\n",
      "   happy     0.587     0.643     0.584    77\n",
      " neutral     0.587     0.333     0.011    95\n",
      "     sad     0.587     0.376     0.703    91\n",
      "surprise     0.587     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.576     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.599441 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.309723  [    0/ 5482]\n",
      "loss: 0.213442  [ 1200/ 5482]\n",
      "loss: 0.201683  [ 2400/ 5482]\n",
      "loss: 0.313212  [ 3600/ 5482]\n",
      "loss: 0.275684  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.677     0.657    99\n",
      " disgust     0.579     0.798     0.776    107\n",
      "    fear     0.579     0.585     0.688    80\n",
      "   happy     0.579     0.551     0.636    77\n",
      " neutral     0.579     0.000     0.000    95\n",
      "     sad     0.579     0.393     0.703    91\n",
      "surprise     0.579     0.661     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.523     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.731219 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.228190  [    0/ 5482]\n",
      "loss: 0.310384  [ 1200/ 5482]\n",
      "loss: 0.468595  [ 2400/ 5482]\n",
      "loss: 0.253429  [ 3600/ 5482]\n",
      "loss: 0.237800  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.705     0.677    99\n",
      " disgust     0.589     0.804     0.804    107\n",
      "    fear     0.589     0.543     0.625    80\n",
      "   happy     0.589     0.593     0.662    77\n",
      " neutral     0.589     0.250     0.011    95\n",
      "     sad     0.589     0.392     0.681    91\n",
      "surprise     0.589     0.618     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.558     0.593    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.667613 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.255010  [    0/ 5482]\n",
      "loss: 0.504337  [ 1200/ 5482]\n",
      "loss: 0.322292  [ 2400/ 5482]\n",
      "loss: 0.476273  [ 3600/ 5482]\n",
      "loss: 0.235221  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.673     0.707    99\n",
      " disgust     0.587     0.702     0.813    107\n",
      "    fear     0.587     0.607     0.637    80\n",
      "   happy     0.587     0.588     0.610    77\n",
      " neutral     0.587     0.143     0.011    95\n",
      "     sad     0.587     0.406     0.692    91\n",
      "surprise     0.587     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.545     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.566867 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.281415  [    0/ 5482]\n",
      "loss: 0.215740  [ 1200/ 5482]\n",
      "loss: 0.443609  [ 2400/ 5482]\n",
      "loss: 0.308134  [ 3600/ 5482]\n",
      "loss: 0.132548  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.615     0.795     0.667    99\n",
      " disgust     0.615     0.765     0.822    107\n",
      "    fear     0.615     0.632     0.750    80\n",
      "   happy     0.615     0.630     0.662    77\n",
      " neutral     0.615     0.250     0.011    95\n",
      "     sad     0.615     0.402     0.747    91\n",
      "surprise     0.615     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.615     0.589     0.619    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.595448 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep112_acc_61.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep96_acc_61\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep112_acc_61\"! Old accuracy: 60.7, new accuracy: 61.5\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.190046  [    0/ 5482]\n",
      "loss: 0.157832  [ 1200/ 5482]\n",
      "loss: 0.180072  [ 2400/ 5482]\n",
      "loss: 0.183139  [ 3600/ 5482]\n",
      "loss: 0.263800  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.610     0.758    99\n",
      " disgust     0.577     0.788     0.766    107\n",
      "    fear     0.577     0.609     0.525    80\n",
      "   happy     0.577     0.490     0.623    77\n",
      " neutral     0.577     0.250     0.021    95\n",
      "     sad     0.577     0.423     0.692    91\n",
      "surprise     0.577     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.550     0.577    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.662356 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.270588  [    0/ 5482]\n",
      "loss: 0.081776  [ 1200/ 5482]\n",
      "loss: 0.363626  [ 2400/ 5482]\n",
      "loss: 0.100392  [ 3600/ 5482]\n",
      "loss: 0.253962  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.607     0.747     0.717    99\n",
      " disgust     0.607     0.777     0.813    107\n",
      "    fear     0.607     0.634     0.650    80\n",
      "   happy     0.607     0.556     0.649    77\n",
      " neutral     0.607     0.000     0.000    95\n",
      "     sad     0.607     0.392     0.758    91\n",
      "surprise     0.607     0.759     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.607     0.552     0.609    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.584218 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.120474  [    0/ 5482]\n",
      "loss: 0.216357  [ 1200/ 5482]\n",
      "loss: 0.114422  [ 2400/ 5482]\n",
      "loss: 0.325285  [ 3600/ 5482]\n",
      "loss: 0.158684  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.598     0.714     0.707    99\n",
      " disgust     0.598     0.784     0.813    107\n",
      "    fear     0.598     0.585     0.600    80\n",
      "   happy     0.598     0.593     0.662    77\n",
      " neutral     0.598     0.400     0.021    95\n",
      "     sad     0.598     0.401     0.692    91\n",
      "surprise     0.598     0.620     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.598     0.585     0.602    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.569221 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.173211  [    0/ 5482]\n",
      "loss: 0.151227  [ 1200/ 5482]\n",
      "loss: 0.238305  [ 2400/ 5482]\n",
      "loss: 0.143440  [ 3600/ 5482]\n",
      "loss: 0.682291  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.695     0.667    99\n",
      " disgust     0.592     0.731     0.813    107\n",
      "    fear     0.592     0.618     0.688    80\n",
      "   happy     0.592     0.608     0.623    77\n",
      " neutral     0.592     0.000     0.000    95\n",
      "     sad     0.592     0.386     0.703    91\n",
      "surprise     0.592     0.683     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.531     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.566056 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.274017  [    0/ 5482]\n",
      "loss: 0.154421  [ 1200/ 5482]\n",
      "loss: 0.141778  [ 2400/ 5482]\n",
      "loss: 0.237751  [ 3600/ 5482]\n",
      "loss: 0.213999  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.603     0.730     0.737    99\n",
      " disgust     0.603     0.761     0.832    107\n",
      "    fear     0.603     0.671     0.613    80\n",
      "   happy     0.603     0.532     0.649    77\n",
      " neutral     0.603     0.000     0.000    95\n",
      "     sad     0.603     0.395     0.703    91\n",
      "surprise     0.603     0.705     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.603     0.542     0.606    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.572375 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.140574  [    0/ 5482]\n",
      "loss: 0.080610  [ 1200/ 5482]\n",
      "loss: 0.322467  [ 2400/ 5482]\n",
      "loss: 0.300928  [ 3600/ 5482]\n",
      "loss: 0.266704  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.598     0.670     0.717    99\n",
      " disgust     0.598     0.748     0.776    107\n",
      "    fear     0.598     0.705     0.688    80\n",
      "   happy     0.598     0.583     0.636    77\n",
      " neutral     0.598     0.333     0.021    95\n",
      "     sad     0.598     0.385     0.736    91\n",
      "surprise     0.598     0.745     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.598     0.596     0.600    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.643397 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.191303  [    0/ 5482]\n",
      "loss: 0.081669  [ 1200/ 5482]\n",
      "loss: 0.222694  [ 2400/ 5482]\n",
      "loss: 0.095163  [ 3600/ 5482]\n",
      "loss: 0.351917  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.611     0.713     0.727    99\n",
      " disgust     0.611     0.774     0.832    107\n",
      "    fear     0.611     0.675     0.675    80\n",
      "   happy     0.611     0.573     0.662    77\n",
      " neutral     0.611     0.000     0.000    95\n",
      "     sad     0.611     0.410     0.725    91\n",
      "surprise     0.611     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.611     0.544     0.613    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.610378 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.076570  [    0/ 5482]\n",
      "loss: 0.196935  [ 1200/ 5482]\n",
      "loss: 0.125716  [ 2400/ 5482]\n",
      "loss: 0.029725  [ 3600/ 5482]\n",
      "loss: 0.220333  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.686     0.727    99\n",
      " disgust     0.579     0.783     0.776    107\n",
      "    fear     0.579     0.677     0.550    80\n",
      "   happy     0.579     0.476     0.636    77\n",
      " neutral     0.579     0.286     0.021    95\n",
      "     sad     0.579     0.401     0.714    91\n",
      "surprise     0.579     0.613     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.560     0.578    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.746154 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.214162  [    0/ 5482]\n",
      "loss: 0.285038  [ 1200/ 5482]\n",
      "loss: 0.186284  [ 2400/ 5482]\n",
      "loss: 0.433685  [ 3600/ 5482]\n",
      "loss: 0.270220  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.714     0.707    99\n",
      " disgust     0.585     0.775     0.804    107\n",
      "    fear     0.585     0.636     0.613    80\n",
      "   happy     0.585     0.561     0.597    77\n",
      " neutral     0.585     0.000     0.000    95\n",
      "     sad     0.585     0.381     0.758    91\n",
      "surprise     0.585     0.649     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.531     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.638938 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.169994  [    0/ 5482]\n",
      "loss: 0.225552  [ 1200/ 5482]\n",
      "loss: 0.110290  [ 2400/ 5482]\n",
      "loss: 0.130439  [ 3600/ 5482]\n",
      "loss: 0.436913  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.655     0.727    99\n",
      " disgust     0.584     0.757     0.785    107\n",
      "    fear     0.584     0.681     0.588    80\n",
      "   happy     0.584     0.549     0.649    77\n",
      " neutral     0.584     0.000     0.000    95\n",
      "     sad     0.584     0.392     0.736    91\n",
      "surprise     0.584     0.655     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.527     0.582    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.689881 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.188513  [    0/ 5482]\n",
      "loss: 0.317834  [ 1200/ 5482]\n",
      "loss: 0.325063  [ 2400/ 5482]\n",
      "loss: 0.205900  [ 3600/ 5482]\n",
      "loss: 0.263695  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.779     0.677    99\n",
      " disgust     0.589     0.702     0.813    107\n",
      "    fear     0.589     0.598     0.613    80\n",
      "   happy     0.589     0.588     0.649    77\n",
      " neutral     0.589     0.000     0.000    95\n",
      "     sad     0.589     0.399     0.736    91\n",
      "surprise     0.589     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.529     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.691938 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.373430  [    0/ 5482]\n",
      "loss: 0.132315  [ 1200/ 5482]\n",
      "loss: 0.117977  [ 2400/ 5482]\n",
      "loss: 0.318527  [ 3600/ 5482]\n",
      "loss: 0.120818  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.605     0.793     0.657    99\n",
      " disgust     0.605     0.736     0.832    107\n",
      "    fear     0.605     0.548     0.713    80\n",
      "   happy     0.605     0.623     0.623    77\n",
      " neutral     0.605     0.400     0.021    95\n",
      "     sad     0.605     0.423     0.725    91\n",
      "surprise     0.605     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.605     0.596     0.608    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.704474 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.330402  [    0/ 5482]\n",
      "loss: 0.206691  [ 1200/ 5482]\n",
      "loss: 0.390944  [ 2400/ 5482]\n",
      "loss: 0.301821  [ 3600/ 5482]\n",
      "loss: 0.144338  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.711     0.697    99\n",
      " disgust     0.595     0.761     0.832    107\n",
      "    fear     0.595     0.635     0.588    80\n",
      "   happy     0.595     0.556     0.649    77\n",
      " neutral     0.595     0.667     0.021    95\n",
      "     sad     0.595     0.411     0.736    91\n",
      "surprise     0.595     0.591     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.619     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.668783 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.268930  [    0/ 5482]\n",
      "loss: 0.283633  [ 1200/ 5482]\n",
      "loss: 0.226228  [ 2400/ 5482]\n",
      "loss: 0.073576  [ 3600/ 5482]\n",
      "loss: 0.149930  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.720     0.727    99\n",
      " disgust     0.600     0.778     0.785    107\n",
      "    fear     0.600     0.690     0.613    80\n",
      "   happy     0.600     0.580     0.662    77\n",
      " neutral     0.600     0.000     0.000    95\n",
      "     sad     0.600     0.392     0.758    91\n",
      "surprise     0.600     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.541     0.603    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.688213 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.222278  [    0/ 5482]\n",
      "loss: 0.327785  [ 1200/ 5482]\n",
      "loss: 0.481648  [ 2400/ 5482]\n",
      "loss: 0.189247  [ 3600/ 5482]\n",
      "loss: 0.211060  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.720     0.677    99\n",
      " disgust     0.600     0.759     0.794    107\n",
      "    fear     0.600     0.733     0.688    80\n",
      "   happy     0.600     0.590     0.597    77\n",
      " neutral     0.600     0.000     0.000    95\n",
      "     sad     0.600     0.386     0.780    91\n",
      "surprise     0.600     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.549     0.604    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.630609 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.255732  [    0/ 5482]\n",
      "loss: 0.164203  [ 1200/ 5482]\n",
      "loss: 0.071777  [ 2400/ 5482]\n",
      "loss: 0.345345  [ 3600/ 5482]\n",
      "loss: 0.334332  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.664     0.717    99\n",
      " disgust     0.592     0.804     0.766    107\n",
      "    fear     0.592     0.689     0.637    80\n",
      "   happy     0.592     0.539     0.623    77\n",
      " neutral     0.592     0.273     0.032    95\n",
      "     sad     0.592     0.400     0.747    91\n",
      "surprise     0.592     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.576     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.672926 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.352204  [    0/ 5482]\n",
      "loss: 0.210574  [ 1200/ 5482]\n",
      "loss: 0.189622  [ 2400/ 5482]\n",
      "loss: 0.098993  [ 3600/ 5482]\n",
      "loss: 0.067399  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.730     0.657    99\n",
      " disgust     0.590     0.796     0.766    107\n",
      "    fear     0.590     0.628     0.675    80\n",
      "   happy     0.590     0.562     0.649    77\n",
      " neutral     0.590     0.250     0.011    95\n",
      "     sad     0.590     0.392     0.736    91\n",
      "surprise     0.590     0.603     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.566     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.712307 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.140662  [    0/ 5482]\n",
      "loss: 0.272119  [ 1200/ 5482]\n",
      "loss: 0.328527  [ 2400/ 5482]\n",
      "loss: 0.251754  [ 3600/ 5482]\n",
      "loss: 0.278007  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.605     0.720     0.727    99\n",
      " disgust     0.605     0.798     0.776    107\n",
      "    fear     0.605     0.655     0.688    80\n",
      "   happy     0.605     0.634     0.584    77\n",
      " neutral     0.605     0.444     0.042    95\n",
      "     sad     0.605     0.391     0.769    91\n",
      "surprise     0.605     0.635     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.605     0.611     0.606    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.690564 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.191193  [    0/ 5482]\n",
      "loss: 0.278015  [ 1200/ 5482]\n",
      "loss: 0.365576  [ 2400/ 5482]\n",
      "loss: 0.147865  [ 3600/ 5482]\n",
      "loss: 0.264737  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.742     0.727    99\n",
      " disgust     0.602     0.746     0.794    107\n",
      "    fear     0.602     0.688     0.662    80\n",
      "   happy     0.602     0.548     0.662    77\n",
      " neutral     0.602     0.143     0.011    95\n",
      "     sad     0.602     0.402     0.725    91\n",
      "surprise     0.602     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.563     0.603    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.752342 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.127610  [    0/ 5482]\n",
      "loss: 0.368948  [ 1200/ 5482]\n",
      "loss: 0.073572  [ 2400/ 5482]\n",
      "loss: 0.254529  [ 3600/ 5482]\n",
      "loss: 0.166407  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.731     0.687    99\n",
      " disgust     0.575     0.708     0.794    107\n",
      "    fear     0.575     0.615     0.600    80\n",
      "   happy     0.575     0.533     0.636    77\n",
      " neutral     0.575     0.333     0.011    95\n",
      "     sad     0.575     0.395     0.681    91\n",
      "surprise     0.575     0.567     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.555     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.852495 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.088807  [    0/ 5482]\n",
      "loss: 0.367038  [ 1200/ 5482]\n",
      "loss: 0.310692  [ 2400/ 5482]\n",
      "loss: 0.168457  [ 3600/ 5482]\n",
      "loss: 0.199751  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.719     0.697    99\n",
      " disgust     0.593     0.759     0.794    107\n",
      "    fear     0.593     0.667     0.675    80\n",
      "   happy     0.593     0.554     0.662    77\n",
      " neutral     0.593     0.333     0.021    95\n",
      "     sad     0.593     0.382     0.692    91\n",
      "surprise     0.593     0.655     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.581     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.741893 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.161957  [    0/ 5482]\n",
      "loss: 0.279888  [ 1200/ 5482]\n",
      "loss: 0.363758  [ 2400/ 5482]\n",
      "loss: 0.270192  [ 3600/ 5482]\n",
      "loss: 0.320059  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.711     0.697    99\n",
      " disgust     0.593     0.798     0.776    107\n",
      "    fear     0.593     0.683     0.700    80\n",
      "   happy     0.593     0.558     0.623    77\n",
      " neutral     0.593     0.333     0.021    95\n",
      "     sad     0.593     0.372     0.736    91\n",
      "surprise     0.593     0.673     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.590     0.594    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.740282 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.136633  [    0/ 5482]\n",
      "loss: 0.217800  [ 1200/ 5482]\n",
      "loss: 0.373340  [ 2400/ 5482]\n",
      "loss: 0.383720  [ 3600/ 5482]\n",
      "loss: 0.172668  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.686     0.727    99\n",
      " disgust     0.579     0.651     0.785    107\n",
      "    fear     0.579     0.712     0.588    80\n",
      "   happy     0.579     0.579     0.571    77\n",
      " neutral     0.579     0.333     0.021    95\n",
      "     sad     0.579     0.395     0.703    91\n",
      "surprise     0.579     0.606     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.566     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.691800 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.078017  [    0/ 5482]\n",
      "loss: 0.290118  [ 1200/ 5482]\n",
      "loss: 0.282119  [ 2400/ 5482]\n",
      "loss: 0.201470  [ 3600/ 5482]\n",
      "loss: 0.248924  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.753     0.707    99\n",
      " disgust     0.602     0.720     0.794    107\n",
      "    fear     0.602     0.740     0.675    80\n",
      "   happy     0.602     0.548     0.662    77\n",
      " neutral     0.602     0.250     0.011    95\n",
      "     sad     0.602     0.390     0.736    91\n",
      "surprise     0.602     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.584     0.604    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.728073 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.181424  [    0/ 5482]\n",
      "loss: 0.180165  [ 1200/ 5482]\n",
      "loss: 0.272444  [ 2400/ 5482]\n",
      "loss: 0.180601  [ 3600/ 5482]\n",
      "loss: 0.226545  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.667     0.687    99\n",
      " disgust     0.587     0.769     0.776    107\n",
      "    fear     0.587     0.762     0.600    80\n",
      "   happy     0.587     0.538     0.649    77\n",
      " neutral     0.587     0.500     0.021    95\n",
      "     sad     0.587     0.379     0.725    91\n",
      "surprise     0.587     0.621     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.605     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.819064 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.159078  [    0/ 5482]\n",
      "loss: 0.184235  [ 1200/ 5482]\n",
      "loss: 0.289097  [ 2400/ 5482]\n",
      "loss: 0.183694  [ 3600/ 5482]\n",
      "loss: 0.269730  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.688     0.778    99\n",
      " disgust     0.593     0.769     0.776    107\n",
      "    fear     0.593     0.671     0.588    80\n",
      "   happy     0.593     0.576     0.636    77\n",
      " neutral     0.593     0.429     0.032    95\n",
      "     sad     0.593     0.387     0.692    91\n",
      "surprise     0.593     0.615     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.591     0.594    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.742697 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.505617  [    0/ 5482]\n",
      "loss: 0.199365  [ 1200/ 5482]\n",
      "loss: 0.213809  [ 2400/ 5482]\n",
      "loss: 0.268839  [ 3600/ 5482]\n",
      "loss: 0.300543  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.756     0.687    99\n",
      " disgust     0.595     0.720     0.794    107\n",
      "    fear     0.595     0.732     0.650    80\n",
      "   happy     0.595     0.590     0.597    77\n",
      " neutral     0.595     0.111     0.011    95\n",
      "     sad     0.595     0.383     0.791    91\n",
      "surprise     0.595     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.570     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.683636 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.136628  [    0/ 5482]\n",
      "loss: 0.181140  [ 1200/ 5482]\n",
      "loss: 0.180525  [ 2400/ 5482]\n",
      "loss: 0.094025  [ 3600/ 5482]\n",
      "loss: 0.147639  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.705     0.677    99\n",
      " disgust     0.593     0.783     0.776    107\n",
      "    fear     0.593     0.641     0.625    80\n",
      "   happy     0.593     0.548     0.662    77\n",
      " neutral     0.593     0.250     0.021    95\n",
      "     sad     0.593     0.412     0.747    91\n",
      "surprise     0.593     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.567     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.762104 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.085133  [    0/ 5482]\n",
      "loss: 0.184213  [ 1200/ 5482]\n",
      "loss: 0.299057  [ 2400/ 5482]\n",
      "loss: 0.298014  [ 3600/ 5482]\n",
      "loss: 0.133723  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.680     0.707    99\n",
      " disgust     0.589     0.716     0.776    107\n",
      "    fear     0.589     0.690     0.613    80\n",
      "   happy     0.589     0.580     0.662    77\n",
      " neutral     0.589     0.200     0.011    95\n",
      "     sad     0.589     0.389     0.714    91\n",
      "surprise     0.589     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.560     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.730845 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.206262  [    0/ 5482]\n",
      "loss: 0.248045  [ 1200/ 5482]\n",
      "loss: 0.245520  [ 2400/ 5482]\n",
      "loss: 0.119960  [ 3600/ 5482]\n",
      "loss: 0.183152  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.687     0.687    99\n",
      " disgust     0.584     0.726     0.766    107\n",
      "    fear     0.584     0.704     0.625    80\n",
      "   happy     0.584     0.495     0.662    77\n",
      " neutral     0.584     0.000     0.000    95\n",
      "     sad     0.584     0.399     0.714    91\n",
      "surprise     0.584     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.529     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.774874 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.381906  [    0/ 5482]\n",
      "loss: 0.279448  [ 1200/ 5482]\n",
      "loss: 0.319822  [ 2400/ 5482]\n",
      "loss: 0.116719  [ 3600/ 5482]\n",
      "loss: 0.196991  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.667     0.687    99\n",
      " disgust     0.582     0.752     0.766    107\n",
      "    fear     0.582     0.696     0.600    80\n",
      "   happy     0.582     0.481     0.649    77\n",
      " neutral     0.582     0.286     0.021    95\n",
      "     sad     0.582     0.406     0.736    91\n",
      "surprise     0.582     0.704     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.570     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.839244 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.271995  [    0/ 5482]\n",
      "loss: 0.232680  [ 1200/ 5482]\n",
      "loss: 0.131071  [ 2400/ 5482]\n",
      "loss: 0.170433  [ 3600/ 5482]\n",
      "loss: 0.249928  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.660     0.687    99\n",
      " disgust     0.577     0.788     0.766    107\n",
      "    fear     0.577     0.657     0.575    80\n",
      "   happy     0.577     0.520     0.662    77\n",
      " neutral     0.577     0.143     0.011    95\n",
      "     sad     0.577     0.394     0.692    91\n",
      "surprise     0.577     0.603     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.538     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.932122 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.407499  [    0/ 5482]\n",
      "loss: 0.071156  [ 1200/ 5482]\n",
      "loss: 0.249726  [ 2400/ 5482]\n",
      "loss: 0.136189  [ 3600/ 5482]\n",
      "loss: 0.432146  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.631     0.707    99\n",
      " disgust     0.574     0.722     0.776    107\n",
      "    fear     0.574     0.707     0.512    80\n",
      "   happy     0.574     0.523     0.597    77\n",
      " neutral     0.574     0.429     0.032    95\n",
      "     sad     0.574     0.409     0.736    91\n",
      "surprise     0.574     0.597     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.574     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.910240 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.315891  [    0/ 5482]\n",
      "loss: 0.131806  [ 1200/ 5482]\n",
      "loss: 0.245613  [ 2400/ 5482]\n",
      "loss: 0.314898  [ 3600/ 5482]\n",
      "loss: 0.252867  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.608     0.737    99\n",
      " disgust     0.575     0.798     0.776    107\n",
      "    fear     0.575     0.662     0.537    80\n",
      "   happy     0.575     0.539     0.623    77\n",
      " neutral     0.575     0.167     0.011    95\n",
      "     sad     0.575     0.386     0.703    91\n",
      "surprise     0.575     0.650     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.544     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.923437 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.162255  [    0/ 5482]\n",
      "loss: 0.253535  [ 1200/ 5482]\n",
      "loss: 0.184130  [ 2400/ 5482]\n",
      "loss: 0.316027  [ 3600/ 5482]\n",
      "loss: 0.131129  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.664     0.737    99\n",
      " disgust     0.580     0.810     0.794    107\n",
      "    fear     0.580     0.614     0.537    80\n",
      "   happy     0.580     0.516     0.636    77\n",
      " neutral     0.580     0.250     0.021    95\n",
      "     sad     0.580     0.385     0.681    91\n",
      "surprise     0.580     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.556     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.865862 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.233520  [    0/ 5482]\n",
      "loss: 0.147539  [ 1200/ 5482]\n",
      "loss: 0.260982  [ 2400/ 5482]\n",
      "loss: 0.199124  [ 3600/ 5482]\n",
      "loss: 0.220730  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.651     0.697    99\n",
      " disgust     0.577     0.752     0.766    107\n",
      "    fear     0.577     0.625     0.562    80\n",
      "   happy     0.577     0.510     0.662    77\n",
      " neutral     0.577     0.200     0.011    95\n",
      "     sad     0.577     0.406     0.714    91\n",
      "surprise     0.577     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.545     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.873197 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.131797  [    0/ 5482]\n",
      "loss: 0.135103  [ 1200/ 5482]\n",
      "loss: 0.134496  [ 2400/ 5482]\n",
      "loss: 0.244986  [ 3600/ 5482]\n",
      "loss: 0.199152  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.673     0.727    99\n",
      " disgust     0.589     0.748     0.776    107\n",
      "    fear     0.589     0.671     0.588    80\n",
      "   happy     0.589     0.553     0.675    77\n",
      " neutral     0.589     0.000     0.000    95\n",
      "     sad     0.589     0.407     0.747    91\n",
      "surprise     0.589     0.649     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.529     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.847215 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.148916  [    0/ 5482]\n",
      "loss: 0.275609  [ 1200/ 5482]\n",
      "loss: 0.253095  [ 2400/ 5482]\n",
      "loss: 0.082516  [ 3600/ 5482]\n",
      "loss: 0.208570  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.598     0.658     0.758    99\n",
      " disgust     0.598     0.781     0.766    107\n",
      "    fear     0.598     0.735     0.625    80\n",
      "   happy     0.598     0.547     0.610    77\n",
      " neutral     0.598     0.429     0.063    95\n",
      "     sad     0.598     0.405     0.747    91\n",
      "surprise     0.598     0.673     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.598     0.604     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.829079 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.484020  [    0/ 5482]\n",
      "loss: 0.315252  [ 1200/ 5482]\n",
      "loss: 0.280073  [ 2400/ 5482]\n",
      "loss: 0.248993  [ 3600/ 5482]\n",
      "loss: 0.093177  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.645     0.717    99\n",
      " disgust     0.580     0.755     0.776    107\n",
      "    fear     0.580     0.698     0.550    80\n",
      "   happy     0.580     0.510     0.636    77\n",
      " neutral     0.580     0.333     0.021    95\n",
      "     sad     0.580     0.388     0.725    91\n",
      "surprise     0.580     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.577     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.897590 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.483598  [    0/ 5482]\n",
      "loss: 0.168724  [ 1200/ 5482]\n",
      "loss: 0.133779  [ 2400/ 5482]\n",
      "loss: 0.158237  [ 3600/ 5482]\n",
      "loss: 0.294028  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.714     0.707    99\n",
      " disgust     0.589     0.764     0.785    107\n",
      "    fear     0.589     0.689     0.637    80\n",
      "   happy     0.589     0.540     0.610    77\n",
      " neutral     0.589     0.167     0.011    95\n",
      "     sad     0.589     0.380     0.747    91\n",
      "surprise     0.589     0.679     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.562     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.808043 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.358424  [    0/ 5482]\n",
      "loss: 0.137960  [ 1200/ 5482]\n",
      "loss: 0.235198  [ 2400/ 5482]\n",
      "loss: 0.153211  [ 3600/ 5482]\n",
      "loss: 0.150789  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.626     0.778    99\n",
      " disgust     0.597     0.778     0.785    107\n",
      "    fear     0.597     0.762     0.600    80\n",
      "   happy     0.597     0.578     0.623    77\n",
      " neutral     0.597     0.286     0.021    95\n",
      "     sad     0.597     0.401     0.736    91\n",
      "surprise     0.597     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.582     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.840704 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.316593  [    0/ 5482]\n",
      "loss: 0.293678  [ 1200/ 5482]\n",
      "loss: 0.256326  [ 2400/ 5482]\n",
      "loss: 0.245296  [ 3600/ 5482]\n",
      "loss: 0.299220  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.686     0.727    99\n",
      " disgust     0.589     0.752     0.766    107\n",
      "    fear     0.589     0.706     0.600    80\n",
      "   happy     0.589     0.576     0.636    77\n",
      " neutral     0.589     0.000     0.000    95\n",
      "     sad     0.589     0.383     0.758    91\n",
      "surprise     0.589     0.661     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.538     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.842290 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.430993  [    0/ 5482]\n",
      "loss: 0.153581  [ 1200/ 5482]\n",
      "loss: 0.354849  [ 2400/ 5482]\n",
      "loss: 0.328521  [ 3600/ 5482]\n",
      "loss: 0.133487  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.699     0.727    99\n",
      " disgust     0.595     0.728     0.776    107\n",
      "    fear     0.595     0.699     0.637    80\n",
      "   happy     0.595     0.625     0.649    77\n",
      " neutral     0.595     0.125     0.011    95\n",
      "     sad     0.595     0.381     0.736    91\n",
      "surprise     0.595     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.565     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.806487 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.333334  [    0/ 5482]\n",
      "loss: 0.271048  [ 1200/ 5482]\n",
      "loss: 0.231158  [ 2400/ 5482]\n",
      "loss: 0.129197  [ 3600/ 5482]\n",
      "loss: 0.270655  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.744     0.677    99\n",
      " disgust     0.597     0.775     0.804    107\n",
      "    fear     0.597     0.675     0.675    80\n",
      "   happy     0.597     0.593     0.623    77\n",
      " neutral     0.597     0.000     0.000    95\n",
      "     sad     0.597     0.386     0.747    91\n",
      "surprise     0.597     0.612     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.541     0.600    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.811460 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.097623  [    0/ 5482]\n",
      "loss: 0.152336  [ 1200/ 5482]\n",
      "loss: 0.306354  [ 2400/ 5482]\n",
      "loss: 0.112562  [ 3600/ 5482]\n",
      "loss: 0.067640  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.676     0.697    99\n",
      " disgust     0.574     0.780     0.794    107\n",
      "    fear     0.574     0.636     0.525    80\n",
      "   happy     0.574     0.538     0.636    77\n",
      " neutral     0.574     0.250     0.021    95\n",
      "     sad     0.574     0.380     0.714    91\n",
      "surprise     0.574     0.603     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.552     0.573    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.925352 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.246584  [    0/ 5482]\n",
      "loss: 0.131238  [ 1200/ 5482]\n",
      "loss: 0.314466  [ 2400/ 5482]\n",
      "loss: 0.268043  [ 3600/ 5482]\n",
      "loss: 0.067761  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.711     0.697    99\n",
      " disgust     0.592     0.819     0.804    107\n",
      "    fear     0.592     0.619     0.650    80\n",
      "   happy     0.592     0.532     0.649    77\n",
      " neutral     0.592     0.000     0.000    95\n",
      "     sad     0.592     0.392     0.714    91\n",
      "surprise     0.592     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.530     0.593    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.866735 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.221202  [    0/ 5482]\n",
      "loss: 0.226503  [ 1200/ 5482]\n",
      "loss: 0.199158  [ 2400/ 5482]\n",
      "loss: 0.283584  [ 3600/ 5482]\n",
      "loss: 0.129567  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.677     0.677    99\n",
      " disgust     0.575     0.741     0.804    107\n",
      "    fear     0.575     0.662     0.562    80\n",
      "   happy     0.575     0.522     0.623    77\n",
      " neutral     0.575     0.250     0.011    95\n",
      "     sad     0.575     0.387     0.736    91\n",
      "surprise     0.575     0.638     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.554     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.851664 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.128812  [    0/ 5482]\n",
      "loss: 0.281585  [ 1200/ 5482]\n",
      "loss: 0.130488  [ 2400/ 5482]\n",
      "loss: 0.251636  [ 3600/ 5482]\n",
      "loss: 0.226358  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.686     0.707    99\n",
      " disgust     0.595     0.810     0.794    107\n",
      "    fear     0.595     0.719     0.575    80\n",
      "   happy     0.595     0.538     0.636    77\n",
      " neutral     0.595     0.000     0.000    95\n",
      "     sad     0.595     0.405     0.769    91\n",
      "surprise     0.595     0.597     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.536     0.598    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.912635 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.245288  [    0/ 5482]\n",
      "loss: 0.331152  [ 1200/ 5482]\n",
      "loss: 0.327761  [ 2400/ 5482]\n",
      "loss: 0.213466  [ 3600/ 5482]\n",
      "loss: 0.543947  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.724     0.717    99\n",
      " disgust     0.589     0.824     0.785    107\n",
      "    fear     0.589     0.667     0.625    80\n",
      "   happy     0.589     0.476     0.636    77\n",
      " neutral     0.589     0.000     0.000    95\n",
      "     sad     0.589     0.384     0.692    91\n",
      "surprise     0.589     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.533     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.906926 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.294222  [    0/ 5482]\n",
      "loss: 0.128577  [ 1200/ 5482]\n",
      "loss: 0.195081  [ 2400/ 5482]\n",
      "loss: 0.063929  [ 3600/ 5482]\n",
      "loss: 0.159923  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.664     0.717    99\n",
      " disgust     0.574     0.775     0.804    107\n",
      "    fear     0.574     0.605     0.575    80\n",
      "   happy     0.574     0.529     0.584    77\n",
      " neutral     0.574     0.200     0.011    95\n",
      "     sad     0.574     0.379     0.703    91\n",
      "surprise     0.574     0.649     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.543     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.876960 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.178382  [    0/ 5482]\n",
      "loss: 0.315750  [ 1200/ 5482]\n",
      "loss: 0.357822  [ 2400/ 5482]\n",
      "loss: 0.109069  [ 3600/ 5482]\n",
      "loss: 0.222870  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.694     0.687    99\n",
      " disgust     0.585     0.789     0.804    107\n",
      "    fear     0.585     0.653     0.588    80\n",
      "   happy     0.585     0.510     0.636    77\n",
      " neutral     0.585     0.000     0.000    95\n",
      "     sad     0.585     0.389     0.714    91\n",
      "surprise     0.585     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.526     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.923422 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.062690  [    0/ 5482]\n",
      "loss: 0.313618  [ 1200/ 5482]\n",
      "loss: 0.142120  [ 2400/ 5482]\n",
      "loss: 0.014889  [ 3600/ 5482]\n",
      "loss: 0.211471  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.722     0.707    99\n",
      " disgust     0.584     0.796     0.804    107\n",
      "    fear     0.584     0.568     0.625    80\n",
      "   happy     0.584     0.500     0.636    77\n",
      " neutral     0.584     0.125     0.011    95\n",
      "     sad     0.584     0.407     0.648    91\n",
      "surprise     0.584     0.621     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.534     0.586    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.932837 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.112149  [    0/ 5482]\n",
      "loss: 0.449762  [ 1200/ 5482]\n",
      "loss: 0.066342  [ 2400/ 5482]\n",
      "loss: 0.309017  [ 3600/ 5482]\n",
      "loss: 0.111965  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.732     0.717    99\n",
      " disgust     0.600     0.761     0.804    107\n",
      "    fear     0.600     0.708     0.637    80\n",
      "   happy     0.600     0.552     0.623    77\n",
      " neutral     0.600     0.500     0.032    95\n",
      "     sad     0.600     0.387     0.736    91\n",
      "surprise     0.600     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.612     0.601    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.832665 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.328931  [    0/ 5482]\n",
      "loss: 0.293844  [ 1200/ 5482]\n",
      "loss: 0.179996  [ 2400/ 5482]\n",
      "loss: 0.081632  [ 3600/ 5482]\n",
      "loss: 0.343086  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.696     0.717    99\n",
      " disgust     0.602     0.773     0.794    107\n",
      "    fear     0.602     0.699     0.637    80\n",
      "   happy     0.602     0.543     0.649    77\n",
      " neutral     0.602     0.286     0.021    95\n",
      "     sad     0.602     0.399     0.736    91\n",
      "surprise     0.602     0.707     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.586     0.604    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.778343 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.251213  [    0/ 5482]\n",
      "loss: 0.151638  [ 1200/ 5482]\n",
      "loss: 0.091587  [ 2400/ 5482]\n",
      "loss: 0.127340  [ 3600/ 5482]\n",
      "loss: 0.179346  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.733     0.667    99\n",
      " disgust     0.592     0.752     0.794    107\n",
      "    fear     0.592     0.633     0.625    80\n",
      "   happy     0.592     0.590     0.636    77\n",
      " neutral     0.592     1.000     0.011    95\n",
      "     sad     0.592     0.381     0.736    91\n",
      "surprise     0.592     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.675     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.856758 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.335567  [    0/ 5482]\n",
      "loss: 0.359893  [ 1200/ 5482]\n",
      "loss: 0.014570  [ 2400/ 5482]\n",
      "loss: 0.297887  [ 3600/ 5482]\n",
      "loss: 0.375743  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.676     0.758    99\n",
      " disgust     0.593     0.727     0.822    107\n",
      "    fear     0.593     0.723     0.588    80\n",
      "   happy     0.593     0.545     0.623    77\n",
      " neutral     0.593     0.333     0.011    95\n",
      "     sad     0.593     0.399     0.736    91\n",
      "surprise     0.593     0.667     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.581     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.889586 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.249176  [    0/ 5482]\n",
      "loss: 0.505748  [ 1200/ 5482]\n",
      "loss: 0.193822  [ 2400/ 5482]\n",
      "loss: 0.229148  [ 3600/ 5482]\n",
      "loss: 0.138293  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.690     0.697    99\n",
      " disgust     0.597     0.755     0.776    107\n",
      "    fear     0.597     0.680     0.637    80\n",
      "   happy     0.597     0.580     0.610    77\n",
      " neutral     0.597     0.750     0.032    95\n",
      "     sad     0.597     0.394     0.758    91\n",
      "surprise     0.597     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.642     0.600    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.919794 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.435698  [    0/ 5482]\n",
      "loss: 0.214544  [ 1200/ 5482]\n",
      "loss: 0.209383  [ 2400/ 5482]\n",
      "loss: 0.174960  [ 3600/ 5482]\n",
      "loss: 0.175600  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.630     0.758    99\n",
      " disgust     0.579     0.781     0.766    107\n",
      "    fear     0.579     0.662     0.562    80\n",
      "   happy     0.579     0.500     0.597    77\n",
      " neutral     0.579     0.000     0.000    95\n",
      "     sad     0.579     0.401     0.736    91\n",
      "surprise     0.579     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.520     0.578    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 2.009211 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.209101  [    0/ 5482]\n",
      "loss: 0.244644  [ 1200/ 5482]\n",
      "loss: 0.274954  [ 2400/ 5482]\n",
      "loss: 0.079400  [ 3600/ 5482]\n",
      "loss: 0.223235  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.654     0.707    99\n",
      " disgust     0.585     0.798     0.776    107\n",
      "    fear     0.585     0.658     0.625    80\n",
      "   happy     0.585     0.595     0.610    77\n",
      " neutral     0.585     0.000     0.000    95\n",
      "     sad     0.585     0.381     0.736    91\n",
      "surprise     0.585     0.606     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.527     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.886207 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.191490  [    0/ 5482]\n",
      "loss: 0.083321  [ 1200/ 5482]\n",
      "loss: 0.188759  [ 2400/ 5482]\n",
      "loss: 0.309625  [ 3600/ 5482]\n",
      "loss: 0.313883  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.721     0.626    99\n",
      " disgust     0.585     0.768     0.804    107\n",
      "    fear     0.585     0.662     0.637    80\n",
      "   happy     0.585     0.521     0.649    77\n",
      " neutral     0.585     0.000     0.000    95\n",
      "     sad     0.585     0.390     0.736    91\n",
      "surprise     0.585     0.641     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.529     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.955252 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.517930  [    0/ 5482]\n",
      "loss: 0.184053  [ 1200/ 5482]\n",
      "loss: 0.143661  [ 2400/ 5482]\n",
      "loss: 0.193875  [ 3600/ 5482]\n",
      "loss: 0.059032  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.695     0.667    99\n",
      " disgust     0.589     0.761     0.804    107\n",
      "    fear     0.589     0.725     0.625    80\n",
      "   happy     0.589     0.511     0.597    77\n",
      " neutral     0.589     0.000     0.000    95\n",
      "     sad     0.589     0.386     0.747    91\n",
      "surprise     0.589     0.662     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.534     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.904278 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.380123  [    0/ 5482]\n",
      "loss: 0.181301  [ 1200/ 5482]\n",
      "loss: 0.253312  [ 2400/ 5482]\n",
      "loss: 0.183870  [ 3600/ 5482]\n",
      "loss: 0.160435  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.645     0.697    99\n",
      " disgust     0.584     0.798     0.813    107\n",
      "    fear     0.584     0.716     0.600    80\n",
      "   happy     0.584     0.480     0.623    77\n",
      " neutral     0.584     0.000     0.000    95\n",
      "     sad     0.584     0.398     0.747    91\n",
      "surprise     0.584     0.692     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.533     0.582    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.928535 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.372588  [    0/ 5482]\n",
      "loss: 0.248775  [ 1200/ 5482]\n",
      "loss: 0.109884  [ 2400/ 5482]\n",
      "loss: 0.128372  [ 3600/ 5482]\n",
      "loss: 0.129582  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.701     0.687    99\n",
      " disgust     0.590     0.789     0.804    107\n",
      "    fear     0.590     0.697     0.575    80\n",
      "   happy     0.590     0.510     0.649    77\n",
      " neutral     0.590     0.750     0.032    95\n",
      "     sad     0.590     0.388     0.758    91\n",
      "surprise     0.590     0.655     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.641     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.915592 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.293534  [    0/ 5482]\n",
      "loss: 0.242559  [ 1200/ 5482]\n",
      "loss: 0.204368  [ 2400/ 5482]\n",
      "loss: 0.176493  [ 3600/ 5482]\n",
      "loss: 0.307380  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.603     0.664     0.717    99\n",
      " disgust     0.603     0.800     0.785    107\n",
      "    fear     0.603     0.726     0.662    80\n",
      "   happy     0.603     0.575     0.597    77\n",
      " neutral     0.603     0.286     0.021    95\n",
      "     sad     0.603     0.389     0.769    91\n",
      "surprise     0.603     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.603     0.595     0.606    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.851805 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.195272  [    0/ 5482]\n",
      "loss: 0.241516  [ 1200/ 5482]\n",
      "loss: 0.195068  [ 2400/ 5482]\n",
      "loss: 0.328034  [ 3600/ 5482]\n",
      "loss: 0.205460  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.778     0.566    99\n",
      " disgust     0.579     0.711     0.804    107\n",
      "    fear     0.579     0.602     0.700    80\n",
      "   happy     0.579     0.719     0.532    77\n",
      " neutral     0.579     0.200     0.011    95\n",
      "     sad     0.579     0.360     0.802    91\n",
      "surprise     0.579     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.578     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.924253 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.147674  [    0/ 5482]\n",
      "loss: 0.203451  [ 1200/ 5482]\n",
      "loss: 0.291189  [ 2400/ 5482]\n",
      "loss: 0.059487  [ 3600/ 5482]\n",
      "loss: 0.377686  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.663     0.697    99\n",
      " disgust     0.579     0.752     0.794    107\n",
      "    fear     0.579     0.628     0.613    80\n",
      "   happy     0.579     0.541     0.597    77\n",
      " neutral     0.579     0.200     0.011    95\n",
      "     sad     0.579     0.391     0.725    91\n",
      "surprise     0.579     0.661     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.548     0.578    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.875464 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.059880  [    0/ 5482]\n",
      "loss: 0.225878  [ 1200/ 5482]\n",
      "loss: 0.318231  [ 2400/ 5482]\n",
      "loss: 0.207554  [ 3600/ 5482]\n",
      "loss: 0.315565  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.695     0.667    99\n",
      " disgust     0.577     0.761     0.804    107\n",
      "    fear     0.577     0.602     0.625    80\n",
      "   happy     0.577     0.568     0.597    77\n",
      " neutral     0.577     0.000     0.000    95\n",
      "     sad     0.577     0.368     0.703    91\n",
      "surprise     0.577     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.526     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.866385 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.179360  [    0/ 5482]\n",
      "loss: 0.313368  [ 1200/ 5482]\n",
      "loss: 0.127596  [ 2400/ 5482]\n",
      "loss: 0.310169  [ 3600/ 5482]\n",
      "loss: 0.394590  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.700     0.707    99\n",
      " disgust     0.579     0.759     0.794    107\n",
      "    fear     0.579     0.657     0.550    80\n",
      "   happy     0.579     0.539     0.623    77\n",
      " neutral     0.579     0.143     0.011    95\n",
      "     sad     0.579     0.384     0.747    91\n",
      "surprise     0.579     0.638     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.546     0.577    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.932935 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.279698  [    0/ 5482]\n",
      "loss: 0.176943  [ 1200/ 5482]\n",
      "loss: 0.111504  [ 2400/ 5482]\n",
      "loss: 0.306047  [ 3600/ 5482]\n",
      "loss: 0.266232  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.598     0.685     0.768    99\n",
      " disgust     0.598     0.802     0.832    107\n",
      "    fear     0.598     0.694     0.537    80\n",
      "   happy     0.598     0.545     0.623    77\n",
      " neutral     0.598     0.667     0.042    95\n",
      "     sad     0.598     0.387     0.736    91\n",
      "surprise     0.598     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.598     0.632     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.887427 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.065342  [    0/ 5482]\n",
      "loss: 0.313179  [ 1200/ 5482]\n",
      "loss: 0.225688  [ 2400/ 5482]\n",
      "loss: 0.246059  [ 3600/ 5482]\n",
      "loss: 0.589044  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.739     0.687    99\n",
      " disgust     0.579     0.768     0.804    107\n",
      "    fear     0.579     0.595     0.588    80\n",
      "   happy     0.579     0.528     0.610    77\n",
      " neutral     0.579     0.000     0.000    95\n",
      "     sad     0.579     0.371     0.725    91\n",
      "surprise     0.579     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.528     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.950482 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.123679  [    0/ 5482]\n",
      "loss: 0.106965  [ 1200/ 5482]\n",
      "loss: 0.108160  [ 2400/ 5482]\n",
      "loss: 0.012014  [ 3600/ 5482]\n",
      "loss: 0.205863  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.676     0.697    99\n",
      " disgust     0.584     0.752     0.794    107\n",
      "    fear     0.584     0.640     0.600    80\n",
      "   happy     0.584     0.588     0.610    77\n",
      " neutral     0.584     0.333     0.011    95\n",
      "     sad     0.584     0.377     0.725    91\n",
      "surprise     0.584     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.573     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.900709 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.177150  [    0/ 5482]\n",
      "loss: 0.115057  [ 1200/ 5482]\n",
      "loss: 0.250187  [ 2400/ 5482]\n",
      "loss: 0.221392  [ 3600/ 5482]\n",
      "loss: 0.205443  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.707     0.707    99\n",
      " disgust     0.590     0.766     0.794    107\n",
      "    fear     0.590     0.686     0.600    80\n",
      "   happy     0.590     0.556     0.649    77\n",
      " neutral     0.590     0.125     0.011    95\n",
      "     sad     0.590     0.376     0.736    91\n",
      "surprise     0.590     0.722     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.563     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.896980 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.276169  [    0/ 5482]\n",
      "loss: 0.314149  [ 1200/ 5482]\n",
      "loss: 0.357848  [ 2400/ 5482]\n",
      "loss: 0.194130  [ 3600/ 5482]\n",
      "loss: 0.060841  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.670     0.717    99\n",
      " disgust     0.589     0.794     0.794    107\n",
      "    fear     0.589     0.647     0.550    80\n",
      "   happy     0.589     0.575     0.649    77\n",
      " neutral     0.589     0.167     0.011    95\n",
      "     sad     0.589     0.384     0.725    91\n",
      "surprise     0.589     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.556     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.897798 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.202984  [    0/ 5482]\n",
      "loss: 0.243520  [ 1200/ 5482]\n",
      "loss: 0.076559  [ 2400/ 5482]\n",
      "loss: 0.292473  [ 3600/ 5482]\n",
      "loss: 0.242301  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.670     0.758    99\n",
      " disgust     0.580     0.780     0.794    107\n",
      "    fear     0.580     0.700     0.525    80\n",
      "   happy     0.580     0.520     0.662    77\n",
      " neutral     0.580     0.286     0.021    95\n",
      "     sad     0.580     0.377     0.670    91\n",
      "surprise     0.580     0.613     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.564     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.966357 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.155081  [    0/ 5482]\n",
      "loss: 0.192017  [ 1200/ 5482]\n",
      "loss: 0.194355  [ 2400/ 5482]\n",
      "loss: 0.176975  [ 3600/ 5482]\n",
      "loss: 0.014617  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.647     0.758    99\n",
      " disgust     0.582     0.778     0.785    107\n",
      "    fear     0.582     0.644     0.588    80\n",
      "   happy     0.582     0.568     0.597    77\n",
      " neutral     0.582     0.000     0.000    95\n",
      "     sad     0.582     0.384     0.725    91\n",
      "surprise     0.582     0.725     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.535     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.938634 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.171642  [    0/ 5482]\n",
      "loss: 0.305519  [ 1200/ 5482]\n",
      "loss: 0.176725  [ 2400/ 5482]\n",
      "loss: 0.185100  [ 3600/ 5482]\n",
      "loss: 0.063046  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.695     0.737    99\n",
      " disgust     0.579     0.823     0.738    107\n",
      "    fear     0.579     0.636     0.613    80\n",
      "   happy     0.579     0.510     0.662    77\n",
      " neutral     0.579     0.167     0.011    95\n",
      "     sad     0.579     0.367     0.681    91\n",
      "surprise     0.579     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.552     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 2.046618 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.237991  [    0/ 5482]\n",
      "loss: 0.111119  [ 1200/ 5482]\n",
      "loss: 0.279162  [ 2400/ 5482]\n",
      "loss: 0.312610  [ 3600/ 5482]\n",
      "loss: 0.090977  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.694     0.687    99\n",
      " disgust     0.587     0.771     0.785    107\n",
      "    fear     0.587     0.589     0.700    80\n",
      "   happy     0.587     0.632     0.623    77\n",
      " neutral     0.587     0.000     0.000    95\n",
      "     sad     0.587     0.369     0.714    91\n",
      "surprise     0.587     0.712     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.538     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.887093 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.194570  [    0/ 5482]\n",
      "loss: 0.172896  [ 1200/ 5482]\n",
      "loss: 0.283739  [ 2400/ 5482]\n",
      "loss: 0.072228  [ 3600/ 5482]\n",
      "loss: 0.058621  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.695     0.737    99\n",
      " disgust     0.580     0.776     0.776    107\n",
      "    fear     0.580     0.652     0.562    80\n",
      "   happy     0.580     0.510     0.649    77\n",
      " neutral     0.580     0.333     0.032    95\n",
      "     sad     0.580     0.376     0.681    91\n",
      "surprise     0.580     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.573     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 2.047241 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.310302  [    0/ 5482]\n",
      "loss: 0.241223  [ 1200/ 5482]\n",
      "loss: 0.179479  [ 2400/ 5482]\n",
      "loss: 0.079192  [ 3600/ 5482]\n",
      "loss: 0.385185  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.767     0.697    99\n",
      " disgust     0.593     0.761     0.804    107\n",
      "    fear     0.593     0.681     0.613    80\n",
      "   happy     0.593     0.556     0.649    77\n",
      " neutral     0.593     0.000     0.000    95\n",
      "     sad     0.593     0.376     0.747    91\n",
      "surprise     0.593     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.544     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.953114 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.282910  [    0/ 5482]\n",
      "loss: 0.126647  [ 1200/ 5482]\n",
      "loss: 0.009756  [ 2400/ 5482]\n",
      "loss: 0.074351  [ 3600/ 5482]\n",
      "loss: 0.175167  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.727     0.727    99\n",
      " disgust     0.587     0.750     0.785    107\n",
      "    fear     0.587     0.644     0.588    80\n",
      "   happy     0.587     0.581     0.649    77\n",
      " neutral     0.587     0.167     0.011    95\n",
      "     sad     0.587     0.366     0.703    91\n",
      "surprise     0.587     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.559     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 2.052481 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.156856  [    0/ 5482]\n",
      "loss: 0.149747  [ 1200/ 5482]\n",
      "loss: 0.194190  [ 2400/ 5482]\n",
      "loss: 0.357110  [ 3600/ 5482]\n",
      "loss: 0.126371  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.761     0.677    99\n",
      " disgust     0.587     0.707     0.813    107\n",
      "    fear     0.587     0.609     0.662    80\n",
      "   happy     0.587     0.615     0.623    77\n",
      " neutral     0.587     0.000     0.000    95\n",
      "     sad     0.587     0.377     0.725    91\n",
      "surprise     0.587     0.673     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.535     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.962704 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.307523  [    0/ 5482]\n",
      "loss: 0.207088  [ 1200/ 5482]\n",
      "loss: 0.078111  [ 2400/ 5482]\n",
      "loss: 0.124644  [ 3600/ 5482]\n",
      "loss: 0.257694  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.707     0.707    99\n",
      " disgust     0.582     0.748     0.804    107\n",
      "    fear     0.582     0.630     0.575    80\n",
      "   happy     0.582     0.593     0.623    77\n",
      " neutral     0.582     0.200     0.011    95\n",
      "     sad     0.582     0.377     0.692    91\n",
      "surprise     0.582     0.586     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.549     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.949768 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.130312  [    0/ 5482]\n",
      "loss: 0.247651  [ 1200/ 5482]\n",
      "loss: 0.248083  [ 2400/ 5482]\n",
      "loss: 0.197121  [ 3600/ 5482]\n",
      "loss: 0.159465  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.670     0.697    99\n",
      " disgust     0.584     0.802     0.794    107\n",
      "    fear     0.584     0.658     0.625    80\n",
      "   happy     0.584     0.495     0.662    77\n",
      " neutral     0.584     0.000     0.000    95\n",
      "     sad     0.584     0.398     0.725    91\n",
      "surprise     0.584     0.686     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.530     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.979467 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.175261  [    0/ 5482]\n",
      "loss: 0.058574  [ 1200/ 5482]\n",
      "loss: 0.256555  [ 2400/ 5482]\n",
      "loss: 0.063693  [ 3600/ 5482]\n",
      "loss: 0.357901  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.611     0.720     0.727    99\n",
      " disgust     0.611     0.761     0.804    107\n",
      "    fear     0.611     0.705     0.688    80\n",
      "   happy     0.611     0.613     0.636    77\n",
      " neutral     0.611     0.286     0.021    95\n",
      "     sad     0.611     0.392     0.780    91\n",
      "surprise     0.611     0.745     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.611     0.603     0.611    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.836865 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.310442  [    0/ 5482]\n",
      "loss: 0.212640  [ 1200/ 5482]\n",
      "loss: 0.197608  [ 2400/ 5482]\n",
      "loss: 0.180232  [ 3600/ 5482]\n",
      "loss: 0.122378  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.719     0.697    99\n",
      " disgust     0.600     0.766     0.794    107\n",
      "    fear     0.600     0.667     0.650    80\n",
      "   happy     0.600     0.549     0.649    77\n",
      " neutral     0.600     0.222     0.021    95\n",
      "     sad     0.600     0.409     0.714    91\n",
      "surprise     0.600     0.652     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.569     0.604    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.847489 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.093454  [    0/ 5482]\n",
      "loss: 0.058797  [ 1200/ 5482]\n",
      "loss: 0.127125  [ 2400/ 5482]\n",
      "loss: 0.466406  [ 3600/ 5482]\n",
      "loss: 0.160315  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.729     0.707    99\n",
      " disgust     0.595     0.697     0.776    107\n",
      "    fear     0.595     0.732     0.650    80\n",
      "   happy     0.595     0.548     0.597    77\n",
      " neutral     0.595     0.375     0.032    95\n",
      "     sad     0.595     0.397     0.758    91\n",
      "surprise     0.595     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.595     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.948448 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.174341  [    0/ 5482]\n",
      "loss: 0.284890  [ 1200/ 5482]\n",
      "loss: 0.222211  [ 2400/ 5482]\n",
      "loss: 0.227071  [ 3600/ 5482]\n",
      "loss: 0.221437  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.600     0.713     0.727    99\n",
      " disgust     0.600     0.790     0.776    107\n",
      "    fear     0.600     0.679     0.688    80\n",
      "   happy     0.600     0.554     0.597    77\n",
      " neutral     0.600     0.333     0.032    95\n",
      "     sad     0.600     0.386     0.747    91\n",
      "surprise     0.600     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.600     0.595     0.601    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.981554 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.240495  [    0/ 5482]\n",
      "loss: 0.241057  [ 1200/ 5482]\n",
      "loss: 0.148096  [ 2400/ 5482]\n",
      "loss: 0.333839  [ 3600/ 5482]\n",
      "loss: 0.213533  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.652     0.758    99\n",
      " disgust     0.597     0.790     0.776    107\n",
      "    fear     0.597     0.700     0.613    80\n",
      "   happy     0.597     0.582     0.597    77\n",
      " neutral     0.597     0.125     0.011    95\n",
      "     sad     0.597     0.400     0.747    91\n",
      "surprise     0.597     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.560     0.598    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.968258 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.241142  [    0/ 5482]\n",
      "loss: 0.238741  [ 1200/ 5482]\n",
      "loss: 0.162413  [ 2400/ 5482]\n",
      "loss: 0.239778  [ 3600/ 5482]\n",
      "loss: 0.061892  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.734     0.697    99\n",
      " disgust     0.593     0.752     0.794    107\n",
      "    fear     0.593     0.667     0.650    80\n",
      "   happy     0.593     0.634     0.584    77\n",
      " neutral     0.593     0.250     0.021    95\n",
      "     sad     0.593     0.367     0.758    91\n",
      "surprise     0.593     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.585     0.594    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.957752 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.223846  [    0/ 5482]\n",
      "loss: 0.108917  [ 1200/ 5482]\n",
      "loss: 0.176527  [ 2400/ 5482]\n",
      "loss: 0.204631  [ 3600/ 5482]\n",
      "loss: 0.171527  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.701     0.687    99\n",
      " disgust     0.580     0.748     0.804    107\n",
      "    fear     0.580     0.635     0.588    80\n",
      "   happy     0.580     0.570     0.584    77\n",
      " neutral     0.580     0.125     0.011    95\n",
      "     sad     0.580     0.384     0.747    91\n",
      "surprise     0.580     0.650     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.545     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.955135 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.080065  [    0/ 5482]\n",
      "loss: 0.271288  [ 1200/ 5482]\n",
      "loss: 0.300693  [ 2400/ 5482]\n",
      "loss: 0.057439  [ 3600/ 5482]\n",
      "loss: 0.242953  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.691     0.768    99\n",
      " disgust     0.589     0.746     0.794    107\n",
      "    fear     0.589     0.622     0.575    80\n",
      "   happy     0.589     0.517     0.597    77\n",
      " neutral     0.589     0.375     0.032    95\n",
      "     sad     0.589     0.396     0.692    91\n",
      "surprise     0.589     0.714     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.580     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.961772 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.168541  [    0/ 5482]\n",
      "loss: 0.149218  [ 1200/ 5482]\n",
      "loss: 0.244127  [ 2400/ 5482]\n",
      "loss: 0.147803  [ 3600/ 5482]\n",
      "loss: 0.226860  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.682     0.758    99\n",
      " disgust     0.602     0.748     0.804    107\n",
      "    fear     0.602     0.721     0.613    80\n",
      "   happy     0.602     0.579     0.571    77\n",
      " neutral     0.602     0.300     0.032    95\n",
      "     sad     0.602     0.393     0.769    91\n",
      "surprise     0.602     0.755     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.597     0.600    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.973940 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.221027  [    0/ 5482]\n",
      "loss: 0.108549  [ 1200/ 5482]\n",
      "loss: 0.460233  [ 2400/ 5482]\n",
      "loss: 0.150178  [ 3600/ 5482]\n",
      "loss: 0.237838  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.692     0.727    99\n",
      " disgust     0.582     0.739     0.794    107\n",
      "    fear     0.582     0.667     0.500    80\n",
      "   happy     0.582     0.511     0.610    77\n",
      " neutral     0.582     0.429     0.032    95\n",
      "     sad     0.582     0.415     0.747    91\n",
      "surprise     0.582     0.588     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.577     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.983974 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.126533  [    0/ 5482]\n",
      "loss: 0.123555  [ 1200/ 5482]\n",
      "loss: 0.240167  [ 2400/ 5482]\n",
      "loss: 0.059504  [ 3600/ 5482]\n",
      "loss: 0.099463  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.684     0.677    99\n",
      " disgust     0.587     0.810     0.757    107\n",
      "    fear     0.587     0.614     0.675    80\n",
      "   happy     0.587     0.563     0.636    77\n",
      " neutral     0.587     0.111     0.011    95\n",
      "     sad     0.587     0.383     0.736    91\n",
      "surprise     0.587     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.557     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.941810 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.188861  [    0/ 5482]\n",
      "loss: 0.085010  [ 1200/ 5482]\n",
      "loss: 0.187152  [ 2400/ 5482]\n",
      "loss: 0.126097  [ 3600/ 5482]\n",
      "loss: 0.074980  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.598     0.652     0.758    99\n",
      " disgust     0.598     0.806     0.776    107\n",
      "    fear     0.598     0.676     0.600    80\n",
      "   happy     0.598     0.538     0.649    77\n",
      " neutral     0.598     0.286     0.021    95\n",
      "     sad     0.598     0.412     0.725    91\n",
      "surprise     0.598     0.672     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.598     0.577     0.600    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 2.000032 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.176507  [    0/ 5482]\n",
      "loss: 0.167833  [ 1200/ 5482]\n",
      "loss: 0.057633  [ 2400/ 5482]\n",
      "loss: 0.150832  [ 3600/ 5482]\n",
      "loss: 0.243746  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.667     0.707    99\n",
      " disgust     0.589     0.757     0.813    107\n",
      "    fear     0.589     0.639     0.575    80\n",
      "   happy     0.589     0.516     0.623    77\n",
      " neutral     0.589     0.222     0.021    95\n",
      "     sad     0.589     0.420     0.747    91\n",
      "surprise     0.589     0.704     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.561     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.974939 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.120811  [    0/ 5482]\n",
      "loss: 0.189255  [ 1200/ 5482]\n",
      "loss: 0.190272  [ 2400/ 5482]\n",
      "loss: 0.238568  [ 3600/ 5482]\n",
      "loss: 0.243465  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.658     0.737    99\n",
      " disgust     0.584     0.773     0.794    107\n",
      "    fear     0.584     0.672     0.562    80\n",
      "   happy     0.584     0.529     0.597    77\n",
      " neutral     0.584     0.125     0.011    95\n",
      "     sad     0.584     0.395     0.747    91\n",
      "surprise     0.584     0.691     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.549     0.582    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.986271 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.126466  [    0/ 5482]\n",
      "loss: 0.009344  [ 1200/ 5482]\n",
      "loss: 0.216001  [ 2400/ 5482]\n",
      "loss: 0.010906  [ 3600/ 5482]\n",
      "loss: 0.058460  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.758     0.697    99\n",
      " disgust     0.597     0.773     0.794    107\n",
      "    fear     0.597     0.577     0.700    80\n",
      "   happy     0.597     0.561     0.597    77\n",
      " neutral     0.597     0.250     0.011    95\n",
      "     sad     0.597     0.389     0.747    91\n",
      "surprise     0.597     0.765     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.582     0.598    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.980676 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.311953  [    0/ 5482]\n",
      "loss: 0.110957  [ 1200/ 5482]\n",
      "loss: 0.327205  [ 2400/ 5482]\n",
      "loss: 0.182302  [ 3600/ 5482]\n",
      "loss: 0.241455  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.686     0.707    99\n",
      " disgust     0.582     0.744     0.813    107\n",
      "    fear     0.582     0.648     0.575    80\n",
      "   happy     0.582     0.547     0.610    77\n",
      " neutral     0.582     0.000     0.000    95\n",
      "     sad     0.582     0.391     0.725    91\n",
      "surprise     0.582     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.527     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.948175 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.143654  [    0/ 5482]\n",
      "loss: 0.567977  [ 1200/ 5482]\n",
      "loss: 0.173334  [ 2400/ 5482]\n",
      "loss: 0.224234  [ 3600/ 5482]\n",
      "loss: 0.275282  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.661     0.727    99\n",
      " disgust     0.592     0.832     0.785    107\n",
      "    fear     0.592     0.681     0.588    80\n",
      "   happy     0.592     0.590     0.597    77\n",
      " neutral     0.592     0.200     0.021    95\n",
      "     sad     0.592     0.383     0.769    91\n",
      "surprise     0.592     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.573     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 2.003033 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.338335  [    0/ 5482]\n",
      "loss: 0.192120  [ 1200/ 5482]\n",
      "loss: 0.302080  [ 2400/ 5482]\n",
      "loss: 0.223518  [ 3600/ 5482]\n",
      "loss: 0.197904  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.667     0.727    99\n",
      " disgust     0.590     0.800     0.748    107\n",
      "    fear     0.590     0.637     0.637    80\n",
      "   happy     0.590     0.531     0.662    77\n",
      " neutral     0.590     0.125     0.011    95\n",
      "     sad     0.590     0.402     0.747    91\n",
      "surprise     0.590     0.755     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.560     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.990855 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.172424  [    0/ 5482]\n",
      "loss: 0.194657  [ 1200/ 5482]\n",
      "loss: 0.215065  [ 2400/ 5482]\n",
      "loss: 0.350819  [ 3600/ 5482]\n",
      "loss: 0.374632  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.670     0.717    99\n",
      " disgust     0.585     0.766     0.794    107\n",
      "    fear     0.585     0.625     0.562    80\n",
      "   happy     0.585     0.579     0.571    77\n",
      " neutral     0.585     0.375     0.032    95\n",
      "     sad     0.585     0.384     0.725    91\n",
      "surprise     0.585     0.662     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.580     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 2.013186 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.134729  [    0/ 5482]\n",
      "loss: 0.190770  [ 1200/ 5482]\n",
      "loss: 0.110667  [ 2400/ 5482]\n",
      "loss: 0.337032  [ 3600/ 5482]\n",
      "loss: 0.173423  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.719     0.697    99\n",
      " disgust     0.595     0.765     0.822    107\n",
      "    fear     0.595     0.653     0.588    80\n",
      "   happy     0.595     0.592     0.584    77\n",
      " neutral     0.595     0.167     0.011    95\n",
      "     sad     0.595     0.393     0.769    91\n",
      "surprise     0.595     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.562     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.996002 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.123439  [    0/ 5482]\n",
      "loss: 0.115274  [ 1200/ 5482]\n",
      "loss: 0.287193  [ 2400/ 5482]\n",
      "loss: 0.324505  [ 3600/ 5482]\n",
      "loss: 0.266049  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.661     0.727    99\n",
      " disgust     0.585     0.759     0.794    107\n",
      "    fear     0.585     0.712     0.525    80\n",
      "   happy     0.585     0.541     0.597    77\n",
      " neutral     0.585     0.200     0.021    95\n",
      "     sad     0.585     0.407     0.791    91\n",
      "surprise     0.585     0.655     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.562     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 2.016298 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.073383  [    0/ 5482]\n",
      "loss: 0.147206  [ 1200/ 5482]\n",
      "loss: 0.173501  [ 2400/ 5482]\n",
      "loss: 0.176586  [ 3600/ 5482]\n",
      "loss: 0.402557  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.637     0.727    99\n",
      " disgust     0.589     0.752     0.794    107\n",
      "    fear     0.589     0.681     0.588    80\n",
      "   happy     0.589     0.573     0.610    77\n",
      " neutral     0.589     0.167     0.011    95\n",
      "     sad     0.589     0.404     0.758    91\n",
      "surprise     0.589     0.679     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.556     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.959846 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.192308  [    0/ 5482]\n",
      "loss: 0.209533  [ 1200/ 5482]\n",
      "loss: 0.156495  [ 2400/ 5482]\n",
      "loss: 0.332384  [ 3600/ 5482]\n",
      "loss: 0.255594  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.667     0.747    99\n",
      " disgust     0.582     0.739     0.794    107\n",
      "    fear     0.582     0.638     0.550    80\n",
      "   happy     0.582     0.565     0.623    77\n",
      " neutral     0.582     0.125     0.011    95\n",
      "     sad     0.582     0.402     0.725    91\n",
      "surprise     0.582     0.638     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.539     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.952259 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.122311  [    0/ 5482]\n",
      "loss: 0.257232  [ 1200/ 5482]\n",
      "loss: 0.354808  [ 2400/ 5482]\n",
      "loss: 0.235987  [ 3600/ 5482]\n",
      "loss: 0.102646  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.660     0.707    99\n",
      " disgust     0.567     0.764     0.785    107\n",
      "    fear     0.567     0.614     0.537    80\n",
      "   happy     0.567     0.516     0.610    77\n",
      " neutral     0.567     0.000     0.000    95\n",
      "     sad     0.567     0.395     0.703    91\n",
      "surprise     0.567     0.594     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.506     0.567    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 2.013287 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.240782  [    0/ 5482]\n",
      "loss: 0.140273  [ 1200/ 5482]\n",
      "loss: 0.063679  [ 2400/ 5482]\n",
      "loss: 0.141672  [ 3600/ 5482]\n",
      "loss: 0.323655  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.605     0.686     0.707    99\n",
      " disgust     0.605     0.810     0.794    107\n",
      "    fear     0.605     0.626     0.713    80\n",
      "   happy     0.605     0.635     0.610    77\n",
      " neutral     0.605     0.154     0.021    95\n",
      "     sad     0.605     0.399     0.736    91\n",
      "surprise     0.605     0.719     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.605     0.576     0.608    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.941421 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.105340  [    0/ 5482]\n",
      "loss: 0.220662  [ 1200/ 5482]\n",
      "loss: 0.287750  [ 2400/ 5482]\n",
      "loss: 0.153761  [ 3600/ 5482]\n",
      "loss: 0.103059  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.657     0.717    99\n",
      " disgust     0.589     0.771     0.785    107\n",
      "    fear     0.589     0.681     0.613    80\n",
      "   happy     0.589     0.576     0.636    77\n",
      " neutral     0.589     0.000     0.000    95\n",
      "     sad     0.589     0.396     0.736    91\n",
      "surprise     0.589     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.532     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.906032 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.142518  [    0/ 5482]\n",
      "loss: 0.475244  [ 1200/ 5482]\n",
      "loss: 0.215218  [ 2400/ 5482]\n",
      "loss: 0.119999  [ 3600/ 5482]\n",
      "loss: 0.160245  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.636     0.707    99\n",
      " disgust     0.582     0.764     0.785    107\n",
      "    fear     0.582     0.686     0.600    80\n",
      "   happy     0.582     0.537     0.571    77\n",
      " neutral     0.582     0.091     0.011    95\n",
      "     sad     0.582     0.402     0.769    91\n",
      "surprise     0.582     0.717     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.547     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 2.021381 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.111736  [    0/ 5482]\n",
      "loss: 0.070933  [ 1200/ 5482]\n",
      "loss: 0.190710  [ 2400/ 5482]\n",
      "loss: 0.170567  [ 3600/ 5482]\n",
      "loss: 0.342361  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.643     0.747    99\n",
      " disgust     0.592     0.798     0.813    107\n",
      "    fear     0.592     0.616     0.562    80\n",
      "   happy     0.592     0.577     0.584    77\n",
      " neutral     0.592     0.300     0.032    95\n",
      "     sad     0.592     0.405     0.747    91\n",
      "surprise     0.592     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.575     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 2.051318 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.205029  [    0/ 5482]\n",
      "loss: 0.250799  [ 1200/ 5482]\n",
      "loss: 0.105647  [ 2400/ 5482]\n",
      "loss: 0.173048  [ 3600/ 5482]\n",
      "loss: 0.244724  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.655     0.727    99\n",
      " disgust     0.585     0.783     0.776    107\n",
      "    fear     0.585     0.721     0.550    80\n",
      "   happy     0.585     0.535     0.597    77\n",
      " neutral     0.585     0.250     0.021    95\n",
      "     sad     0.585     0.394     0.758    91\n",
      "surprise     0.585     0.641     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.568     0.586    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 2.073705 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.291545  [    0/ 5482]\n",
      "loss: 0.137456  [ 1200/ 5482]\n",
      "loss: 0.223058  [ 2400/ 5482]\n",
      "loss: 0.075054  [ 3600/ 5482]\n",
      "loss: 0.156794  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.593     0.673     0.687    99\n",
      " disgust     0.593     0.766     0.794    107\n",
      "    fear     0.593     0.694     0.625    80\n",
      "   happy     0.593     0.544     0.636    77\n",
      " neutral     0.593     0.400     0.021    95\n",
      "     sad     0.593     0.391     0.747    91\n",
      "surprise     0.593     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.593     0.596     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.995918 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.223808  [    0/ 5482]\n",
      "loss: 0.142684  [ 1200/ 5482]\n",
      "loss: 0.437244  [ 2400/ 5482]\n",
      "loss: 0.623358  [ 3600/ 5482]\n",
      "loss: 0.072418  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.723     0.687    99\n",
      " disgust     0.585     0.725     0.813    107\n",
      "    fear     0.585     0.677     0.550    80\n",
      "   happy     0.585     0.516     0.623    77\n",
      " neutral     0.585     0.375     0.032    95\n",
      "     sad     0.585     0.407     0.747    91\n",
      "surprise     0.585     0.619     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.578     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.949864 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.306915  [    0/ 5482]\n",
      "loss: 0.116049  [ 1200/ 5482]\n",
      "loss: 0.282223  [ 2400/ 5482]\n",
      "loss: 0.123422  [ 3600/ 5482]\n",
      "loss: 0.223337  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.710     0.717    99\n",
      " disgust     0.585     0.720     0.794    107\n",
      "    fear     0.585     0.643     0.562    80\n",
      "   happy     0.585     0.522     0.623    77\n",
      " neutral     0.585     0.375     0.032    95\n",
      "     sad     0.585     0.406     0.736    91\n",
      "surprise     0.585     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.578     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.984508 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.057904  [    0/ 5482]\n",
      "loss: 0.290418  [ 1200/ 5482]\n",
      "loss: 0.122506  [ 2400/ 5482]\n",
      "loss: 0.053835  [ 3600/ 5482]\n",
      "loss: 0.208208  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.689     0.717    99\n",
      " disgust     0.589     0.723     0.804    107\n",
      "    fear     0.589     0.721     0.550    80\n",
      "   happy     0.589     0.563     0.636    77\n",
      " neutral     0.589     0.200     0.021    95\n",
      "     sad     0.589     0.391     0.747    91\n",
      "surprise     0.589     0.696     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.569     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.964021 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.222666  [    0/ 5482]\n",
      "loss: 0.240336  [ 1200/ 5482]\n",
      "loss: 0.216235  [ 2400/ 5482]\n",
      "loss: 0.237163  [ 3600/ 5482]\n",
      "loss: 0.395531  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.664     0.717    99\n",
      " disgust     0.585     0.733     0.794    107\n",
      "    fear     0.585     0.623     0.600    80\n",
      "   happy     0.585     0.535     0.597    77\n",
      " neutral     0.585     0.375     0.032    95\n",
      "     sad     0.585     0.411     0.736    91\n",
      "surprise     0.585     0.698     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.577     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.958306 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.323373  [    0/ 5482]\n",
      "loss: 0.123713  [ 1200/ 5482]\n",
      "loss: 0.123402  [ 2400/ 5482]\n",
      "loss: 0.501999  [ 3600/ 5482]\n",
      "loss: 0.103388  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.676     0.717    99\n",
      " disgust     0.585     0.739     0.794    107\n",
      "    fear     0.585     0.676     0.575    80\n",
      "   happy     0.585     0.534     0.610    77\n",
      " neutral     0.585     0.400     0.021    95\n",
      "     sad     0.585     0.391     0.747    91\n",
      "surprise     0.585     0.691     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.587     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.988418 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.058958  [    0/ 5482]\n",
      "loss: 0.171664  [ 1200/ 5482]\n",
      "loss: 0.339087  [ 2400/ 5482]\n",
      "loss: 0.073058  [ 3600/ 5482]\n",
      "loss: 0.103523  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.686     0.707    99\n",
      " disgust     0.595     0.724     0.785    107\n",
      "    fear     0.595     0.654     0.662    80\n",
      "   happy     0.595     0.566     0.610    77\n",
      " neutral     0.595     0.250     0.011    95\n",
      "     sad     0.595     0.404     0.758    91\n",
      "surprise     0.595     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.574     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.931203 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.106020  [    0/ 5482]\n",
      "loss: 0.221424  [ 1200/ 5482]\n",
      "loss: 0.188437  [ 2400/ 5482]\n",
      "loss: 0.384811  [ 3600/ 5482]\n",
      "loss: 0.378505  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.663     0.697    99\n",
      " disgust     0.579     0.724     0.785    107\n",
      "    fear     0.579     0.632     0.600    80\n",
      "   happy     0.579     0.557     0.571    77\n",
      " neutral     0.579     0.200     0.011    95\n",
      "     sad     0.579     0.392     0.758    91\n",
      "surprise     0.579     0.704     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.553     0.578    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.961033 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.102766  [    0/ 5482]\n",
      "loss: 0.101135  [ 1200/ 5482]\n",
      "loss: 0.346532  [ 2400/ 5482]\n",
      "loss: 0.188261  [ 3600/ 5482]\n",
      "loss: 0.009320  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.716     0.737    99\n",
      " disgust     0.589     0.750     0.785    107\n",
      "    fear     0.589     0.613     0.575    80\n",
      "   happy     0.589     0.573     0.610    77\n",
      " neutral     0.589     0.250     0.021    95\n",
      "     sad     0.589     0.401     0.736    91\n",
      "surprise     0.589     0.625     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.561     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.933950 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.193790  [    0/ 5482]\n",
      "loss: 0.217352  [ 1200/ 5482]\n",
      "loss: 0.102464  [ 2400/ 5482]\n",
      "loss: 0.216811  [ 3600/ 5482]\n",
      "loss: 0.217528  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.695     0.737    99\n",
      " disgust     0.590     0.802     0.757    107\n",
      "    fear     0.590     0.676     0.600    80\n",
      "   happy     0.590     0.590     0.597    77\n",
      " neutral     0.590     0.083     0.011    95\n",
      "     sad     0.590     0.389     0.791    91\n",
      "surprise     0.590     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.558     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.951335 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.290936  [    0/ 5482]\n",
      "loss: 0.191169  [ 1200/ 5482]\n",
      "loss: 0.265696  [ 2400/ 5482]\n",
      "loss: 0.057161  [ 3600/ 5482]\n",
      "loss: 0.127799  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.595     0.717     0.717    99\n",
      " disgust     0.595     0.779     0.757    107\n",
      "    fear     0.595     0.654     0.662    80\n",
      "   happy     0.595     0.541     0.597    77\n",
      " neutral     0.595     0.000     0.000    95\n",
      "     sad     0.595     0.390     0.758    91\n",
      "surprise     0.595     0.754     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.595     0.548     0.600    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.954038 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.141921  [    0/ 5482]\n",
      "loss: 0.155188  [ 1200/ 5482]\n",
      "loss: 0.171510  [ 2400/ 5482]\n",
      "loss: 0.240341  [ 3600/ 5482]\n",
      "loss: 0.164990  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.720     0.727    99\n",
      " disgust     0.590     0.754     0.804    107\n",
      "    fear     0.590     0.658     0.600    80\n",
      "   happy     0.590     0.530     0.571    77\n",
      " neutral     0.590     0.143     0.011    95\n",
      "     sad     0.590     0.396     0.736    91\n",
      "surprise     0.590     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.551     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.942110 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.185351  [    0/ 5482]\n",
      "loss: 0.127164  [ 1200/ 5482]\n",
      "loss: 0.201490  [ 2400/ 5482]\n",
      "loss: 0.215209  [ 3600/ 5482]\n",
      "loss: 0.141966  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.685     0.747    99\n",
      " disgust     0.582     0.727     0.822    107\n",
      "    fear     0.582     0.661     0.487    80\n",
      "   happy     0.582     0.505     0.597    77\n",
      " neutral     0.582     0.333     0.011    95\n",
      "     sad     0.582     0.402     0.747    91\n",
      "surprise     0.582     0.661     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.568     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 2.016351 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.061400  [    0/ 5482]\n",
      "loss: 0.209783  [ 1200/ 5482]\n",
      "loss: 0.061286  [ 2400/ 5482]\n",
      "loss: 0.197788  [ 3600/ 5482]\n",
      "loss: 0.220006  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.689     0.717    99\n",
      " disgust     0.580     0.737     0.785    107\n",
      "    fear     0.580     0.648     0.575    80\n",
      "   happy     0.580     0.579     0.571    77\n",
      " neutral     0.580     0.167     0.011    95\n",
      "     sad     0.580     0.387     0.714    91\n",
      "surprise     0.580     0.597     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.543     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.990526 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.137633  [    0/ 5482]\n",
      "loss: 0.008191  [ 1200/ 5482]\n",
      "loss: 0.072887  [ 2400/ 5482]\n",
      "loss: 0.139711  [ 3600/ 5482]\n",
      "loss: 0.166132  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.713     0.727    99\n",
      " disgust     0.589     0.759     0.794    107\n",
      "    fear     0.589     0.638     0.550    80\n",
      "   happy     0.589     0.571     0.571    77\n",
      " neutral     0.589     0.167     0.011    95\n",
      "     sad     0.589     0.393     0.725    91\n",
      "surprise     0.589     0.610     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.550     0.593    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.973369 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.236119  [    0/ 5482]\n",
      "loss: 0.074649  [ 1200/ 5482]\n",
      "loss: 0.290737  [ 2400/ 5482]\n",
      "loss: 0.176710  [ 3600/ 5482]\n",
      "loss: 0.110553  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.683     0.717    99\n",
      " disgust     0.587     0.752     0.794    107\n",
      "    fear     0.587     0.635     0.588    80\n",
      "   happy     0.587     0.557     0.571    77\n",
      " neutral     0.587     0.250     0.021    95\n",
      "     sad     0.587     0.401     0.736    91\n",
      "surprise     0.587     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.561     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.937122 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.239308  [    0/ 5482]\n",
      "loss: 0.273913  [ 1200/ 5482]\n",
      "loss: 0.439509  [ 2400/ 5482]\n",
      "loss: 0.281186  [ 3600/ 5482]\n",
      "loss: 0.171996  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.692     0.727    99\n",
      " disgust     0.580     0.718     0.785    107\n",
      "    fear     0.580     0.667     0.550    80\n",
      "   happy     0.580     0.543     0.571    77\n",
      " neutral     0.580     0.125     0.011    95\n",
      "     sad     0.580     0.387     0.736    91\n",
      "surprise     0.580     0.689     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.546     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 2.015983 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.145557  [    0/ 5482]\n",
      "loss: 0.197067  [ 1200/ 5482]\n",
      "loss: 0.189516  [ 2400/ 5482]\n",
      "loss: 0.165567  [ 3600/ 5482]\n",
      "loss: 0.123805  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.676     0.717    99\n",
      " disgust     0.577     0.818     0.757    107\n",
      "    fear     0.577     0.580     0.637    80\n",
      "   happy     0.577     0.531     0.558    77\n",
      " neutral     0.577     0.000     0.000    95\n",
      "     sad     0.577     0.389     0.714    91\n",
      "surprise     0.577     0.612     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.515     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 2.096398 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.187780  [    0/ 5482]\n",
      "loss: 0.122067  [ 1200/ 5482]\n",
      "loss: 0.253917  [ 2400/ 5482]\n",
      "loss: 0.303421  [ 3600/ 5482]\n",
      "loss: 0.096835  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.724     0.717    99\n",
      " disgust     0.587     0.769     0.776    107\n",
      "    fear     0.587     0.586     0.637    80\n",
      "   happy     0.587     0.557     0.571    77\n",
      " neutral     0.587     0.200     0.021    95\n",
      "     sad     0.587     0.390     0.736    91\n",
      "surprise     0.587     0.714     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.563     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.967155 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.148447  [    0/ 5482]\n",
      "loss: 0.233953  [ 1200/ 5482]\n",
      "loss: 0.168745  [ 2400/ 5482]\n",
      "loss: 0.305808  [ 3600/ 5482]\n",
      "loss: 0.059372  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.724     0.717    99\n",
      " disgust     0.574     0.726     0.766    107\n",
      "    fear     0.574     0.614     0.537    80\n",
      "   happy     0.574     0.577     0.584    77\n",
      " neutral     0.574     0.100     0.011    95\n",
      "     sad     0.574     0.382     0.714    91\n",
      "surprise     0.574     0.606     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.533     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.920111 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.063418  [    0/ 5482]\n",
      "loss: 0.305065  [ 1200/ 5482]\n",
      "loss: 0.335759  [ 2400/ 5482]\n",
      "loss: 0.072137  [ 3600/ 5482]\n",
      "loss: 0.156793  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.572     0.692     0.747    99\n",
      " disgust     0.572     0.759     0.766    107\n",
      "    fear     0.572     0.608     0.562    80\n",
      "   happy     0.572     0.489     0.584    77\n",
      " neutral     0.572     0.333     0.032    95\n",
      "     sad     0.572     0.393     0.703    91\n",
      "surprise     0.572     0.632     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.572     0.558     0.569    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 2.013538 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.135816  [    0/ 5482]\n",
      "loss: 0.332329  [ 1200/ 5482]\n",
      "loss: 0.122051  [ 2400/ 5482]\n",
      "loss: 0.059182  [ 3600/ 5482]\n",
      "loss: 0.163017  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.673     0.727    99\n",
      " disgust     0.567     0.774     0.766    107\n",
      "    fear     0.567     0.564     0.550    80\n",
      "   happy     0.567     0.516     0.610    77\n",
      " neutral     0.567     0.143     0.011    95\n",
      "     sad     0.567     0.382     0.714    91\n",
      "surprise     0.567     0.686     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.534     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 2.015966 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.237805  [    0/ 5482]\n",
      "loss: 0.192331  [ 1200/ 5482]\n",
      "loss: 0.299520  [ 2400/ 5482]\n",
      "loss: 0.282133  [ 3600/ 5482]\n",
      "loss: 0.307862  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.713     0.727    99\n",
      " disgust     0.577     0.735     0.776    107\n",
      "    fear     0.577     0.579     0.550    80\n",
      "   happy     0.577     0.616     0.584    77\n",
      " neutral     0.577     0.000     0.000    95\n",
      "     sad     0.577     0.379     0.703    91\n",
      "surprise     0.577     0.638     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.523     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.962373 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.201248  [    0/ 5482]\n",
      "loss: 0.162993  [ 1200/ 5482]\n",
      "loss: 0.276134  [ 2400/ 5482]\n",
      "loss: 0.255400  [ 3600/ 5482]\n",
      "loss: 0.138493  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.569     0.704     0.768    99\n",
      " disgust     0.569     0.761     0.776    107\n",
      "    fear     0.569     0.603     0.512    80\n",
      "   happy     0.569     0.524     0.571    77\n",
      " neutral     0.569     0.100     0.011    95\n",
      "     sad     0.569     0.373     0.725    91\n",
      "surprise     0.569     0.667     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.569     0.533     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.959489 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.280987  [    0/ 5482]\n",
      "loss: 0.186164  [ 1200/ 5482]\n",
      "loss: 0.254648  [ 2400/ 5482]\n",
      "loss: 0.248314  [ 3600/ 5482]\n",
      "loss: 0.120859  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.702     0.737    99\n",
      " disgust     0.577     0.737     0.785    107\n",
      "    fear     0.577     0.603     0.588    80\n",
      "   happy     0.577     0.561     0.597    77\n",
      " neutral     0.577     0.111     0.011    95\n",
      "     sad     0.577     0.378     0.714    91\n",
      "surprise     0.577     0.706     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.542     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 2.011681 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.137162  [    0/ 5482]\n",
      "loss: 0.138494  [ 1200/ 5482]\n",
      "loss: 0.124664  [ 2400/ 5482]\n",
      "loss: 0.300285  [ 3600/ 5482]\n",
      "loss: 0.172775  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.723     0.737    99\n",
      " disgust     0.585     0.802     0.794    107\n",
      "    fear     0.585     0.573     0.588    80\n",
      "   happy     0.585     0.534     0.610    77\n",
      " neutral     0.585     0.100     0.011    95\n",
      "     sad     0.585     0.398     0.725    91\n",
      "surprise     0.585     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.542     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.970552 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.268141  [    0/ 5482]\n",
      "loss: 0.148510  [ 1200/ 5482]\n",
      "loss: 0.147304  [ 2400/ 5482]\n",
      "loss: 0.301228  [ 3600/ 5482]\n",
      "loss: 0.259649  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.587     0.714     0.758    99\n",
      " disgust     0.587     0.790     0.776    107\n",
      "    fear     0.587     0.584     0.562    80\n",
      "   happy     0.587     0.511     0.597    77\n",
      " neutral     0.587     0.286     0.021    95\n",
      "     sad     0.587     0.412     0.747    91\n",
      "surprise     0.587     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.587     0.562     0.586    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.962720 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.166771  [    0/ 5482]\n",
      "loss: 0.147830  [ 1200/ 5482]\n",
      "loss: 0.057832  [ 2400/ 5482]\n",
      "loss: 0.065804  [ 3600/ 5482]\n",
      "loss: 0.315779  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.712     0.747    99\n",
      " disgust     0.589     0.761     0.776    107\n",
      "    fear     0.589     0.595     0.588    80\n",
      "   happy     0.589     0.558     0.623    77\n",
      " neutral     0.589     0.125     0.011    95\n",
      "     sad     0.589     0.398     0.747    91\n",
      "surprise     0.589     0.717     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.552     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.952247 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.164325  [    0/ 5482]\n",
      "loss: 0.186080  [ 1200/ 5482]\n",
      "loss: 0.059246  [ 2400/ 5482]\n",
      "loss: 0.323637  [ 3600/ 5482]\n",
      "loss: 0.007378  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.729     0.788    99\n",
      " disgust     0.585     0.743     0.785    107\n",
      "    fear     0.585     0.597     0.537    80\n",
      "   happy     0.585     0.580     0.610    77\n",
      " neutral     0.585     0.125     0.011    95\n",
      "     sad     0.585     0.393     0.725    91\n",
      "surprise     0.585     0.623     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.542     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.959164 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.225365  [    0/ 5482]\n",
      "loss: 0.146620  [ 1200/ 5482]\n",
      "loss: 0.005411  [ 2400/ 5482]\n",
      "loss: 0.174982  [ 3600/ 5482]\n",
      "loss: 0.251107  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.731     0.768    99\n",
      " disgust     0.590     0.746     0.794    107\n",
      "    fear     0.590     0.657     0.575    80\n",
      "   happy     0.590     0.573     0.610    77\n",
      " neutral     0.590     0.077     0.011    95\n",
      "     sad     0.590     0.391     0.692    91\n",
      "surprise     0.590     0.636     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.544     0.591    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.874623 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.216289  [    0/ 5482]\n",
      "loss: 0.170910  [ 1200/ 5482]\n",
      "loss: 0.220961  [ 2400/ 5482]\n",
      "loss: 0.183168  [ 3600/ 5482]\n",
      "loss: 0.144766  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.730     0.737    99\n",
      " disgust     0.677     0.757     0.785    107\n",
      "    fear     0.677     0.644     0.588    80\n",
      "   happy     0.677     0.558     0.623    77\n",
      " neutral     0.677     0.709     0.642    95\n",
      "     sad     0.677     0.617     0.637    91\n",
      "surprise     0.677     0.700     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.674     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.877457 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep255_acc_68.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep112_acc_61\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep255_acc_68\"! Old accuracy: 61.5, new accuracy: 67.7\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.130941  [    0/ 5482]\n",
      "loss: 0.101180  [ 1200/ 5482]\n",
      "loss: 0.218754  [ 2400/ 5482]\n",
      "loss: 0.118138  [ 3600/ 5482]\n",
      "loss: 0.121479  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.738     0.768    99\n",
      " disgust     0.687     0.766     0.794    107\n",
      "    fear     0.687     0.630     0.575    80\n",
      "   happy     0.687     0.560     0.610    77\n",
      " neutral     0.687     0.719     0.726    95\n",
      "     sad     0.687     0.651     0.615    91\n",
      "surprise     0.687     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.681     0.678    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.844119 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep256_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep255_acc_68\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep256_acc_69\"! Old accuracy: 67.7, new accuracy: 68.7\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.185747  [    0/ 5482]\n",
      "loss: 0.142572  [ 1200/ 5482]\n",
      "loss: 0.059037  [ 2400/ 5482]\n",
      "loss: 0.170808  [ 3600/ 5482]\n",
      "loss: 0.092400  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.731     0.768    99\n",
      " disgust     0.687     0.739     0.794    107\n",
      "    fear     0.687     0.645     0.613    80\n",
      "   happy     0.687     0.592     0.584    77\n",
      " neutral     0.687     0.708     0.716    95\n",
      "     sad     0.687     0.663     0.604    91\n",
      "surprise     0.687     0.683     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.680     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.858562 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.184427  [    0/ 5482]\n",
      "loss: 0.044000  [ 1200/ 5482]\n",
      "loss: 0.091049  [ 2400/ 5482]\n",
      "loss: 0.138492  [ 3600/ 5482]\n",
      "loss: 0.161930  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.768     0.737    99\n",
      " disgust     0.692     0.763     0.813    107\n",
      "    fear     0.692     0.646     0.637    80\n",
      "   happy     0.692     0.576     0.636    77\n",
      " neutral     0.692     0.688     0.695    95\n",
      "     sad     0.692     0.679     0.604    91\n",
      "surprise     0.692     0.683     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.686     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.762967 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep258_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep256_acc_69\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep258_acc_69\"! Old accuracy: 68.7, new accuracy: 69.2\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.138125  [    0/ 5482]\n",
      "loss: 0.091518  [ 1200/ 5482]\n",
      "loss: 0.117759  [ 2400/ 5482]\n",
      "loss: 0.181909  [ 3600/ 5482]\n",
      "loss: 0.042012  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.757     0.788    99\n",
      " disgust     0.702     0.750     0.813    107\n",
      "    fear     0.702     0.667     0.650    80\n",
      "   happy     0.702     0.580     0.610    77\n",
      " neutral     0.702     0.690     0.726    95\n",
      "     sad     0.702     0.697     0.582    91\n",
      "surprise     0.702     0.750     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.699     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.840468 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep259_acc_70.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep258_acc_69\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep259_acc_70\"! Old accuracy: 69.2, new accuracy: 70.2\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.158426  [    0/ 5482]\n",
      "loss: 0.040578  [ 1200/ 5482]\n",
      "loss: 0.055704  [ 2400/ 5482]\n",
      "loss: 0.141216  [ 3600/ 5482]\n",
      "loss: 0.179670  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.694     0.758    99\n",
      " disgust     0.698     0.761     0.832    107\n",
      "    fear     0.698     0.685     0.625    80\n",
      "   happy     0.698     0.565     0.623    77\n",
      " neutral     0.698     0.680     0.737    95\n",
      "     sad     0.698     0.718     0.615    91\n",
      "surprise     0.698     0.826     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.704     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.802906 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.099846  [    0/ 5482]\n",
      "loss: 0.077041  [ 1200/ 5482]\n",
      "loss: 0.137001  [ 2400/ 5482]\n",
      "loss: 0.117489  [ 3600/ 5482]\n",
      "loss: 0.598521  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.710     0.768    99\n",
      " disgust     0.689     0.787     0.794    107\n",
      "    fear     0.689     0.644     0.588    80\n",
      "   happy     0.689     0.528     0.610    77\n",
      " neutral     0.689     0.739     0.716    95\n",
      "     sad     0.689     0.696     0.604    91\n",
      "surprise     0.689     0.677     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.683     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.866919 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.155955  [    0/ 5482]\n",
      "loss: 0.157487  [ 1200/ 5482]\n",
      "loss: 0.113605  [ 2400/ 5482]\n",
      "loss: 0.171744  [ 3600/ 5482]\n",
      "loss: 0.066656  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.745     0.737    99\n",
      " disgust     0.682     0.748     0.776    107\n",
      "    fear     0.682     0.647     0.550    80\n",
      "   happy     0.682     0.584     0.584    77\n",
      " neutral     0.682     0.651     0.747    95\n",
      "     sad     0.682     0.698     0.659    91\n",
      "surprise     0.682     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.676     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.940200 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.196911  [    0/ 5482]\n",
      "loss: 0.220494  [ 1200/ 5482]\n",
      "loss: 0.108263  [ 2400/ 5482]\n",
      "loss: 0.063656  [ 3600/ 5482]\n",
      "loss: 0.165636  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.726     0.778    99\n",
      " disgust     0.684     0.755     0.776    107\n",
      "    fear     0.684     0.608     0.600    80\n",
      "   happy     0.684     0.597     0.597    77\n",
      " neutral     0.684     0.673     0.737    95\n",
      "     sad     0.684     0.709     0.615    91\n",
      "surprise     0.684     0.673     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.677     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.849601 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.133798  [    0/ 5482]\n",
      "loss: 0.167912  [ 1200/ 5482]\n",
      "loss: 0.159173  [ 2400/ 5482]\n",
      "loss: 0.070025  [ 3600/ 5482]\n",
      "loss: 0.124239  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.708     0.758    99\n",
      " disgust     0.693     0.739     0.794    107\n",
      "    fear     0.693     0.640     0.600    80\n",
      "   happy     0.693     0.584     0.584    77\n",
      " neutral     0.693     0.720     0.758    95\n",
      "     sad     0.693     0.760     0.626    91\n",
      "surprise     0.693     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.687     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.845465 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.158629  [    0/ 5482]\n",
      "loss: 0.137751  [ 1200/ 5482]\n",
      "loss: 0.141021  [ 2400/ 5482]\n",
      "loss: 0.103864  [ 3600/ 5482]\n",
      "loss: 0.282468  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.742     0.727    99\n",
      " disgust     0.697     0.759     0.794    107\n",
      "    fear     0.697     0.662     0.613    80\n",
      "   happy     0.697     0.627     0.610    77\n",
      " neutral     0.697     0.685     0.779    95\n",
      "     sad     0.697     0.720     0.593    91\n",
      "surprise     0.697     0.638     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.690     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.803102 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.152734  [    0/ 5482]\n",
      "loss: 0.142803  [ 1200/ 5482]\n",
      "loss: 0.197746  [ 2400/ 5482]\n",
      "loss: 0.147894  [ 3600/ 5482]\n",
      "loss: 0.057901  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.713     0.727    99\n",
      " disgust     0.680     0.774     0.766    107\n",
      "    fear     0.680     0.644     0.588    80\n",
      "   happy     0.680     0.603     0.610    77\n",
      " neutral     0.680     0.676     0.747    95\n",
      "     sad     0.680     0.671     0.582    91\n",
      "surprise     0.680     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.673     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.808957 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.161537  [    0/ 5482]\n",
      "loss: 0.172175  [ 1200/ 5482]\n",
      "loss: 0.094640  [ 2400/ 5482]\n",
      "loss: 0.103032  [ 3600/ 5482]\n",
      "loss: 0.129395  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.704     0.768    99\n",
      " disgust     0.677     0.780     0.794    107\n",
      "    fear     0.677     0.646     0.525    80\n",
      "   happy     0.677     0.548     0.597    77\n",
      " neutral     0.677     0.689     0.747    95\n",
      "     sad     0.677     0.708     0.560    91\n",
      "surprise     0.677     0.609     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.669     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.849247 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.084655  [    0/ 5482]\n",
      "loss: 0.069483  [ 1200/ 5482]\n",
      "loss: 0.083216  [ 2400/ 5482]\n",
      "loss: 0.092049  [ 3600/ 5482]\n",
      "loss: 0.092680  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.676     0.758    99\n",
      " disgust     0.687     0.766     0.794    107\n",
      "    fear     0.687     0.645     0.613    80\n",
      "   happy     0.687     0.570     0.584    77\n",
      " neutral     0.687     0.737     0.737    95\n",
      "     sad     0.687     0.684     0.571    91\n",
      "surprise     0.687     0.694     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.681     0.680    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.816872 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.188600  [    0/ 5482]\n",
      "loss: 0.138128  [ 1200/ 5482]\n",
      "loss: 0.109171  [ 2400/ 5482]\n",
      "loss: 0.234525  [ 3600/ 5482]\n",
      "loss: 0.029732  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.724     0.768    99\n",
      " disgust     0.692     0.790     0.776    107\n",
      "    fear     0.692     0.645     0.613    80\n",
      "   happy     0.692     0.585     0.623    77\n",
      " neutral     0.692     0.703     0.747    95\n",
      "     sad     0.692     0.708     0.560    91\n",
      "surprise     0.692     0.638     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.685     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.852557 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.264883  [    0/ 5482]\n",
      "loss: 0.448048  [ 1200/ 5482]\n",
      "loss: 0.142775  [ 2400/ 5482]\n",
      "loss: 0.115921  [ 3600/ 5482]\n",
      "loss: 0.185758  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.682     0.758    99\n",
      " disgust     0.698     0.790     0.776    107\n",
      "    fear     0.698     0.676     0.625    80\n",
      "   happy     0.698     0.613     0.597    77\n",
      " neutral     0.698     0.723     0.768    95\n",
      "     sad     0.698     0.720     0.593    91\n",
      "surprise     0.698     0.643     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.692     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.850444 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.104788  [    0/ 5482]\n",
      "loss: 0.112950  [ 1200/ 5482]\n",
      "loss: 0.138290  [ 2400/ 5482]\n",
      "loss: 0.079245  [ 3600/ 5482]\n",
      "loss: 0.072203  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.743     0.758    99\n",
      " disgust     0.705     0.783     0.776    107\n",
      "    fear     0.705     0.708     0.575    80\n",
      "   happy     0.705     0.543     0.649    77\n",
      " neutral     0.705     0.709     0.768    95\n",
      "     sad     0.705     0.716     0.637    91\n",
      "surprise     0.705     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.704     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.874411 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep271_acc_70.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep259_acc_70\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep271_acc_70\"! Old accuracy: 70.2, new accuracy: 70.5\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.138061  [    0/ 5482]\n",
      "loss: 0.143305  [ 1200/ 5482]\n",
      "loss: 0.064767  [ 2400/ 5482]\n",
      "loss: 0.093456  [ 3600/ 5482]\n",
      "loss: 0.082222  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.709     0.737    99\n",
      " disgust     0.697     0.785     0.785    107\n",
      "    fear     0.697     0.671     0.637    80\n",
      "   happy     0.697     0.573     0.610    77\n",
      " neutral     0.697     0.699     0.758    95\n",
      "     sad     0.697     0.687     0.626    91\n",
      "surprise     0.697     0.732     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.694     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.843944 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.114896  [    0/ 5482]\n",
      "loss: 0.063154  [ 1200/ 5482]\n",
      "loss: 0.145939  [ 2400/ 5482]\n",
      "loss: 0.067458  [ 3600/ 5482]\n",
      "loss: 0.078120  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.729     0.707    99\n",
      " disgust     0.689     0.741     0.804    107\n",
      "    fear     0.689     0.629     0.550    80\n",
      "   happy     0.689     0.541     0.597    77\n",
      " neutral     0.689     0.742     0.758    95\n",
      "     sad     0.689     0.731     0.626    91\n",
      "surprise     0.689     0.662     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.682     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.882162 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.034761  [    0/ 5482]\n",
      "loss: 0.044394  [ 1200/ 5482]\n",
      "loss: 0.057779  [ 2400/ 5482]\n",
      "loss: 0.198982  [ 3600/ 5482]\n",
      "loss: 0.045747  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.644     0.768    99\n",
      " disgust     0.687     0.755     0.776    107\n",
      "    fear     0.687     0.681     0.613    80\n",
      "   happy     0.687     0.587     0.571    77\n",
      " neutral     0.687     0.667     0.758    95\n",
      "     sad     0.687     0.764     0.604    91\n",
      "surprise     0.687     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.689     0.678    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.899539 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.044631  [    0/ 5482]\n",
      "loss: 0.040536  [ 1200/ 5482]\n",
      "loss: 0.180830  [ 2400/ 5482]\n",
      "loss: 0.086400  [ 3600/ 5482]\n",
      "loss: 0.050892  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.652     0.737    99\n",
      " disgust     0.697     0.833     0.748    107\n",
      "    fear     0.697     0.662     0.662    80\n",
      "   happy     0.697     0.580     0.610    77\n",
      " neutral     0.697     0.717     0.747    95\n",
      "     sad     0.697     0.702     0.648    91\n",
      "surprise     0.697     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.696     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.903369 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.180858  [    0/ 5482]\n",
      "loss: 0.078150  [ 1200/ 5482]\n",
      "loss: 0.096557  [ 2400/ 5482]\n",
      "loss: 0.040048  [ 3600/ 5482]\n",
      "loss: 0.074903  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.694     0.758    99\n",
      " disgust     0.698     0.785     0.785    107\n",
      "    fear     0.698     0.634     0.562    80\n",
      "   happy     0.698     0.630     0.597    77\n",
      " neutral     0.698     0.730     0.768    95\n",
      "     sad     0.698     0.714     0.659    91\n",
      "surprise     0.698     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.690     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.853655 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.059212  [    0/ 5482]\n",
      "loss: 0.080002  [ 1200/ 5482]\n",
      "loss: 0.148001  [ 2400/ 5482]\n",
      "loss: 0.119190  [ 3600/ 5482]\n",
      "loss: 0.163213  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.661     0.747    99\n",
      " disgust     0.692     0.804     0.766    107\n",
      "    fear     0.692     0.724     0.525    80\n",
      "   happy     0.692     0.608     0.584    77\n",
      " neutral     0.692     0.694     0.789    95\n",
      "     sad     0.692     0.709     0.670    91\n",
      "surprise     0.692     0.614     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.688     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.892649 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.063699  [    0/ 5482]\n",
      "loss: 0.067767  [ 1200/ 5482]\n",
      "loss: 0.034458  [ 2400/ 5482]\n",
      "loss: 0.042074  [ 3600/ 5482]\n",
      "loss: 0.035693  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.689     0.737    99\n",
      " disgust     0.690     0.778     0.785    107\n",
      "    fear     0.690     0.657     0.550    80\n",
      "   happy     0.690     0.580     0.610    77\n",
      " neutral     0.690     0.706     0.758    95\n",
      "     sad     0.690     0.737     0.615    91\n",
      "surprise     0.690     0.643     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.684     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.861849 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.043264  [    0/ 5482]\n",
      "loss: 0.077785  [ 1200/ 5482]\n",
      "loss: 0.038756  [ 2400/ 5482]\n",
      "loss: 0.078211  [ 3600/ 5482]\n",
      "loss: 0.049169  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.692     0.727    99\n",
      " disgust     0.697     0.802     0.757    107\n",
      "    fear     0.697     0.635     0.588    80\n",
      "   happy     0.697     0.597     0.597    77\n",
      " neutral     0.697     0.667     0.779    95\n",
      "     sad     0.697     0.769     0.659    91\n",
      "surprise     0.697     0.692     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.694     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.884882 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.083156  [    0/ 5482]\n",
      "loss: 0.058774  [ 1200/ 5482]\n",
      "loss: 0.125316  [ 2400/ 5482]\n",
      "loss: 0.055669  [ 3600/ 5482]\n",
      "loss: 0.054603  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.702     0.737    99\n",
      " disgust     0.695     0.802     0.757    107\n",
      "    fear     0.695     0.625     0.625    80\n",
      "   happy     0.695     0.590     0.597    77\n",
      " neutral     0.695     0.664     0.768    95\n",
      "     sad     0.695     0.726     0.670    91\n",
      "surprise     0.695     0.755     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.695     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.919550 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.086347  [    0/ 5482]\n",
      "loss: 0.030716  [ 1200/ 5482]\n",
      "loss: 0.047126  [ 2400/ 5482]\n",
      "loss: 0.040507  [ 3600/ 5482]\n",
      "loss: 0.070617  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.709     0.737    99\n",
      " disgust     0.695     0.835     0.757    107\n",
      "    fear     0.695     0.620     0.613    80\n",
      "   happy     0.695     0.575     0.597    77\n",
      " neutral     0.695     0.643     0.779    95\n",
      "     sad     0.695     0.753     0.670    91\n",
      "surprise     0.695     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.695     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.925838 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.043885  [    0/ 5482]\n",
      "loss: 0.035812  [ 1200/ 5482]\n",
      "loss: 0.176603  [ 2400/ 5482]\n",
      "loss: 0.083894  [ 3600/ 5482]\n",
      "loss: 0.064933  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.698     0.677    99\n",
      " disgust     0.689     0.796     0.804    107\n",
      "    fear     0.689     0.618     0.588    80\n",
      "   happy     0.689     0.588     0.610    77\n",
      " neutral     0.689     0.658     0.768    95\n",
      "     sad     0.689     0.773     0.637    91\n",
      "surprise     0.689     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.684     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.895443 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.063257  [    0/ 5482]\n",
      "loss: 0.065865  [ 1200/ 5482]\n",
      "loss: 0.034934  [ 2400/ 5482]\n",
      "loss: 0.059406  [ 3600/ 5482]\n",
      "loss: 0.102166  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.729     0.707    99\n",
      " disgust     0.697     0.780     0.794    107\n",
      "    fear     0.697     0.603     0.588    80\n",
      "   happy     0.697     0.575     0.597    77\n",
      " neutral     0.697     0.673     0.779    95\n",
      "     sad     0.697     0.784     0.637    91\n",
      "surprise     0.697     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.694     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 1.960880 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.036481  [    0/ 5482]\n",
      "loss: 0.032223  [ 1200/ 5482]\n",
      "loss: 0.053793  [ 2400/ 5482]\n",
      "loss: 0.043268  [ 3600/ 5482]\n",
      "loss: 0.043289  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.708     0.687    99\n",
      " disgust     0.705     0.800     0.785    107\n",
      "    fear     0.705     0.701     0.675    80\n",
      "   happy     0.705     0.625     0.584    77\n",
      " neutral     0.705     0.638     0.779    95\n",
      "     sad     0.705     0.753     0.670    91\n",
      "surprise     0.705     0.698     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.703     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.892687 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.049713  [    0/ 5482]\n",
      "loss: 0.045511  [ 1200/ 5482]\n",
      "loss: 0.037686  [ 2400/ 5482]\n",
      "loss: 0.068124  [ 3600/ 5482]\n",
      "loss: 0.052473  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.723     0.737    99\n",
      " disgust     0.705     0.796     0.804    107\n",
      "    fear     0.705     0.671     0.637    80\n",
      "   happy     0.705     0.595     0.610    77\n",
      " neutral     0.705     0.664     0.789    95\n",
      "     sad     0.705     0.770     0.626    91\n",
      "surprise     0.705     0.695     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.702     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 1.886900 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.025906  [    0/ 5482]\n",
      "loss: 0.118221  [ 1200/ 5482]\n",
      "loss: 0.049183  [ 2400/ 5482]\n",
      "loss: 0.079633  [ 3600/ 5482]\n",
      "loss: 0.031984  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.699     0.727    99\n",
      " disgust     0.700     0.814     0.776    107\n",
      "    fear     0.700     0.672     0.562    80\n",
      "   happy     0.700     0.543     0.649    77\n",
      " neutral     0.700     0.718     0.779    95\n",
      "     sad     0.700     0.819     0.648    91\n",
      "surprise     0.700     0.620     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.698     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.933555 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.042760  [    0/ 5482]\n",
      "loss: 0.057503  [ 1200/ 5482]\n",
      "loss: 0.052890  [ 2400/ 5482]\n",
      "loss: 0.056549  [ 3600/ 5482]\n",
      "loss: 0.054726  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.713     0.756     0.687    99\n",
      " disgust     0.713     0.789     0.804    107\n",
      "    fear     0.713     0.681     0.613    80\n",
      "   happy     0.713     0.613     0.597    77\n",
      " neutral     0.713     0.685     0.800    95\n",
      "     sad     0.713     0.741     0.692    91\n",
      "surprise     0.713     0.691     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.713     0.708     0.709    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 1.912275 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep287_acc_71.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep271_acc_70\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep287_acc_71\"! Old accuracy: 70.5, new accuracy: 71.3\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.064281  [    0/ 5482]\n",
      "loss: 0.033387  [ 1200/ 5482]\n",
      "loss: 0.060755  [ 2400/ 5482]\n",
      "loss: 0.050300  [ 3600/ 5482]\n",
      "loss: 0.052046  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.699     0.727    99\n",
      " disgust     0.702     0.771     0.785    107\n",
      "    fear     0.702     0.705     0.537    80\n",
      "   happy     0.702     0.578     0.623    77\n",
      " neutral     0.702     0.692     0.779    95\n",
      "     sad     0.702     0.782     0.670    91\n",
      "surprise     0.702     0.667     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.699     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.906074 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.025973  [    0/ 5482]\n",
      "loss: 0.092829  [ 1200/ 5482]\n",
      "loss: 0.099794  [ 2400/ 5482]\n",
      "loss: 0.018281  [ 3600/ 5482]\n",
      "loss: 0.091896  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.732     0.717    99\n",
      " disgust     0.702     0.837     0.766    107\n",
      "    fear     0.702     0.667     0.575    80\n",
      "   happy     0.702     0.573     0.610    77\n",
      " neutral     0.702     0.636     0.789    95\n",
      "     sad     0.702     0.775     0.681    91\n",
      "surprise     0.702     0.682     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.700     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.047784 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.045049  [    0/ 5482]\n",
      "loss: 0.062106  [ 1200/ 5482]\n",
      "loss: 0.070272  [ 2400/ 5482]\n",
      "loss: 0.073874  [ 3600/ 5482]\n",
      "loss: 0.043482  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.644     0.768    99\n",
      " disgust     0.682     0.802     0.757    107\n",
      "    fear     0.682     0.692     0.562    80\n",
      "   happy     0.682     0.549     0.584    77\n",
      " neutral     0.682     0.638     0.779    95\n",
      "     sad     0.682     0.760     0.626    91\n",
      "surprise     0.682     0.717     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.686     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 2.050562 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.062600  [    0/ 5482]\n",
      "loss: 0.041866  [ 1200/ 5482]\n",
      "loss: 0.036462  [ 2400/ 5482]\n",
      "loss: 0.059017  [ 3600/ 5482]\n",
      "loss: 0.034425  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.723     0.737    99\n",
      " disgust     0.703     0.806     0.776    107\n",
      "    fear     0.703     0.690     0.613    80\n",
      "   happy     0.703     0.548     0.597    77\n",
      " neutral     0.703     0.689     0.768    95\n",
      "     sad     0.703     0.736     0.703    91\n",
      "surprise     0.703     0.707     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.700     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.953527 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.057547  [    0/ 5482]\n",
      "loss: 0.051828  [ 1200/ 5482]\n",
      "loss: 0.405426  [ 2400/ 5482]\n",
      "loss: 0.028349  [ 3600/ 5482]\n",
      "loss: 0.047215  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.755     0.747    99\n",
      " disgust     0.693     0.798     0.776    107\n",
      "    fear     0.693     0.667     0.575    80\n",
      "   happy     0.693     0.583     0.636    77\n",
      " neutral     0.693     0.646     0.768    95\n",
      "     sad     0.693     0.722     0.571    91\n",
      "surprise     0.693     0.657     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.690     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.959558 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.041917  [    0/ 5482]\n",
      "loss: 0.037887  [ 1200/ 5482]\n",
      "loss: 0.056914  [ 2400/ 5482]\n",
      "loss: 0.041454  [ 3600/ 5482]\n",
      "loss: 0.043260  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.715     0.777     0.737    99\n",
      " disgust     0.715     0.814     0.776    107\n",
      "    fear     0.715     0.653     0.588    80\n",
      "   happy     0.715     0.602     0.649    77\n",
      " neutral     0.715     0.701     0.789    95\n",
      "     sad     0.715     0.785     0.681    91\n",
      "surprise     0.715     0.630     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.715     0.709     0.711    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 1.854407 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep293_acc_71.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep287_acc_71\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep293_acc_71\"! Old accuracy: 71.3, new accuracy: 71.5\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.174457  [    0/ 5482]\n",
      "loss: 0.031392  [ 1200/ 5482]\n",
      "loss: 0.037439  [ 2400/ 5482]\n",
      "loss: 0.059238  [ 3600/ 5482]\n",
      "loss: 0.050252  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.760     0.737    99\n",
      " disgust     0.692     0.808     0.785    107\n",
      "    fear     0.692     0.667     0.550    80\n",
      "   happy     0.692     0.500     0.623    77\n",
      " neutral     0.692     0.673     0.758    95\n",
      "     sad     0.692     0.735     0.670    91\n",
      "surprise     0.692     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.690     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.976716 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.041841  [    0/ 5482]\n",
      "loss: 0.035143  [ 1200/ 5482]\n",
      "loss: 0.035987  [ 2400/ 5482]\n",
      "loss: 0.055254  [ 3600/ 5482]\n",
      "loss: 0.025465  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.793     0.737    99\n",
      " disgust     0.695     0.780     0.794    107\n",
      "    fear     0.695     0.672     0.537    80\n",
      "   happy     0.695     0.551     0.636    77\n",
      " neutral     0.695     0.673     0.779    95\n",
      "     sad     0.695     0.725     0.637    91\n",
      "surprise     0.695     0.636     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.690     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 1.969746 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.039435  [    0/ 5482]\n",
      "loss: 0.029684  [ 1200/ 5482]\n",
      "loss: 0.065193  [ 2400/ 5482]\n",
      "loss: 0.017197  [ 3600/ 5482]\n",
      "loss: 0.026245  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.740     0.717    99\n",
      " disgust     0.690     0.804     0.804    107\n",
      "    fear     0.690     0.631     0.512    80\n",
      "   happy     0.690     0.545     0.623    77\n",
      " neutral     0.690     0.689     0.768    95\n",
      "     sad     0.690     0.781     0.626    91\n",
      "surprise     0.690     0.600     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.684     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.996594 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.045195  [    0/ 5482]\n",
      "loss: 0.035388  [ 1200/ 5482]\n",
      "loss: 0.141594  [ 2400/ 5482]\n",
      "loss: 0.050804  [ 3600/ 5482]\n",
      "loss: 0.045737  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.705     0.747    99\n",
      " disgust     0.690     0.802     0.757    107\n",
      "    fear     0.690     0.678     0.500    80\n",
      "   happy     0.690     0.568     0.649    77\n",
      " neutral     0.690     0.718     0.779    95\n",
      "     sad     0.690     0.757     0.615    91\n",
      "surprise     0.690     0.575     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.686     0.686    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.071190 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.049632  [    0/ 5482]\n",
      "loss: 0.090809  [ 1200/ 5482]\n",
      "loss: 0.035268  [ 2400/ 5482]\n",
      "loss: 0.351248  [ 3600/ 5482]\n",
      "loss: 0.047819  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.694     0.778    99\n",
      " disgust     0.695     0.804     0.766    107\n",
      "    fear     0.695     0.712     0.525    80\n",
      "   happy     0.695     0.570     0.636    77\n",
      " neutral     0.695     0.676     0.768    95\n",
      "     sad     0.695     0.763     0.637    91\n",
      "surprise     0.695     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.693     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.043792 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.037926  [    0/ 5482]\n",
      "loss: 0.056525  [ 1200/ 5482]\n",
      "loss: 0.034013  [ 2400/ 5482]\n",
      "loss: 0.046424  [ 3600/ 5482]\n",
      "loss: 0.044434  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.747     0.747    99\n",
      " disgust     0.693     0.778     0.785    107\n",
      "    fear     0.693     0.606     0.537    80\n",
      "   happy     0.693     0.544     0.636    77\n",
      " neutral     0.693     0.701     0.789    95\n",
      "     sad     0.693     0.803     0.670    91\n",
      "surprise     0.693     0.627     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.687     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.026622 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.042695  [    0/ 5482]\n",
      "loss: 0.055865  [ 1200/ 5482]\n",
      "loss: 0.040942  [ 2400/ 5482]\n",
      "loss: 0.095282  [ 3600/ 5482]\n",
      "loss: 0.043958  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.713     0.727    99\n",
      " disgust     0.682     0.779     0.757    107\n",
      "    fear     0.682     0.687     0.575    80\n",
      "   happy     0.682     0.539     0.623    77\n",
      " neutral     0.682     0.664     0.768    95\n",
      "     sad     0.682     0.775     0.604    91\n",
      "surprise     0.682     0.603     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.680     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 2.018432 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.036737  [    0/ 5482]\n",
      "loss: 0.029829  [ 1200/ 5482]\n",
      "loss: 0.013761  [ 2400/ 5482]\n",
      "loss: 0.034181  [ 3600/ 5482]\n",
      "loss: 0.031771  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.784     0.697    99\n",
      " disgust     0.700     0.785     0.785    107\n",
      "    fear     0.700     0.639     0.575    80\n",
      "   happy     0.700     0.551     0.636    77\n",
      " neutral     0.700     0.670     0.789    95\n",
      "     sad     0.700     0.772     0.670    91\n",
      "surprise     0.700     0.683     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.698     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 1.953002 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.047244  [    0/ 5482]\n",
      "loss: 0.054126  [ 1200/ 5482]\n",
      "loss: 0.028307  [ 2400/ 5482]\n",
      "loss: 0.041953  [ 3600/ 5482]\n",
      "loss: 0.033584  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.722     0.707    99\n",
      " disgust     0.689     0.764     0.757    107\n",
      "    fear     0.689     0.648     0.575    80\n",
      "   happy     0.689     0.618     0.610    77\n",
      " neutral     0.689     0.673     0.800    95\n",
      "     sad     0.689     0.720     0.648    91\n",
      "surprise     0.689     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.682     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.088111 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.042569  [    0/ 5482]\n",
      "loss: 0.020968  [ 1200/ 5482]\n",
      "loss: 0.100480  [ 2400/ 5482]\n",
      "loss: 0.027425  [ 3600/ 5482]\n",
      "loss: 0.034264  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.735     0.727    99\n",
      " disgust     0.672     0.769     0.748    107\n",
      "    fear     0.672     0.656     0.500    80\n",
      "   happy     0.672     0.557     0.571    77\n",
      " neutral     0.672     0.670     0.768    95\n",
      "     sad     0.672     0.713     0.626    91\n",
      "surprise     0.672     0.557     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.665     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 2.088273 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.071902  [    0/ 5482]\n",
      "loss: 0.043700  [ 1200/ 5482]\n",
      "loss: 0.054724  [ 2400/ 5482]\n",
      "loss: 0.029984  [ 3600/ 5482]\n",
      "loss: 0.016083  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.683     0.717    99\n",
      " disgust     0.692     0.818     0.757    107\n",
      "    fear     0.692     0.671     0.613    80\n",
      "   happy     0.692     0.590     0.597    77\n",
      " neutral     0.692     0.650     0.800    95\n",
      "     sad     0.692     0.720     0.648    91\n",
      "surprise     0.692     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.690     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.099415 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.027324  [    0/ 5482]\n",
      "loss: 0.038111  [ 1200/ 5482]\n",
      "loss: 0.042965  [ 2400/ 5482]\n",
      "loss: 0.095777  [ 3600/ 5482]\n",
      "loss: 0.033348  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.708     0.687    99\n",
      " disgust     0.692     0.822     0.776    107\n",
      "    fear     0.692     0.627     0.650    80\n",
      "   happy     0.692     0.568     0.597    77\n",
      " neutral     0.692     0.676     0.789    95\n",
      "     sad     0.692     0.711     0.648    91\n",
      "surprise     0.692     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.689     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.120225 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.033367  [    0/ 5482]\n",
      "loss: 0.040412  [ 1200/ 5482]\n",
      "loss: 0.037654  [ 2400/ 5482]\n",
      "loss: 0.054304  [ 3600/ 5482]\n",
      "loss: 0.049187  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.682     0.737    99\n",
      " disgust     0.684     0.763     0.813    107\n",
      "    fear     0.684     0.672     0.487    80\n",
      "   happy     0.684     0.490     0.649    77\n",
      " neutral     0.684     0.755     0.779    95\n",
      "     sad     0.684     0.818     0.593    91\n",
      "surprise     0.684     0.615     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.685     0.674    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 2.124265 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.326670  [    0/ 5482]\n",
      "loss: 0.081724  [ 1200/ 5482]\n",
      "loss: 0.044122  [ 2400/ 5482]\n",
      "loss: 0.042095  [ 3600/ 5482]\n",
      "loss: 0.033916  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.745     0.707    99\n",
      " disgust     0.698     0.824     0.785    107\n",
      "    fear     0.698     0.627     0.650    80\n",
      "   happy     0.698     0.593     0.623    77\n",
      " neutral     0.698     0.682     0.768    95\n",
      "     sad     0.698     0.698     0.659    91\n",
      "surprise     0.698     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.693     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 2.003371 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.052173  [    0/ 5482]\n",
      "loss: 0.037713  [ 1200/ 5482]\n",
      "loss: 0.047071  [ 2400/ 5482]\n",
      "loss: 0.053569  [ 3600/ 5482]\n",
      "loss: 0.020094  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.737     0.707    99\n",
      " disgust     0.703     0.796     0.804    107\n",
      "    fear     0.703     0.648     0.575    80\n",
      "   happy     0.703     0.613     0.597    77\n",
      " neutral     0.703     0.685     0.779    95\n",
      "     sad     0.703     0.741     0.692    91\n",
      "surprise     0.703     0.647     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.695     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 1.944046 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.036890  [    0/ 5482]\n",
      "loss: 0.029085  [ 1200/ 5482]\n",
      "loss: 0.024994  [ 2400/ 5482]\n",
      "loss: 0.043638  [ 3600/ 5482]\n",
      "loss: 0.031380  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.685     0.707     0.707    99\n",
      " disgust     0.685     0.783     0.776    107\n",
      "    fear     0.685     0.639     0.575    80\n",
      "   happy     0.685     0.535     0.597    77\n",
      " neutral     0.685     0.673     0.779    95\n",
      "     sad     0.685     0.772     0.670    91\n",
      "surprise     0.685     0.655     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.685     0.681     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 2.034786 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.036580  [    0/ 5482]\n",
      "loss: 0.300189  [ 1200/ 5482]\n",
      "loss: 0.033002  [ 2400/ 5482]\n",
      "loss: 0.021415  [ 3600/ 5482]\n",
      "loss: 0.036054  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.707     0.707    99\n",
      " disgust     0.689     0.748     0.804    107\n",
      "    fear     0.689     0.667     0.525    80\n",
      "   happy     0.689     0.530     0.571    77\n",
      " neutral     0.689     0.713     0.811    95\n",
      "     sad     0.689     0.766     0.648    91\n",
      "surprise     0.689     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.682     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.970193 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.058035  [    0/ 5482]\n",
      "loss: 0.026577  [ 1200/ 5482]\n",
      "loss: 0.029191  [ 2400/ 5482]\n",
      "loss: 0.069227  [ 3600/ 5482]\n",
      "loss: 0.061551  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.697     0.697    99\n",
      " disgust     0.687     0.764     0.785    107\n",
      "    fear     0.687     0.643     0.562    80\n",
      "   happy     0.687     0.512     0.558    77\n",
      " neutral     0.687     0.731     0.800    95\n",
      "     sad     0.687     0.756     0.681    91\n",
      "surprise     0.687     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.680     0.677    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.050988 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.087064  [    0/ 5482]\n",
      "loss: 0.018325  [ 1200/ 5482]\n",
      "loss: 0.027352  [ 2400/ 5482]\n",
      "loss: 0.080281  [ 3600/ 5482]\n",
      "loss: 0.033385  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.696     0.717    99\n",
      " disgust     0.693     0.808     0.748    107\n",
      "    fear     0.693     0.662     0.588    80\n",
      "   happy     0.693     0.580     0.610    77\n",
      " neutral     0.693     0.670     0.789    95\n",
      "     sad     0.693     0.747     0.681    91\n",
      "surprise     0.693     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.689     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.058926 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.037565  [    0/ 5482]\n",
      "loss: 0.032301  [ 1200/ 5482]\n",
      "loss: 0.021377  [ 2400/ 5482]\n",
      "loss: 0.030063  [ 3600/ 5482]\n",
      "loss: 0.017103  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.700     0.707    99\n",
      " disgust     0.695     0.823     0.738    107\n",
      "    fear     0.695     0.630     0.575    80\n",
      "   happy     0.695     0.575     0.649    77\n",
      " neutral     0.695     0.688     0.789    95\n",
      "     sad     0.695     0.772     0.670    91\n",
      "surprise     0.695     0.652     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.691     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.048545 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.037428  [    0/ 5482]\n",
      "loss: 0.029892  [ 1200/ 5482]\n",
      "loss: 0.033785  [ 2400/ 5482]\n",
      "loss: 0.060862  [ 3600/ 5482]\n",
      "loss: 0.023466  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.756     0.657    99\n",
      " disgust     0.689     0.823     0.738    107\n",
      "    fear     0.689     0.605     0.613    80\n",
      "   happy     0.689     0.526     0.662    77\n",
      " neutral     0.689     0.698     0.779    95\n",
      "     sad     0.689     0.756     0.681    91\n",
      "surprise     0.689     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.687     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.115403 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.066236  [    0/ 5482]\n",
      "loss: 0.037623  [ 1200/ 5482]\n",
      "loss: 0.020750  [ 2400/ 5482]\n",
      "loss: 0.021015  [ 3600/ 5482]\n",
      "loss: 0.042001  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.761     0.677    99\n",
      " disgust     0.698     0.806     0.776    107\n",
      "    fear     0.698     0.633     0.625    80\n",
      "   happy     0.698     0.547     0.675    77\n",
      " neutral     0.698     0.667     0.779    95\n",
      "     sad     0.698     0.750     0.659    91\n",
      "surprise     0.698     0.741     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.701     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 2.093529 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.020347  [    0/ 5482]\n",
      "loss: 0.021759  [ 1200/ 5482]\n",
      "loss: 0.021287  [ 2400/ 5482]\n",
      "loss: 0.038236  [ 3600/ 5482]\n",
      "loss: 0.008303  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.734     0.697    99\n",
      " disgust     0.697     0.783     0.776    107\n",
      "    fear     0.697     0.662     0.588    80\n",
      "   happy     0.697     0.544     0.636    77\n",
      " neutral     0.697     0.691     0.800    95\n",
      "     sad     0.697     0.744     0.637    91\n",
      "surprise     0.697     0.705     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.695     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.032973 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.017979  [    0/ 5482]\n",
      "loss: 0.013307  [ 1200/ 5482]\n",
      "loss: 0.026459  [ 2400/ 5482]\n",
      "loss: 0.032080  [ 3600/ 5482]\n",
      "loss: 0.036453  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.716     0.687    99\n",
      " disgust     0.687     0.818     0.757    107\n",
      "    fear     0.687     0.643     0.562    80\n",
      "   happy     0.687     0.510     0.688    77\n",
      " neutral     0.687     0.667     0.758    95\n",
      "     sad     0.687     0.797     0.604    91\n",
      "surprise     0.687     0.692     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.692     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.194306 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.029819  [    0/ 5482]\n",
      "loss: 0.024524  [ 1200/ 5482]\n",
      "loss: 0.035275  [ 2400/ 5482]\n",
      "loss: 0.010323  [ 3600/ 5482]\n",
      "loss: 0.072056  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.703     0.717    99\n",
      " disgust     0.687     0.773     0.794    107\n",
      "    fear     0.687     0.656     0.525    80\n",
      "   happy     0.687     0.522     0.623    77\n",
      " neutral     0.687     0.698     0.779    95\n",
      "     sad     0.687     0.766     0.648    91\n",
      "surprise     0.687     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.684     0.678    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.050751 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.063286  [    0/ 5482]\n",
      "loss: 0.018826  [ 1200/ 5482]\n",
      "loss: 0.023923  [ 2400/ 5482]\n",
      "loss: 0.022776  [ 3600/ 5482]\n",
      "loss: 0.039075  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.734     0.697    99\n",
      " disgust     0.693     0.785     0.785    107\n",
      "    fear     0.693     0.667     0.600    80\n",
      "   happy     0.693     0.533     0.623    77\n",
      " neutral     0.693     0.679     0.800    95\n",
      "     sad     0.693     0.781     0.626    91\n",
      "surprise     0.693     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.691     0.686    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.056845 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.023677  [    0/ 5482]\n",
      "loss: 0.051910  [ 1200/ 5482]\n",
      "loss: 0.031809  [ 2400/ 5482]\n",
      "loss: 0.142087  [ 3600/ 5482]\n",
      "loss: 0.064198  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.689     0.717    99\n",
      " disgust     0.689     0.771     0.785    107\n",
      "    fear     0.689     0.652     0.537    80\n",
      "   happy     0.689     0.538     0.636    77\n",
      " neutral     0.689     0.705     0.779    95\n",
      "     sad     0.689     0.779     0.659    91\n",
      "surprise     0.689     0.661     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.685     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.118807 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.011125  [    0/ 5482]\n",
      "loss: 0.114358  [ 1200/ 5482]\n",
      "loss: 0.027406  [ 2400/ 5482]\n",
      "loss: 0.037448  [ 3600/ 5482]\n",
      "loss: 0.025060  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.753     0.707    99\n",
      " disgust     0.693     0.806     0.776    107\n",
      "    fear     0.693     0.649     0.600    80\n",
      "   happy     0.693     0.532     0.649    77\n",
      " neutral     0.693     0.673     0.758    95\n",
      "     sad     0.693     0.728     0.648    91\n",
      "surprise     0.693     0.707     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.692     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.093307 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.016311  [    0/ 5482]\n",
      "loss: 0.016813  [ 1200/ 5482]\n",
      "loss: 0.026611  [ 2400/ 5482]\n",
      "loss: 0.033442  [ 3600/ 5482]\n",
      "loss: 0.024159  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.729     0.707    99\n",
      " disgust     0.700     0.814     0.776    107\n",
      "    fear     0.700     0.653     0.613    80\n",
      "   happy     0.700     0.617     0.649    77\n",
      " neutral     0.700     0.670     0.768    95\n",
      "     sad     0.700     0.705     0.681    91\n",
      "surprise     0.700     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.695     0.693    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.092717 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.021020  [    0/ 5482]\n",
      "loss: 0.029772  [ 1200/ 5482]\n",
      "loss: 0.018339  [ 2400/ 5482]\n",
      "loss: 0.024204  [ 3600/ 5482]\n",
      "loss: 0.011163  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.718     0.747    99\n",
      " disgust     0.695     0.778     0.785    107\n",
      "    fear     0.695     0.677     0.525    80\n",
      "   happy     0.695     0.537     0.662    77\n",
      " neutral     0.695     0.695     0.768    95\n",
      "     sad     0.695     0.756     0.648    91\n",
      "surprise     0.695     0.695     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.694     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.123154 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.047842  [    0/ 5482]\n",
      "loss: 0.019467  [ 1200/ 5482]\n",
      "loss: 0.027886  [ 2400/ 5482]\n",
      "loss: 0.022766  [ 3600/ 5482]\n",
      "loss: 0.037983  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.713     0.713     0.727    99\n",
      " disgust     0.713     0.812     0.766    107\n",
      "    fear     0.713     0.712     0.650    80\n",
      "   happy     0.713     0.576     0.636    77\n",
      " neutral     0.713     0.679     0.779    95\n",
      "     sad     0.713     0.747     0.681    91\n",
      "surprise     0.713     0.759     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.713     0.714     0.709    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 2.043191 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.022256  [    0/ 5482]\n",
      "loss: 0.020113  [ 1200/ 5482]\n",
      "loss: 0.044283  [ 2400/ 5482]\n",
      "loss: 0.043099  [ 3600/ 5482]\n",
      "loss: 0.027079  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.719     0.697    99\n",
      " disgust     0.695     0.785     0.785    107\n",
      "    fear     0.695     0.690     0.613    80\n",
      "   happy     0.695     0.515     0.662    77\n",
      " neutral     0.695     0.695     0.768    95\n",
      "     sad     0.695     0.787     0.648    91\n",
      "surprise     0.695     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.696     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.111484 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.033189  [    0/ 5482]\n",
      "loss: 0.017738  [ 1200/ 5482]\n",
      "loss: 0.016937  [ 2400/ 5482]\n",
      "loss: 0.029433  [ 3600/ 5482]\n",
      "loss: 0.022787  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.711     0.697    99\n",
      " disgust     0.700     0.820     0.766    107\n",
      "    fear     0.700     0.691     0.588    80\n",
      "   happy     0.700     0.535     0.688    77\n",
      " neutral     0.700     0.695     0.768    95\n",
      "     sad     0.700     0.756     0.681    91\n",
      "surprise     0.700     0.695     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.701     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.136299 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.028299  [    0/ 5482]\n",
      "loss: 0.014846  [ 1200/ 5482]\n",
      "loss: 0.021913  [ 2400/ 5482]\n",
      "loss: 0.023622  [ 3600/ 5482]\n",
      "loss: 0.029693  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.728     0.677    99\n",
      " disgust     0.697     0.806     0.776    107\n",
      "    fear     0.697     0.676     0.625    80\n",
      "   happy     0.697     0.516     0.636    77\n",
      " neutral     0.697     0.737     0.768    95\n",
      "     sad     0.697     0.714     0.659    91\n",
      "surprise     0.697     0.683     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.694     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.112092 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.024387  [    0/ 5482]\n",
      "loss: 0.026069  [ 1200/ 5482]\n",
      "loss: 0.020151  [ 2400/ 5482]\n",
      "loss: 0.032991  [ 3600/ 5482]\n",
      "loss: 0.042057  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.737     0.707    99\n",
      " disgust     0.692     0.777     0.813    107\n",
      "    fear     0.692     0.632     0.537    80\n",
      "   happy     0.692     0.558     0.623    77\n",
      " neutral     0.692     0.723     0.768    95\n",
      "     sad     0.692     0.753     0.604    91\n",
      "surprise     0.692     0.613     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.685     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.132278 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.019305  [    0/ 5482]\n",
      "loss: 0.049070  [ 1200/ 5482]\n",
      "loss: 0.041011  [ 2400/ 5482]\n",
      "loss: 0.034445  [ 3600/ 5482]\n",
      "loss: 0.051992  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.683     0.717    99\n",
      " disgust     0.695     0.794     0.757    107\n",
      "    fear     0.695     0.671     0.613    80\n",
      "   happy     0.695     0.593     0.623    77\n",
      " neutral     0.695     0.681     0.811    95\n",
      "     sad     0.695     0.738     0.648    91\n",
      "surprise     0.695     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.692     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.149666 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.020110  [    0/ 5482]\n",
      "loss: 0.024399  [ 1200/ 5482]\n",
      "loss: 0.011963  [ 2400/ 5482]\n",
      "loss: 0.019338  [ 3600/ 5482]\n",
      "loss: 0.020397  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.707     0.707    99\n",
      " disgust     0.695     0.765     0.822    107\n",
      "    fear     0.695     0.640     0.600    80\n",
      "   happy     0.695     0.545     0.623    77\n",
      " neutral     0.695     0.733     0.779    95\n",
      "     sad     0.695     0.792     0.626    91\n",
      "surprise     0.695     0.650     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.690     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.142251 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.022727  [    0/ 5482]\n",
      "loss: 0.024105  [ 1200/ 5482]\n",
      "loss: 0.016099  [ 2400/ 5482]\n",
      "loss: 0.032811  [ 3600/ 5482]\n",
      "loss: 0.032218  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.742     0.697    99\n",
      " disgust     0.693     0.773     0.794    107\n",
      "    fear     0.693     0.639     0.575    80\n",
      "   happy     0.693     0.521     0.649    77\n",
      " neutral     0.693     0.714     0.789    95\n",
      "     sad     0.693     0.769     0.659    91\n",
      "surprise     0.693     0.679     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.691     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.203123 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.023884  [    0/ 5482]\n",
      "loss: 0.023497  [ 1200/ 5482]\n",
      "loss: 0.008235  [ 2400/ 5482]\n",
      "loss: 0.023062  [ 3600/ 5482]\n",
      "loss: 0.019907  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.700     0.707    99\n",
      " disgust     0.693     0.769     0.776    107\n",
      "    fear     0.693     0.662     0.588    80\n",
      "   happy     0.693     0.598     0.636    77\n",
      " neutral     0.693     0.709     0.768    95\n",
      "     sad     0.693     0.747     0.648    91\n",
      "surprise     0.693     0.627     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.687     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.183388 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.016628  [    0/ 5482]\n",
      "loss: 0.029019  [ 1200/ 5482]\n",
      "loss: 0.021412  [ 2400/ 5482]\n",
      "loss: 0.022379  [ 3600/ 5482]\n",
      "loss: 0.037490  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.708     0.742     0.697    99\n",
      " disgust     0.708     0.785     0.785    107\n",
      "    fear     0.708     0.662     0.662    80\n",
      "   happy     0.708     0.593     0.623    77\n",
      " neutral     0.708     0.694     0.789    95\n",
      "     sad     0.708     0.741     0.659    91\n",
      "surprise     0.708     0.717     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.708     0.705     0.703    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 2.218331 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.021656  [    0/ 5482]\n",
      "loss: 0.030388  [ 1200/ 5482]\n",
      "loss: 0.122106  [ 2400/ 5482]\n",
      "loss: 0.011784  [ 3600/ 5482]\n",
      "loss: 0.027963  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.758     0.697    99\n",
      " disgust     0.702     0.783     0.776    107\n",
      "    fear     0.702     0.676     0.625    80\n",
      "   happy     0.702     0.593     0.623    77\n",
      " neutral     0.702     0.673     0.779    95\n",
      "     sad     0.702     0.732     0.659    91\n",
      "surprise     0.702     0.667     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.697     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.255111 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.033317  [    0/ 5482]\n",
      "loss: 0.020192  [ 1200/ 5482]\n",
      "loss: 0.025134  [ 2400/ 5482]\n",
      "loss: 0.055462  [ 3600/ 5482]\n",
      "loss: 0.018104  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.782     0.687    99\n",
      " disgust     0.687     0.798     0.776    107\n",
      "    fear     0.687     0.636     0.525    80\n",
      "   happy     0.687     0.481     0.662    77\n",
      " neutral     0.687     0.723     0.768    95\n",
      "     sad     0.687     0.740     0.626    91\n",
      "surprise     0.687     0.652     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.687     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.278884 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.014730  [    0/ 5482]\n",
      "loss: 0.017987  [ 1200/ 5482]\n",
      "loss: 0.022335  [ 2400/ 5482]\n",
      "loss: 0.017800  [ 3600/ 5482]\n",
      "loss: 0.018559  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.708     0.727     0.727    99\n",
      " disgust     0.708     0.778     0.785    107\n",
      "    fear     0.708     0.681     0.613    80\n",
      "   happy     0.708     0.561     0.597    77\n",
      " neutral     0.708     0.706     0.811    95\n",
      "     sad     0.708     0.713     0.681    91\n",
      "surprise     0.708     0.792     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.708     0.708     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 2.173968 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.029867  [    0/ 5482]\n",
      "loss: 0.031558  [ 1200/ 5482]\n",
      "loss: 0.027140  [ 2400/ 5482]\n",
      "loss: 0.018183  [ 3600/ 5482]\n",
      "loss: 0.028382  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.682     0.737    99\n",
      " disgust     0.692     0.778     0.785    107\n",
      "    fear     0.692     0.656     0.500    80\n",
      "   happy     0.692     0.558     0.623    77\n",
      " neutral     0.692     0.692     0.779    95\n",
      "     sad     0.692     0.744     0.670    91\n",
      "surprise     0.692     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.689     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.241064 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.034978  [    0/ 5482]\n",
      "loss: 0.014465  [ 1200/ 5482]\n",
      "loss: 0.032610  [ 2400/ 5482]\n",
      "loss: 0.027661  [ 3600/ 5482]\n",
      "loss: 0.015026  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.783     0.657    99\n",
      " disgust     0.689     0.780     0.794    107\n",
      "    fear     0.689     0.632     0.600    80\n",
      "   happy     0.689     0.548     0.662    77\n",
      " neutral     0.689     0.702     0.768    95\n",
      "     sad     0.689     0.733     0.604    91\n",
      "surprise     0.689     0.614     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.685     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.288441 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.045542  [    0/ 5482]\n",
      "loss: 0.017094  [ 1200/ 5482]\n",
      "loss: 0.026549  [ 2400/ 5482]\n",
      "loss: 0.023143  [ 3600/ 5482]\n",
      "loss: 0.019231  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.688     0.667    99\n",
      " disgust     0.669     0.775     0.738    107\n",
      "    fear     0.669     0.641     0.512    80\n",
      "   happy     0.669     0.500     0.623    77\n",
      " neutral     0.669     0.695     0.768    95\n",
      "     sad     0.669     0.702     0.648    91\n",
      "surprise     0.669     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.667     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 2.352422 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.019265  [    0/ 5482]\n",
      "loss: 0.018734  [ 1200/ 5482]\n",
      "loss: 0.022386  [ 2400/ 5482]\n",
      "loss: 0.021295  [ 3600/ 5482]\n",
      "loss: 0.021632  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.673     0.687    99\n",
      " disgust     0.675     0.781     0.766    107\n",
      "    fear     0.675     0.667     0.550    80\n",
      "   happy     0.675     0.535     0.597    77\n",
      " neutral     0.675     0.685     0.800    95\n",
      "     sad     0.675     0.700     0.615    91\n",
      "surprise     0.675     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.671     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 2.334149 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.013568  [    0/ 5482]\n",
      "loss: 0.009508  [ 1200/ 5482]\n",
      "loss: 0.018504  [ 2400/ 5482]\n",
      "loss: 0.018643  [ 3600/ 5482]\n",
      "loss: 0.020548  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.762     0.646    99\n",
      " disgust     0.687     0.778     0.785    107\n",
      "    fear     0.687     0.671     0.588    80\n",
      "   happy     0.687     0.578     0.623    77\n",
      " neutral     0.687     0.702     0.768    95\n",
      "     sad     0.687     0.699     0.637    91\n",
      "surprise     0.687     0.577     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.681     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.241015 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.019161  [    0/ 5482]\n",
      "loss: 0.017697  [ 1200/ 5482]\n",
      "loss: 0.022475  [ 2400/ 5482]\n",
      "loss: 0.073710  [ 3600/ 5482]\n",
      "loss: 0.029665  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.676     0.697    99\n",
      " disgust     0.690     0.769     0.776    107\n",
      "    fear     0.690     0.676     0.575    80\n",
      "   happy     0.690     0.595     0.571    77\n",
      " neutral     0.690     0.667     0.800    95\n",
      "     sad     0.690     0.762     0.670    91\n",
      "surprise     0.690     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.686     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.270143 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.035874  [    0/ 5482]\n",
      "loss: 0.023151  [ 1200/ 5482]\n",
      "loss: 0.020245  [ 2400/ 5482]\n",
      "loss: 0.019584  [ 3600/ 5482]\n",
      "loss: 0.030435  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.691     0.677    99\n",
      " disgust     0.689     0.769     0.776    107\n",
      "    fear     0.689     0.697     0.575    80\n",
      "   happy     0.689     0.597     0.597    77\n",
      " neutral     0.689     0.644     0.800    95\n",
      "     sad     0.689     0.726     0.670    91\n",
      "surprise     0.689     0.683     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.687     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.244451 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.015850  [    0/ 5482]\n",
      "loss: 0.015414  [ 1200/ 5482]\n",
      "loss: 0.057091  [ 2400/ 5482]\n",
      "loss: 0.012195  [ 3600/ 5482]\n",
      "loss: 0.029017  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.726     0.697    99\n",
      " disgust     0.690     0.788     0.766    107\n",
      "    fear     0.690     0.646     0.525    80\n",
      "   happy     0.690     0.544     0.636    77\n",
      " neutral     0.690     0.714     0.789    95\n",
      "     sad     0.690     0.747     0.648    91\n",
      "surprise     0.690     0.625     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.684     0.686    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.105543 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.027468  [    0/ 5482]\n",
      "loss: 0.021437  [ 1200/ 5482]\n",
      "loss: 0.024390  [ 2400/ 5482]\n",
      "loss: 0.017580  [ 3600/ 5482]\n",
      "loss: 0.016513  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.731     0.687    99\n",
      " disgust     0.693     0.776     0.776    107\n",
      "    fear     0.693     0.622     0.575    80\n",
      "   happy     0.693     0.600     0.623    77\n",
      " neutral     0.693     0.681     0.811    95\n",
      "     sad     0.693     0.784     0.637    91\n",
      "surprise     0.693     0.623     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.688     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.181165 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.016382  [    0/ 5482]\n",
      "loss: 0.106312  [ 1200/ 5482]\n",
      "loss: 0.044168  [ 2400/ 5482]\n",
      "loss: 0.025981  [ 3600/ 5482]\n",
      "loss: 0.012466  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.764     0.687    99\n",
      " disgust     0.697     0.746     0.794    107\n",
      "    fear     0.697     0.652     0.537    80\n",
      "   happy     0.697     0.527     0.636    77\n",
      " neutral     0.697     0.740     0.779    95\n",
      "     sad     0.697     0.782     0.670    91\n",
      "surprise     0.697     0.643     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.693     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.208574 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.022199  [    0/ 5482]\n",
      "loss: 0.013010  [ 1200/ 5482]\n",
      "loss: 0.021175  [ 2400/ 5482]\n",
      "loss: 0.016146  [ 3600/ 5482]\n",
      "loss: 0.023050  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.699     0.727    99\n",
      " disgust     0.700     0.798     0.776    107\n",
      "    fear     0.700     0.733     0.550    80\n",
      "   happy     0.700     0.533     0.623    77\n",
      " neutral     0.700     0.708     0.789    95\n",
      "     sad     0.700     0.762     0.670    91\n",
      "surprise     0.700     0.657     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.699     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.211724 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.024477  [    0/ 5482]\n",
      "loss: 0.015866  [ 1200/ 5482]\n",
      "loss: 0.023233  [ 2400/ 5482]\n",
      "loss: 0.028312  [ 3600/ 5482]\n",
      "loss: 0.021916  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.685     0.747    99\n",
      " disgust     0.703     0.798     0.776    107\n",
      "    fear     0.703     0.690     0.613    80\n",
      "   happy     0.703     0.589     0.558    77\n",
      " neutral     0.703     0.661     0.800    95\n",
      "     sad     0.703     0.775     0.681    91\n",
      "surprise     0.703     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.701     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.224684 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.029848  [    0/ 5482]\n",
      "loss: 0.037398  [ 1200/ 5482]\n",
      "loss: 0.026263  [ 2400/ 5482]\n",
      "loss: 0.010949  [ 3600/ 5482]\n",
      "loss: 0.022013  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.726     0.697    99\n",
      " disgust     0.705     0.806     0.776    107\n",
      "    fear     0.705     0.662     0.637    80\n",
      "   happy     0.705     0.605     0.597    77\n",
      " neutral     0.705     0.682     0.789    95\n",
      "     sad     0.705     0.738     0.681    91\n",
      "surprise     0.705     0.677     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.700     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.208981 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.014841  [    0/ 5482]\n",
      "loss: 0.032480  [ 1200/ 5482]\n",
      "loss: 0.017060  [ 2400/ 5482]\n",
      "loss: 0.021805  [ 3600/ 5482]\n",
      "loss: 0.010449  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.711     0.798     0.677    99\n",
      " disgust     0.711     0.824     0.785    107\n",
      "    fear     0.711     0.679     0.662    80\n",
      "   happy     0.711     0.558     0.623    77\n",
      " neutral     0.711     0.679     0.779    95\n",
      "     sad     0.711     0.727     0.703    91\n",
      "surprise     0.711     0.698     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.711     0.709     0.707    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.203368 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.017632  [    0/ 5482]\n",
      "loss: 0.018652  [ 1200/ 5482]\n",
      "loss: 0.035046  [ 2400/ 5482]\n",
      "loss: 0.012608  [ 3600/ 5482]\n",
      "loss: 0.014122  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.707     0.707    99\n",
      " disgust     0.695     0.776     0.776    107\n",
      "    fear     0.695     0.723     0.588    80\n",
      "   happy     0.695     0.562     0.584    77\n",
      " neutral     0.695     0.673     0.800    95\n",
      "     sad     0.695     0.735     0.670    91\n",
      "surprise     0.695     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.692     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.251958 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.031964  [    0/ 5482]\n",
      "loss: 0.013042  [ 1200/ 5482]\n",
      "loss: 0.020941  [ 2400/ 5482]\n",
      "loss: 0.021770  [ 3600/ 5482]\n",
      "loss: 0.025323  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.701     0.687    99\n",
      " disgust     0.695     0.796     0.766    107\n",
      "    fear     0.695     0.723     0.588    80\n",
      "   happy     0.695     0.605     0.597    77\n",
      " neutral     0.695     0.697     0.800    95\n",
      "     sad     0.695     0.698     0.659    91\n",
      "surprise     0.695     0.608     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.690     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.283420 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.016330  [    0/ 5482]\n",
      "loss: 0.027662  [ 1200/ 5482]\n",
      "loss: 0.021696  [ 2400/ 5482]\n",
      "loss: 0.015399  [ 3600/ 5482]\n",
      "loss: 0.016221  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.701     0.687    99\n",
      " disgust     0.693     0.746     0.794    107\n",
      "    fear     0.693     0.710     0.613    80\n",
      "   happy     0.693     0.603     0.571    77\n",
      " neutral     0.693     0.692     0.779    95\n",
      "     sad     0.693     0.723     0.659    91\n",
      "surprise     0.693     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.688     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.275355 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.016344  [    0/ 5482]\n",
      "loss: 0.019057  [ 1200/ 5482]\n",
      "loss: 0.013476  [ 2400/ 5482]\n",
      "loss: 0.031663  [ 3600/ 5482]\n",
      "loss: 0.033479  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.737     0.737    99\n",
      " disgust     0.702     0.771     0.785    107\n",
      "    fear     0.702     0.677     0.525    80\n",
      "   happy     0.702     0.553     0.610    77\n",
      " neutral     0.702     0.679     0.800    95\n",
      "     sad     0.702     0.790     0.703    91\n",
      "surprise     0.702     0.677     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.698     0.693    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.246513 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.011245  [    0/ 5482]\n",
      "loss: 0.024119  [ 1200/ 5482]\n",
      "loss: 0.017725  [ 2400/ 5482]\n",
      "loss: 0.012192  [ 3600/ 5482]\n",
      "loss: 0.020623  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.753     0.707    99\n",
      " disgust     0.705     0.796     0.766    107\n",
      "    fear     0.705     0.653     0.588    80\n",
      "   happy     0.705     0.567     0.662    77\n",
      " neutral     0.705     0.685     0.800    95\n",
      "     sad     0.705     0.765     0.681    91\n",
      "surprise     0.705     0.700     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.703     0.699    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.211592 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.026986  [    0/ 5482]\n",
      "loss: 0.022477  [ 1200/ 5482]\n",
      "loss: 0.017666  [ 2400/ 5482]\n",
      "loss: 0.017813  [ 3600/ 5482]\n",
      "loss: 0.021538  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.760     0.737    99\n",
      " disgust     0.693     0.722     0.776    107\n",
      "    fear     0.693     0.616     0.562    80\n",
      "   happy     0.693     0.644     0.610    77\n",
      " neutral     0.693     0.653     0.811    95\n",
      "     sad     0.693     0.763     0.637    91\n",
      "surprise     0.693     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.691     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.285069 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.013985  [    0/ 5482]\n",
      "loss: 0.016723  [ 1200/ 5482]\n",
      "loss: 0.016050  [ 2400/ 5482]\n",
      "loss: 0.017990  [ 3600/ 5482]\n",
      "loss: 0.015687  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.795     0.667    99\n",
      " disgust     0.697     0.750     0.757    107\n",
      "    fear     0.697     0.662     0.637    80\n",
      "   happy     0.697     0.588     0.649    77\n",
      " neutral     0.697     0.645     0.821    95\n",
      "     sad     0.697     0.718     0.670    91\n",
      "surprise     0.697     0.745     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.700     0.689    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.253832 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.013004  [    0/ 5482]\n",
      "loss: 0.015762  [ 1200/ 5482]\n",
      "loss: 0.018351  [ 2400/ 5482]\n",
      "loss: 0.015530  [ 3600/ 5482]\n",
      "loss: 0.065615  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.744     0.677    99\n",
      " disgust     0.692     0.788     0.766    107\n",
      "    fear     0.692     0.692     0.562    80\n",
      "   happy     0.692     0.568     0.649    77\n",
      " neutral     0.692     0.670     0.768    95\n",
      "     sad     0.692     0.741     0.659    91\n",
      "surprise     0.692     0.616     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.689     0.689    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.298284 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.051681  [    0/ 5482]\n",
      "loss: 0.027639  [ 1200/ 5482]\n",
      "loss: 0.016615  [ 2400/ 5482]\n",
      "loss: 0.029061  [ 3600/ 5482]\n",
      "loss: 0.010658  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.773     0.687    99\n",
      " disgust     0.693     0.798     0.776    107\n",
      "    fear     0.693     0.628     0.613    80\n",
      "   happy     0.693     0.540     0.610    77\n",
      " neutral     0.693     0.682     0.789    95\n",
      "     sad     0.693     0.726     0.670    91\n",
      "surprise     0.693     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.689     0.686    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.211890 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.036471  [    0/ 5482]\n",
      "loss: 0.010600  [ 1200/ 5482]\n",
      "loss: 0.016606  [ 2400/ 5482]\n",
      "loss: 0.042250  [ 3600/ 5482]\n",
      "loss: 0.020537  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.755     0.717    99\n",
      " disgust     0.702     0.830     0.776    107\n",
      "    fear     0.702     0.644     0.588    80\n",
      "   happy     0.702     0.551     0.636    77\n",
      " neutral     0.702     0.673     0.779    95\n",
      "     sad     0.702     0.744     0.670    91\n",
      "surprise     0.702     0.694     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.699     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.173021 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.019750  [    0/ 5482]\n",
      "loss: 0.016801  [ 1200/ 5482]\n",
      "loss: 0.014653  [ 2400/ 5482]\n",
      "loss: 0.016007  [ 3600/ 5482]\n",
      "loss: 0.024077  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.714     0.758    99\n",
      " disgust     0.692     0.779     0.757    107\n",
      "    fear     0.692     0.712     0.588    80\n",
      "   happy     0.692     0.573     0.610    77\n",
      " neutral     0.692     0.630     0.789    95\n",
      "     sad     0.692     0.753     0.670    91\n",
      "surprise     0.692     0.679     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.692     0.680    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.242072 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.032122  [    0/ 5482]\n",
      "loss: 0.011631  [ 1200/ 5482]\n",
      "loss: 0.027746  [ 2400/ 5482]\n",
      "loss: 0.013363  [ 3600/ 5482]\n",
      "loss: 0.016421  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.725     0.747    99\n",
      " disgust     0.703     0.828     0.766    107\n",
      "    fear     0.703     0.691     0.588    80\n",
      "   happy     0.703     0.563     0.636    77\n",
      " neutral     0.703     0.670     0.789    95\n",
      "     sad     0.703     0.714     0.659    91\n",
      "surprise     0.703     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.702     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.219691 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.029985  [    0/ 5482]\n",
      "loss: 0.015715  [ 1200/ 5482]\n",
      "loss: 0.010984  [ 2400/ 5482]\n",
      "loss: 0.013208  [ 3600/ 5482]\n",
      "loss: 0.009896  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.695     0.737    99\n",
      " disgust     0.697     0.806     0.776    107\n",
      "    fear     0.697     0.677     0.525    80\n",
      "   happy     0.697     0.568     0.649    77\n",
      " neutral     0.697     0.679     0.779    95\n",
      "     sad     0.697     0.720     0.648    91\n",
      "surprise     0.697     0.721     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.695     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.230116 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.017683  [    0/ 5482]\n",
      "loss: 0.016772  [ 1200/ 5482]\n",
      "loss: 0.016404  [ 2400/ 5482]\n",
      "loss: 0.015371  [ 3600/ 5482]\n",
      "loss: 0.015978  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.713     0.727    99\n",
      " disgust     0.700     0.808     0.748    107\n",
      "    fear     0.700     0.691     0.588    80\n",
      "   happy     0.700     0.588     0.649    77\n",
      " neutral     0.700     0.655     0.800    95\n",
      "     sad     0.700     0.726     0.670    91\n",
      "surprise     0.700     0.719     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.700     0.693    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.243307 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.023488  [    0/ 5482]\n",
      "loss: 0.046559  [ 1200/ 5482]\n",
      "loss: 0.017084  [ 2400/ 5482]\n",
      "loss: 0.059077  [ 3600/ 5482]\n",
      "loss: 0.023003  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.714     0.758    99\n",
      " disgust     0.697     0.786     0.757    107\n",
      "    fear     0.697     0.657     0.550    80\n",
      "   happy     0.697     0.563     0.636    77\n",
      " neutral     0.697     0.673     0.800    95\n",
      "     sad     0.697     0.759     0.659    91\n",
      "surprise     0.697     0.714     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.695     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.228395 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.017725  [    0/ 5482]\n",
      "loss: 0.018706  [ 1200/ 5482]\n",
      "loss: 0.012758  [ 2400/ 5482]\n",
      "loss: 0.012733  [ 3600/ 5482]\n",
      "loss: 0.014377  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.716     0.737    99\n",
      " disgust     0.692     0.776     0.776    107\n",
      "    fear     0.692     0.656     0.500    80\n",
      "   happy     0.692     0.573     0.610    77\n",
      " neutral     0.692     0.688     0.789    95\n",
      "     sad     0.692     0.744     0.670    91\n",
      "surprise     0.692     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.685     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.246043 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.023833  [    0/ 5482]\n",
      "loss: 0.021956  [ 1200/ 5482]\n",
      "loss: 0.015243  [ 2400/ 5482]\n",
      "loss: 0.008891  [ 3600/ 5482]\n",
      "loss: 0.017269  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.708     0.758     0.697    99\n",
      " disgust     0.708     0.798     0.776    107\n",
      "    fear     0.708     0.645     0.613    80\n",
      "   happy     0.708     0.627     0.610    77\n",
      " neutral     0.708     0.691     0.800    95\n",
      "     sad     0.708     0.727     0.703    91\n",
      "surprise     0.708     0.667     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.708     0.702     0.703    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 2.192884 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.023629  [    0/ 5482]\n",
      "loss: 0.083113  [ 1200/ 5482]\n",
      "loss: 0.011212  [ 2400/ 5482]\n",
      "loss: 0.015106  [ 3600/ 5482]\n",
      "loss: 0.024430  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.711     0.720     0.727    99\n",
      " disgust     0.711     0.781     0.766    107\n",
      "    fear     0.711     0.691     0.588    80\n",
      "   happy     0.711     0.657     0.597    77\n",
      " neutral     0.711     0.694     0.811    95\n",
      "     sad     0.711     0.744     0.703    91\n",
      "surprise     0.711     0.657     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.711     0.706     0.707    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 2.219149 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.020182  [    0/ 5482]\n",
      "loss: 0.021168  [ 1200/ 5482]\n",
      "loss: 0.013875  [ 2400/ 5482]\n",
      "loss: 0.005011  [ 3600/ 5482]\n",
      "loss: 0.021623  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.707     0.722     0.707    99\n",
      " disgust     0.707     0.787     0.794    107\n",
      "    fear     0.707     0.644     0.588    80\n",
      "   happy     0.707     0.618     0.610    77\n",
      " neutral     0.707     0.710     0.800    95\n",
      "     sad     0.707     0.747     0.714    91\n",
      "surprise     0.707     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.707     0.699     0.698    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 2.279808 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.022844  [    0/ 5482]\n",
      "loss: 0.024319  [ 1200/ 5482]\n",
      "loss: 0.010941  [ 2400/ 5482]\n",
      "loss: 0.019243  [ 3600/ 5482]\n",
      "loss: 0.021069  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.704     0.768    99\n",
      " disgust     0.703     0.808     0.785    107\n",
      "    fear     0.703     0.667     0.550    80\n",
      "   happy     0.703     0.603     0.610    77\n",
      " neutral     0.703     0.697     0.800    95\n",
      "     sad     0.703     0.719     0.703    91\n",
      "surprise     0.703     0.679     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.697     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.221960 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.017004  [    0/ 5482]\n",
      "loss: 0.007739  [ 1200/ 5482]\n",
      "loss: 0.022308  [ 2400/ 5482]\n",
      "loss: 0.015719  [ 3600/ 5482]\n",
      "loss: 0.015194  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.737     0.707    99\n",
      " disgust     0.700     0.773     0.794    107\n",
      "    fear     0.700     0.652     0.562    80\n",
      "   happy     0.700     0.595     0.610    77\n",
      " neutral     0.700     0.738     0.800    95\n",
      "     sad     0.700     0.779     0.659    91\n",
      "surprise     0.700     0.571     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.692     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.236848 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.011734  [    0/ 5482]\n",
      "loss: 0.014666  [ 1200/ 5482]\n",
      "loss: 0.013030  [ 2400/ 5482]\n",
      "loss: 0.014792  [ 3600/ 5482]\n",
      "loss: 0.010275  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.708     0.710     0.717    99\n",
      " disgust     0.708     0.802     0.794    107\n",
      "    fear     0.708     0.677     0.550    80\n",
      "   happy     0.708     0.617     0.649    77\n",
      " neutral     0.708     0.701     0.789    95\n",
      "     sad     0.708     0.782     0.670    91\n",
      "surprise     0.708     0.630     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.708     0.703     0.704    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 2.262300 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.010323  [    0/ 5482]\n",
      "loss: 0.014036  [ 1200/ 5482]\n",
      "loss: 0.075610  [ 2400/ 5482]\n",
      "loss: 0.011651  [ 3600/ 5482]\n",
      "loss: 0.021404  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.720     0.677    99\n",
      " disgust     0.705     0.782     0.804    107\n",
      "    fear     0.705     0.667     0.600    80\n",
      "   happy     0.705     0.570     0.636    77\n",
      " neutral     0.705     0.724     0.800    95\n",
      "     sad     0.705     0.756     0.648    91\n",
      "surprise     0.705     0.682     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.700     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.192866 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.016374  [    0/ 5482]\n",
      "loss: 0.014593  [ 1200/ 5482]\n",
      "loss: 0.023006  [ 2400/ 5482]\n",
      "loss: 0.013699  [ 3600/ 5482]\n",
      "loss: 0.215501  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.718     0.750     0.758    99\n",
      " disgust     0.718     0.750     0.785    107\n",
      "    fear     0.718     0.675     0.650    80\n",
      "   happy     0.718     0.652     0.584    77\n",
      " neutral     0.718     0.684     0.842    95\n",
      "     sad     0.718     0.759     0.659    91\n",
      "surprise     0.718     0.750     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.718     0.717     0.710    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 2.173833 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep374_acc_72.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep293_acc_71\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR8/Run_Nr_0/conv/emo_reco_best_ep374_acc_72\"! Old accuracy: 71.5, new accuracy: 71.8\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.015234  [    0/ 5482]\n",
      "loss: 0.014153  [ 1200/ 5482]\n",
      "loss: 0.016425  [ 2400/ 5482]\n",
      "loss: 0.015110  [ 3600/ 5482]\n",
      "loss: 0.018186  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.718     0.747    99\n",
      " disgust     0.695     0.716     0.776    107\n",
      "    fear     0.695     0.667     0.625    80\n",
      "   happy     0.695     0.622     0.597    77\n",
      " neutral     0.695     0.718     0.779    95\n",
      "     sad     0.695     0.690     0.637    91\n",
      "surprise     0.695     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.691     0.686    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.161035 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.009683  [    0/ 5482]\n",
      "loss: 0.022320  [ 1200/ 5482]\n",
      "loss: 0.019012  [ 2400/ 5482]\n",
      "loss: 0.019825  [ 3600/ 5482]\n",
      "loss: 0.017942  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.737     0.737    99\n",
      " disgust     0.700     0.802     0.794    107\n",
      "    fear     0.700     0.672     0.562    80\n",
      "   happy     0.700     0.556     0.584    77\n",
      " neutral     0.700     0.701     0.789    95\n",
      "     sad     0.700     0.750     0.626    91\n",
      "surprise     0.700     0.635     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.693     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.255741 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.022022  [    0/ 5482]\n",
      "loss: 0.011820  [ 1200/ 5482]\n",
      "loss: 0.016445  [ 2400/ 5482]\n",
      "loss: 0.009100  [ 3600/ 5482]\n",
      "loss: 0.014801  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.724     0.717    99\n",
      " disgust     0.682     0.781     0.766    107\n",
      "    fear     0.682     0.662     0.537    80\n",
      "   happy     0.682     0.523     0.597    77\n",
      " neutral     0.682     0.688     0.789    95\n",
      "     sad     0.682     0.702     0.648    91\n",
      "surprise     0.682     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.677     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 2.275731 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.011324  [    0/ 5482]\n",
      "loss: 0.013693  [ 1200/ 5482]\n",
      "loss: 0.024198  [ 2400/ 5482]\n",
      "loss: 0.016546  [ 3600/ 5482]\n",
      "loss: 0.016181  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.693     0.707    99\n",
      " disgust     0.689     0.781     0.766    107\n",
      "    fear     0.689     0.653     0.588    80\n",
      "   happy     0.689     0.542     0.584    77\n",
      " neutral     0.689     0.667     0.800    95\n",
      "     sad     0.689     0.750     0.659    91\n",
      "surprise     0.689     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.688     0.680    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.371898 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.019918  [    0/ 5482]\n",
      "loss: 0.016067  [ 1200/ 5482]\n",
      "loss: 0.012686  [ 2400/ 5482]\n",
      "loss: 0.013536  [ 3600/ 5482]\n",
      "loss: 0.016333  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.693     0.707    99\n",
      " disgust     0.684     0.759     0.794    107\n",
      "    fear     0.684     0.642     0.537    80\n",
      "   happy     0.684     0.542     0.584    77\n",
      " neutral     0.684     0.678     0.821    95\n",
      "     sad     0.684     0.734     0.637    91\n",
      "surprise     0.684     0.717     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.681     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 2.305445 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.044338  [    0/ 5482]\n",
      "loss: 0.009578  [ 1200/ 5482]\n",
      "loss: 0.014222  [ 2400/ 5482]\n",
      "loss: 0.012276  [ 3600/ 5482]\n",
      "loss: 0.023136  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.700     0.707    99\n",
      " disgust     0.689     0.818     0.757    107\n",
      "    fear     0.689     0.640     0.600    80\n",
      "   happy     0.689     0.528     0.610    77\n",
      " neutral     0.689     0.667     0.800    95\n",
      "     sad     0.689     0.725     0.637    91\n",
      "surprise     0.689     0.755     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.690     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.285435 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.012399  [    0/ 5482]\n",
      "loss: 0.017497  [ 1200/ 5482]\n",
      "loss: 0.017904  [ 2400/ 5482]\n",
      "loss: 0.009638  [ 3600/ 5482]\n",
      "loss: 0.013875  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.729     0.707    99\n",
      " disgust     0.705     0.764     0.785    107\n",
      "    fear     0.705     0.700     0.613    80\n",
      "   happy     0.705     0.556     0.584    77\n",
      " neutral     0.705     0.700     0.811    95\n",
      "     sad     0.705     0.759     0.659    91\n",
      "surprise     0.705     0.703     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.702     0.700    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.256842 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.008054  [    0/ 5482]\n",
      "loss: 0.011257  [ 1200/ 5482]\n",
      "loss: 0.012835  [ 2400/ 5482]\n",
      "loss: 0.013949  [ 3600/ 5482]\n",
      "loss: 0.008876  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.769     0.707    99\n",
      " disgust     0.697     0.802     0.794    107\n",
      "    fear     0.697     0.589     0.662    80\n",
      "   happy     0.697     0.516     0.623    77\n",
      " neutral     0.697     0.712     0.779    95\n",
      "     sad     0.697     0.737     0.615    91\n",
      "surprise     0.697     0.780     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.701     0.689    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.304429 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.015527  [    0/ 5482]\n",
      "loss: 0.024236  [ 1200/ 5482]\n",
      "loss: 0.015771  [ 2400/ 5482]\n",
      "loss: 0.013932  [ 3600/ 5482]\n",
      "loss: 0.019396  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.730     0.737    99\n",
      " disgust     0.698     0.789     0.804    107\n",
      "    fear     0.698     0.639     0.575    80\n",
      "   happy     0.698     0.582     0.597    77\n",
      " neutral     0.698     0.681     0.811    95\n",
      "     sad     0.698     0.740     0.626    91\n",
      "surprise     0.698     0.683     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.692     0.689    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 2.282499 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.017479  [    0/ 5482]\n",
      "loss: 0.015759  [ 1200/ 5482]\n",
      "loss: 0.008693  [ 2400/ 5482]\n",
      "loss: 0.007240  [ 3600/ 5482]\n",
      "loss: 0.016907  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.739     0.687    99\n",
      " disgust     0.690     0.787     0.794    107\n",
      "    fear     0.690     0.635     0.588    80\n",
      "   happy     0.690     0.542     0.584    77\n",
      " neutral     0.690     0.670     0.811    95\n",
      "     sad     0.690     0.740     0.626    91\n",
      "surprise     0.690     0.689     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.686     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.311481 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.015088  [    0/ 5482]\n",
      "loss: 0.014785  [ 1200/ 5482]\n",
      "loss: 0.012374  [ 2400/ 5482]\n",
      "loss: 0.020759  [ 3600/ 5482]\n",
      "loss: 0.010969  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.729     0.707    99\n",
      " disgust     0.700     0.765     0.822    107\n",
      "    fear     0.700     0.710     0.550    80\n",
      "   happy     0.700     0.516     0.610    77\n",
      " neutral     0.700     0.733     0.779    95\n",
      "     sad     0.700     0.773     0.637    91\n",
      "surprise     0.700     0.657     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.698     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.282325 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.013044  [    0/ 5482]\n",
      "loss: 0.012686  [ 1200/ 5482]\n",
      "loss: 0.013022  [ 2400/ 5482]\n",
      "loss: 0.013802  [ 3600/ 5482]\n",
      "loss: 0.032621  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.737     0.737    99\n",
      " disgust     0.695     0.778     0.785    107\n",
      "    fear     0.695     0.608     0.562    80\n",
      "   happy     0.695     0.541     0.597    77\n",
      " neutral     0.695     0.717     0.800    95\n",
      "     sad     0.695     0.778     0.615    91\n",
      "surprise     0.695     0.667     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.689     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.301886 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.014963  [    0/ 5482]\n",
      "loss: 0.013865  [ 1200/ 5482]\n",
      "loss: 0.017657  [ 2400/ 5482]\n",
      "loss: 0.014778  [ 3600/ 5482]\n",
      "loss: 0.015094  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.682     0.737    99\n",
      " disgust     0.690     0.759     0.794    107\n",
      "    fear     0.690     0.656     0.500    80\n",
      "   happy     0.690     0.552     0.623    77\n",
      " neutral     0.690     0.712     0.779    95\n",
      "     sad     0.690     0.767     0.615    91\n",
      "surprise     0.690     0.682     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.687     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.288041 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.015648  [    0/ 5482]\n",
      "loss: 0.015042  [ 1200/ 5482]\n",
      "loss: 0.011208  [ 2400/ 5482]\n",
      "loss: 0.023687  [ 3600/ 5482]\n",
      "loss: 0.014398  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.782     0.687    99\n",
      " disgust     0.697     0.798     0.776    107\n",
      "    fear     0.697     0.616     0.562    80\n",
      "   happy     0.697     0.551     0.636    77\n",
      " neutral     0.697     0.694     0.811    95\n",
      "     sad     0.697     0.728     0.648    91\n",
      "surprise     0.697     0.677     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.692     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.301327 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.017404  [    0/ 5482]\n",
      "loss: 0.009784  [ 1200/ 5482]\n",
      "loss: 0.018111  [ 2400/ 5482]\n",
      "loss: 0.010157  [ 3600/ 5482]\n",
      "loss: 0.015944  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.767     0.697    99\n",
      " disgust     0.697     0.755     0.776    107\n",
      "    fear     0.697     0.671     0.588    80\n",
      "   happy     0.697     0.511     0.584    77\n",
      " neutral     0.697     0.733     0.811    95\n",
      "     sad     0.697     0.747     0.648    91\n",
      "surprise     0.697     0.662     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.692     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.253665 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.016174  [    0/ 5482]\n",
      "loss: 0.014785  [ 1200/ 5482]\n",
      "loss: 0.011215  [ 2400/ 5482]\n",
      "loss: 0.012221  [ 3600/ 5482]\n",
      "loss: 0.011450  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.753     0.707    99\n",
      " disgust     0.700     0.757     0.785    107\n",
      "    fear     0.700     0.681     0.613    80\n",
      "   happy     0.700     0.557     0.571    77\n",
      " neutral     0.700     0.710     0.800    95\n",
      "     sad     0.700     0.694     0.648    91\n",
      "surprise     0.700     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.695     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.285082 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.016675  [    0/ 5482]\n",
      "loss: 0.017113  [ 1200/ 5482]\n",
      "loss: 0.008432  [ 2400/ 5482]\n",
      "loss: 0.028522  [ 3600/ 5482]\n",
      "loss: 0.015569  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.707     0.707    99\n",
      " disgust     0.692     0.781     0.766    107\n",
      "    fear     0.692     0.714     0.500    80\n",
      "   happy     0.692     0.522     0.623    77\n",
      " neutral     0.692     0.694     0.811    95\n",
      "     sad     0.692     0.747     0.648    91\n",
      "surprise     0.692     0.676     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.692     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.299344 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.018570  [    0/ 5482]\n",
      "loss: 0.012387  [ 1200/ 5482]\n",
      "loss: 0.014560  [ 2400/ 5482]\n",
      "loss: 0.020776  [ 3600/ 5482]\n",
      "loss: 0.009749  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.773     0.687    99\n",
      " disgust     0.692     0.746     0.822    107\n",
      "    fear     0.692     0.679     0.475    80\n",
      "   happy     0.692     0.511     0.610    77\n",
      " neutral     0.692     0.716     0.821    95\n",
      "     sad     0.692     0.770     0.626    91\n",
      "surprise     0.692     0.630     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.689     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.283252 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.006858  [    0/ 5482]\n",
      "loss: 0.013591  [ 1200/ 5482]\n",
      "loss: 0.015146  [ 2400/ 5482]\n",
      "loss: 0.011488  [ 3600/ 5482]\n",
      "loss: 0.010971  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.705     0.737     0.737    99\n",
      " disgust     0.705     0.798     0.776    107\n",
      "    fear     0.705     0.682     0.562    80\n",
      "   happy     0.705     0.538     0.636    77\n",
      " neutral     0.705     0.688     0.811    95\n",
      "     sad     0.705     0.808     0.648    91\n",
      "surprise     0.705     0.677     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.705     0.704     0.699    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.312591 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.012470  [    0/ 5482]\n",
      "loss: 0.006899  [ 1200/ 5482]\n",
      "loss: 0.012484  [ 2400/ 5482]\n",
      "loss: 0.014177  [ 3600/ 5482]\n",
      "loss: 0.014898  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.686     0.727    99\n",
      " disgust     0.695     0.796     0.766    107\n",
      "    fear     0.695     0.667     0.550    80\n",
      "   happy     0.695     0.523     0.597    77\n",
      " neutral     0.695     0.688     0.811    95\n",
      "     sad     0.695     0.792     0.670    91\n",
      "surprise     0.695     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.695     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.305616 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.012414  [    0/ 5482]\n",
      "loss: 0.014363  [ 1200/ 5482]\n",
      "loss: 0.012165  [ 2400/ 5482]\n",
      "loss: 0.009941  [ 3600/ 5482]\n",
      "loss: 0.015678  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.726     0.697    99\n",
      " disgust     0.697     0.739     0.766    107\n",
      "    fear     0.697     0.630     0.575    80\n",
      "   happy     0.697     0.575     0.597    77\n",
      " neutral     0.697     0.681     0.811    95\n",
      "     sad     0.697     0.785     0.681    91\n",
      "surprise     0.697     0.729     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.695     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.330155 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.016563  [    0/ 5482]\n",
      "loss: 0.007526  [ 1200/ 5482]\n",
      "loss: 0.009440  [ 2400/ 5482]\n",
      "loss: 0.006321  [ 3600/ 5482]\n",
      "loss: 0.009224  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.667     0.788    99\n",
      " disgust     0.693     0.806     0.776    107\n",
      "    fear     0.693     0.672     0.537    80\n",
      "   happy     0.693     0.505     0.610    77\n",
      " neutral     0.693     0.717     0.800    95\n",
      "     sad     0.693     0.821     0.604    91\n",
      "surprise     0.693     0.683     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.696     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.467675 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.027443  [    0/ 5482]\n",
      "loss: 0.014357  [ 1200/ 5482]\n",
      "loss: 0.015693  [ 2400/ 5482]\n",
      "loss: 0.011843  [ 3600/ 5482]\n",
      "loss: 0.033547  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.731     0.768    99\n",
      " disgust     0.700     0.783     0.776    107\n",
      "    fear     0.700     0.656     0.500    80\n",
      "   happy     0.700     0.510     0.649    77\n",
      " neutral     0.700     0.733     0.811    95\n",
      "     sad     0.700     0.811     0.659    91\n",
      "surprise     0.700     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.698     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.351285 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.008627  [    0/ 5482]\n",
      "loss: 0.015406  [ 1200/ 5482]\n",
      "loss: 0.014802  [ 2400/ 5482]\n",
      "loss: 0.016261  [ 3600/ 5482]\n",
      "loss: 0.012271  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.664     0.778    99\n",
      " disgust     0.687     0.779     0.757    107\n",
      "    fear     0.687     0.695     0.512    80\n",
      "   happy     0.687     0.544     0.636    77\n",
      " neutral     0.687     0.688     0.789    95\n",
      "     sad     0.687     0.734     0.637    91\n",
      "surprise     0.687     0.717     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.689     0.676    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.417727 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.012639  [    0/ 5482]\n",
      "loss: 0.016647  [ 1200/ 5482]\n",
      "loss: 0.014447  [ 2400/ 5482]\n",
      "loss: 0.015221  [ 3600/ 5482]\n",
      "loss: 0.015556  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.733     0.747    99\n",
      " disgust     0.690     0.783     0.776    107\n",
      "    fear     0.690     0.592     0.525    80\n",
      "   happy     0.690     0.526     0.662    77\n",
      " neutral     0.690     0.709     0.768    95\n",
      "     sad     0.690     0.841     0.637    91\n",
      "surprise     0.690     0.635     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.688     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.471862 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.029184  [    0/ 5482]\n",
      "loss: 0.012572  [ 1200/ 5482]\n",
      "loss: 0.007516  [ 2400/ 5482]\n",
      "loss: 0.014118  [ 3600/ 5482]\n",
      "loss: 0.016471  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.733     0.747    99\n",
      " disgust     0.695     0.796     0.766    107\n",
      "    fear     0.695     0.610     0.625    80\n",
      "   happy     0.695     0.528     0.610    77\n",
      " neutral     0.695     0.698     0.779    95\n",
      "     sad     0.695     0.792     0.626    91\n",
      "surprise     0.695     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.694     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.394207 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.014513  [    0/ 5482]\n",
      "loss: 0.013037  [ 1200/ 5482]\n",
      "loss: 0.014526  [ 2400/ 5482]\n",
      "loss: 0.012718  [ 3600/ 5482]\n",
      "loss: 0.011148  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.780     0.717    99\n",
      " disgust     0.689     0.822     0.776    107\n",
      "    fear     0.689     0.677     0.550    80\n",
      "   happy     0.689     0.468     0.662    77\n",
      " neutral     0.689     0.698     0.779    95\n",
      "     sad     0.689     0.733     0.604    91\n",
      "surprise     0.689     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.692     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.424261 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.010550  [    0/ 5482]\n",
      "loss: 0.008989  [ 1200/ 5482]\n",
      "loss: 0.013557  [ 2400/ 5482]\n",
      "loss: 0.017717  [ 3600/ 5482]\n",
      "loss: 0.011714  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.697     0.772     0.717    99\n",
      " disgust     0.697     0.785     0.785    107\n",
      "    fear     0.697     0.610     0.625    80\n",
      "   happy     0.697     0.566     0.610    77\n",
      " neutral     0.697     0.725     0.779    95\n",
      "     sad     0.697     0.760     0.626    91\n",
      "surprise     0.697     0.609     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.697     0.690     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.335329 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.013156  [    0/ 5482]\n",
      "loss: 0.005392  [ 1200/ 5482]\n",
      "loss: 0.007225  [ 2400/ 5482]\n",
      "loss: 0.015213  [ 3600/ 5482]\n",
      "loss: 0.006986  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.685     0.768    99\n",
      " disgust     0.684     0.764     0.757    107\n",
      "    fear     0.684     0.688     0.550    80\n",
      "   happy     0.684     0.560     0.545    77\n",
      " neutral     0.684     0.682     0.768    95\n",
      "     sad     0.684     0.723     0.659    91\n",
      "surprise     0.684     0.641     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.677     0.674    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 2.339184 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.011056  [    0/ 5482]\n",
      "loss: 0.008027  [ 1200/ 5482]\n",
      "loss: 0.009570  [ 2400/ 5482]\n",
      "loss: 0.009934  [ 3600/ 5482]\n",
      "loss: 0.014150  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.760     0.737    99\n",
      " disgust     0.702     0.812     0.766    107\n",
      "    fear     0.702     0.657     0.550    80\n",
      "   happy     0.702     0.557     0.636    77\n",
      " neutral     0.702     0.713     0.811    95\n",
      "     sad     0.702     0.750     0.659    91\n",
      "surprise     0.702     0.614     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.695     0.695    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.309488 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.011263  [    0/ 5482]\n",
      "loss: 0.016824  [ 1200/ 5482]\n",
      "loss: 0.008405  [ 2400/ 5482]\n",
      "loss: 0.009996  [ 3600/ 5482]\n",
      "loss: 0.016475  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.750     0.758    99\n",
      " disgust     0.700     0.764     0.785    107\n",
      "    fear     0.700     0.672     0.537    80\n",
      "   happy     0.700     0.568     0.649    77\n",
      " neutral     0.700     0.700     0.811    95\n",
      "     sad     0.700     0.740     0.626    91\n",
      "surprise     0.700     0.672     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.695     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.241427 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.010542  [    0/ 5482]\n",
      "loss: 0.014286  [ 1200/ 5482]\n",
      "loss: 0.011777  [ 2400/ 5482]\n",
      "loss: 0.009683  [ 3600/ 5482]\n",
      "loss: 0.012755  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.735     0.727    99\n",
      " disgust     0.695     0.810     0.757    107\n",
      "    fear     0.695     0.625     0.562    80\n",
      "   happy     0.695     0.538     0.636    77\n",
      " neutral     0.695     0.681     0.811    95\n",
      "     sad     0.695     0.778     0.615    91\n",
      "surprise     0.695     0.688     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.694     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.236308 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.040559  [    0/ 5482]\n",
      "loss: 0.012086  [ 1200/ 5482]\n",
      "loss: 0.007339  [ 2400/ 5482]\n",
      "loss: 0.016420  [ 3600/ 5482]\n",
      "loss: 0.014502  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.783     0.727    99\n",
      " disgust     0.703     0.804     0.766    107\n",
      "    fear     0.703     0.618     0.588    80\n",
      "   happy     0.703     0.549     0.649    77\n",
      " neutral     0.703     0.721     0.789    95\n",
      "     sad     0.703     0.753     0.637    91\n",
      "surprise     0.703     0.662     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.699     0.699    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.267343 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.012284  [    0/ 5482]\n",
      "loss: 0.010971  [ 1200/ 5482]\n",
      "loss: 0.017857  [ 2400/ 5482]\n",
      "loss: 0.008876  [ 3600/ 5482]\n",
      "loss: 0.009861  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.728     0.758    99\n",
      " disgust     0.703     0.822     0.776    107\n",
      "    fear     0.703     0.677     0.550    80\n",
      "   happy     0.703     0.500     0.636    77\n",
      " neutral     0.703     0.728     0.789    95\n",
      "     sad     0.703     0.766     0.648    91\n",
      "surprise     0.703     0.698     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.703     0.697    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.268724 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.033832  [    0/ 5482]\n",
      "loss: 0.014546  [ 1200/ 5482]\n",
      "loss: 0.013589  [ 2400/ 5482]\n",
      "loss: 0.004552  [ 3600/ 5482]\n",
      "loss: 0.019233  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.742     0.727    99\n",
      " disgust     0.690     0.822     0.776    107\n",
      "    fear     0.690     0.661     0.512    80\n",
      "   happy     0.690     0.490     0.636    77\n",
      " neutral     0.690     0.708     0.789    95\n",
      "     sad     0.690     0.770     0.626    91\n",
      "surprise     0.690     0.629     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.689     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.354894 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.013967  [    0/ 5482]\n",
      "loss: 0.010899  [ 1200/ 5482]\n",
      "loss: 0.010441  [ 2400/ 5482]\n",
      "loss: 0.011035  [ 3600/ 5482]\n",
      "loss: 0.049046  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.725     0.747    99\n",
      " disgust     0.693     0.781     0.766    107\n",
      "    fear     0.693     0.587     0.550    80\n",
      "   happy     0.693     0.578     0.623    77\n",
      " neutral     0.693     0.735     0.789    95\n",
      "     sad     0.693     0.781     0.626    91\n",
      "surprise     0.693     0.614     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.686     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.289399 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.011024  [    0/ 5482]\n",
      "loss: 0.012237  [ 1200/ 5482]\n",
      "loss: 0.009699  [ 2400/ 5482]\n",
      "loss: 0.011715  [ 3600/ 5482]\n",
      "loss: 0.015556  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.723     0.687    99\n",
      " disgust     0.689     0.820     0.766    107\n",
      "    fear     0.689     0.630     0.575    80\n",
      "   happy     0.689     0.500     0.662    77\n",
      " neutral     0.689     0.697     0.800    95\n",
      "     sad     0.689     0.797     0.604    91\n",
      "surprise     0.689     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.691     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.369168 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.019478  [    0/ 5482]\n",
      "loss: 0.009680  [ 1200/ 5482]\n",
      "loss: 0.011383  [ 2400/ 5482]\n",
      "loss: 0.016640  [ 3600/ 5482]\n",
      "loss: 0.008155  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.694     0.778    99\n",
      " disgust     0.702     0.781     0.766    107\n",
      "    fear     0.702     0.639     0.575    80\n",
      "   happy     0.702     0.620     0.636    77\n",
      " neutral     0.702     0.709     0.821    95\n",
      "     sad     0.702     0.750     0.626    91\n",
      "surprise     0.702     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.697     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.275824 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.007036  [    0/ 5482]\n",
      "loss: 0.010183  [ 1200/ 5482]\n",
      "loss: 0.006055  [ 2400/ 5482]\n",
      "loss: 0.023569  [ 3600/ 5482]\n",
      "loss: 0.008185  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.661     0.788    99\n",
      " disgust     0.692     0.814     0.738    107\n",
      "    fear     0.692     0.657     0.550    80\n",
      "   happy     0.692     0.585     0.623    77\n",
      " neutral     0.692     0.700     0.811    95\n",
      "     sad     0.692     0.775     0.604    91\n",
      "surprise     0.692     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.689     0.684    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.410583 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.008078  [    0/ 5482]\n",
      "loss: 0.011180  [ 1200/ 5482]\n",
      "loss: 0.012824  [ 2400/ 5482]\n",
      "loss: 0.009486  [ 3600/ 5482]\n",
      "loss: 0.147722  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.680     0.707    99\n",
      " disgust     0.680     0.790     0.738    107\n",
      "    fear     0.680     0.603     0.588    80\n",
      "   happy     0.680     0.537     0.662    77\n",
      " neutral     0.680     0.716     0.768    95\n",
      "     sad     0.680     0.824     0.615    91\n",
      "surprise     0.680     0.609     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.680     0.674    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 2.417616 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.012751  [    0/ 5482]\n",
      "loss: 0.396107  [ 1200/ 5482]\n",
      "loss: 0.011142  [ 2400/ 5482]\n",
      "loss: 0.011568  [ 3600/ 5482]\n",
      "loss: 0.005308  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.703     0.709     0.737    99\n",
      " disgust     0.703     0.800     0.785    107\n",
      "    fear     0.703     0.701     0.588    80\n",
      "   happy     0.703     0.562     0.649    77\n",
      " neutral     0.703     0.735     0.789    95\n",
      "     sad     0.703     0.694     0.648    91\n",
      "surprise     0.703     0.695     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.703     0.699     0.696    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.430583 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.005929  [    0/ 5482]\n",
      "loss: 0.016478  [ 1200/ 5482]\n",
      "loss: 0.010055  [ 2400/ 5482]\n",
      "loss: 0.014599  [ 3600/ 5482]\n",
      "loss: 0.013855  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.750     0.697    99\n",
      " disgust     0.695     0.790     0.776    107\n",
      "    fear     0.695     0.625     0.625    80\n",
      "   happy     0.695     0.544     0.636    77\n",
      " neutral     0.695     0.721     0.789    95\n",
      "     sad     0.695     0.753     0.637    91\n",
      "surprise     0.695     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.690     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.447713 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.012129  [    0/ 5482]\n",
      "loss: 0.009152  [ 1200/ 5482]\n",
      "loss: 0.033668  [ 2400/ 5482]\n",
      "loss: 0.038146  [ 3600/ 5482]\n",
      "loss: 0.014500  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.693     0.753     0.707    99\n",
      " disgust     0.693     0.716     0.776    107\n",
      "    fear     0.693     0.654     0.637    80\n",
      "   happy     0.693     0.495     0.623    77\n",
      " neutral     0.693     0.720     0.811    95\n",
      "     sad     0.693     0.817     0.637    91\n",
      "surprise     0.693     0.750     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.693     0.700     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 2.420442 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.013017  [    0/ 5482]\n",
      "loss: 0.011055  [ 1200/ 5482]\n",
      "loss: 0.028781  [ 2400/ 5482]\n",
      "loss: 0.020168  [ 3600/ 5482]\n",
      "loss: 0.011783  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.753     0.707    99\n",
      " disgust     0.687     0.798     0.738    107\n",
      "    fear     0.687     0.634     0.562    80\n",
      "   happy     0.687     0.500     0.662    77\n",
      " neutral     0.687     0.725     0.779    95\n",
      "     sad     0.687     0.747     0.648    91\n",
      "surprise     0.687     0.641     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.685     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.498850 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.007989  [    0/ 5482]\n",
      "loss: 0.008603  [ 1200/ 5482]\n",
      "loss: 0.007826  [ 2400/ 5482]\n",
      "loss: 0.017376  [ 3600/ 5482]\n",
      "loss: 0.005794  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.732     0.717    99\n",
      " disgust     0.695     0.808     0.748    107\n",
      "    fear     0.695     0.608     0.600    80\n",
      "   happy     0.695     0.522     0.623    77\n",
      " neutral     0.695     0.724     0.800    95\n",
      "     sad     0.695     0.753     0.637    91\n",
      "surprise     0.695     0.705     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.693     0.690    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.440384 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.007843  [    0/ 5482]\n",
      "loss: 0.009022  [ 1200/ 5482]\n",
      "loss: 0.008299  [ 2400/ 5482]\n",
      "loss: 0.011769  [ 3600/ 5482]\n",
      "loss: 0.010239  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.713     0.727    99\n",
      " disgust     0.690     0.781     0.766    107\n",
      "    fear     0.690     0.603     0.550    80\n",
      "   happy     0.690     0.532     0.649    77\n",
      " neutral     0.690     0.733     0.811    95\n",
      "     sad     0.690     0.797     0.604    91\n",
      "surprise     0.690     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.687     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.447042 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.015518  [    0/ 5482]\n",
      "loss: 0.129934  [ 1200/ 5482]\n",
      "loss: 0.009329  [ 2400/ 5482]\n",
      "loss: 0.015390  [ 3600/ 5482]\n",
      "loss: 0.013079  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.702     0.730     0.737    99\n",
      " disgust     0.702     0.774     0.766    107\n",
      "    fear     0.702     0.658     0.600    80\n",
      "   happy     0.702     0.557     0.636    77\n",
      " neutral     0.702     0.703     0.821    95\n",
      "     sad     0.702     0.781     0.626    91\n",
      "surprise     0.702     0.695     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.702     0.699     0.694    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 2.442506 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.017273  [    0/ 5482]\n",
      "loss: 0.009747  [ 1200/ 5482]\n",
      "loss: 0.025948  [ 2400/ 5482]\n",
      "loss: 0.016136  [ 3600/ 5482]\n",
      "loss: 0.010675  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.798     0.677    99\n",
      " disgust     0.687     0.798     0.776    107\n",
      "    fear     0.687     0.610     0.588    80\n",
      "   happy     0.687     0.485     0.649    77\n",
      " neutral     0.687     0.728     0.789    95\n",
      "     sad     0.687     0.737     0.615    91\n",
      "surprise     0.687     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.687     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.556825 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.008536  [    0/ 5482]\n",
      "loss: 0.013534  [ 1200/ 5482]\n",
      "loss: 0.014360  [ 2400/ 5482]\n",
      "loss: 0.013153  [ 3600/ 5482]\n",
      "loss: 0.011961  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.698     0.756     0.687    99\n",
      " disgust     0.698     0.785     0.785    107\n",
      "    fear     0.698     0.590     0.613    80\n",
      "   happy     0.698     0.551     0.636    77\n",
      " neutral     0.698     0.743     0.789    95\n",
      "     sad     0.698     0.776     0.648    91\n",
      "surprise     0.698     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.698     0.694     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 2.440517 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.007669  [    0/ 5482]\n",
      "loss: 0.009781  [ 1200/ 5482]\n",
      "loss: 0.008159  [ 2400/ 5482]\n",
      "loss: 0.010705  [ 3600/ 5482]\n",
      "loss: 0.011394  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.700     0.747     0.687    99\n",
      " disgust     0.700     0.775     0.804    107\n",
      "    fear     0.700     0.628     0.613    80\n",
      "   happy     0.700     0.540     0.610    77\n",
      " neutral     0.700     0.720     0.811    95\n",
      "     sad     0.700     0.759     0.659    91\n",
      "surprise     0.700     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.700     0.696     0.691    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 2.477519 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.004794  [    0/ 5482]\n",
      "loss: 0.013437  [ 1200/ 5482]\n",
      "loss: 0.008383  [ 2400/ 5482]\n",
      "loss: 0.009950  [ 3600/ 5482]\n",
      "loss: 0.024468  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.767     0.697    99\n",
      " disgust     0.692     0.810     0.794    107\n",
      "    fear     0.692     0.570     0.613    80\n",
      "   happy     0.692     0.522     0.610    77\n",
      " neutral     0.692     0.705     0.779    95\n",
      "     sad     0.692     0.806     0.593    91\n",
      "surprise     0.692     0.657     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.691     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 2.550879 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.009405  [    0/ 5482]\n",
      "loss: 0.006177  [ 1200/ 5482]\n",
      "loss: 0.007827  [ 2400/ 5482]\n",
      "loss: 0.010794  [ 3600/ 5482]\n",
      "loss: 0.012761  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.695     0.773     0.687    99\n",
      " disgust     0.695     0.780     0.794    107\n",
      "    fear     0.695     0.568     0.625    80\n",
      "   happy     0.695     0.545     0.623    77\n",
      " neutral     0.695     0.737     0.768    95\n",
      "     sad     0.695     0.759     0.659    91\n",
      "surprise     0.695     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.695     0.692     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 2.485786 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.008863  [    0/ 5482]\n",
      "loss: 0.008145  [ 1200/ 5482]\n",
      "loss: 0.011793  [ 2400/ 5482]\n",
      "loss: 0.006568  [ 3600/ 5482]\n",
      "loss: 0.007527  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.730     0.657    99\n",
      " disgust     0.680     0.786     0.757    107\n",
      "    fear     0.680     0.551     0.613    80\n",
      "   happy     0.680     0.480     0.623    77\n",
      " neutral     0.680     0.730     0.768    95\n",
      "     sad     0.680     0.831     0.648    91\n",
      "surprise     0.680     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.685     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 2.585801 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.012424  [    0/ 5482]\n",
      "loss: 0.009540  [ 1200/ 5482]\n",
      "loss: 0.012109  [ 2400/ 5482]\n",
      "loss: 0.011731  [ 3600/ 5482]\n",
      "loss: 0.006225  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.721     0.758    99\n",
      " disgust     0.687     0.825     0.748    107\n",
      "    fear     0.687     0.603     0.588    80\n",
      "   happy     0.687     0.495     0.584    77\n",
      " neutral     0.687     0.723     0.768    95\n",
      "     sad     0.687     0.795     0.637    91\n",
      "surprise     0.687     0.621     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.683     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.539570 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.007448  [    0/ 5482]\n",
      "loss: 0.015347  [ 1200/ 5482]\n",
      "loss: 0.306986  [ 2400/ 5482]\n",
      "loss: 0.006882  [ 3600/ 5482]\n",
      "loss: 0.011882  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.702     0.737    99\n",
      " disgust     0.682     0.774     0.766    107\n",
      "    fear     0.682     0.600     0.562    80\n",
      "   happy     0.682     0.548     0.597    77\n",
      " neutral     0.682     0.708     0.789    95\n",
      "     sad     0.682     0.746     0.582    91\n",
      "surprise     0.682     0.656     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.676     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 2.434486 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.008869  [    0/ 5482]\n",
      "loss: 0.007554  [ 1200/ 5482]\n",
      "loss: 0.013692  [ 2400/ 5482]\n",
      "loss: 0.012038  [ 3600/ 5482]\n",
      "loss: 0.007931  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.685     0.776     0.667    99\n",
      " disgust     0.685     0.812     0.766    107\n",
      "    fear     0.685     0.580     0.588    80\n",
      "   happy     0.685     0.529     0.597    77\n",
      " neutral     0.685     0.710     0.800    95\n",
      "     sad     0.685     0.767     0.615    91\n",
      "surprise     0.685     0.592     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.685     0.681     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 2.588268 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.009571  [    0/ 5482]\n",
      "loss: 0.007506  [ 1200/ 5482]\n",
      "loss: 0.016932  [ 2400/ 5482]\n",
      "loss: 0.008324  [ 3600/ 5482]\n",
      "loss: 0.010159  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.798     0.677    99\n",
      " disgust     0.687     0.820     0.766    107\n",
      "    fear     0.687     0.557     0.613    80\n",
      "   happy     0.687     0.500     0.597    77\n",
      " neutral     0.687     0.710     0.800    95\n",
      "     sad     0.687     0.776     0.648    91\n",
      "surprise     0.687     0.635     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.685     0.680    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.601854 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.009702  [    0/ 5482]\n",
      "loss: 0.007778  [ 1200/ 5482]\n",
      "loss: 0.009625  [ 2400/ 5482]\n",
      "loss: 0.009884  [ 3600/ 5482]\n",
      "loss: 0.012792  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.685     0.723     0.687    99\n",
      " disgust     0.685     0.804     0.766    107\n",
      "    fear     0.685     0.615     0.600    80\n",
      "   happy     0.685     0.494     0.571    77\n",
      " neutral     0.685     0.672     0.821    95\n",
      "     sad     0.685     0.744     0.637    91\n",
      "surprise     0.685     0.755     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.685     0.687     0.677    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 2.605195 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.007889  [    0/ 5482]\n",
      "loss: 0.026435  [ 1200/ 5482]\n",
      "loss: 0.005419  [ 2400/ 5482]\n",
      "loss: 0.014381  [ 3600/ 5482]\n",
      "loss: 0.009425  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.740     0.747    99\n",
      " disgust     0.687     0.792     0.748    107\n",
      "    fear     0.687     0.587     0.550    80\n",
      "   happy     0.687     0.535     0.597    77\n",
      " neutral     0.687     0.700     0.811    95\n",
      "     sad     0.687     0.766     0.648    91\n",
      "surprise     0.687     0.639     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.680     0.677    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 2.589934 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.007638  [    0/ 5482]\n",
      "loss: 0.007251  [ 1200/ 5482]\n",
      "loss: 0.012482  [ 2400/ 5482]\n",
      "loss: 0.007011  [ 3600/ 5482]\n",
      "loss: 0.010599  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.747     0.717    99\n",
      " disgust     0.679     0.802     0.757    107\n",
      "    fear     0.679     0.566     0.537    80\n",
      "   happy     0.679     0.511     0.597    77\n",
      " neutral     0.679     0.710     0.800    95\n",
      "     sad     0.679     0.746     0.582    91\n",
      "surprise     0.679     0.629     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.673     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 2.618727 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.012884  [    0/ 5482]\n",
      "loss: 0.011043  [ 1200/ 5482]\n",
      "loss: 0.014247  [ 2400/ 5482]\n",
      "loss: 0.009915  [ 3600/ 5482]\n",
      "loss: 0.015562  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.713     0.727    99\n",
      " disgust     0.684     0.810     0.757    107\n",
      "    fear     0.684     0.615     0.600    80\n",
      "   happy     0.684     0.529     0.584    77\n",
      " neutral     0.684     0.710     0.800    95\n",
      "     sad     0.684     0.737     0.615    91\n",
      "surprise     0.684     0.619     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.676     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 2.619668 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.008905  [    0/ 5482]\n",
      "loss: 0.012985  [ 1200/ 5482]\n",
      "loss: 0.012270  [ 2400/ 5482]\n",
      "loss: 0.011582  [ 3600/ 5482]\n",
      "loss: 0.007344  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.737     0.707    99\n",
      " disgust     0.689     0.802     0.757    107\n",
      "    fear     0.689     0.597     0.575    80\n",
      "   happy     0.689     0.526     0.662    77\n",
      " neutral     0.689     0.692     0.779    95\n",
      "     sad     0.689     0.773     0.637    91\n",
      "surprise     0.689     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.688     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 2.596543 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.007676  [    0/ 5482]\n",
      "loss: 0.012432  [ 1200/ 5482]\n",
      "loss: 0.015494  [ 2400/ 5482]\n",
      "loss: 0.005651  [ 3600/ 5482]\n",
      "loss: 0.005130  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.702     0.737    99\n",
      " disgust     0.690     0.798     0.738    107\n",
      "    fear     0.690     0.645     0.613    80\n",
      "   happy     0.690     0.562     0.584    77\n",
      " neutral     0.690     0.670     0.811    95\n",
      "     sad     0.690     0.766     0.648    91\n",
      "surprise     0.690     0.661     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.686     0.682    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 2.566717 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.011000  [    0/ 5482]\n",
      "loss: 0.013512  [ 1200/ 5482]\n",
      "loss: 0.006898  [ 2400/ 5482]\n",
      "loss: 0.006418  [ 3600/ 5482]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mexp_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_em\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:37\u001B[0m, in \u001B[0;36mExperimentsTrainer.train_em\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m trail \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials_per_model_type):\n\u001B[1;32m     36\u001B[0m     lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_lr \u001B[38;5;241m/\u001B[39m  (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_quotient \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m trail)\n\u001B[0;32m---> 37\u001B[0m     highest_acc_c, higest_epoch_c, higest_true_c, higest_pred_c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_conv_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs_per_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrail\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# generate Report\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     SSGenModelTrainer\u001B[38;5;241m.\u001B[39mgenAndSaveEvaluation(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrail\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/conv\u001B[39m\u001B[38;5;124m\"\u001B[39m, higest_true_c, higest_pred_c, highest_acc_c, higest_epoch_c, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvolutional\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mencoded_dataset\u001B[38;5;241m.\u001B[39mlabel_list)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:57\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_conv_model_test\u001B[0;34m(self, lr, epochs, current_run)\u001B[0m\n\u001B[1;32m     55\u001B[0m model \u001B[38;5;241m=\u001B[39m SSConvModel3Sec(num_emotions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list), xSize\u001B[38;5;241m=\u001B[39mx_size, ySize\u001B[38;5;241m=\u001B[39my_size)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     56\u001B[0m save_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_run\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/conv/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 57\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:84\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_model_test\u001B[0;34m(self, lr, epochs, model, save_dir, bs)\u001B[0m\n\u001B[1;32m     76\u001B[0m trainDS, testDs \u001B[38;5;241m=\u001B[39m train_val_dataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, val_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m     77\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SSGenModelTrainer(lr\u001B[38;5;241m=\u001B[39mlr, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model\u001B[38;5;241m=\u001B[39mmodel, train_dataset\u001B[38;5;241m=\u001B[39mtrainDS,\n\u001B[1;32m     78\u001B[0m                             eval_dataset\u001B[38;5;241m=\u001B[39mtestDs,\n\u001B[1;32m     79\u001B[0m                             device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, labelList\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     82\u001B[0m                             save_highest_acc_min_acc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_highest_acc_min_acc,\n\u001B[1;32m     83\u001B[0m                             model_path \u001B[38;5;241m=\u001B[39m save_dir, regularize_dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregularize_dims)\n\u001B[0;32m---> 84\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:75\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# this is the trainloop\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# --------------------- testloop and evaluation- ---------------\u001B[39;00m\n\u001B[1;32m     78\u001B[0m acc, true, preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loop(test_dataloader, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:119\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer, epoch)\u001B[0m\n\u001B[1;32m    117\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    118\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m--> 119\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    122\u001B[0m     loss, current \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem(), batch \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(X)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 23\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/optim/sgd.py:151\u001B[0m, in \u001B[0;36mSGD.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m             momentum_buffer_list\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmomentum_buffer\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 151\u001B[0m \u001B[43msgd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m    \u001B[49m\u001B[43md_p_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmomentum_buffer_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmomentum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdampening\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdampening\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnesterov\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnesterov\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_sparse_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_sparse_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;66;03m# update momentum_buffers in state\u001B[39;00m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p, momentum_buffer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/optim/sgd.py:202\u001B[0m, in \u001B[0;36msgd\u001B[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    200\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_sgd\n\u001B[0;32m--> 202\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m     \u001B[49m\u001B[43md_p_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmomentum_buffer_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdampening\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdampening\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m     \u001B[49m\u001B[43mnesterov\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnesterov\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m     \u001B[49m\u001B[43mhas_sparse_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_sparse_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/optim/sgd.py:238\u001B[0m, in \u001B[0;36m_single_tensor_sgd\u001B[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001B[0m\n\u001B[1;32m    236\u001B[0m     momentum_buffer_list[i] \u001B[38;5;241m=\u001B[39m buf\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 238\u001B[0m     \u001B[43mbuf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd_(d_p, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m dampening)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nesterov:\n\u001B[1;32m    241\u001B[0m     d_p \u001B[38;5;241m=\u001B[39m d_p\u001B[38;5;241m.\u001B[39madd(buf, alpha\u001B[38;5;241m=\u001B[39mmomentum)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 10:00:38.227854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 10:00:38.701976: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 10:00:38.702028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 10:00:38.702032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 267178 samples and validating with randomly splitted 14063 samples\n",
      "0: soundstream total loss: 545563.781, soundstream recon loss: 0.603 | discr (scale 1) loss: 2.001 | discr (scale 0.5) loss: 2.002 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to ../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32\n",
      "0: saving model to ../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32\n",
      "1: soundstream total loss: 660274.273, soundstream recon loss: 0.702 | discr (scale 1) loss: 1.989 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 1.997\n",
      "2: soundstream total loss: 526542.078, soundstream recon loss: 0.547 | discr (scale 1) loss: 1.966 | discr (scale 0.5) loss: 1.990 | discr (scale 0.25) loss: 2.001\n",
      "3: soundstream total loss: 158701.781, soundstream recon loss: 0.173 | discr (scale 1) loss: 1.958 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 2.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 37\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#soundstream.load(\"../content/soundstream/verision0.12.1/00_10000_1e-4_bs6_gae8_dml320-32/soundstream.42500.pt\")\u001B[39;00m\n\u001B[1;32m     20\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SoundStreamTrainer(\n\u001B[1;32m     21\u001B[0m     soundstream,\n\u001B[1;32m     22\u001B[0m     folder \u001B[38;5;241m=\u001B[39m  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/home/ckwdani/Music/libri\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m#standard lr 3e-4,\u001B[39;00m\n\u001B[1;32m     35\u001B[0m )\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m---> 37\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:455\u001B[0m, in \u001B[0;36mSoundStreamTrainer.train\u001B[0;34m(self, log_fn)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, log_fn \u001B[38;5;241m=\u001B[39m noop):\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_train_steps:\n\u001B[0;32m--> 455\u001B[0m         logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m         log_fn(logs)\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining complete\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:357\u001B[0m, in \u001B[0;36mSoundStreamTrainer.train_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    354\u001B[0m     multiscale_discr_optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad_accum_every):\n\u001B[0;32m--> 357\u001B[0m     wave, \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdl_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m     wave \u001B[38;5;241m=\u001B[39m wave\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    360\u001B[0m     discr_losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoundstream(\n\u001B[1;32m    361\u001B[0m         wave,\n\u001B[1;32m    362\u001B[0m         apply_grad_penalty \u001B[38;5;241m=\u001B[39m apply_grad_penalty,\n\u001B[1;32m    363\u001B[0m         return_discr_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    364\u001B[0m         return_discr_losses_separately \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    365\u001B[0m     )\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:72\u001B[0m, in \u001B[0;36mcycle\u001B[0;34m(dl)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcycle\u001B[39m(dl):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 72\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dl:\n\u001B[1;32m     73\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/accelerate/data_loader.py:383\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    382\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m send_to_device(current_batch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 383\u001B[0m next_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m current_batch\n\u001B[1;32m    385\u001B[0m current_batch \u001B[38;5;241m=\u001B[39m next_batch\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/data.py:67\u001B[0m, in \u001B[0;36mSoundDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     65\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfiles[idx]\n\u001B[0;32m---> 67\u001B[0m     data, sample_hz \u001B[38;5;241m=\u001B[39m \u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m data\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mone of your audio file (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) is empty. please remove it from your folder\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;66;03m# the audio has more than 1 channel, convert to mono\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:222\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _fallback_load_fileobj(filepath, frame_offset, num_frames, normalize, channels_first, \u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    221\u001B[0m     filepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(filepath)\n\u001B[0;32m--> 222\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msox_io_load_audio_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\n\u001B[1;32m    224\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_ops.py:442\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "#\n",
    "# import numpy as np\n",
    "# from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "#\n",
    "# soundstream = SoundStream(\n",
    "#\n",
    "#     codebook_size = 2048,\n",
    "#     rq_num_quantizers = 12,\n",
    "#     attn_window_size = 128,       # local attention receptive field at bottleneck\n",
    "#     attn_depth = 3\n",
    "#     #target_sample_hz=16000\n",
    "# )\n",
    "#\n",
    "# #soundstream.load(\"../content/soundstream/verision0.12.1/00_10000_1e-4_bs6_gae8_dml320-32/soundstream.42500.pt\")\n",
    "#\n",
    "# trainer = SoundStreamTrainer(\n",
    "#     soundstream,\n",
    "#     folder =  '/home/ckwdani/Music/libri',\n",
    "#     #'/home/ckwdani/Music/train-clean-100',\n",
    "#     batch_size = 6,\n",
    "#     grad_accum_every = 8,         # effective batch size of 32\n",
    "#     data_max_length = 320 * 32,\n",
    "#     save_results_every=1000,\n",
    "#     num_train_steps = 60001,\n",
    "#     save_model_every= 500,\n",
    "#     results_folder = '../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32',\n",
    "#     #lr=1e-4\n",
    "# #    lr = 5e-5,\n",
    "#     #lr = 5e-5\n",
    "#     #standard lr 3e-4,\n",
    "# ).cuda()\n",
    "#\n",
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
