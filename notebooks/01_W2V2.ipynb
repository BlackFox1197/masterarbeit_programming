{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 18:51:02 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"utils\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/w2v_emotion_model\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/soundstream_lstm\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import utils.wav2Vec_utils as w2vU\n",
    "import utils.audio_dataset_utils as ADU\n",
    "import network_models.soundstream_lstm.LSTM_dataset as lds\n",
    "import network_models.soundstream_lstm.CombinedEmoDataset_7_Emo as ced\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "importlib.reload(w2vU)\n",
    "importlib.reload(lds)\n",
    "importlib.reload(ced)\n",
    "#\n",
    "# #tess = lds.AudioEmotionTessDataset(directory=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\", device=\"cpu\")\n",
    "emo_dataset = ced.CombinedEmoDataSet_7_emos(directory_tess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\",\n",
    "                                             directory_cafe=\"/home/ckwdani/Music/emotionDatasets/converted_mono/cafe\",\n",
    "                                             directory_ravdess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/RAVDESS Audio_Speech_Actors_01-24\",\n",
    "                                             directory_mesd=\"/home/ckwdani/Music/emotionDatasets/converted_mono/mesd\",\n",
    "                                             device=\"cpu\")\n",
    "processor, sr = w2vU.init_w2v2(num_labels=len(emo_dataset.label_list), label_list=emo_dataset.label_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# maxnum = 0\n",
    "# maxnum1 = 0\n",
    "# maxnum2 = 0\n",
    "# maxindex = 0\n",
    "# for i in range(0, len(emo_dataset) - 1):\n",
    "#     lo = emo_dataset[i][0].cpu()\n",
    "#\n",
    "#     #size = (processor(lo)[\"input_values\"][0][0].shape)[0]\n",
    "#     size = lo.shape[1]\n",
    "#     maxnum = max(maxnum, size)\n",
    "#     if(maxnum != size):\n",
    "#         maxnum1 = max(maxnum1, size)\n",
    "#     if(maxnum != size and maxnum1 != size):\n",
    "#         maxnum2 = max(maxnum2, size)\n",
    "#     if(maxnum == size):\n",
    "#         maxindex = i\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.25\n",
      "7.531125\n",
      "7.2345\n"
     ]
    }
   ],
   "source": [
    "# print(100000 / 16000)\n",
    "# print(maxnum1 / 16000)\n",
    "# print(maxnum2 / 16000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# f,s = torchaudio.load(emo_dataset.dataFrame[\"path\"][4640])\n",
    "# len(f[0])\n",
    "# lo = emo_dataset[6000][0].cpu()\n",
    "# len(lo[0])\n",
    "# processor(lo, return_tensors=\"pt\")\n",
    "# print(processor(lo[0], return_tensors=\"pt\"), lo[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...generating encoding\r"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# importlib.reload(lds)\n",
    "#\n",
    "# loaded = emo_dataset[500][0].cpu()\n",
    "# samplerate = emo_dataset.samplerate\n",
    "#\n",
    "newSet = lds.AudioEmotionTessWav2VecDataset(emo_dataset, processor= processor, sampling_rate=sr)\n",
    "#res = torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=sr)(loaded)\n",
    "#output = processor(res)\n",
    "#processor.pad()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Debug old model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import network_models.w2v_emotion_model.custom_model as cm\n",
    "import  network_models.w2v_emotion_model.custom_collator as cc\n",
    "import gc\n",
    "import importlib\n",
    "device = \"cpu\"\n",
    "#importlib.reload(cm)\n",
    "# import network_models.soundstream_lstm.model_trainer as ct\n",
    "#\n",
    "#model = cm.W2V_EmotionClassifierSevenEmos().to(device=device)\n",
    "model_name_or_path = \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "gc.collect()\n",
    "importlib.reload(cm)\n",
    "#tess = lds.AudioEmotionTessDataset(directory=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\")\n",
    "\n",
    "model = cm.Wav2Vec2ForSpeechClassification(model_name_or_path=model_name_or_path, pooling_mode=\"mean\", device=device).to(\"cpu\")\n",
    "model2 = cm.W2V_EmotionClassifierSevenEmos().to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f762b84a6c0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_517752/824063702.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asarray(newSet[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0.3689, 0.0625, 0.3361, 0.0012, 0.1861, 0.0440, 0.0012]],\n       grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = cc.DataCollatorCTCWithPadding(processor=processor, padding=True, num_labels=len(newSet.dataSet.label_list)).collate_fn\n",
    "#loader = DataLoader(newSet, shuffle=True, num_workers=2 ,collate_fn=data_collator)\n",
    "arr = np.asarray(newSet[0])\n",
    "vals = data_collator(arr[None, :])\n",
    "\n",
    "#vals = enumerate(loader)[0]\n",
    "model(vals[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2PreTrainedModel, Wav2Vec2Model, Wav2Vec2Config\n",
    "\n",
    "\n",
    "config = Wav2Vec2Config(name_or_path=model_name_or_path)\n",
    "wav2vec2 = Wav2Vec2Model(config).to(device)\n",
    "def gen(data):\n",
    "    outputs = wav2vec2(data)\n",
    "    return torch.mean(outputs[0], dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.9008, -0.3096,  0.7530,  0.1439,  0.7899,  0.3786, -0.2956,  0.4463,\n         -0.7978,  0.4299, -0.4700, -0.2694,  0.4110, -0.8196, -0.5054,  0.0825,\n          0.6343, -1.6062, -0.3723, -0.2128, -0.6787, -0.6757, -0.5471,  0.0829,\n          0.9843, -0.1209,  0.7534,  0.2680,  0.2844,  0.8103, -0.6863,  0.2118,\n         -0.0171,  1.2855, -1.9144, -0.5711,  0.0812,  0.4870, -0.9308,  0.3020,\n          0.0284,  0.5335,  0.1846,  0.7637,  0.4484, -0.5277, -0.4951,  0.5641,\n          0.7578, -0.0168, -0.1442,  0.3758, -0.2371,  0.1367,  1.2959,  0.0593,\n         -0.4436, -1.4407,  0.2164, -0.8998,  0.0868, -0.8606,  0.0490,  0.3743,\n         -0.5595, -1.6432,  0.2320,  0.7943,  0.8324,  0.2879, -0.0803, -0.4916,\n         -0.5705,  0.5791,  0.1537, -0.0113, -0.9397, -0.1012, -1.2689, -0.7750,\n          0.4140, -0.1385, -0.7271,  0.0056,  0.2039,  0.7528,  0.1659, -0.4618,\n         -0.6326, -0.8201,  0.2963,  0.1479,  0.3667, -0.5136, -0.4786, -0.5223,\n         -0.2813, -0.6745,  0.5832, -0.1231,  0.0305, -0.0558, -0.5586, -0.7598,\n         -0.3148, -0.9541,  0.3590, -0.6519,  0.4677,  1.4426, -0.0402,  0.4099,\n         -0.6977, -1.3039, -0.1071,  0.1917, -0.2701,  0.0423, -0.2059,  1.1072,\n         -0.0486,  0.2033, -0.6086, -0.2729,  0.2590, -1.2793,  0.9792,  0.5758,\n         -0.6666, -0.3449, -0.0184,  0.3565, -0.3267,  0.6944,  0.3408,  0.1775,\n          0.3687,  0.4232, -0.7443, -0.5758,  0.5687,  0.6333, -0.4207, -0.5773,\n         -0.0209, -0.0415, -0.0062,  1.0404, -0.7665, -0.8027,  1.7974, -0.2465,\n          0.5377, -0.4328, -0.2455,  0.3108,  0.6319, -0.0268,  0.3247,  0.1849,\n          0.9173,  0.0847, -0.0609,  0.2227,  0.1240,  0.4456,  0.8093,  0.4082,\n         -0.7321,  0.1535,  0.4154, -0.3122,  0.1085,  0.2875, -0.2182,  0.7370,\n         -0.6429,  0.1970,  0.8180, -0.1908,  0.8651, -1.0335,  0.9490,  0.0992,\n          1.2886, -0.0711,  1.1166,  0.5379,  0.5478,  0.3214, -0.1607, -0.3017,\n         -0.2087, -0.5338,  0.1420,  0.5671, -0.2226, -0.5921,  0.6578,  0.4588,\n          0.0800, -0.0990, -0.1867,  0.3667,  0.2636, -0.2173, -0.0874, -1.1936,\n         -0.6068, -0.0640, -0.2398,  0.8829,  0.5670,  0.1244, -0.2832,  1.4531,\n         -0.8336, -0.5508, -0.6649, -0.3381,  0.8882,  0.2130, -1.0906, -0.1592,\n          0.7525, -0.4104, -0.7311, -1.1238,  0.0616,  0.4067,  0.5922,  0.1676,\n         -0.0101,  0.1083, -0.2014,  0.4983,  0.1582, -0.2621, -0.2700,  0.0428,\n         -0.4253,  0.9820,  0.6181, -0.8821, -0.1291,  0.4245,  1.0491,  1.2294,\n          0.2817, -0.7482, -0.0382,  0.7401, -0.3615,  1.3272,  0.3320, -0.6687,\n         -0.4155, -0.4310, -1.6232,  0.3365, -0.1662,  0.0489, -0.1239, -0.0445,\n          0.4560,  1.7426,  0.0334, -0.4589, -0.6346, -0.8333,  0.0338, -0.7888,\n         -0.4108,  0.3107,  0.2674, -0.0092,  0.8272,  0.0847,  0.4473,  0.0992,\n          0.2364, -0.4174,  0.1206, -0.0244,  1.0782,  0.0761, -0.3463,  0.5962,\n          0.3193, -0.9143,  0.2084, -0.5105,  0.4210,  0.4750,  0.2815, -0.3743,\n         -0.0326, -0.9252, -0.1276, -0.2568,  0.8220,  0.9908, -0.3606, -0.7610,\n         -0.4371, -0.1178, -0.2531, -0.2417, -0.6675, -1.1157,  0.4885,  0.0218,\n          0.1271, -0.3705,  0.2274, -0.6677, -0.5302, -1.7367, -0.3071,  0.0084,\n          0.2365,  0.4159, -0.0226,  0.1633, -0.6448,  0.1679,  0.4719, -0.6009,\n         -0.6915,  0.0677, -0.1195,  0.6447, -0.1627,  0.4971,  0.7334, -0.3025,\n         -0.5806, -0.8201, -0.1345, -0.4370, -0.4951,  0.1657, -0.0361, -0.3027,\n          1.1800,  1.7278, -0.7638,  0.3103,  0.3288, -0.3269,  1.3144, -0.6705,\n          0.4996,  0.4550, -0.0732,  0.7115, -0.1081, -0.1374, -0.8486,  0.3418,\n          0.6549, -0.0515,  0.6899, -0.0328,  0.6912, -0.0943, -0.5443,  0.7115,\n          0.1912,  0.1166,  0.0451, -0.1907,  0.3178,  0.5819,  0.8709,  0.4618,\n         -1.3410,  1.0147,  0.6363,  0.4131,  0.5815, -0.3616, -1.0839, -1.1492,\n         -0.1211,  0.3137, -0.2345,  1.3695, -0.7573,  0.2838, -0.0724, -0.4134,\n         -0.0336,  0.4147, -0.2709, -0.4595, -1.1042,  0.6559, -1.4796, -0.7049,\n          0.3356, -0.8915, -0.5253,  0.3142,  1.2530, -0.4242, -0.2530,  0.3767,\n         -0.0859, -0.4411, -0.0079, -0.8908, -0.4457, -0.3896, -0.4408,  1.3881,\n         -1.5033,  0.2742, -1.0897,  0.5496, -0.3810, -0.5320, -1.0864,  0.4491,\n          0.3158,  0.7080, -0.4340, -0.8839,  0.2668,  0.0558, -0.0873, -0.5025,\n          0.6085,  0.0064,  0.4841,  1.6612, -0.8187,  0.3700, -0.5416,  0.2274,\n         -0.8269,  0.2117,  0.7928, -0.5560, -0.1476,  0.6690,  0.3355,  0.3481,\n         -0.0832,  0.8785,  0.0110,  0.1542,  0.5900, -0.1141,  0.1359,  0.2456,\n         -1.0918, -0.8512,  0.9963, -0.1982,  0.1473,  0.1542,  1.0294, -0.5509,\n          0.7578,  0.2640,  0.7581,  0.2850, -0.8354, -0.0845, -0.3974, -0.1861,\n          0.3662, -0.4715,  0.0052, -0.2328, -0.1235, -0.2397, -0.4019, -0.4053,\n          1.1638,  1.0043,  0.4631,  0.2028,  0.3276, -0.0255, -0.4385, -0.1188,\n         -0.9335,  0.2076, -0.1621, -0.1935, -1.6145,  0.8119,  0.8435, -0.3110,\n         -0.1649,  0.0181,  0.7754, -0.0681, -0.6413,  0.5652, -1.0847, -0.3433,\n         -0.6629, -0.1604, -0.7169,  0.3999,  0.0368, -0.7826, -0.2972,  0.1853,\n         -0.8404,  0.6300, -0.8728,  0.7355,  0.3218,  0.3588, -0.7099, -0.7291,\n         -0.9263,  0.6084, -0.2694, -0.5182,  0.8089, -0.1229,  0.7907,  0.2604,\n          0.3902, -0.2149, -0.2183, -0.3916, -1.0203, -0.0994, -0.2480,  0.2578,\n         -0.7084, -0.2495,  0.4939,  0.8010, -0.7785, -0.2278,  0.3648,  0.0383,\n          0.1838,  0.2036,  0.4161,  0.0184, -0.1863, -0.3213, -0.1845,  0.8063,\n         -0.8249, -0.3121, -0.0518,  1.0121, -0.3304,  0.4550, -0.8110, -0.1888,\n          0.1111,  1.0904, -1.0291, -1.4509, -0.7050,  0.1231, -0.3176,  0.5016,\n         -0.3823,  0.4164, -0.1373, -1.0923,  0.3532,  0.7016,  1.4093,  0.2808,\n         -0.3033,  0.7652,  0.3935,  0.9417,  0.2567,  0.4073, -0.0192,  0.3811,\n         -0.0664,  0.1124,  0.8096,  1.1994, -0.3121,  0.6812,  0.0081, -0.1108,\n         -0.5135,  0.1516, -0.6683, -1.1270, -0.2697, -0.5343,  0.6840, -0.2400,\n          0.2292, -0.8512,  0.5815, -0.9013,  0.0145, -0.4549,  0.1921, -1.1736,\n          0.5979, -0.0258, -0.6500,  0.1725,  0.6599,  1.1916, -0.4611,  0.0843,\n         -0.1808,  0.3157, -0.6657, -0.3740,  0.0606,  0.4962,  0.2276,  0.2781,\n          0.9606,  0.0307, -0.7318,  0.2716, -0.4042, -0.4511, -0.4066,  0.7103,\n         -1.3087,  0.1162,  0.2552, -0.6446,  0.0535, -0.1233, -0.6747, -0.5401,\n         -0.4567,  0.6482,  0.8648, -1.2437,  0.4797,  0.7604, -0.0135,  0.1274,\n         -0.3529,  0.2425, -0.2107,  1.1303, -0.5964, -0.6007, -0.0683,  0.4791,\n          0.6555,  0.2681, -0.9262,  1.0684,  0.4577, -0.4545,  1.3595, -0.5989,\n         -0.1368,  0.0436,  0.6754, -0.6439,  0.1394, -0.1349, -0.7907, -0.0691,\n          0.4150,  0.1426, -0.7590, -0.4979, -0.3573,  1.2835, -0.7314, -1.4695,\n         -0.0134, -0.7213,  1.3176,  0.8772, -0.1383,  0.0543,  0.9858, -0.5716,\n          0.3720, -0.4258,  0.3900,  0.6240,  0.8436,  0.3079,  0.0249,  0.4073,\n          0.7368,  0.0152,  0.6071, -0.4494, -0.1813, -0.5259, -1.2798,  1.3034,\n         -1.4171,  0.3661,  0.5179,  0.2543, -0.3889,  0.7227,  0.4862,  0.5791,\n          0.3886, -0.1270,  0.3762, -0.0883,  0.9477, -0.1508,  0.1754,  0.7229,\n          0.2709,  0.9602, -1.2405, -0.3502,  0.2156, -0.6130, -0.1030, -0.8635,\n         -0.5421,  1.2743,  0.2945, -0.5390,  0.8284,  0.7609, -1.1148,  0.6761,\n         -0.1399, -1.2161, -0.2364, -0.0744,  1.0323,  0.6032, -0.8050, -0.7956,\n          0.8585, -0.3703, -0.0418, -0.6519,  0.1589, -0.3439, -0.5469,  0.0669,\n          1.3828,  1.1040,  0.4368,  0.6973, -1.3283,  0.5924, -0.8074,  0.8324,\n          0.6605,  0.3067, -0.0754, -0.8020, -0.1666,  0.1274, -0.3576,  0.7970]],\n       grad_fn=<MeanBackward1>)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = gen(vals[0])\n",
    "#model2(enc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training old Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# importlib.reload(ADU)\n",
    "# trainSet, evalSet = ADU.train_val_dataset(newSet, 0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# emo_dataset.label_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# importlib.reload(ct)\n",
    "# trainer = ct.ModelTrainer(model=model, need_reshape=False, train_dataset=trainSet, eval_dataset=evalSet, device=device, batch_size=8, save_model_every=500, num_epochs=5001, model_path=\"content/classifier/W2VClassifier/Nr1/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "#\n",
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...generating encoding\r"
     ]
    }
   ],
   "source": [
    "importlib.reload(w2vU)\n",
    "importlib.reload(lds)\n",
    "importlib.reload(ced)\n",
    "device = \"cpu\"\n",
    "# emo_dataset = ced.CombinedEmoDataSet_7_emos(directory_tess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\",\n",
    "#                                              directory_cafe=\"/home/ckwdani/Music/emotionDatasets/converted_mono/cafe\",\n",
    "#                                              directory_ravdess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/RAVDESS Audio_Speech_Actors_01-24\",\n",
    "#                                              directory_mesd=\"/home/ckwdani/Music/emotionDatasets/converted_mono/mesd\",\n",
    "#                                              device=\"cpu\")\n",
    "emo_dataset = lds.AudioEmotionTessDataset(directory=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\", device=device)\n",
    "processor, sr = w2vU.init_w2v2(num_labels=len(emo_dataset.label_list), label_list=emo_dataset.label_list, device=device)\n",
    "#processor.to(device)\n",
    "#\n",
    "#torch.save(model.state_dict(), \"content/classifier/W2VClassifier/Nr1/model_3363.pth\")\n",
    "\n",
    "newSet = lds.AudioEmotionTessWav2VecDataset(emo_dataset, processor= processor, sampling_rate=sr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2Config, Wav2Vec2Model\n",
    "import network_models.soundstream_lstm.LSTM_dataset as lds\n",
    "import network_models.w2v_emotion_model.custom_model as cm\n",
    "import gc\n",
    "import network_models.soundstream_lstm.model_trainer as ct\n",
    "model_name_or_path = \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "gc.collect()\n",
    "importlib.reload(cm)\n",
    "#tess = lds.AudioEmotionTessDataset(directory=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\")\n",
    "\n",
    "model = cm.Wav2Vec2ForSpeechClassification(model_name_or_path=model_name_or_path, pooling_mode=\"mean\", device=device)\n",
    "# # config\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     num_labels=len(tess.label_list),\n",
    "#     label2id={label: i for i, label in enumerate(tess.label_list)},\n",
    "#     id2label={i: label for i, label in enumerate(tess.label_list)},\n",
    "#     finetuning_task=\"wav2vec2_clf\",\n",
    "# )\n",
    "# setattr(config, 'pooling_mode', pooling_mode)\n",
    "#\n",
    "# processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "# target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "#\n",
    "# conf = Wav2Vec2Config(name_or_path=model_name_or_path)\n",
    "# model = Wav2Vec2Model(conf)\n",
    "\n",
    "#model(tess[0][0], tess[0][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 0.00907809, -0.08372961, -0.13303371, ...,  0.0525817 ,\n         0.04243086,  0.03082989], dtype=float32),\n array([1]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSet[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import  network_models.w2v_emotion_model.custom_collator as cc\n",
    "import  network_models.w2v_emotion_model.custom_trainer as ctct\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "trainSet, evalSet = ADU.train_val_dataset(newSet, 0.02)\n",
    "trainSet, evalSet = ADU.train_val_dataset(evalSet, 0.1)\n",
    "\n",
    "importlib.reload(ctct)\n",
    "importlib.reload(cm)\n",
    "importlib.reload(cc)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == np.argmax(p.label_ids, axis=1)).astype(np.float32).mean().item()}\n",
    "\n",
    "model.freeze_feature_extractor()\n",
    "data_collator = cc.DataCollatorCTCWithPadding(processor=processor, padding=True, num_labels=len(newSet.dataSet.label_list))\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"content/W2VClassifier/Nr2\",\n",
    "#     # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     num_train_epochs=10.0,\n",
    "#     fp16=True,\n",
    "#     save_steps=100,\n",
    "#     eval_steps=100,\n",
    "#     logging_steps=100,\n",
    "#     learning_rate=1e-4,\n",
    "#     save_total_limit=2,\n",
    "# )\n",
    "\n",
    "# trainer = ctct.CTCTrainer(\n",
    "#     model=model,\n",
    "#     data_collator=data_collator,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=trainSet,\n",
    "#     eval_dataset=evalSet,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "# )\n",
    "\n",
    "import network_models.soundstream_lstm.model_trainer as ct1\n",
    "\n",
    "\n",
    "# importlib.reload(ct1)\n",
    "# trainer = ct1.ModelTrainer(model=model, need_reshape=False, train_dataset=trainSet, eval_dataset=evalSet, device=device, batch_size=8, save_model_every=500, num_epochs=5001, model_path=\"content/classifier/W2VClassifier/Nr1/\", data_collator=data_collator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.945971  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.945413 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.942434  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.944779 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.945729  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.944705 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.937100  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.944342 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.907578  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.943351 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.960479  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.943211 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.954976  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.942728 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.962167  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.942149 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.926746  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.941849 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.953983  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.941910 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.949798  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.941316 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.979265  [    0/   50]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 1.941140 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ct1)\n",
    "trainer = ct1.ModelTrainer(model=model, need_reshape=False, train_dataset=trainSet, eval_dataset=evalSet, device=device, batch_size=2, save_model_every=500, num_epochs=5001, model_path=\"content/classifier/W2VClassifier/Nr1/\", data_collator=data_collator)\n",
    "gc.collect()\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "[2]\n",
      "[5]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundstream_lstm/model_trainer.py:51\u001B[0m, in \u001B[0;36mModelTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     49\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memo_reco_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loop(test_dataloader, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundstream_lstm/model_trainer.py:60\u001B[0m, in \u001B[0;36mModelTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_loop\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataloader, model, loss_fn, optimizer):\n\u001B[1;32m     59\u001B[0m     size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m     61\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneed_reshape):\n\u001B[1;32m     62\u001B[0m             X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m512\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m400\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
