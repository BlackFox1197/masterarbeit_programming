{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 17:27:10.603270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 17:27:11.156645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:27:11.156796: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 17:27:11.156802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils.audio_dataset_utils import train_val_dataset\n",
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 12\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final/\"\n",
    "trials_per_model_type = 1\n",
    "epochs_per_model = 1000\n",
    "save_highest_acc_min_acc = 0.5\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks_clip/content/datasets/soundstream_encoded/allEncodings_noInducednoStimuli_3_5_sec_v12_1_basic.pkl\", device=\"cuda\")\n",
    "    #csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient, save_highest_acc_min_acc=save_highest_acc_min_acc, seed=3333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.061443  [ 1200/ 4873]\n",
      "loss: 1.932718  [ 2400/ 4873]\n",
      "loss: 1.954849  [ 3600/ 4873]\n",
      "loss: 1.891762  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.164     0.000     0.000    105\n",
      " disgust     0.164     0.000     0.000    109\n",
      "    fear     0.164     0.000     0.000    80\n",
      "   happy     0.164     1.000     0.160    81\n",
      " neutral     0.164     0.000     0.000    84\n",
      "     sad     0.164     0.146     1.000    87\n",
      "surprise     0.164     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.164     0.164     0.166    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 16.4%, Avg loss: 1.980615 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.881003  [ 1200/ 4873]\n",
      "loss: 1.966375  [ 2400/ 4873]\n",
      "loss: 2.033295  [ 3600/ 4873]\n",
      "loss: 1.996691  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.161     0.000     0.000    105\n",
      " disgust     0.161     0.000     0.000    109\n",
      "    fear     0.161     0.000     0.000    80\n",
      "   happy     0.161     0.786     0.136    81\n",
      " neutral     0.161     0.000     0.000    84\n",
      "     sad     0.161     0.146     1.000    87\n",
      "surprise     0.161     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.161     0.133     0.162    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 16.1%, Avg loss: 1.948576 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.032190  [ 1200/ 4873]\n",
      "loss: 1.798020  [ 2400/ 4873]\n",
      "loss: 2.031765  [ 3600/ 4873]\n",
      "loss: 1.902628  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.177     0.000     0.000    105\n",
      " disgust     0.177     0.000     0.000    109\n",
      "    fear     0.177     0.000     0.000    80\n",
      "   happy     0.177     0.331     0.556    81\n",
      " neutral     0.177     0.000     0.000    84\n",
      "     sad     0.177     0.133     0.724    87\n",
      "surprise     0.177     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.177     0.066     0.183    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 17.7%, Avg loss: 1.916107 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.019827  [ 1200/ 4873]\n",
      "loss: 1.818405  [ 2400/ 4873]\n",
      "loss: 1.973478  [ 3600/ 4873]\n",
      "loss: 1.835705  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.190     0.000     0.000    105\n",
      " disgust     0.190     0.000     0.000    109\n",
      "    fear     0.190     0.000     0.000    80\n",
      "   happy     0.190     0.345     0.630    81\n",
      " neutral     0.190     0.000     0.000    84\n",
      "     sad     0.190     0.141     0.747    87\n",
      "surprise     0.190     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.190     0.069     0.197    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.905550 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.902292  [ 1200/ 4873]\n",
      "loss: 1.802156  [ 2400/ 4873]\n",
      "loss: 1.967000  [ 3600/ 4873]\n",
      "loss: 1.825016  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.151     0.000     0.000    105\n",
      " disgust     0.151     0.000     0.000    109\n",
      "    fear     0.151     0.000     0.000    80\n",
      "   happy     0.151     0.174     0.877    81\n",
      " neutral     0.151     0.000     0.000    84\n",
      "     sad     0.151     0.104     0.241    87\n",
      "surprise     0.151     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.151     0.040     0.160    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 15.1%, Avg loss: 1.985992 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.915296  [ 1200/ 4873]\n",
      "loss: 1.848046  [ 2400/ 4873]\n",
      "loss: 2.000118  [ 3600/ 4873]\n",
      "loss: 1.899586  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.175     0.000     0.000    105\n",
      " disgust     0.175     0.000     0.000    109\n",
      "    fear     0.175     0.000     0.000    80\n",
      "   happy     0.175     0.418     0.284    81\n",
      " neutral     0.175     0.000     0.000    84\n",
      "     sad     0.175     0.151     0.966    87\n",
      "surprise     0.175     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.175     0.081     0.178    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 1.923675 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.735167  [ 1200/ 4873]\n",
      "loss: 1.813350  [ 2400/ 4873]\n",
      "loss: 1.794718  [ 3600/ 4873]\n",
      "loss: 1.733494  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.215     0.000     0.000    105\n",
      " disgust     0.215     0.000     0.000    109\n",
      "    fear     0.215     0.000     0.000    80\n",
      "   happy     0.215     0.294     0.741    81\n",
      " neutral     0.215     0.000     0.000    84\n",
      "     sad     0.215     0.175     0.816    87\n",
      "surprise     0.215     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.215     0.067     0.222    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.5%, Avg loss: 1.883441 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.757770  [ 1200/ 4873]\n",
      "loss: 1.733119  [ 2400/ 4873]\n",
      "loss: 1.756317  [ 3600/ 4873]\n",
      "loss: 1.806189  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.210     0.000     0.000    105\n",
      " disgust     0.210     0.000     0.000    109\n",
      "    fear     0.210     0.000     0.000    80\n",
      "   happy     0.210     0.394     0.617    81\n",
      " neutral     0.210     0.000     0.000    84\n",
      "     sad     0.210     0.161     0.897    87\n",
      "surprise     0.210     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.210     0.079     0.216    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.0%, Avg loss: 1.872213 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.843790  [ 1200/ 4873]\n",
      "loss: 1.757811  [ 2400/ 4873]\n",
      "loss: 1.875593  [ 3600/ 4873]\n",
      "loss: 1.820761  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.216     0.000     0.000    105\n",
      " disgust     0.216     0.000     0.000    109\n",
      "    fear     0.216     0.000     0.000    80\n",
      "   happy     0.216     0.307     0.667    81\n",
      " neutral     0.216     0.000     0.000    84\n",
      "     sad     0.216     0.184     0.897    87\n",
      "surprise     0.216     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.216     0.070     0.223    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.6%, Avg loss: 1.867804 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.879132  [ 1200/ 4873]\n",
      "loss: 1.770591  [ 2400/ 4873]\n",
      "loss: 1.857619  [ 3600/ 4873]\n",
      "loss: 1.752676  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.218     0.000     0.000    105\n",
      " disgust     0.218     0.000     0.000    109\n",
      "    fear     0.218     0.122     0.062    80\n",
      "   happy     0.218     0.325     0.667    81\n",
      " neutral     0.218     0.000     0.000    84\n",
      "     sad     0.218     0.184     0.851    87\n",
      "surprise     0.218     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.218     0.090     0.226    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.8%, Avg loss: 1.857085 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.893000  [ 1200/ 4873]\n",
      "loss: 1.827118  [ 2400/ 4873]\n",
      "loss: 1.881714  [ 3600/ 4873]\n",
      "loss: 1.793237  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.267     0.000     0.000    105\n",
      " disgust     0.267     0.412     0.321    109\n",
      "    fear     0.267     0.115     0.087    80\n",
      "   happy     0.267     0.406     0.642    81\n",
      " neutral     0.267     0.000     0.000    84\n",
      "     sad     0.267     0.205     0.793    87\n",
      "surprise     0.267     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.267     0.163     0.263    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 1.838087 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.775563  [ 1200/ 4873]\n",
      "loss: 1.926073  [ 2400/ 4873]\n",
      "loss: 1.903445  [ 3600/ 4873]\n",
      "loss: 1.843830  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.289     0.000     0.000    105\n",
      " disgust     0.289     0.425     0.413    109\n",
      "    fear     0.289     0.148     0.100    80\n",
      "   happy     0.289     0.413     0.704    81\n",
      " neutral     0.289     0.000     0.000    84\n",
      "     sad     0.289     0.212     0.759    87\n",
      "surprise     0.289     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.289     0.171     0.282    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.9%, Avg loss: 1.824096 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.782298  [ 1200/ 4873]\n",
      "loss: 1.893604  [ 2400/ 4873]\n",
      "loss: 1.833932  [ 3600/ 4873]\n",
      "loss: 1.681534  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.302     0.253     0.581    105\n",
      " disgust     0.302     0.474     0.495    109\n",
      "    fear     0.302     0.167     0.037    80\n",
      "   happy     0.302     0.314     0.728    81\n",
      " neutral     0.302     0.000     0.000    84\n",
      "     sad     0.302     0.143     0.080    87\n",
      "surprise     0.302     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.302     0.193     0.275    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 1.824332 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.763487  [ 1200/ 4873]\n",
      "loss: 1.998790  [ 2400/ 4873]\n",
      "loss: 1.659151  [ 3600/ 4873]\n",
      "loss: 1.804581  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.303     0.315     0.429    105\n",
      " disgust     0.303     0.378     0.569    109\n",
      "    fear     0.303     0.145     0.100    80\n",
      "   happy     0.303     0.287     0.765    81\n",
      " neutral     0.303     0.000     0.000    84\n",
      "     sad     0.303     0.250     0.092    87\n",
      "surprise     0.303     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.303     0.196     0.279    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 1.831926 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.847547  [ 1200/ 4873]\n",
      "loss: 1.844641  [ 2400/ 4873]\n",
      "loss: 1.655956  [ 3600/ 4873]\n",
      "loss: 1.601487  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.339     0.246     0.571    105\n",
      " disgust     0.339     0.446     0.752    109\n",
      "    fear     0.339     0.219     0.087    80\n",
      "   happy     0.339     0.385     0.704    81\n",
      " neutral     0.339     0.000     0.000    84\n",
      "     sad     0.339     0.500     0.011    87\n",
      "surprise     0.339     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.339     0.256     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.800935 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.905847  [ 1200/ 4873]\n",
      "loss: 1.891189  [ 2400/ 4873]\n",
      "loss: 1.762803  [ 3600/ 4873]\n",
      "loss: 2.002592  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.313     0.253     0.533    105\n",
      " disgust     0.313     0.405     0.606    109\n",
      "    fear     0.313     0.212     0.175    80\n",
      "   happy     0.313     0.344     0.679    81\n",
      " neutral     0.313     0.000     0.000    84\n",
      "     sad     0.313     0.000     0.000    87\n",
      "surprise     0.313     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.313     0.173     0.285    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 1.799118 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.765157  [ 1200/ 4873]\n",
      "loss: 1.892098  [ 2400/ 4873]\n",
      "loss: 1.864402  [ 3600/ 4873]\n",
      "loss: 1.943802  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.303     0.514    105\n",
      " disgust     0.321     0.396     0.679    109\n",
      "    fear     0.321     0.143     0.062    80\n",
      "   happy     0.321     0.300     0.778    81\n",
      " neutral     0.321     0.000     0.000    84\n",
      "     sad     0.321     0.000     0.000    87\n",
      "surprise     0.321     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.163     0.290    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.795791 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.560178  [ 1200/ 4873]\n",
      "loss: 1.784039  [ 2400/ 4873]\n",
      "loss: 1.774486  [ 3600/ 4873]\n",
      "loss: 1.643106  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.272     0.657    105\n",
      " disgust     0.321     0.481     0.587    109\n",
      "    fear     0.321     0.226     0.087    80\n",
      "   happy     0.321     0.295     0.691    81\n",
      " neutral     0.321     0.000     0.000    84\n",
      "     sad     0.321     0.000     0.000    87\n",
      "surprise     0.321     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.182     0.289    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.788928 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.610263  [ 1200/ 4873]\n",
      "loss: 1.661206  [ 2400/ 4873]\n",
      "loss: 1.794668  [ 3600/ 4873]\n",
      "loss: 1.728107  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.349     0.275     0.638    105\n",
      " disgust     0.349     0.426     0.798    109\n",
      "    fear     0.349     0.118     0.025    80\n",
      "   happy     0.349     0.396     0.704    81\n",
      " neutral     0.349     0.000     0.000    84\n",
      "     sad     0.349     0.000     0.000    87\n",
      "surprise     0.349     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.349     0.174     0.309    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 1.760743 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.851248  [ 1200/ 4873]\n",
      "loss: 1.731402  [ 2400/ 4873]\n",
      "loss: 1.874709  [ 3600/ 4873]\n",
      "loss: 1.757061  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.318     0.307     0.590    105\n",
      " disgust     0.318     0.472     0.624    109\n",
      "    fear     0.318     0.115     0.037    80\n",
      "   happy     0.318     0.276     0.716    81\n",
      " neutral     0.318     0.000     0.000    84\n",
      "     sad     0.318     0.083     0.011    87\n",
      "surprise     0.318     0.125     0.031    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.318     0.197     0.287    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 1.791231 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.794464  [ 1200/ 4873]\n",
      "loss: 1.786058  [ 2400/ 4873]\n",
      "loss: 1.811874  [ 3600/ 4873]\n",
      "loss: 1.723610  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.320     0.260     0.562    105\n",
      " disgust     0.320     0.415     0.624    109\n",
      "    fear     0.320     0.214     0.113    80\n",
      "   happy     0.320     0.343     0.728    81\n",
      " neutral     0.320     0.000     0.000    84\n",
      "     sad     0.320     0.000     0.000    87\n",
      "surprise     0.320     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.320     0.176     0.290    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 1.765289 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.762728  [ 1200/ 4873]\n",
      "loss: 1.849482  [ 2400/ 4873]\n",
      "loss: 1.728198  [ 3600/ 4873]\n",
      "loss: 1.714724  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.293     0.268     0.429    105\n",
      " disgust     0.293     0.402     0.624    109\n",
      "    fear     0.293     0.091     0.037    80\n",
      "   happy     0.293     0.268     0.778    81\n",
      " neutral     0.293     0.000     0.000    84\n",
      "     sad     0.293     0.000     0.000    87\n",
      "surprise     0.293     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.293     0.147     0.267    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 29.3%, Avg loss: 1.796971 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.620328  [ 1200/ 4873]\n",
      "loss: 1.577048  [ 2400/ 4873]\n",
      "loss: 1.621501  [ 3600/ 4873]\n",
      "loss: 1.712601  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.246     0.667    105\n",
      " disgust     0.338     0.597     0.651    109\n",
      "    fear     0.338     0.100     0.025    80\n",
      "   happy     0.338     0.353     0.741    81\n",
      " neutral     0.338     0.000     0.000    84\n",
      "     sad     0.338     0.000     0.000    87\n",
      "surprise     0.338     0.500     0.047    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.256     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.729820 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.838794  [ 1200/ 4873]\n",
      "loss: 1.764296  [ 2400/ 4873]\n",
      "loss: 1.875771  [ 3600/ 4873]\n",
      "loss: 1.852597  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.308     0.505    105\n",
      " disgust     0.321     0.412     0.670    109\n",
      "    fear     0.321     0.156     0.062    80\n",
      "   happy     0.321     0.294     0.790    81\n",
      " neutral     0.321     0.000     0.000    84\n",
      "     sad     0.321     0.000     0.000    87\n",
      "surprise     0.321     0.167     0.016    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.191     0.292    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.755670 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.854092  [ 1200/ 4873]\n",
      "loss: 1.471140  [ 2400/ 4873]\n",
      "loss: 1.837812  [ 3600/ 4873]\n",
      "loss: 1.585055  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.316     0.241     0.505    105\n",
      " disgust     0.316     0.503     0.679    109\n",
      "    fear     0.316     0.111     0.037    80\n",
      "   happy     0.316     0.303     0.753    81\n",
      " neutral     0.316     0.000     0.000    84\n",
      "     sad     0.316     0.000     0.000    87\n",
      "surprise     0.316     0.250     0.031    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.316     0.201     0.286    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 1.741277 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.595543  [ 1200/ 4873]\n",
      "loss: 1.706580  [ 2400/ 4873]\n",
      "loss: 1.625387  [ 3600/ 4873]\n",
      "loss: 1.680891  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.330     0.234     0.619    105\n",
      " disgust     0.330     0.497     0.697    109\n",
      "    fear     0.330     0.231     0.037    80\n",
      "   happy     0.330     0.351     0.667    81\n",
      " neutral     0.330     0.000     0.000    84\n",
      "     sad     0.330     0.000     0.000    87\n",
      "surprise     0.330     0.500     0.047    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.330     0.259     0.295    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.721173 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.803997  [ 1200/ 4873]\n",
      "loss: 1.760621  [ 2400/ 4873]\n",
      "loss: 1.818158  [ 3600/ 4873]\n",
      "loss: 1.719825  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.311     0.252     0.590    105\n",
      " disgust     0.311     0.483     0.633    109\n",
      "    fear     0.311     0.143     0.025    80\n",
      "   happy     0.311     0.292     0.704    81\n",
      " neutral     0.311     0.000     0.000    84\n",
      "     sad     0.311     0.000     0.000    87\n",
      "surprise     0.311     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.311     0.167     0.279    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.1%, Avg loss: 1.727947 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.592322  [ 1200/ 4873]\n",
      "loss: 1.794195  [ 2400/ 4873]\n",
      "loss: 1.486291  [ 3600/ 4873]\n",
      "loss: 1.527123  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.318     0.258     0.619    105\n",
      " disgust     0.318     0.557     0.587    109\n",
      "    fear     0.318     0.105     0.025    80\n",
      "   happy     0.318     0.300     0.741    81\n",
      " neutral     0.318     0.000     0.000    84\n",
      "     sad     0.318     0.000     0.000    87\n",
      "surprise     0.318     0.231     0.047    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.318     0.207     0.288    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 1.725737 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.457584  [ 1200/ 4873]\n",
      "loss: 1.736477  [ 2400/ 4873]\n",
      "loss: 1.634400  [ 3600/ 4873]\n",
      "loss: 1.769162  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.284     0.600    105\n",
      " disgust     0.336     0.503     0.679    109\n",
      "    fear     0.336     0.188     0.037    80\n",
      "   happy     0.336     0.300     0.778    81\n",
      " neutral     0.336     0.000     0.000    84\n",
      "     sad     0.336     0.000     0.000    87\n",
      "surprise     0.336     0.286     0.031    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.223     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.700812 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.892348  [ 1200/ 4873]\n",
      "loss: 1.551345  [ 2400/ 4873]\n",
      "loss: 1.688058  [ 3600/ 4873]\n",
      "loss: 1.760409  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.240     0.505    105\n",
      " disgust     0.321     0.441     0.752    109\n",
      "    fear     0.321     0.167     0.075    80\n",
      "   happy     0.321     0.340     0.679    81\n",
      " neutral     0.321     0.000     0.000    84\n",
      "     sad     0.321     0.000     0.000    87\n",
      "surprise     0.321     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.170     0.287    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.683160 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.895702  [ 1200/ 4873]\n",
      "loss: 1.632838  [ 2400/ 4873]\n",
      "loss: 1.590683  [ 3600/ 4873]\n",
      "loss: 1.619826  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.315     0.249     0.486    105\n",
      " disgust     0.315     0.447     0.697    109\n",
      "    fear     0.315     0.138     0.050    80\n",
      "   happy     0.315     0.303     0.741    81\n",
      " neutral     0.315     0.000     0.000    84\n",
      "     sad     0.315     0.000     0.000    87\n",
      "surprise     0.315     0.143     0.016    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.315     0.183     0.284    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.5%, Avg loss: 1.695550 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.433394  [ 1200/ 4873]\n",
      "loss: 1.714062  [ 2400/ 4873]\n",
      "loss: 1.846760  [ 3600/ 4873]\n",
      "loss: 1.570203  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.341     0.249     0.676    105\n",
      " disgust     0.341     0.521     0.798    109\n",
      "    fear     0.341     0.294     0.062    80\n",
      "   happy     0.341     0.338     0.556    81\n",
      " neutral     0.341     0.000     0.000    84\n",
      "     sad     0.341     0.000     0.000    87\n",
      "surprise     0.341     0.000     0.000    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.341     0.200     0.299    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.666816 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.525494  [ 1200/ 4873]\n",
      "loss: 1.594767  [ 2400/ 4873]\n",
      "loss: 1.672417  [ 3600/ 4873]\n",
      "loss: 1.557854  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.339     0.250     0.648    105\n",
      " disgust     0.339     0.551     0.743    109\n",
      "    fear     0.339     0.167     0.025    80\n",
      "   happy     0.339     0.316     0.679    81\n",
      " neutral     0.339     0.000     0.000    84\n",
      "     sad     0.339     0.000     0.000    87\n",
      "surprise     0.339     0.250     0.016    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.339     0.219     0.301    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.644906 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.468852  [ 1200/ 4873]\n",
      "loss: 1.880097  [ 2400/ 4873]\n",
      "loss: 1.876540  [ 3600/ 4873]\n",
      "loss: 1.561283  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.237     0.600    105\n",
      " disgust     0.336     0.591     0.716    109\n",
      "    fear     0.336     0.211     0.050    80\n",
      "   happy     0.336     0.320     0.704    81\n",
      " neutral     0.336     0.000     0.000    84\n",
      "     sad     0.336     0.000     0.000    87\n",
      "surprise     0.336     0.200     0.047    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.223     0.302    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.637244 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.633123  [ 1200/ 4873]\n",
      "loss: 1.689546  [ 2400/ 4873]\n",
      "loss: 1.950207  [ 3600/ 4873]\n",
      "loss: 1.695290  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.281     0.629    105\n",
      " disgust     0.344     0.500     0.743    109\n",
      "    fear     0.344     0.179     0.062    80\n",
      "   happy     0.344     0.314     0.667    81\n",
      " neutral     0.344     0.000     0.000    84\n",
      "     sad     0.344     1.000     0.011    87\n",
      "surprise     0.344     0.250     0.047    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.360     0.308    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.636453 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.477695  [ 1200/ 4873]\n",
      "loss: 1.720267  [ 2400/ 4873]\n",
      "loss: 1.396400  [ 3600/ 4873]\n",
      "loss: 1.739006  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.389     0.335     0.619    105\n",
      " disgust     0.389     0.437     0.826    109\n",
      "    fear     0.389     0.188     0.075    80\n",
      "   happy     0.389     0.417     0.679    81\n",
      " neutral     0.389     0.000     0.000    84\n",
      "     sad     0.389     1.000     0.011    87\n",
      "surprise     0.389     0.444     0.312    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.389     0.403     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.623681 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.071630  [ 1200/ 4873]\n",
      "loss: 1.454414  [ 2400/ 4873]\n",
      "loss: 1.748922  [ 3600/ 4873]\n",
      "loss: 1.389991  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.288     0.619    105\n",
      " disgust     0.366     0.431     0.835    109\n",
      "    fear     0.366     0.174     0.050    80\n",
      "   happy     0.366     0.410     0.593    81\n",
      " neutral     0.366     0.000     0.000    84\n",
      "     sad     0.366     0.000     0.000    87\n",
      "surprise     0.366     0.455     0.234    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.251     0.333    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.612229 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.350576  [ 1200/ 4873]\n",
      "loss: 1.622253  [ 2400/ 4873]\n",
      "loss: 1.851018  [ 3600/ 4873]\n",
      "loss: 1.642221  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.380     0.258     0.562    105\n",
      " disgust     0.380     0.485     0.752    109\n",
      "    fear     0.380     0.133     0.025    80\n",
      "   happy     0.380     0.470     0.667    81\n",
      " neutral     0.380     0.000     0.000    84\n",
      "     sad     0.380     0.000     0.000    87\n",
      "surprise     0.380     0.427     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.380     0.253     0.365    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.615032 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.522293  [ 1200/ 4873]\n",
      "loss: 1.600925  [ 2400/ 4873]\n",
      "loss: 1.506949  [ 3600/ 4873]\n",
      "loss: 1.693925  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.261     0.562    105\n",
      " disgust     0.390     0.542     0.716    109\n",
      "    fear     0.390     0.231     0.037    80\n",
      "   happy     0.390     0.452     0.704    81\n",
      " neutral     0.390     0.000     0.000    84\n",
      "     sad     0.390     0.000     0.000    87\n",
      "surprise     0.390     0.406     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.270     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.624889 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.594682  [ 1200/ 4873]\n",
      "loss: 1.628533  [ 2400/ 4873]\n",
      "loss: 1.602906  [ 3600/ 4873]\n",
      "loss: 1.499317  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.385     0.285     0.600    105\n",
      " disgust     0.385     0.455     0.826    109\n",
      "    fear     0.385     0.188     0.037    80\n",
      "   happy     0.385     0.465     0.580    81\n",
      " neutral     0.385     0.000     0.000    84\n",
      "     sad     0.385     0.000     0.000    87\n",
      "surprise     0.385     0.432     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.385     0.261     0.363    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 1.592268 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.552194  [ 1200/ 4873]\n",
      "loss: 1.705905  [ 2400/ 4873]\n",
      "loss: 1.473744  [ 3600/ 4873]\n",
      "loss: 1.647887  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.269     0.648    105\n",
      " disgust     0.393     0.571     0.734    109\n",
      "    fear     0.393     0.059     0.013    80\n",
      "   happy     0.393     0.506     0.556    81\n",
      " neutral     0.393     0.000     0.000    84\n",
      "     sad     0.393     0.000     0.000    87\n",
      "surprise     0.393     0.414     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.260     0.381    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.587330 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.303520  [ 1200/ 4873]\n",
      "loss: 1.676694  [ 2400/ 4873]\n",
      "loss: 1.889590  [ 3600/ 4873]\n",
      "loss: 1.620301  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.402     0.267     0.600    105\n",
      " disgust     0.402     0.515     0.771    109\n",
      "    fear     0.402     0.211     0.050    80\n",
      "   happy     0.402     0.600     0.556    81\n",
      " neutral     0.402     0.000     0.000    84\n",
      "     sad     0.402     0.000     0.000    87\n",
      "surprise     0.402     0.419     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.402     0.287     0.392    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.582549 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.496492  [ 1200/ 4873]\n",
      "loss: 1.615508  [ 2400/ 4873]\n",
      "loss: 1.714544  [ 3600/ 4873]\n",
      "loss: 1.220846  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.286     0.667    105\n",
      " disgust     0.400     0.513     0.743    109\n",
      "    fear     0.400     0.200     0.025    80\n",
      "   happy     0.400     0.521     0.617    81\n",
      " neutral     0.400     0.000     0.000    84\n",
      "     sad     0.400     0.000     0.000    87\n",
      "surprise     0.400     0.410     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.276     0.385    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.572808 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.791398  [ 1200/ 4873]\n",
      "loss: 1.638976  [ 2400/ 4873]\n",
      "loss: 1.501179  [ 3600/ 4873]\n",
      "loss: 1.458075  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.392     0.249     0.657    105\n",
      " disgust     0.392     0.580     0.761    109\n",
      "    fear     0.392     0.059     0.013    80\n",
      "   happy     0.392     0.545     0.519    81\n",
      " neutral     0.392     0.000     0.000    84\n",
      "     sad     0.392     0.000     0.000    87\n",
      "surprise     0.392     0.458     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.392     0.270     0.377    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.557345 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.612704  [ 1200/ 4873]\n",
      "loss: 1.453044  [ 2400/ 4873]\n",
      "loss: 1.791993  [ 3600/ 4873]\n",
      "loss: 1.581870  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.410     0.261     0.667    105\n",
      " disgust     0.410     0.614     0.716    109\n",
      "    fear     0.410     0.083     0.013    80\n",
      "   happy     0.410     0.548     0.630    81\n",
      " neutral     0.410     0.000     0.000    84\n",
      "     sad     0.410     0.200     0.011    87\n",
      "surprise     0.410     0.467     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.410     0.311     0.400    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.560223 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.435611  [ 1200/ 4873]\n",
      "loss: 1.436219  [ 2400/ 4873]\n",
      "loss: 1.515634  [ 3600/ 4873]\n",
      "loss: 1.492438  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.389     0.236     0.619    105\n",
      " disgust     0.389     0.700     0.578    109\n",
      "    fear     0.389     0.250     0.013    80\n",
      "   happy     0.389     0.559     0.642    81\n",
      " neutral     0.389     0.000     0.000    84\n",
      "     sad     0.389     1.000     0.011    87\n",
      "surprise     0.389     0.377     0.859    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.389     0.446     0.389    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.604702 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.601136  [ 1200/ 4873]\n",
      "loss: 1.646655  [ 2400/ 4873]\n",
      "loss: 1.926313  [ 3600/ 4873]\n",
      "loss: 1.665499  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.392     0.258     0.638    105\n",
      " disgust     0.392     0.526     0.835    109\n",
      "    fear     0.392     0.286     0.075    80\n",
      "   happy     0.392     0.534     0.580    81\n",
      " neutral     0.392     0.000     0.000    84\n",
      "     sad     0.392     0.000     0.000    87\n",
      "surprise     0.392     0.431     0.438    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.392     0.291     0.367    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.571219 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.488496  [ 1200/ 4873]\n",
      "loss: 1.782551  [ 2400/ 4873]\n",
      "loss: 1.903005  [ 3600/ 4873]\n",
      "loss: 1.828413  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.420     0.253     0.648    105\n",
      " disgust     0.420     0.632     0.725    109\n",
      "    fear     0.420     0.167     0.013    80\n",
      "   happy     0.420     0.596     0.654    81\n",
      " neutral     0.420     0.000     0.000    84\n",
      "     sad     0.420     0.222     0.023    87\n",
      "surprise     0.420     0.473     0.828    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.420     0.335     0.413    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.525844 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.273609  [ 1200/ 4873]\n",
      "loss: 1.569256  [ 2400/ 4873]\n",
      "loss: 1.597731  [ 3600/ 4873]\n",
      "loss: 1.265714  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.384     0.248     0.571    105\n",
      " disgust     0.384     0.550     0.761    109\n",
      "    fear     0.384     0.158     0.037    80\n",
      "   happy     0.384     0.462     0.667    81\n",
      " neutral     0.384     0.000     0.000    84\n",
      "     sad     0.384     0.000     0.000    87\n",
      "surprise     0.384     0.436     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.384     0.265     0.367    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 1.551901 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.742661  [ 1200/ 4873]\n",
      "loss: 1.843490  [ 2400/ 4873]\n",
      "loss: 1.598592  [ 3600/ 4873]\n",
      "loss: 1.617046  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.397     0.248     0.638    105\n",
      " disgust     0.397     0.550     0.807    109\n",
      "    fear     0.397     0.071     0.013    80\n",
      "   happy     0.397     0.582     0.568    81\n",
      " neutral     0.397     0.000     0.000    84\n",
      "     sad     0.397     0.333     0.023    87\n",
      "surprise     0.397     0.469     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.397     0.322     0.378    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.536825 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.346899  [ 1200/ 4873]\n",
      "loss: 1.304430  [ 2400/ 4873]\n",
      "loss: 1.661982  [ 3600/ 4873]\n",
      "loss: 1.307090  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.407     0.267     0.552    105\n",
      " disgust     0.407     0.497     0.761    109\n",
      "    fear     0.407     0.200     0.025    80\n",
      "   happy     0.407     0.613     0.605    81\n",
      " neutral     0.407     0.000     0.000    84\n",
      "     sad     0.407     0.111     0.011    87\n",
      "surprise     0.407     0.433     0.859    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.407     0.303     0.402    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.518959 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.511578  [ 1200/ 4873]\n",
      "loss: 1.377138  [ 2400/ 4873]\n",
      "loss: 1.689711  [ 3600/ 4873]\n",
      "loss: 1.546225  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.402     0.257     0.638    105\n",
      " disgust     0.402     0.588     0.734    109\n",
      "    fear     0.402     0.071     0.013    80\n",
      "   happy     0.402     0.597     0.568    81\n",
      " neutral     0.402     0.000     0.000    84\n",
      "     sad     0.402     0.250     0.034    87\n",
      "surprise     0.402     0.436     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.402     0.314     0.391    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.519953 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.648762  [ 1200/ 4873]\n",
      "loss: 1.707996  [ 2400/ 4873]\n",
      "loss: 1.323797  [ 3600/ 4873]\n",
      "loss: 1.444814  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.416     0.263     0.638    105\n",
      " disgust     0.416     0.609     0.716    109\n",
      "    fear     0.416     0.111     0.013    80\n",
      "   happy     0.416     0.593     0.630    81\n",
      " neutral     0.416     0.000     0.000    84\n",
      "     sad     0.416     0.235     0.046    87\n",
      "surprise     0.416     0.461     0.828    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.416     0.325     0.410    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.499363 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.299980  [ 1200/ 4873]\n",
      "loss: 1.716737  [ 2400/ 4873]\n",
      "loss: 1.508823  [ 3600/ 4873]\n",
      "loss: 1.464344  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.228     0.562    105\n",
      " disgust     0.390     0.619     0.716    109\n",
      "    fear     0.390     0.222     0.050    80\n",
      "   happy     0.390     0.459     0.617    81\n",
      " neutral     0.390     0.000     0.000    84\n",
      "     sad     0.390     0.250     0.069    87\n",
      "surprise     0.390     0.554     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.333     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.528056 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.581920  [ 1200/ 4873]\n",
      "loss: 1.603918  [ 2400/ 4873]\n",
      "loss: 1.437591  [ 3600/ 4873]\n",
      "loss: 1.493648  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.403     0.282     0.581    105\n",
      " disgust     0.403     0.497     0.798    109\n",
      "    fear     0.403     0.125     0.037    80\n",
      "   happy     0.403     0.570     0.605    81\n",
      " neutral     0.403     0.000     0.000    84\n",
      "     sad     0.403     0.241     0.080    87\n",
      "surprise     0.403     0.487     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.403     0.315     0.387    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.514931 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.422363  [ 1200/ 4873]\n",
      "loss: 1.327786  [ 2400/ 4873]\n",
      "loss: 1.591758  [ 3600/ 4873]\n",
      "loss: 1.236965  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.241     0.667    105\n",
      " disgust     0.393     0.632     0.725    109\n",
      "    fear     0.393     0.111     0.013    80\n",
      "   happy     0.393     0.575     0.519    81\n",
      " neutral     0.393     0.000     0.000    84\n",
      "     sad     0.393     0.263     0.057    87\n",
      "surprise     0.393     0.462     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.326     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.508395 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.394344  [ 1200/ 4873]\n",
      "loss: 1.334528  [ 2400/ 4873]\n",
      "loss: 1.695223  [ 3600/ 4873]\n",
      "loss: 1.548503  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.407     0.264     0.600    105\n",
      " disgust     0.407     0.539     0.752    109\n",
      "    fear     0.407     0.250     0.013    80\n",
      "   happy     0.407     0.564     0.654    81\n",
      " neutral     0.407     0.000     0.000    84\n",
      "     sad     0.407     0.172     0.057    87\n",
      "surprise     0.407     0.478     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.407     0.324     0.395    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.517500 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.311344  [ 1200/ 4873]\n",
      "loss: 1.226334  [ 2400/ 4873]\n",
      "loss: 1.651129  [ 3600/ 4873]\n",
      "loss: 1.203000  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.415     0.283     0.619    105\n",
      " disgust     0.415     0.503     0.826    109\n",
      "    fear     0.415     0.167     0.025    80\n",
      "   happy     0.415     0.603     0.506    81\n",
      " neutral     0.415     0.000     0.000    84\n",
      "     sad     0.415     0.360     0.103    87\n",
      "surprise     0.415     0.479     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.415     0.342     0.400    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.502495 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.482479  [ 1200/ 4873]\n",
      "loss: 1.681039  [ 2400/ 4873]\n",
      "loss: 1.421496  [ 3600/ 4873]\n",
      "loss: 1.248744  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.433     0.286     0.581    105\n",
      " disgust     0.433     0.545     0.780    109\n",
      "    fear     0.433     0.077     0.013    80\n",
      "   happy     0.433     0.610     0.617    81\n",
      " neutral     0.433     0.000     0.000    84\n",
      "     sad     0.433     0.538     0.161    87\n",
      "surprise     0.433     0.442     0.828    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.433     0.357     0.426    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.3%, Avg loss: 1.465571 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.278382  [ 1200/ 4873]\n",
      "loss: 1.184223  [ 2400/ 4873]\n",
      "loss: 1.544897  [ 3600/ 4873]\n",
      "loss: 1.400663  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.416     0.367     0.105    105\n",
      " disgust     0.416     0.548     0.789    109\n",
      "    fear     0.416     0.091     0.013    80\n",
      "   happy     0.416     0.568     0.568    81\n",
      " neutral     0.416     0.000     0.000    84\n",
      "     sad     0.416     0.277     0.770    87\n",
      "surprise     0.416     0.483     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.416     0.333     0.417    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.488466 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.403386  [ 1200/ 4873]\n",
      "loss: 1.474080  [ 2400/ 4873]\n",
      "loss: 1.551509  [ 3600/ 4873]\n",
      "loss: 1.641353  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.420     0.244     0.105    105\n",
      " disgust     0.420     0.627     0.725    109\n",
      "    fear     0.420     0.083     0.013    80\n",
      "   happy     0.420     0.567     0.630    81\n",
      " neutral     0.420     0.000     0.000    84\n",
      "     sad     0.420     0.265     0.690    87\n",
      "surprise     0.420     0.486     0.844    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.420     0.325     0.429    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.456895 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.737756  [ 1200/ 4873]\n",
      "loss: 1.435655  [ 2400/ 4873]\n",
      "loss: 1.560682  [ 3600/ 4873]\n",
      "loss: 1.494557  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.413     0.455     0.095    105\n",
      " disgust     0.413     0.524     0.798    109\n",
      "    fear     0.413     0.077     0.025    80\n",
      "   happy     0.413     0.514     0.704    81\n",
      " neutral     0.413     0.000     0.000    84\n",
      "     sad     0.413     0.282     0.655    87\n",
      "surprise     0.413     0.470     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.413     0.332     0.412    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.500994 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.191897  [ 1200/ 4873]\n",
      "loss: 1.127979  [ 2400/ 4873]\n",
      "loss: 1.684453  [ 3600/ 4873]\n",
      "loss: 1.060716  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.415     0.394     0.124    105\n",
      " disgust     0.415     0.475     0.780    109\n",
      "    fear     0.415     0.133     0.025    80\n",
      "   happy     0.415     0.524     0.667    81\n",
      " neutral     0.415     0.000     0.000    84\n",
      "     sad     0.415     0.283     0.586    87\n",
      "surprise     0.415     0.480     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.415     0.327     0.419    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.469242 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.466314  [ 1200/ 4873]\n",
      "loss: 1.386524  [ 2400/ 4873]\n",
      "loss: 1.597913  [ 3600/ 4873]\n",
      "loss: 1.223741  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.407     0.260     0.124    105\n",
      " disgust     0.407     0.558     0.752    109\n",
      "    fear     0.407     0.167     0.013    80\n",
      "   happy     0.407     0.556     0.494    81\n",
      " neutral     0.407     0.000     0.000    84\n",
      "     sad     0.407     0.278     0.724    87\n",
      "surprise     0.407     0.454     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.407     0.324     0.410    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.487255 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.736045  [ 1200/ 4873]\n",
      "loss: 1.783842  [ 2400/ 4873]\n",
      "loss: 1.526394  [ 3600/ 4873]\n",
      "loss: 1.264637  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.416     0.318     0.133    105\n",
      " disgust     0.416     0.548     0.780    109\n",
      "    fear     0.416     0.160     0.050    80\n",
      "   happy     0.416     0.578     0.593    81\n",
      " neutral     0.416     0.000     0.000    84\n",
      "     sad     0.416     0.266     0.655    87\n",
      "surprise     0.416     0.517     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.416     0.341     0.419    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.462558 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.471292  [ 1200/ 4873]\n",
      "loss: 1.488966  [ 2400/ 4873]\n",
      "loss: 1.256854  [ 3600/ 4873]\n",
      "loss: 1.266092  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.439     0.435     0.190    105\n",
      " disgust     0.439     0.534     0.798    109\n",
      "    fear     0.439     0.200     0.037    80\n",
      "   happy     0.439     0.595     0.617    81\n",
      " neutral     0.439     0.000     0.000    84\n",
      "     sad     0.439     0.298     0.713    87\n",
      "surprise     0.439     0.489     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.439     0.364     0.439    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.478539 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.681829  [ 1200/ 4873]\n",
      "loss: 1.312477  [ 2400/ 4873]\n",
      "loss: 1.758899  [ 3600/ 4873]\n",
      "loss: 1.553426  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.407     0.308     0.152    105\n",
      " disgust     0.407     0.590     0.725    109\n",
      "    fear     0.407     0.050     0.013    80\n",
      "   happy     0.407     0.524     0.543    81\n",
      " neutral     0.407     0.000     0.000    84\n",
      "     sad     0.407     0.275     0.770    87\n",
      "surprise     0.407     0.539     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.407     0.326     0.406    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.470958 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.194401  [ 1200/ 4873]\n",
      "loss: 1.047615  [ 2400/ 4873]\n",
      "loss: 1.474979  [ 3600/ 4873]\n",
      "loss: 1.416750  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.297     0.105    105\n",
      " disgust     0.400     0.551     0.697    109\n",
      "    fear     0.400     0.111     0.013    80\n",
      "   happy     0.400     0.575     0.568    81\n",
      " neutral     0.400     0.000     0.000    84\n",
      "     sad     0.400     0.258     0.759    87\n",
      "surprise     0.400     0.489     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.326     0.404    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.466647 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.117830  [ 1200/ 4873]\n",
      "loss: 1.696110  [ 2400/ 4873]\n",
      "loss: 1.142440  [ 3600/ 4873]\n",
      "loss: 1.338013  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.421     0.467     0.133    105\n",
      " disgust     0.421     0.518     0.789    109\n",
      "    fear     0.421     0.087     0.025    80\n",
      "   happy     0.421     0.495     0.667    81\n",
      " neutral     0.421     0.000     0.000    84\n",
      "     sad     0.421     0.305     0.690    87\n",
      "surprise     0.421     0.482     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.421     0.336     0.421    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.500684 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.867990  [ 1200/ 4873]\n",
      "loss: 1.204857  [ 2400/ 4873]\n",
      "loss: 1.334271  [ 3600/ 4873]\n",
      "loss: 1.136630  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.457     0.648     0.333    105\n",
      " disgust     0.457     0.625     0.734    109\n",
      "    fear     0.457     0.133     0.025    80\n",
      "   happy     0.457     0.544     0.605    81\n",
      " neutral     0.457     0.000     0.000    84\n",
      "     sad     0.457     0.279     0.782    87\n",
      "surprise     0.457     0.570     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.457     0.400     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.448501 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.225019  [ 1200/ 4873]\n",
      "loss: 1.671047  [ 2400/ 4873]\n",
      "loss: 1.408297  [ 3600/ 4873]\n",
      "loss: 1.579913  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.456     0.594     0.390    105\n",
      " disgust     0.456     0.588     0.706    109\n",
      "    fear     0.456     0.182     0.025    80\n",
      "   happy     0.456     0.505     0.593    81\n",
      " neutral     0.456     0.000     0.000    84\n",
      "     sad     0.456     0.313     0.724    87\n",
      "surprise     0.456     0.456     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.456     0.377     0.453    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.470889 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.339626  [ 1200/ 4873]\n",
      "loss: 1.491934  [ 2400/ 4873]\n",
      "loss: 1.275136  [ 3600/ 4873]\n",
      "loss: 1.282556  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.466     0.606     0.381    105\n",
      " disgust     0.466     0.551     0.743    109\n",
      "    fear     0.466     0.115     0.037    80\n",
      "   happy     0.466     0.510     0.605    81\n",
      " neutral     0.466     0.000     0.000    84\n",
      "     sad     0.466     0.352     0.713    87\n",
      "surprise     0.466     0.495     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.466     0.376     0.464    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 1.454830 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.238304  [ 1200/ 4873]\n",
      "loss: 0.971328  [ 2400/ 4873]\n",
      "loss: 1.514107  [ 3600/ 4873]\n",
      "loss: 1.260471  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.457     0.610     0.448    105\n",
      " disgust     0.457     0.542     0.706    109\n",
      "    fear     0.457     0.111     0.025    80\n",
      "   happy     0.457     0.539     0.593    81\n",
      " neutral     0.457     0.000     0.000    84\n",
      "     sad     0.457     0.336     0.575    87\n",
      "surprise     0.457     0.407     0.859    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.457     0.364     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.450673 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.308381  [ 1200/ 4873]\n",
      "loss: 1.723062  [ 2400/ 4873]\n",
      "loss: 1.591175  [ 3600/ 4873]\n",
      "loss: 1.520129  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.560     0.581    105\n",
      " disgust     0.484     0.644     0.697    109\n",
      "    fear     0.484     0.217     0.062    80\n",
      "   happy     0.484     0.575     0.617    81\n",
      " neutral     0.484     0.000     0.000    84\n",
      "     sad     0.484     0.349     0.609    87\n",
      "surprise     0.484     0.413     0.781    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.394     0.478    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.430391 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.171059  [ 1200/ 4873]\n",
      "loss: 1.180794  [ 2400/ 4873]\n",
      "loss: 1.419794  [ 3600/ 4873]\n",
      "loss: 1.288517  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.470     0.543     0.486    105\n",
      " disgust     0.470     0.607     0.679    109\n",
      "    fear     0.470     0.174     0.050    80\n",
      "   happy     0.470     0.435     0.580    81\n",
      " neutral     0.470     0.000     0.000    84\n",
      "     sad     0.470     0.373     0.724    87\n",
      "surprise     0.470     0.511     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.470     0.377     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 1.467257 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.259796  [ 1200/ 4873]\n",
      "loss: 1.436267  [ 2400/ 4873]\n",
      "loss: 1.215379  [ 3600/ 4873]\n",
      "loss: 1.250158  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.596     0.590    105\n",
      " disgust     0.490     0.642     0.706    109\n",
      "    fear     0.490     0.130     0.037    80\n",
      "   happy     0.490     0.523     0.568    81\n",
      " neutral     0.490     0.000     0.000    84\n",
      "     sad     0.490     0.354     0.724    87\n",
      "surprise     0.490     0.495     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.391     0.482    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.435723 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.367474  [ 1200/ 4873]\n",
      "loss: 1.013812  [ 2400/ 4873]\n",
      "loss: 1.097568  [ 3600/ 4873]\n",
      "loss: 1.724515  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.562     0.514    105\n",
      " disgust     0.479     0.585     0.725    109\n",
      "    fear     0.479     0.154     0.050    80\n",
      "   happy     0.479     0.554     0.630    81\n",
      " neutral     0.479     0.000     0.000    84\n",
      "     sad     0.479     0.337     0.644    87\n",
      "surprise     0.479     0.505     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.385     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.420644 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.409250  [ 1200/ 4873]\n",
      "loss: 1.495865  [ 2400/ 4873]\n",
      "loss: 1.056496  [ 3600/ 4873]\n",
      "loss: 1.676210  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.464     0.500     0.533    105\n",
      " disgust     0.464     0.637     0.661    109\n",
      "    fear     0.464     0.045     0.013    80\n",
      "   happy     0.464     0.482     0.654    81\n",
      " neutral     0.464     0.000     0.000    84\n",
      "     sad     0.464     0.352     0.586    87\n",
      "surprise     0.464     0.463     0.781    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.464     0.354     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.423054 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.499692  [ 1200/ 4873]\n",
      "loss: 1.583341  [ 2400/ 4873]\n",
      "loss: 1.200717  [ 3600/ 4873]\n",
      "loss: 1.372585  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.545     0.581    105\n",
      " disgust     0.498     0.622     0.725    109\n",
      "    fear     0.498     0.194     0.075    80\n",
      "   happy     0.498     0.570     0.654    81\n",
      " neutral     0.498     0.000     0.000    84\n",
      "     sad     0.498     0.358     0.678    87\n",
      "surprise     0.498     0.561     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.407     0.490    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.419985 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.318193  [ 1200/ 4873]\n",
      "loss: 1.441152  [ 2400/ 4873]\n",
      "loss: 1.472089  [ 3600/ 4873]\n",
      "loss: 1.668874  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.621     0.514    105\n",
      " disgust     0.477     0.562     0.743    109\n",
      "    fear     0.477     0.250     0.125    80\n",
      "   happy     0.477     0.495     0.642    81\n",
      " neutral     0.477     0.000     0.000    84\n",
      "     sad     0.477     0.333     0.598    87\n",
      "surprise     0.477     0.538     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.400     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.408196 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.005952  [ 1200/ 4873]\n",
      "loss: 1.510976  [ 2400/ 4873]\n",
      "loss: 1.641170  [ 3600/ 4873]\n",
      "loss: 1.046616  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.539     0.590    105\n",
      " disgust     0.507     0.599     0.752    109\n",
      "    fear     0.507     0.346     0.113    80\n",
      "   happy     0.507     0.554     0.630    81\n",
      " neutral     0.507     0.000     0.000    84\n",
      "     sad     0.507     0.371     0.644    87\n",
      "surprise     0.507     0.551     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.423     0.499    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.381030 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep80_acc_51.md \n",
      "\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep80_acc_51\"!  new accuracy: 50.7\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.258275  [ 1200/ 4873]\n",
      "loss: 1.653319  [ 2400/ 4873]\n",
      "loss: 1.647019  [ 3600/ 4873]\n",
      "loss: 1.325986  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.571     0.495    105\n",
      " disgust     0.485     0.607     0.679    109\n",
      "    fear     0.485     0.200     0.062    80\n",
      "   happy     0.485     0.509     0.716    81\n",
      " neutral     0.485     0.000     0.000    84\n",
      "     sad     0.485     0.369     0.667    87\n",
      "surprise     0.485     0.485     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.392     0.484    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.409054 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.517437  [ 1200/ 4873]\n",
      "loss: 1.833038  [ 2400/ 4873]\n",
      "loss: 1.042795  [ 3600/ 4873]\n",
      "loss: 1.763804  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.449     0.543    105\n",
      " disgust     0.485     0.676     0.651    109\n",
      "    fear     0.485     0.318     0.087    80\n",
      "   happy     0.485     0.561     0.679    81\n",
      " neutral     0.485     0.000     0.000    84\n",
      "     sad     0.485     0.351     0.701    87\n",
      "surprise     0.485     0.536     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.413     0.481    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.385438 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.944099  [ 1200/ 4873]\n",
      "loss: 1.328813  [ 2400/ 4873]\n",
      "loss: 1.523133  [ 3600/ 4873]\n",
      "loss: 1.011933  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.495     0.608     0.562    105\n",
      " disgust     0.495     0.600     0.716    109\n",
      "    fear     0.495     0.238     0.062    80\n",
      "   happy     0.495     0.535     0.654    81\n",
      " neutral     0.495     0.000     0.000    84\n",
      "     sad     0.495     0.358     0.736    87\n",
      "surprise     0.495     0.512     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.495     0.407     0.486    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.395738 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.334282  [ 1200/ 4873]\n",
      "loss: 1.472816  [ 2400/ 4873]\n",
      "loss: 1.684124  [ 3600/ 4873]\n",
      "loss: 1.621535  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.616     0.505    105\n",
      " disgust     0.493     0.573     0.752    109\n",
      "    fear     0.493     0.194     0.087    80\n",
      "   happy     0.493     0.500     0.691    81\n",
      " neutral     0.493     0.000     0.000    84\n",
      "     sad     0.493     0.362     0.667    87\n",
      "surprise     0.493     0.616     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.409     0.487    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.415496 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.180474  [ 1200/ 4873]\n",
      "loss: 1.277203  [ 2400/ 4873]\n",
      "loss: 1.793223  [ 3600/ 4873]\n",
      "loss: 0.896721  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.567     0.524    105\n",
      " disgust     0.485     0.633     0.697    109\n",
      "    fear     0.485     0.262     0.200    80\n",
      "   happy     0.485     0.477     0.642    81\n",
      " neutral     0.485     0.000     0.000    84\n",
      "     sad     0.485     0.374     0.632    87\n",
      "surprise     0.485     0.553     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.409     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.373976 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.790322  [ 1200/ 4873]\n",
      "loss: 1.600268  [ 2400/ 4873]\n",
      "loss: 1.010272  [ 3600/ 4873]\n",
      "loss: 1.231864  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.517     0.571    105\n",
      " disgust     0.492     0.592     0.679    109\n",
      "    fear     0.492     0.341     0.188    80\n",
      "   happy     0.492     0.568     0.568    81\n",
      " neutral     0.492     0.000     0.000    84\n",
      "     sad     0.492     0.377     0.655    87\n",
      "surprise     0.492     0.516     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.416     0.487    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.404099 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.988954  [ 1200/ 4873]\n",
      "loss: 1.429630  [ 2400/ 4873]\n",
      "loss: 1.463922  [ 3600/ 4873]\n",
      "loss: 0.900431  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.530     0.660     0.610    105\n",
      " disgust     0.530     0.619     0.761    109\n",
      "    fear     0.530     0.469     0.287    80\n",
      "   happy     0.530     0.587     0.667    81\n",
      " neutral     0.530     0.000     0.000    84\n",
      "     sad     0.530     0.333     0.678    87\n",
      "surprise     0.530     0.656     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.530     0.475     0.518    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.368607 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep87_acc_53.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep80_acc_51\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep87_acc_53\"! Old accuracy: 50.7, new accuracy: 53.0\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.279812  [ 1200/ 4873]\n",
      "loss: 1.346966  [ 2400/ 4873]\n",
      "loss: 1.155356  [ 3600/ 4873]\n",
      "loss: 1.118781  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.660     0.590    105\n",
      " disgust     0.515     0.600     0.798    109\n",
      "    fear     0.515     0.400     0.200    80\n",
      "   happy     0.515     0.595     0.617    81\n",
      " neutral     0.515     0.000     0.000    84\n",
      "     sad     0.515     0.337     0.690    87\n",
      "surprise     0.515     0.565     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.451     0.501    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.379649 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 2.186761  [ 1200/ 4873]\n",
      "loss: 1.050056  [ 2400/ 4873]\n",
      "loss: 1.546578  [ 3600/ 4873]\n",
      "loss: 1.214327  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.602     0.562    105\n",
      " disgust     0.507     0.595     0.716    109\n",
      "    fear     0.507     0.328     0.263    80\n",
      "   happy     0.507     0.588     0.617    81\n",
      " neutral     0.507     0.000     0.000    84\n",
      "     sad     0.507     0.380     0.655    87\n",
      "surprise     0.507     0.537     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.433     0.500    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.397549 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.234368  [ 1200/ 4873]\n",
      "loss: 1.248632  [ 2400/ 4873]\n",
      "loss: 1.043665  [ 3600/ 4873]\n",
      "loss: 1.153328  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.622     0.581    105\n",
      " disgust     0.518     0.553     0.807    109\n",
      "    fear     0.518     0.406     0.350    80\n",
      "   happy     0.518     0.627     0.642    81\n",
      " neutral     0.518     0.000     0.000    84\n",
      "     sad     0.518     0.348     0.632    87\n",
      "surprise     0.518     0.744     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.471     0.502    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.392358 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.328297  [ 1200/ 4873]\n",
      "loss: 1.127132  [ 2400/ 4873]\n",
      "loss: 1.333667  [ 3600/ 4873]\n",
      "loss: 1.159869  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.551     0.619    105\n",
      " disgust     0.498     0.597     0.679    109\n",
      "    fear     0.498     0.391     0.225    80\n",
      "   happy     0.498     0.672     0.506    81\n",
      " neutral     0.498     0.000     0.000    84\n",
      "     sad     0.498     0.358     0.724    87\n",
      "surprise     0.498     0.506     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.439     0.489    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.429294 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.486133  [ 1200/ 4873]\n",
      "loss: 1.467480  [ 2400/ 4873]\n",
      "loss: 1.299133  [ 3600/ 4873]\n",
      "loss: 1.253483  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.522     0.562    105\n",
      " disgust     0.511     0.638     0.743    109\n",
      "    fear     0.511     0.429     0.300    80\n",
      "   happy     0.511     0.600     0.556    81\n",
      " neutral     0.511     1.000     0.012    84\n",
      "     sad     0.511     0.368     0.655    87\n",
      "surprise     0.511     0.542     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.585     0.504    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.355454 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.285787  [ 1200/ 4873]\n",
      "loss: 1.459010  [ 2400/ 4873]\n",
      "loss: 1.347652  [ 3600/ 4873]\n",
      "loss: 1.378208  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.632     0.571    105\n",
      " disgust     0.533     0.571     0.807    109\n",
      "    fear     0.533     0.518     0.362    80\n",
      "   happy     0.533     0.692     0.556    81\n",
      " neutral     0.533     0.000     0.000    84\n",
      "     sad     0.533     0.350     0.724    87\n",
      "surprise     0.533     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.492     0.521    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.371003 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep93_acc_53.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep87_acc_53\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep93_acc_53\"! Old accuracy: 53.0, new accuracy: 53.3\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.986188  [ 1200/ 4873]\n",
      "loss: 1.653852  [ 2400/ 4873]\n",
      "loss: 1.512104  [ 3600/ 4873]\n",
      "loss: 1.557186  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.497     0.583     0.600    105\n",
      " disgust     0.497     0.612     0.780    109\n",
      "    fear     0.497     0.373     0.312    80\n",
      "   happy     0.497     0.642     0.420    81\n",
      " neutral     0.497     0.000     0.000    84\n",
      "     sad     0.497     0.368     0.644    87\n",
      "surprise     0.497     0.440     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.497     0.431     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.400521 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.278778  [ 1200/ 4873]\n",
      "loss: 1.027274  [ 2400/ 4873]\n",
      "loss: 1.229409  [ 3600/ 4873]\n",
      "loss: 1.311186  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.641     0.562    105\n",
      " disgust     0.510     0.566     0.752    109\n",
      "    fear     0.510     0.387     0.300    80\n",
      "   happy     0.510     0.583     0.605    81\n",
      " neutral     0.510     0.333     0.012    84\n",
      "     sad     0.510     0.362     0.632    87\n",
      "surprise     0.510     0.569     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.492     0.501    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.370529 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.141477  [ 1200/ 4873]\n",
      "loss: 1.661836  [ 2400/ 4873]\n",
      "loss: 1.487645  [ 3600/ 4873]\n",
      "loss: 1.561115  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.528     0.663     0.562    105\n",
      " disgust     0.528     0.567     0.780    109\n",
      "    fear     0.528     0.480     0.450    80\n",
      "   happy     0.528     0.676     0.593    81\n",
      " neutral     0.528     0.000     0.000    84\n",
      "     sad     0.528     0.337     0.655    87\n",
      "surprise     0.528     0.673     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.528     0.485     0.517    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.359813 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.277942  [ 1200/ 4873]\n",
      "loss: 0.801305  [ 2400/ 4873]\n",
      "loss: 1.062404  [ 3600/ 4873]\n",
      "loss: 0.931439  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.565     0.619    105\n",
      " disgust     0.508     0.635     0.606    109\n",
      "    fear     0.508     0.558     0.300    80\n",
      "   happy     0.508     0.750     0.556    81\n",
      " neutral     0.508     0.333     0.012    84\n",
      "     sad     0.508     0.341     0.805    87\n",
      "surprise     0.508     0.487     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.524     0.501    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.422956 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.089225  [ 1200/ 4873]\n",
      "loss: 1.518257  [ 2400/ 4873]\n",
      "loss: 1.340112  [ 3600/ 4873]\n",
      "loss: 1.273825  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.648     0.543    105\n",
      " disgust     0.508     0.573     0.789    109\n",
      "    fear     0.508     0.366     0.325    80\n",
      "   happy     0.508     0.629     0.543    81\n",
      " neutral     0.508     0.000     0.000    84\n",
      "     sad     0.508     0.358     0.655    87\n",
      "surprise     0.508     0.580     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.451     0.497    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.367592 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.320055  [ 1200/ 4873]\n",
      "loss: 1.273148  [ 2400/ 4873]\n",
      "loss: 1.772252  [ 3600/ 4873]\n",
      "loss: 1.712463  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.520     0.526     0.581    105\n",
      " disgust     0.520     0.701     0.624    109\n",
      "    fear     0.520     0.382     0.425    80\n",
      "   happy     0.520     0.635     0.667    81\n",
      " neutral     0.520     0.600     0.036    84\n",
      "     sad     0.520     0.376     0.609    87\n",
      "surprise     0.520     0.571     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.520     0.542     0.518    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.358709 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.461160  [ 1200/ 4873]\n",
      "loss: 1.430963  [ 2400/ 4873]\n",
      "loss: 1.647618  [ 3600/ 4873]\n",
      "loss: 1.289808  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.538     0.533    105\n",
      " disgust     0.508     0.587     0.743    109\n",
      "    fear     0.508     0.406     0.487    80\n",
      "   happy     0.508     0.700     0.605    81\n",
      " neutral     0.508     0.429     0.036    84\n",
      "     sad     0.508     0.329     0.563    87\n",
      "surprise     0.508     0.717     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.529     0.498    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.370158 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 1.321433  [ 1200/ 4873]\n",
      "loss: 1.513289  [ 2400/ 4873]\n",
      "loss: 1.297331  [ 3600/ 4873]\n",
      "loss: 0.897564  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.530     0.685     0.581    105\n",
      " disgust     0.530     0.619     0.716    109\n",
      "    fear     0.530     0.465     0.412    80\n",
      "   happy     0.530     0.639     0.654    81\n",
      " neutral     0.530     0.125     0.012    84\n",
      "     sad     0.530     0.326     0.701    87\n",
      "surprise     0.530     0.783     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.530     0.520     0.520    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.325003 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 1.082007  [ 1200/ 4873]\n",
      "loss: 1.174706  [ 2400/ 4873]\n",
      "loss: 2.116987  [ 3600/ 4873]\n",
      "loss: 1.128597  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.523     0.610     0.581    105\n",
      " disgust     0.523     0.661     0.716    109\n",
      "    fear     0.523     0.512     0.263    80\n",
      "   happy     0.523     0.780     0.568    81\n",
      " neutral     0.523     0.400     0.024    84\n",
      "     sad     0.523     0.347     0.793    87\n",
      "surprise     0.523     0.477     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.523     0.541     0.514    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.364799 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 1.173305  [ 1200/ 4873]\n",
      "loss: 1.303867  [ 2400/ 4873]\n",
      "loss: 1.326488  [ 3600/ 4873]\n",
      "loss: 0.946831  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.543     0.727     0.610    105\n",
      " disgust     0.543     0.596     0.743    109\n",
      "    fear     0.543     0.371     0.412    80\n",
      "   happy     0.543     0.635     0.667    81\n",
      " neutral     0.543     0.400     0.024    84\n",
      "     sad     0.543     0.379     0.667    87\n",
      "surprise     0.543     0.722     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.543     0.547     0.533    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.330956 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep103_acc_54.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep93_acc_53\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep103_acc_54\"! Old accuracy: 53.3, new accuracy: 54.3\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 1.266274  [ 1200/ 4873]\n",
      "loss: 1.463496  [ 2400/ 4873]\n",
      "loss: 1.439778  [ 3600/ 4873]\n",
      "loss: 1.022705  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.536     0.636     0.600    105\n",
      " disgust     0.536     0.690     0.734    109\n",
      "    fear     0.536     0.417     0.375    80\n",
      "   happy     0.536     0.676     0.617    81\n",
      " neutral     0.536     0.308     0.048    84\n",
      "     sad     0.536     0.341     0.667    87\n",
      "surprise     0.536     0.636     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.536     0.529     0.528    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.339357 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 1.402610  [ 1200/ 4873]\n",
      "loss: 1.635290  [ 2400/ 4873]\n",
      "loss: 0.790980  [ 3600/ 4873]\n",
      "loss: 1.459569  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.690     0.552    105\n",
      " disgust     0.533     0.670     0.670    109\n",
      "    fear     0.533     0.391     0.450    80\n",
      "   happy     0.533     0.667     0.667    81\n",
      " neutral     0.533     0.333     0.048    84\n",
      "     sad     0.533     0.339     0.701    87\n",
      "surprise     0.533     0.750     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.549     0.528    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.327847 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 1.184418  [ 1200/ 4873]\n",
      "loss: 1.006777  [ 2400/ 4873]\n",
      "loss: 1.250145  [ 3600/ 4873]\n",
      "loss: 1.738679  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.562     0.713     0.543    105\n",
      " disgust     0.562     0.636     0.752    109\n",
      "    fear     0.562     0.384     0.475    80\n",
      "   happy     0.562     0.582     0.654    81\n",
      " neutral     0.562     0.564     0.631    84\n",
      "     sad     0.562     0.342     0.287    87\n",
      "surprise     0.562     0.795     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.562     0.574     0.556    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.328873 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep106_acc_56.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep103_acc_54\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep106_acc_56\"! Old accuracy: 54.3, new accuracy: 56.2\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.988467  [ 1200/ 4873]\n",
      "loss: 1.404430  [ 2400/ 4873]\n",
      "loss: 1.342770  [ 3600/ 4873]\n",
      "loss: 1.295175  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.554     0.561     0.524    105\n",
      " disgust     0.554     0.673     0.661    109\n",
      "    fear     0.554     0.444     0.400    80\n",
      "   happy     0.554     0.647     0.679    81\n",
      " neutral     0.554     0.544     0.667    84\n",
      "     sad     0.554     0.393     0.276    87\n",
      "surprise     0.554     0.524     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.554     0.541     0.556    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.360763 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.979561  [ 1200/ 4873]\n",
      "loss: 1.267751  [ 2400/ 4873]\n",
      "loss: 1.708940  [ 3600/ 4873]\n",
      "loss: 1.137473  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.556     0.686     0.562    105\n",
      " disgust     0.556     0.624     0.670    109\n",
      "    fear     0.556     0.474     0.450    80\n",
      "   happy     0.556     0.639     0.654    81\n",
      " neutral     0.556     0.517     0.714    84\n",
      "     sad     0.556     0.281     0.207    87\n",
      "surprise     0.556     0.588     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.556     0.544     0.555    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.357922 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.330474  [ 1200/ 4873]\n",
      "loss: 1.394653  [ 2400/ 4873]\n",
      "loss: 1.307969  [ 3600/ 4873]\n",
      "loss: 0.899645  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.569     0.688     0.610    105\n",
      " disgust     0.569     0.664     0.706    109\n",
      "    fear     0.569     0.481     0.487    80\n",
      "   happy     0.569     0.649     0.593    81\n",
      " neutral     0.569     0.496     0.738    84\n",
      "     sad     0.569     0.308     0.276    87\n",
      "surprise     0.569     0.767     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.569     0.579     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.307631 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep109_acc_57.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep106_acc_56\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep109_acc_57\"! Old accuracy: 56.2, new accuracy: 56.9\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 1.241232  [ 1200/ 4873]\n",
      "loss: 1.267589  [ 2400/ 4873]\n",
      "loss: 1.285163  [ 3600/ 4873]\n",
      "loss: 1.427183  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.744     0.581    105\n",
      " disgust     0.557     0.664     0.688    109\n",
      "    fear     0.557     0.500     0.500    80\n",
      "   happy     0.557     0.642     0.531    81\n",
      " neutral     0.557     0.475     0.690    84\n",
      "     sad     0.557     0.250     0.253    87\n",
      "surprise     0.557     0.707     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.569     0.555    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.312565 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 1.218031  [ 1200/ 4873]\n",
      "loss: 1.255783  [ 2400/ 4873]\n",
      "loss: 0.871935  [ 3600/ 4873]\n",
      "loss: 0.775053  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.635     0.581    105\n",
      " disgust     0.585     0.639     0.716    109\n",
      "    fear     0.585     0.512     0.537    80\n",
      "   happy     0.585     0.640     0.679    81\n",
      " neutral     0.585     0.562     0.643    84\n",
      "     sad     0.585     0.414     0.276    87\n",
      "surprise     0.585     0.618     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.574     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.302205 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep111_acc_59.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep109_acc_57\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep111_acc_59\"! Old accuracy: 56.9, new accuracy: 58.5\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 1.260314  [ 1200/ 4873]\n",
      "loss: 1.003675  [ 2400/ 4873]\n",
      "loss: 1.430607  [ 3600/ 4873]\n",
      "loss: 1.272090  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.500     0.581    105\n",
      " disgust     0.525     0.738     0.541    109\n",
      "    fear     0.525     0.420     0.425    80\n",
      "   happy     0.525     0.688     0.654    81\n",
      " neutral     0.525     0.505     0.643    84\n",
      "     sad     0.525     0.250     0.172    87\n",
      "surprise     0.525     0.530     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.519     0.529    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.401950 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.483282  [ 1200/ 4873]\n",
      "loss: 0.998573  [ 2400/ 4873]\n",
      "loss: 1.593883  [ 3600/ 4873]\n",
      "loss: 1.293927  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.670     0.619    105\n",
      " disgust     0.577     0.703     0.651    109\n",
      "    fear     0.577     0.462     0.525    80\n",
      "   happy     0.577     0.634     0.642    81\n",
      " neutral     0.577     0.514     0.667    84\n",
      "     sad     0.577     0.388     0.299    87\n",
      "surprise     0.577     0.635     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.572     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.338630 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 1.359487  [ 1200/ 4873]\n",
      "loss: 1.326486  [ 2400/ 4873]\n",
      "loss: 1.039284  [ 3600/ 4873]\n",
      "loss: 1.033692  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.747     0.562    105\n",
      " disgust     0.590     0.572     0.798    109\n",
      "    fear     0.590     0.488     0.525    80\n",
      "   happy     0.590     0.767     0.568    81\n",
      " neutral     0.590     0.607     0.643    84\n",
      "     sad     0.590     0.381     0.460    87\n",
      "surprise     0.590     0.821     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.626     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.354511 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep114_acc_59.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep111_acc_59\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep114_acc_59\"! Old accuracy: 58.5, new accuracy: 59.0\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 1.582246  [ 1200/ 4873]\n",
      "loss: 1.483115  [ 2400/ 4873]\n",
      "loss: 0.887230  [ 3600/ 4873]\n",
      "loss: 1.053862  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.776     0.562    105\n",
      " disgust     0.585     0.682     0.670    109\n",
      "    fear     0.585     0.433     0.525    80\n",
      "   happy     0.585     0.671     0.605    81\n",
      " neutral     0.585     0.621     0.702    84\n",
      "     sad     0.585     0.362     0.437    87\n",
      "surprise     0.585     0.649     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.599     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.352115 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 1.461023  [ 1200/ 4873]\n",
      "loss: 1.119810  [ 2400/ 4873]\n",
      "loss: 0.879008  [ 3600/ 4873]\n",
      "loss: 1.231807  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.691     0.619    105\n",
      " disgust     0.574     0.690     0.734    109\n",
      "    fear     0.574     0.472     0.425    80\n",
      "   happy     0.574     0.617     0.617    81\n",
      " neutral     0.574     0.542     0.619    84\n",
      "     sad     0.574     0.318     0.310    87\n",
      "surprise     0.574     0.636     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.567     0.569    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.305391 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 1.346199  [ 1200/ 4873]\n",
      "loss: 1.248592  [ 2400/ 4873]\n",
      "loss: 1.393232  [ 3600/ 4873]\n",
      "loss: 1.555472  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.657     0.638    105\n",
      " disgust     0.579     0.625     0.780    109\n",
      "    fear     0.579     0.441     0.512    80\n",
      "   happy     0.579     0.621     0.506    81\n",
      " neutral     0.579     0.578     0.702    84\n",
      "     sad     0.579     0.426     0.333    87\n",
      "surprise     0.579     0.721     0.484    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.581     0.565    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.324227 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 1.139888  [ 1200/ 4873]\n",
      "loss: 0.869582  [ 2400/ 4873]\n",
      "loss: 0.993997  [ 3600/ 4873]\n",
      "loss: 0.705702  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.574     0.690     0.571    105\n",
      " disgust     0.574     0.717     0.697    109\n",
      "    fear     0.574     0.529     0.463    80\n",
      "   happy     0.574     0.632     0.679    81\n",
      " neutral     0.574     0.496     0.679    84\n",
      "     sad     0.574     0.300     0.276    87\n",
      "surprise     0.574     0.631     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.574     0.571     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.311281 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.725281  [ 1200/ 4873]\n",
      "loss: 1.215989  [ 2400/ 4873]\n",
      "loss: 1.098066  [ 3600/ 4873]\n",
      "loss: 1.403012  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.625     0.619    105\n",
      " disgust     0.584     0.780     0.651    109\n",
      "    fear     0.584     0.514     0.475    80\n",
      "   happy     0.584     0.662     0.654    81\n",
      " neutral     0.584     0.552     0.690    84\n",
      "     sad     0.584     0.309     0.333    87\n",
      "surprise     0.584     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.589     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.282035 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 1.308491  [ 1200/ 4873]\n",
      "loss: 1.012076  [ 2400/ 4873]\n",
      "loss: 0.938266  [ 3600/ 4873]\n",
      "loss: 0.944085  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.561     0.625     0.571    105\n",
      " disgust     0.561     0.723     0.624    109\n",
      "    fear     0.561     0.431     0.588    80\n",
      "   happy     0.561     0.589     0.691    81\n",
      " neutral     0.561     0.573     0.655    84\n",
      "     sad     0.561     0.311     0.264    87\n",
      "surprise     0.561     0.717     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.561     0.567     0.558    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.277382 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 1.375468  [ 1200/ 4873]\n",
      "loss: 0.622938  [ 2400/ 4873]\n",
      "loss: 1.344126  [ 3600/ 4873]\n",
      "loss: 1.141609  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.564     0.630     0.600    105\n",
      " disgust     0.564     0.675     0.780    109\n",
      "    fear     0.564     0.432     0.512    80\n",
      "   happy     0.564     0.681     0.605    81\n",
      " neutral     0.564     0.540     0.643    84\n",
      "     sad     0.564     0.256     0.230    87\n",
      "surprise     0.564     0.821     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.564     0.576     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.288919 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.633339  [ 1200/ 4873]\n",
      "loss: 0.897549  [ 2400/ 4873]\n",
      "loss: 1.312382  [ 3600/ 4873]\n",
      "loss: 1.255622  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.646     0.590    105\n",
      " disgust     0.567     0.690     0.734    109\n",
      "    fear     0.567     0.463     0.475    80\n",
      "   happy     0.567     0.641     0.617    81\n",
      " neutral     0.567     0.509     0.702    84\n",
      "     sad     0.567     0.254     0.184    87\n",
      "surprise     0.567     0.695     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.557     0.563    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.303567 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.978875  [ 1200/ 4873]\n",
      "loss: 1.217359  [ 2400/ 4873]\n",
      "loss: 1.228044  [ 3600/ 4873]\n",
      "loss: 0.682364  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.567     0.612     0.600    105\n",
      " disgust     0.567     0.647     0.706    109\n",
      "    fear     0.567     0.493     0.425    80\n",
      "   happy     0.567     0.704     0.617    81\n",
      " neutral     0.567     0.567     0.702    84\n",
      "     sad     0.567     0.276     0.241    87\n",
      "surprise     0.567     0.618     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.567     0.560     0.564    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.265314 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.890024  [ 1200/ 4873]\n",
      "loss: 1.054916  [ 2400/ 4873]\n",
      "loss: 1.435703  [ 3600/ 4873]\n",
      "loss: 1.036566  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.569     0.543     0.657    105\n",
      " disgust     0.569     0.714     0.642    109\n",
      "    fear     0.569     0.603     0.475    80\n",
      "   happy     0.569     0.687     0.568    81\n",
      " neutral     0.569     0.504     0.690    84\n",
      "     sad     0.569     0.294     0.230    87\n",
      "surprise     0.569     0.639     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.569     0.569     0.569    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.268613 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 1.358867  [ 1200/ 4873]\n",
      "loss: 1.401456  [ 2400/ 4873]\n",
      "loss: 1.027427  [ 3600/ 4873]\n",
      "loss: 1.266482  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.685     0.600    105\n",
      " disgust     0.589     0.690     0.734    109\n",
      "    fear     0.589     0.506     0.512    80\n",
      "   happy     0.589     0.708     0.568    81\n",
      " neutral     0.589     0.535     0.726    84\n",
      "     sad     0.589     0.358     0.333    87\n",
      "surprise     0.589     0.639     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.589     0.583    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.272874 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 1.169604  [ 1200/ 4873]\n",
      "loss: 1.229740  [ 2400/ 4873]\n",
      "loss: 1.436853  [ 3600/ 4873]\n",
      "loss: 1.148816  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.582     0.587     0.581    105\n",
      " disgust     0.582     0.718     0.725    109\n",
      "    fear     0.582     0.489     0.550    80\n",
      "   happy     0.582     0.727     0.593    81\n",
      " neutral     0.582     0.576     0.679    84\n",
      "     sad     0.582     0.326     0.322    87\n",
      "surprise     0.582     0.691     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.582     0.588     0.577    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.250000 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.989270  [ 1200/ 4873]\n",
      "loss: 1.413864  [ 2400/ 4873]\n",
      "loss: 0.989169  [ 3600/ 4873]\n",
      "loss: 1.132647  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.610     0.610    105\n",
      " disgust     0.580     0.722     0.716    109\n",
      "    fear     0.580     0.465     0.412    80\n",
      "   happy     0.580     0.637     0.630    81\n",
      " neutral     0.580     0.588     0.679    84\n",
      "     sad     0.580     0.352     0.425    87\n",
      "surprise     0.580     0.773     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.592     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.266996 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 1.027820  [ 1200/ 4873]\n",
      "loss: 1.674615  [ 2400/ 4873]\n",
      "loss: 0.919962  [ 3600/ 4873]\n",
      "loss: 0.913890  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.579     0.714     0.571    105\n",
      " disgust     0.579     0.684     0.734    109\n",
      "    fear     0.579     0.507     0.450    80\n",
      "   happy     0.579     0.593     0.667    81\n",
      " neutral     0.579     0.552     0.631    84\n",
      "     sad     0.579     0.364     0.368    87\n",
      "surprise     0.579     0.603     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.579     0.574     0.574    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 1.326810 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 1.074897  [ 1200/ 4873]\n",
      "loss: 1.286350  [ 2400/ 4873]\n",
      "loss: 1.328845  [ 3600/ 4873]\n",
      "loss: 1.239631  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.633     0.657    105\n",
      " disgust     0.589     0.743     0.688    109\n",
      "    fear     0.589     0.417     0.537    80\n",
      "   happy     0.589     0.658     0.617    81\n",
      " neutral     0.589     0.622     0.667    84\n",
      "     sad     0.589     0.385     0.402    87\n",
      "surprise     0.589     0.775     0.484    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.605     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.270021 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 1.094189  [ 1200/ 4873]\n",
      "loss: 0.784930  [ 2400/ 4873]\n",
      "loss: 1.022279  [ 3600/ 4873]\n",
      "loss: 1.227394  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.589     0.663     0.543    105\n",
      " disgust     0.589     0.646     0.670    109\n",
      "    fear     0.589     0.430     0.463    80\n",
      "   happy     0.589     0.646     0.654    81\n",
      " neutral     0.589     0.707     0.690    84\n",
      "     sad     0.589     0.440     0.460    87\n",
      "surprise     0.589     0.586     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.589     0.588     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.307128 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 1.038261  [ 1200/ 4873]\n",
      "loss: 0.757690  [ 2400/ 4873]\n",
      "loss: 1.050470  [ 3600/ 4873]\n",
      "loss: 1.729545  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.716     0.600    105\n",
      " disgust     0.575     0.699     0.725    109\n",
      "    fear     0.575     0.432     0.475    80\n",
      "   happy     0.575     0.649     0.617    81\n",
      " neutral     0.575     0.528     0.667    84\n",
      "     sad     0.575     0.309     0.287    87\n",
      "surprise     0.575     0.702     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.576     0.571    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.254914 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.647809  [ 1200/ 4873]\n",
      "loss: 1.702048  [ 2400/ 4873]\n",
      "loss: 1.004506  [ 3600/ 4873]\n",
      "loss: 1.024027  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.584     0.680     0.629    105\n",
      " disgust     0.584     0.719     0.633    109\n",
      "    fear     0.584     0.471     0.600    80\n",
      "   happy     0.584     0.576     0.605    81\n",
      " neutral     0.584     0.633     0.679    84\n",
      "     sad     0.584     0.393     0.379    87\n",
      "surprise     0.584     0.607     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.584     0.583     0.579    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.269218 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 1.346896  [ 1200/ 4873]\n",
      "loss: 1.719553  [ 2400/ 4873]\n",
      "loss: 0.985162  [ 3600/ 4873]\n",
      "loss: 1.215759  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.554     0.695     0.543    105\n",
      " disgust     0.554     0.657     0.615    109\n",
      "    fear     0.554     0.478     0.537    80\n",
      "   happy     0.554     0.596     0.654    81\n",
      " neutral     0.554     0.509     0.690    84\n",
      "     sad     0.554     0.287     0.287    87\n",
      "surprise     0.554     0.761     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.554     0.569     0.553    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.322345 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 1.044579  [ 1200/ 4873]\n",
      "loss: 1.212345  [ 2400/ 4873]\n",
      "loss: 1.040493  [ 3600/ 4873]\n",
      "loss: 0.919961  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.592     0.612     0.600    105\n",
      " disgust     0.592     0.636     0.706    109\n",
      "    fear     0.592     0.453     0.537    80\n",
      "   happy     0.592     0.640     0.679    81\n",
      " neutral     0.592     0.688     0.631    84\n",
      "     sad     0.592     0.440     0.379    87\n",
      "surprise     0.592     0.698     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.592     0.595     0.587    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.247341 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep134_acc_59.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep114_acc_59\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep134_acc_59\"! Old accuracy: 59.0, new accuracy: 59.2\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 1.051248  [ 1200/ 4873]\n",
      "loss: 0.795462  [ 2400/ 4873]\n",
      "loss: 1.049852  [ 3600/ 4873]\n",
      "loss: 1.113509  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.575     0.740     0.543    105\n",
      " disgust     0.575     0.710     0.651    109\n",
      "    fear     0.575     0.400     0.500    80\n",
      "   happy     0.575     0.602     0.654    81\n",
      " neutral     0.575     0.619     0.619    84\n",
      "     sad     0.575     0.378     0.356    87\n",
      "surprise     0.575     0.595     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.575     0.578     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.324894 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.894260  [ 1200/ 4873]\n",
      "loss: 1.162446  [ 2400/ 4873]\n",
      "loss: 0.971692  [ 3600/ 4873]\n",
      "loss: 0.739443  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.613     0.677     0.600    105\n",
      " disgust     0.613     0.600     0.826    109\n",
      "    fear     0.613     0.464     0.487    80\n",
      "   happy     0.613     0.712     0.642    81\n",
      " neutral     0.613     0.697     0.631    84\n",
      "     sad     0.613     0.490     0.563    87\n",
      "surprise     0.613     0.824     0.438    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.613     0.638     0.598    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.271442 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep136_acc_61.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep134_acc_59\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep136_acc_61\"! Old accuracy: 59.2, new accuracy: 61.3\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 1.020100  [ 1200/ 4873]\n",
      "loss: 0.956774  [ 2400/ 4873]\n",
      "loss: 1.513182  [ 3600/ 4873]\n",
      "loss: 0.699310  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.628     0.636     0.648    105\n",
      " disgust     0.628     0.709     0.670    109\n",
      "    fear     0.628     0.531     0.537    80\n",
      "   happy     0.628     0.707     0.654    81\n",
      " neutral     0.628     0.612     0.619    84\n",
      "     sad     0.628     0.515     0.598    87\n",
      "surprise     0.628     0.724     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.628     0.633     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.209393 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep137_acc_63.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep136_acc_61\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep137_acc_63\"! Old accuracy: 61.3, new accuracy: 62.8\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 1.127225  [ 1200/ 4873]\n",
      "loss: 1.941038  [ 2400/ 4873]\n",
      "loss: 1.159959  [ 3600/ 4873]\n",
      "loss: 0.996167  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.581     0.581    105\n",
      " disgust     0.577     0.738     0.569    109\n",
      "    fear     0.577     0.500     0.438    80\n",
      "   happy     0.577     0.558     0.654    81\n",
      " neutral     0.577     0.631     0.631    84\n",
      "     sad     0.577     0.431     0.506    87\n",
      "surprise     0.577     0.629     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.581     0.581    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.346022 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.619284  [ 1200/ 4873]\n",
      "loss: 0.876092  [ 2400/ 4873]\n",
      "loss: 1.089247  [ 3600/ 4873]\n",
      "loss: 1.326296  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.743     0.524    105\n",
      " disgust     0.631     0.701     0.752    109\n",
      "    fear     0.631     0.427     0.662    80\n",
      "   happy     0.631     0.632     0.679    81\n",
      " neutral     0.631     0.757     0.667    84\n",
      "     sad     0.631     0.549     0.517    87\n",
      "surprise     0.631     0.750     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.651     0.630    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.261456 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep139_acc_63.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep137_acc_63\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep139_acc_63\"! Old accuracy: 62.8, new accuracy: 63.1\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.880459  [ 1200/ 4873]\n",
      "loss: 0.784094  [ 2400/ 4873]\n",
      "loss: 1.649957  [ 3600/ 4873]\n",
      "loss: 1.224528  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.610     0.732     0.571    105\n",
      " disgust     0.610     0.764     0.624    109\n",
      "    fear     0.610     0.452     0.525    80\n",
      "   happy     0.610     0.564     0.704    81\n",
      " neutral     0.610     0.743     0.619    84\n",
      "     sad     0.610     0.510     0.563    87\n",
      "surprise     0.610     0.557     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.610     0.617     0.613    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.304609 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 1.288669  [ 1200/ 4873]\n",
      "loss: 1.107137  [ 2400/ 4873]\n",
      "loss: 0.973859  [ 3600/ 4873]\n",
      "loss: 0.803150  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.611     0.692     0.600    105\n",
      " disgust     0.611     0.670     0.706    109\n",
      "    fear     0.611     0.476     0.500    80\n",
      "   happy     0.611     0.671     0.580    81\n",
      " neutral     0.611     0.771     0.643    84\n",
      "     sad     0.611     0.460     0.655    87\n",
      "surprise     0.611     0.625     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.611     0.624     0.605    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.272980 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 1.301569  [ 1200/ 4873]\n",
      "loss: 0.828049  [ 2400/ 4873]\n",
      "loss: 1.209328  [ 3600/ 4873]\n",
      "loss: 0.932036  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.750     0.571    105\n",
      " disgust     0.636     0.657     0.807    109\n",
      "    fear     0.636     0.505     0.575    80\n",
      "   happy     0.636     0.653     0.580    81\n",
      " neutral     0.636     0.771     0.643    84\n",
      "     sad     0.636     0.509     0.632    87\n",
      "surprise     0.636     0.691     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.648     0.629    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.229698 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep142_acc_64.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep139_acc_63\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep142_acc_64\"! Old accuracy: 63.1, new accuracy: 63.6\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 1.356937  [ 1200/ 4873]\n",
      "loss: 1.651627  [ 2400/ 4873]\n",
      "loss: 1.306438  [ 3600/ 4873]\n",
      "loss: 1.137385  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.680     0.648    105\n",
      " disgust     0.638     0.761     0.642    109\n",
      "    fear     0.638     0.548     0.500    80\n",
      "   happy     0.638     0.716     0.593    81\n",
      " neutral     0.638     0.741     0.714    84\n",
      "     sad     0.638     0.441     0.736    87\n",
      "surprise     0.638     0.750     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.662     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.196933 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep143_acc_64.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep142_acc_64\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep143_acc_64\"! Old accuracy: 63.6, new accuracy: 63.8\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 1.202363  [ 1200/ 4873]\n",
      "loss: 1.267937  [ 2400/ 4873]\n",
      "loss: 1.287912  [ 3600/ 4873]\n",
      "loss: 1.165243  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.590     0.713     0.590    105\n",
      " disgust     0.590     0.755     0.651    109\n",
      "    fear     0.590     0.383     0.550    80\n",
      "   happy     0.590     0.650     0.642    81\n",
      " neutral     0.590     0.720     0.702    84\n",
      "     sad     0.590     0.426     0.563    87\n",
      "surprise     0.590     0.622     0.359    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.590     0.610     0.580    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.281725 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 1.463138  [ 1200/ 4873]\n",
      "loss: 1.144613  [ 2400/ 4873]\n",
      "loss: 0.936980  [ 3600/ 4873]\n",
      "loss: 0.761343  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.691     0.638    105\n",
      " disgust     0.633     0.727     0.734    109\n",
      "    fear     0.633     0.535     0.475    80\n",
      "   happy     0.633     0.658     0.593    81\n",
      " neutral     0.633     0.663     0.702    84\n",
      "     sad     0.633     0.476     0.575    87\n",
      "surprise     0.633     0.677     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.632     0.629    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.223782 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 1.348353  [ 1200/ 4873]\n",
      "loss: 1.009189  [ 2400/ 4873]\n",
      "loss: 1.162193  [ 3600/ 4873]\n",
      "loss: 0.541645  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.608     0.733     0.600    105\n",
      " disgust     0.608     0.735     0.661    109\n",
      "    fear     0.608     0.405     0.613    80\n",
      "   happy     0.608     0.576     0.654    81\n",
      " neutral     0.608     0.662     0.631    84\n",
      "     sad     0.608     0.533     0.563    87\n",
      "surprise     0.608     0.780     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.608     0.632     0.603    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.245367 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 1.002611  [ 1200/ 4873]\n",
      "loss: 1.011437  [ 2400/ 4873]\n",
      "loss: 0.921568  [ 3600/ 4873]\n",
      "loss: 1.132560  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.603     0.600     0.629    105\n",
      " disgust     0.603     0.787     0.642    109\n",
      "    fear     0.603     0.469     0.562    80\n",
      "   happy     0.603     0.607     0.630    81\n",
      " neutral     0.603     0.618     0.655    84\n",
      "     sad     0.603     0.551     0.494    87\n",
      "surprise     0.603     0.594     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.603     0.604     0.601    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.237124 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 1.217761  [ 1200/ 4873]\n",
      "loss: 0.910977  [ 2400/ 4873]\n",
      "loss: 1.016211  [ 3600/ 4873]\n",
      "loss: 0.807379  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.673     0.648    105\n",
      " disgust     0.633     0.722     0.716    109\n",
      "    fear     0.633     0.514     0.463    80\n",
      "   happy     0.633     0.643     0.556    81\n",
      " neutral     0.633     0.686     0.702    84\n",
      "     sad     0.633     0.527     0.678    87\n",
      "surprise     0.633     0.656     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.632     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.224183 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 1.214686  [ 1200/ 4873]\n",
      "loss: 0.994755  [ 2400/ 4873]\n",
      "loss: 0.977657  [ 3600/ 4873]\n",
      "loss: 1.027185  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.631     0.667    105\n",
      " disgust     0.631     0.725     0.679    109\n",
      "    fear     0.631     0.506     0.562    80\n",
      "   happy     0.631     0.786     0.679    81\n",
      " neutral     0.631     0.648     0.679    84\n",
      "     sad     0.631     0.480     0.563    87\n",
      "surprise     0.631     0.729     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.644     0.625    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.214718 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 1.227696  [ 1200/ 4873]\n",
      "loss: 0.595265  [ 2400/ 4873]\n",
      "loss: 0.880292  [ 3600/ 4873]\n",
      "loss: 0.678843  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.603     0.733     0.600    105\n",
      " disgust     0.603     0.632     0.661    109\n",
      "    fear     0.603     0.465     0.500    80\n",
      "   happy     0.603     0.610     0.617    81\n",
      " neutral     0.603     0.775     0.655    84\n",
      "     sad     0.603     0.417     0.552    87\n",
      "surprise     0.603     0.714     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.603     0.621     0.601    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.267717 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.942154  [ 1200/ 4873]\n",
      "loss: 1.580005  [ 2400/ 4873]\n",
      "loss: 1.366256  [ 3600/ 4873]\n",
      "loss: 0.945704  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.611     0.753     0.581    105\n",
      " disgust     0.611     0.627     0.771    109\n",
      "    fear     0.611     0.443     0.537    80\n",
      "   happy     0.611     0.708     0.630    81\n",
      " neutral     0.611     0.725     0.690    84\n",
      "     sad     0.611     0.414     0.471    87\n",
      "surprise     0.611     0.745     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.611     0.631     0.604    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.252927 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 1.030833  [ 1200/ 4873]\n",
      "loss: 0.914323  [ 2400/ 4873]\n",
      "loss: 1.296799  [ 3600/ 4873]\n",
      "loss: 1.265015  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.626     0.706     0.571    105\n",
      " disgust     0.626     0.772     0.651    109\n",
      "    fear     0.626     0.484     0.550    80\n",
      "   happy     0.626     0.587     0.667    81\n",
      " neutral     0.626     0.732     0.714    84\n",
      "     sad     0.626     0.468     0.598    87\n",
      "surprise     0.626     0.719     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.626     0.638     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.209783 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 1.585830  [ 1200/ 4873]\n",
      "loss: 0.871425  [ 2400/ 4873]\n",
      "loss: 0.937742  [ 3600/ 4873]\n",
      "loss: 1.268182  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.605     0.636     0.600    105\n",
      " disgust     0.605     0.840     0.578    109\n",
      "    fear     0.605     0.532     0.512    80\n",
      "   happy     0.605     0.634     0.642    81\n",
      " neutral     0.605     0.604     0.726    84\n",
      "     sad     0.605     0.430     0.598    87\n",
      "surprise     0.605     0.673     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.605     0.621     0.605    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.241131 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.868624  [ 1200/ 4873]\n",
      "loss: 1.110542  [ 2400/ 4873]\n",
      "loss: 1.282126  [ 3600/ 4873]\n",
      "loss: 1.013835  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.616     0.767     0.533    105\n",
      " disgust     0.616     0.724     0.697    109\n",
      "    fear     0.616     0.457     0.600    80\n",
      "   happy     0.616     0.607     0.630    81\n",
      " neutral     0.616     0.769     0.714    84\n",
      "     sad     0.616     0.444     0.644    87\n",
      "surprise     0.616     0.744     0.453    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.616     0.645     0.610    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.233879 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.884447  [ 1200/ 4873]\n",
      "loss: 0.999959  [ 2400/ 4873]\n",
      "loss: 1.773784  [ 3600/ 4873]\n",
      "loss: 1.349961  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.635     0.629    105\n",
      " disgust     0.643     0.724     0.697    109\n",
      "    fear     0.643     0.552     0.463    80\n",
      "   happy     0.643     0.679     0.654    81\n",
      " neutral     0.643     0.782     0.726    84\n",
      "     sad     0.643     0.504     0.667    87\n",
      "surprise     0.643     0.651     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.647     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.156612 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep155_acc_64.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep143_acc_64\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep155_acc_64\"! Old accuracy: 63.8, new accuracy: 64.3\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.868933  [ 1200/ 4873]\n",
      "loss: 0.981373  [ 2400/ 4873]\n",
      "loss: 1.035073  [ 3600/ 4873]\n",
      "loss: 1.131543  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.621     0.653     0.610    105\n",
      " disgust     0.621     0.735     0.661    109\n",
      "    fear     0.621     0.582     0.400    80\n",
      "   happy     0.621     0.658     0.617    81\n",
      " neutral     0.621     0.659     0.714    84\n",
      "     sad     0.621     0.471     0.644    87\n",
      "surprise     0.621     0.616     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.621     0.625     0.621    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.232052 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.984811  [ 1200/ 4873]\n",
      "loss: 1.212943  [ 2400/ 4873]\n",
      "loss: 0.961420  [ 3600/ 4873]\n",
      "loss: 1.155940  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.626     0.700     0.600    105\n",
      " disgust     0.626     0.658     0.688    109\n",
      "    fear     0.626     0.545     0.525    80\n",
      "   happy     0.626     0.662     0.630    81\n",
      " neutral     0.626     0.771     0.643    84\n",
      "     sad     0.626     0.436     0.667    87\n",
      "surprise     0.626     0.796     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.626     0.653     0.623    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.216642 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 1.202251  [ 1200/ 4873]\n",
      "loss: 1.197274  [ 2400/ 4873]\n",
      "loss: 1.213010  [ 3600/ 4873]\n",
      "loss: 1.474744  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.595     0.629    105\n",
      " disgust     0.631     0.732     0.651    109\n",
      "    fear     0.631     0.580     0.500    80\n",
      "   happy     0.631     0.696     0.593    81\n",
      " neutral     0.631     0.702     0.702    84\n",
      "     sad     0.631     0.495     0.621    87\n",
      "surprise     0.631     0.662     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.637     0.633    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.212150 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.885183  [ 1200/ 4873]\n",
      "loss: 0.808489  [ 2400/ 4873]\n",
      "loss: 0.898075  [ 3600/ 4873]\n",
      "loss: 2.007083  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.630     0.789     0.571    105\n",
      " disgust     0.630     0.618     0.771    109\n",
      "    fear     0.630     0.516     0.588    80\n",
      "   happy     0.630     0.621     0.667    81\n",
      " neutral     0.630     0.810     0.607    84\n",
      "     sad     0.630     0.470     0.632    87\n",
      "surprise     0.630     0.825     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.630     0.664     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.210432 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.884492  [ 1200/ 4873]\n",
      "loss: 0.891308  [ 2400/ 4873]\n",
      "loss: 1.036847  [ 3600/ 4873]\n",
      "loss: 0.687004  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.623     0.634     0.610    105\n",
      " disgust     0.623     0.775     0.633    109\n",
      "    fear     0.623     0.483     0.537    80\n",
      "   happy     0.623     0.684     0.667    81\n",
      " neutral     0.623     0.725     0.690    84\n",
      "     sad     0.623     0.470     0.621    87\n",
      "surprise     0.623     0.667     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.623     0.634     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.227984 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 1.413934  [ 1200/ 4873]\n",
      "loss: 1.207761  [ 2400/ 4873]\n",
      "loss: 0.549341  [ 3600/ 4873]\n",
      "loss: 1.315104  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.765     0.590    105\n",
      " disgust     0.649     0.798     0.688    109\n",
      "    fear     0.649     0.489     0.575    80\n",
      "   happy     0.649     0.667     0.716    81\n",
      " neutral     0.649     0.813     0.726    84\n",
      "     sad     0.649     0.462     0.701    87\n",
      "surprise     0.649     0.702     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.671     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.185556 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep161_acc_65.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep155_acc_64\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep161_acc_65\"! Old accuracy: 64.3, new accuracy: 64.9\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.812679  [ 1200/ 4873]\n",
      "loss: 0.895165  [ 2400/ 4873]\n",
      "loss: 0.906704  [ 3600/ 4873]\n",
      "loss: 0.986482  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.615     0.771     0.514    105\n",
      " disgust     0.615     0.764     0.624    109\n",
      "    fear     0.615     0.519     0.512    80\n",
      "   happy     0.615     0.643     0.667    81\n",
      " neutral     0.615     0.648     0.702    84\n",
      "     sad     0.615     0.438     0.609    87\n",
      "surprise     0.615     0.605     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.615     0.627     0.621    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.286067 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 1.177387  [ 1200/ 4873]\n",
      "loss: 1.149419  [ 2400/ 4873]\n",
      "loss: 0.673746  [ 3600/ 4873]\n",
      "loss: 1.110873  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.684     0.619    105\n",
      " disgust     0.639     0.738     0.697    109\n",
      "    fear     0.639     0.567     0.475    80\n",
      "   happy     0.639     0.564     0.654    81\n",
      " neutral     0.639     0.750     0.714    84\n",
      "     sad     0.639     0.518     0.678    87\n",
      "surprise     0.639     0.684     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.644     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.171237 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 1.297489  [ 1200/ 4873]\n",
      "loss: 1.134054  [ 2400/ 4873]\n",
      "loss: 0.869100  [ 3600/ 4873]\n",
      "loss: 1.174466  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.618     0.770     0.543    105\n",
      " disgust     0.618     0.812     0.633    109\n",
      "    fear     0.618     0.483     0.537    80\n",
      "   happy     0.618     0.580     0.630    81\n",
      " neutral     0.618     0.738     0.702    84\n",
      "     sad     0.618     0.441     0.690    87\n",
      "surprise     0.618     0.655     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.618     0.640     0.618    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.181083 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 1.106928  [ 1200/ 4873]\n",
      "loss: 0.763070  [ 2400/ 4873]\n",
      "loss: 0.949612  [ 3600/ 4873]\n",
      "loss: 1.488051  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.670     0.600    105\n",
      " disgust     0.641     0.722     0.761    109\n",
      "    fear     0.641     0.511     0.588    80\n",
      "   happy     0.641     0.667     0.642    81\n",
      " neutral     0.641     0.789     0.667    84\n",
      "     sad     0.641     0.483     0.667    87\n",
      "surprise     0.641     0.800     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.663     0.632    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.160224 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.786049  [ 1200/ 4873]\n",
      "loss: 0.820746  [ 2400/ 4873]\n",
      "loss: 1.232928  [ 3600/ 4873]\n",
      "loss: 0.782812  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.626     0.670     0.619    105\n",
      " disgust     0.626     0.770     0.706    109\n",
      "    fear     0.626     0.449     0.550    80\n",
      "   happy     0.626     0.593     0.667    81\n",
      " neutral     0.626     0.778     0.667    84\n",
      "     sad     0.626     0.500     0.667    87\n",
      "surprise     0.626     0.778     0.438    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.626     0.648     0.616    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.176989 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 1.223431  [ 1200/ 4873]\n",
      "loss: 0.810159  [ 2400/ 4873]\n",
      "loss: 0.859552  [ 3600/ 4873]\n",
      "loss: 0.868761  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.634     0.697     0.590    105\n",
      " disgust     0.634     0.681     0.725    109\n",
      "    fear     0.634     0.519     0.512    80\n",
      "   happy     0.634     0.659     0.667    81\n",
      " neutral     0.634     0.797     0.655    84\n",
      "     sad     0.634     0.474     0.632    87\n",
      "surprise     0.634     0.695     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.634     0.646     0.632    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.180748 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 1.400581  [ 1200/ 4873]\n",
      "loss: 1.144375  [ 2400/ 4873]\n",
      "loss: 1.150217  [ 3600/ 4873]\n",
      "loss: 1.077204  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.620     0.744     0.552    105\n",
      " disgust     0.620     0.696     0.716    109\n",
      "    fear     0.620     0.537     0.537    80\n",
      "   happy     0.620     0.738     0.556    81\n",
      " neutral     0.620     0.720     0.702    84\n",
      "     sad     0.620     0.403     0.644    87\n",
      "surprise     0.620     0.672     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.620     0.644     0.617    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.192703 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.952732  [ 1200/ 4873]\n",
      "loss: 0.957779  [ 2400/ 4873]\n",
      "loss: 0.932952  [ 3600/ 4873]\n",
      "loss: 0.931630  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.789     0.571    105\n",
      " disgust     0.633     0.735     0.688    109\n",
      "    fear     0.633     0.543     0.550    80\n",
      "   happy     0.633     0.612     0.642    81\n",
      " neutral     0.633     0.716     0.690    84\n",
      "     sad     0.633     0.450     0.667    87\n",
      "surprise     0.633     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.649     0.631    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.230399 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.875424  [ 1200/ 4873]\n",
      "loss: 0.772310  [ 2400/ 4873]\n",
      "loss: 0.500355  [ 3600/ 4873]\n",
      "loss: 0.727703  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.759     0.600    105\n",
      " disgust     0.643     0.634     0.780    109\n",
      "    fear     0.643     0.577     0.562    80\n",
      "   happy     0.643     0.779     0.654    81\n",
      " neutral     0.643     0.770     0.679    84\n",
      "     sad     0.643     0.429     0.621    87\n",
      "surprise     0.643     0.745     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.670     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.183067 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 1.248221  [ 1200/ 4873]\n",
      "loss: 1.322241  [ 2400/ 4873]\n",
      "loss: 1.065026  [ 3600/ 4873]\n",
      "loss: 0.960675  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.644     0.619    105\n",
      " disgust     0.641     0.734     0.734    109\n",
      "    fear     0.641     0.481     0.487    80\n",
      "   happy     0.641     0.667     0.642    81\n",
      " neutral     0.641     0.732     0.714    84\n",
      "     sad     0.641     0.550     0.632    87\n",
      "surprise     0.641     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.641     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.183982 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 1.455175  [ 1200/ 4873]\n",
      "loss: 0.527842  [ 2400/ 4873]\n",
      "loss: 0.433590  [ 3600/ 4873]\n",
      "loss: 1.057896  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.621     0.735     0.581    105\n",
      " disgust     0.621     0.745     0.670    109\n",
      "    fear     0.621     0.467     0.438    80\n",
      "   happy     0.621     0.589     0.691    81\n",
      " neutral     0.621     0.731     0.679    84\n",
      "     sad     0.621     0.469     0.690    87\n",
      "surprise     0.621     0.698     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.621     0.633     0.618    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.175225 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 1.140512  [ 1200/ 4873]\n",
      "loss: 0.893395  [ 2400/ 4873]\n",
      "loss: 1.355673  [ 3600/ 4873]\n",
      "loss: 1.262665  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.663     0.619    105\n",
      " disgust     0.638     0.756     0.624    109\n",
      "    fear     0.638     0.549     0.487    80\n",
      "   happy     0.638     0.628     0.667    81\n",
      " neutral     0.638     0.797     0.702    84\n",
      "     sad     0.638     0.517     0.701    87\n",
      "surprise     0.638     0.589     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.643     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.225232 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.945962  [ 1200/ 4873]\n",
      "loss: 1.418161  [ 2400/ 4873]\n",
      "loss: 1.004426  [ 3600/ 4873]\n",
      "loss: 0.615112  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.726     0.581    105\n",
      " disgust     0.636     0.755     0.651    109\n",
      "    fear     0.636     0.506     0.512    80\n",
      "   happy     0.636     0.600     0.667    81\n",
      " neutral     0.636     0.800     0.667    84\n",
      "     sad     0.636     0.500     0.713    87\n",
      "surprise     0.636     0.642     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.647     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.160177 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 1.175047  [ 1200/ 4873]\n",
      "loss: 1.078617  [ 2400/ 4873]\n",
      "loss: 1.026197  [ 3600/ 4873]\n",
      "loss: 0.631300  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.765     0.590    105\n",
      " disgust     0.662     0.768     0.697    109\n",
      "    fear     0.662     0.536     0.562    80\n",
      "   happy     0.662     0.611     0.679    81\n",
      " neutral     0.662     0.811     0.714    84\n",
      "     sad     0.662     0.533     0.747    87\n",
      "surprise     0.662     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.672     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.114016 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep175_acc_66.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep161_acc_65\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep175_acc_66\"! Old accuracy: 64.9, new accuracy: 66.2\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.642726  [ 1200/ 4873]\n",
      "loss: 0.957197  [ 2400/ 4873]\n",
      "loss: 0.832650  [ 3600/ 4873]\n",
      "loss: 0.992162  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.623     0.759     0.629    105\n",
      " disgust     0.623     0.722     0.716    109\n",
      "    fear     0.623     0.494     0.537    80\n",
      "   happy     0.623     0.541     0.654    81\n",
      " neutral     0.623     0.756     0.702    84\n",
      "     sad     0.623     0.483     0.644    87\n",
      "surprise     0.623     0.694     0.391    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.623     0.636     0.610    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.160010 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 1.193064  [ 1200/ 4873]\n",
      "loss: 1.017488  [ 2400/ 4873]\n",
      "loss: 0.969717  [ 3600/ 4873]\n",
      "loss: 1.197810  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.696     0.610    105\n",
      " disgust     0.651     0.760     0.697    109\n",
      "    fear     0.651     0.506     0.537    80\n",
      "   happy     0.651     0.663     0.704    81\n",
      " neutral     0.651     0.708     0.750    84\n",
      "     sad     0.651     0.518     0.667    87\n",
      "surprise     0.651     0.783     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.662     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.159707 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 1.050282  [ 1200/ 4873]\n",
      "loss: 1.490845  [ 2400/ 4873]\n",
      "loss: 0.507383  [ 3600/ 4873]\n",
      "loss: 1.536554  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.611     0.703     0.610    105\n",
      " disgust     0.611     0.731     0.624    109\n",
      "    fear     0.611     0.479     0.438    80\n",
      "   happy     0.611     0.641     0.617    81\n",
      " neutral     0.611     0.621     0.702    84\n",
      "     sad     0.611     0.479     0.667    87\n",
      "surprise     0.611     0.661     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.611     0.617     0.610    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.221418 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.829925  [ 1200/ 4873]\n",
      "loss: 1.048605  [ 2400/ 4873]\n",
      "loss: 0.879460  [ 3600/ 4873]\n",
      "loss: 1.228652  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.648     0.648    105\n",
      " disgust     0.631     0.792     0.697    109\n",
      "    fear     0.631     0.528     0.475    80\n",
      "   happy     0.631     0.758     0.617    81\n",
      " neutral     0.631     0.706     0.714    84\n",
      "     sad     0.631     0.430     0.667    87\n",
      "surprise     0.631     0.686     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.649     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.146034 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.852830  [ 1200/ 4873]\n",
      "loss: 0.912828  [ 2400/ 4873]\n",
      "loss: 1.720833  [ 3600/ 4873]\n",
      "loss: 0.911677  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.714     0.571    105\n",
      " disgust     0.643     0.729     0.716    109\n",
      "    fear     0.643     0.533     0.500    80\n",
      "   happy     0.643     0.718     0.630    81\n",
      " neutral     0.643     0.795     0.738    84\n",
      "     sad     0.643     0.449     0.713    87\n",
      "surprise     0.643     0.684     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.660     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.149380 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.779633  [ 1200/ 4873]\n",
      "loss: 0.897975  [ 2400/ 4873]\n",
      "loss: 1.072512  [ 3600/ 4873]\n",
      "loss: 0.651543  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.621     0.805     0.590    105\n",
      " disgust     0.621     0.714     0.688    109\n",
      "    fear     0.621     0.487     0.463    80\n",
      "   happy     0.621     0.614     0.630    81\n",
      " neutral     0.621     0.737     0.667    84\n",
      "     sad     0.621     0.417     0.667    87\n",
      "surprise     0.621     0.741     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.621     0.645     0.618    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.163822 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.736870  [ 1200/ 4873]\n",
      "loss: 0.972438  [ 2400/ 4873]\n",
      "loss: 0.738138  [ 3600/ 4873]\n",
      "loss: 0.757416  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.615     0.772     0.581    105\n",
      " disgust     0.615     0.819     0.624    109\n",
      "    fear     0.615     0.500     0.562    80\n",
      "   happy     0.615     0.613     0.605    81\n",
      " neutral     0.615     0.635     0.643    84\n",
      "     sad     0.615     0.428     0.713    87\n",
      "surprise     0.615     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.615     0.645     0.613    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.182962 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 1.181379  [ 1200/ 4873]\n",
      "loss: 0.897083  [ 2400/ 4873]\n",
      "loss: 1.018243  [ 3600/ 4873]\n",
      "loss: 1.370189  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.838     0.543    105\n",
      " disgust     0.641     0.733     0.679    109\n",
      "    fear     0.641     0.453     0.600    80\n",
      "   happy     0.641     0.605     0.642    81\n",
      " neutral     0.641     0.766     0.702    84\n",
      "     sad     0.641     0.541     0.678    87\n",
      "surprise     0.641     0.667     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.658     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.198074 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.642100  [ 1200/ 4873]\n",
      "loss: 1.276591  [ 2400/ 4873]\n",
      "loss: 0.601373  [ 3600/ 4873]\n",
      "loss: 1.675270  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.739     0.648    105\n",
      " disgust     0.636     0.722     0.716    109\n",
      "    fear     0.636     0.562     0.512    80\n",
      "   happy     0.636     0.776     0.556    81\n",
      " neutral     0.636     0.632     0.714    84\n",
      "     sad     0.636     0.444     0.724    87\n",
      "surprise     0.636     0.786     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.666     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.168451 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.792713  [ 1200/ 4873]\n",
      "loss: 1.056791  [ 2400/ 4873]\n",
      "loss: 0.871629  [ 3600/ 4873]\n",
      "loss: 1.039365  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.775     0.590    105\n",
      " disgust     0.639     0.784     0.633    109\n",
      "    fear     0.639     0.538     0.613    80\n",
      "   happy     0.639     0.633     0.704    81\n",
      " neutral     0.639     0.645     0.714    84\n",
      "     sad     0.639     0.496     0.655    87\n",
      "surprise     0.639     0.679     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.650     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.209971 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.883215  [ 1200/ 4873]\n",
      "loss: 1.068289  [ 2400/ 4873]\n",
      "loss: 0.727285  [ 3600/ 4873]\n",
      "loss: 0.756728  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.685     0.581    105\n",
      " disgust     0.636     0.722     0.642    109\n",
      "    fear     0.636     0.533     0.500    80\n",
      "   happy     0.636     0.613     0.704    81\n",
      " neutral     0.636     0.759     0.714    84\n",
      "     sad     0.636     0.480     0.678    87\n",
      "surprise     0.636     0.759     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.650     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.179208 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.803163  [ 1200/ 4873]\n",
      "loss: 0.878757  [ 2400/ 4873]\n",
      "loss: 1.322017  [ 3600/ 4873]\n",
      "loss: 0.503545  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.724     0.600    105\n",
      " disgust     0.648     0.772     0.651    109\n",
      "    fear     0.648     0.520     0.487    80\n",
      "   happy     0.648     0.674     0.716    81\n",
      " neutral     0.648     0.697     0.738    84\n",
      "     sad     0.648     0.505     0.632    87\n",
      "surprise     0.648     0.653     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.649     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.163201 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.924340  [ 1200/ 4873]\n",
      "loss: 0.510535  [ 2400/ 4873]\n",
      "loss: 0.833334  [ 3600/ 4873]\n",
      "loss: 1.176558  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.744     0.638    105\n",
      " disgust     0.643     0.766     0.661    109\n",
      "    fear     0.643     0.482     0.662    80\n",
      "   happy     0.643     0.628     0.667    81\n",
      " neutral     0.643     0.728     0.702    84\n",
      "     sad     0.643     0.521     0.575    87\n",
      "surprise     0.643     0.698     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.652     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.190321 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.565959  [ 1200/ 4873]\n",
      "loss: 0.937201  [ 2400/ 4873]\n",
      "loss: 1.271530  [ 3600/ 4873]\n",
      "loss: 1.314045  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.634     0.778     0.600    105\n",
      " disgust     0.634     0.812     0.633    109\n",
      "    fear     0.634     0.545     0.525    80\n",
      "   happy     0.634     0.600     0.667    81\n",
      " neutral     0.634     0.608     0.738    84\n",
      "     sad     0.634     0.479     0.655    87\n",
      "surprise     0.634     0.714     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.634     0.648     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.187388 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.779793  [ 1200/ 4873]\n",
      "loss: 1.238335  [ 2400/ 4873]\n",
      "loss: 0.954294  [ 3600/ 4873]\n",
      "loss: 0.931995  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.753     0.581    105\n",
      " disgust     0.654     0.737     0.642    109\n",
      "    fear     0.654     0.561     0.575    80\n",
      "   happy     0.654     0.602     0.728    81\n",
      " neutral     0.654     0.768     0.750    84\n",
      "     sad     0.654     0.513     0.690    87\n",
      "surprise     0.654     0.727     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.666     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.159705 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.630254  [ 1200/ 4873]\n",
      "loss: 0.802691  [ 2400/ 4873]\n",
      "loss: 0.677455  [ 3600/ 4873]\n",
      "loss: 0.834391  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.711     0.610    105\n",
      " disgust     0.651     0.736     0.716    109\n",
      "    fear     0.651     0.566     0.537    80\n",
      "   happy     0.651     0.680     0.630    81\n",
      " neutral     0.651     0.773     0.690    84\n",
      "     sad     0.651     0.481     0.736    87\n",
      "surprise     0.651     0.709     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.665     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.130494 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 1.180791  [ 1200/ 4873]\n",
      "loss: 1.165215  [ 2400/ 4873]\n",
      "loss: 0.899041  [ 3600/ 4873]\n",
      "loss: 0.639499  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.826     0.543    105\n",
      " disgust     0.644     0.747     0.651    109\n",
      "    fear     0.644     0.525     0.525    80\n",
      "   happy     0.644     0.641     0.728    81\n",
      " neutral     0.644     0.750     0.750    84\n",
      "     sad     0.644     0.492     0.690    87\n",
      "surprise     0.644     0.603     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.655     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.126253 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.590614  [ 1200/ 4873]\n",
      "loss: 0.841824  [ 2400/ 4873]\n",
      "loss: 1.048201  [ 3600/ 4873]\n",
      "loss: 0.817883  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.797     0.600    105\n",
      " disgust     0.652     0.731     0.725    109\n",
      "    fear     0.652     0.500     0.537    80\n",
      "   happy     0.652     0.663     0.679    81\n",
      " neutral     0.652     0.853     0.690    84\n",
      "     sad     0.652     0.454     0.678    87\n",
      "surprise     0.652     0.732     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.676     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.128972 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.928981  [ 1200/ 4873]\n",
      "loss: 0.926875  [ 2400/ 4873]\n",
      "loss: 1.111600  [ 3600/ 4873]\n",
      "loss: 1.062163  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.773     0.648    105\n",
      " disgust     0.661     0.800     0.697    109\n",
      "    fear     0.661     0.591     0.487    80\n",
      "   happy     0.661     0.693     0.642    81\n",
      " neutral     0.661     0.660     0.762    84\n",
      "     sad     0.661     0.479     0.770    87\n",
      "surprise     0.661     0.755     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.679     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.118641 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 1.209352  [ 1200/ 4873]\n",
      "loss: 0.875992  [ 2400/ 4873]\n",
      "loss: 1.469096  [ 3600/ 4873]\n",
      "loss: 0.752045  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.753     0.610    105\n",
      " disgust     0.644     0.796     0.679    109\n",
      "    fear     0.644     0.547     0.438    80\n",
      "   happy     0.644     0.593     0.667    81\n",
      " neutral     0.644     0.711     0.762    84\n",
      "     sad     0.644     0.485     0.724    87\n",
      "surprise     0.644     0.684     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.653     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.156931 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.837724  [ 1200/ 4873]\n",
      "loss: 1.200437  [ 2400/ 4873]\n",
      "loss: 0.694999  [ 3600/ 4873]\n",
      "loss: 1.035127  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.657     0.619    105\n",
      " disgust     0.651     0.802     0.633    109\n",
      "    fear     0.651     0.495     0.613    80\n",
      "   happy     0.651     0.682     0.716    81\n",
      " neutral     0.651     0.756     0.738    84\n",
      "     sad     0.651     0.550     0.632    87\n",
      "surprise     0.651     0.661     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.658     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.131660 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 1.534397  [ 1200/ 4873]\n",
      "loss: 0.894090  [ 2400/ 4873]\n",
      "loss: 1.280864  [ 3600/ 4873]\n",
      "loss: 1.071965  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.797     0.600    105\n",
      " disgust     0.661     0.849     0.670    109\n",
      "    fear     0.661     0.570     0.562    80\n",
      "   happy     0.661     0.564     0.704    81\n",
      " neutral     0.661     0.718     0.726    84\n",
      "     sad     0.661     0.508     0.747    87\n",
      "surprise     0.661     0.750     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.679     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.137272 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.335247  [ 1200/ 4873]\n",
      "loss: 0.853716  [ 2400/ 4873]\n",
      "loss: 1.222313  [ 3600/ 4873]\n",
      "loss: 0.824085  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.797     0.562    105\n",
      " disgust     0.644     0.738     0.697    109\n",
      "    fear     0.644     0.413     0.537    80\n",
      "   happy     0.644     0.619     0.642    81\n",
      " neutral     0.644     0.855     0.702    84\n",
      "     sad     0.644     0.531     0.782    87\n",
      "surprise     0.644     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.672     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.133538 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.804292  [ 1200/ 4873]\n",
      "loss: 0.755730  [ 2400/ 4873]\n",
      "loss: 0.732557  [ 3600/ 4873]\n",
      "loss: 0.613292  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.783     0.619    105\n",
      " disgust     0.648     0.743     0.716    109\n",
      "    fear     0.648     0.606     0.500    80\n",
      "   happy     0.648     0.591     0.642    81\n",
      " neutral     0.648     0.652     0.714    84\n",
      "     sad     0.648     0.492     0.724    87\n",
      "surprise     0.648     0.771     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.663     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.102530 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.585993  [ 1200/ 4873]\n",
      "loss: 0.663881  [ 2400/ 4873]\n",
      "loss: 0.970270  [ 3600/ 4873]\n",
      "loss: 1.110011  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.851     0.600    105\n",
      " disgust     0.666     0.806     0.688    109\n",
      "    fear     0.666     0.639     0.575    80\n",
      "   happy     0.666     0.706     0.593    81\n",
      " neutral     0.666     0.649     0.726    84\n",
      "     sad     0.666     0.473     0.816    87\n",
      "surprise     0.666     0.712     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.691     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.113691 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep200_acc_67.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep175_acc_66\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep200_acc_67\"! Old accuracy: 66.2, new accuracy: 66.6\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.720577  [ 1200/ 4873]\n",
      "loss: 1.287208  [ 2400/ 4873]\n",
      "loss: 1.078011  [ 3600/ 4873]\n",
      "loss: 0.730747  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.741     0.600    105\n",
      " disgust     0.664     0.721     0.734    109\n",
      "    fear     0.664     0.516     0.588    80\n",
      "   happy     0.664     0.659     0.691    81\n",
      " neutral     0.664     0.892     0.690    84\n",
      "     sad     0.664     0.504     0.713    87\n",
      "surprise     0.664     0.780     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.688     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.136401 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.781931  [ 1200/ 4873]\n",
      "loss: 1.331744  [ 2400/ 4873]\n",
      "loss: 0.824255  [ 3600/ 4873]\n",
      "loss: 0.680592  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.628     0.792     0.581    105\n",
      " disgust     0.628     0.846     0.606    109\n",
      "    fear     0.628     0.565     0.487    80\n",
      "   happy     0.628     0.640     0.679    81\n",
      " neutral     0.628     0.558     0.750    84\n",
      "     sad     0.628     0.435     0.690    87\n",
      "surprise     0.628     0.796     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.628     0.662     0.629    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.137440 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.818694  [ 1200/ 4873]\n",
      "loss: 0.986775  [ 2400/ 4873]\n",
      "loss: 1.121621  [ 3600/ 4873]\n",
      "loss: 0.890272  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.816     0.590    105\n",
      " disgust     0.643     0.723     0.670    109\n",
      "    fear     0.643     0.516     0.613    80\n",
      "   happy     0.643     0.576     0.654    81\n",
      " neutral     0.643     0.779     0.714    84\n",
      "     sad     0.643     0.484     0.701    87\n",
      "surprise     0.643     0.791     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.669     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.143448 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 1.076074  [ 1200/ 4873]\n",
      "loss: 1.207421  [ 2400/ 4873]\n",
      "loss: 0.739355  [ 3600/ 4873]\n",
      "loss: 1.500095  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.620     0.775     0.590    105\n",
      " disgust     0.620     0.685     0.697    109\n",
      "    fear     0.620     0.478     0.400    80\n",
      "   happy     0.620     0.609     0.654    81\n",
      " neutral     0.620     0.682     0.714    84\n",
      "     sad     0.620     0.458     0.632    87\n",
      "surprise     0.620     0.702     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.620     0.627     0.616    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.191598 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 1.062114  [ 1200/ 4873]\n",
      "loss: 1.042674  [ 2400/ 4873]\n",
      "loss: 1.372628  [ 3600/ 4873]\n",
      "loss: 1.299114  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.759     0.629    105\n",
      " disgust     0.657     0.779     0.679    109\n",
      "    fear     0.657     0.587     0.550    80\n",
      "   happy     0.657     0.707     0.716    81\n",
      " neutral     0.657     0.735     0.726    84\n",
      "     sad     0.657     0.449     0.713    87\n",
      "surprise     0.657     0.720     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.677     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.138113 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 1.458780  [ 1200/ 4873]\n",
      "loss: 0.895441  [ 2400/ 4873]\n",
      "loss: 0.666705  [ 3600/ 4873]\n",
      "loss: 0.778907  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.766     0.562    105\n",
      " disgust     0.631     0.831     0.541    109\n",
      "    fear     0.631     0.553     0.525    80\n",
      "   happy     0.631     0.484     0.741    81\n",
      " neutral     0.631     0.756     0.738    84\n",
      "     sad     0.631     0.557     0.678    87\n",
      "surprise     0.631     0.595     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.649     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.256448 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 1.253867  [ 1200/ 4873]\n",
      "loss: 1.395446  [ 2400/ 4873]\n",
      "loss: 1.267236  [ 3600/ 4873]\n",
      "loss: 1.222084  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.677     0.619    105\n",
      " disgust     0.657     0.735     0.761    109\n",
      "    fear     0.657     0.523     0.562    80\n",
      "   happy     0.657     0.667     0.642    81\n",
      " neutral     0.657     0.881     0.702    84\n",
      "     sad     0.657     0.525     0.713    87\n",
      "surprise     0.657     0.673     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.669     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.138166 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.862291  [ 1200/ 4873]\n",
      "loss: 0.858824  [ 2400/ 4873]\n",
      "loss: 1.254841  [ 3600/ 4873]\n",
      "loss: 0.642729  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.813     0.581    105\n",
      " disgust     0.651     0.752     0.725    109\n",
      "    fear     0.651     0.544     0.537    80\n",
      "   happy     0.651     0.739     0.630    81\n",
      " neutral     0.651     0.732     0.714    84\n",
      "     sad     0.651     0.438     0.736    87\n",
      "surprise     0.651     0.722     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.677     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.173081 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.824023  [ 1200/ 4873]\n",
      "loss: 0.434269  [ 2400/ 4873]\n",
      "loss: 1.270040  [ 3600/ 4873]\n",
      "loss: 0.803846  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.838     0.543    105\n",
      " disgust     0.641     0.743     0.716    109\n",
      "    fear     0.641     0.476     0.613    80\n",
      "   happy     0.641     0.633     0.704    81\n",
      " neutral     0.641     0.740     0.679    84\n",
      "     sad     0.641     0.470     0.632    87\n",
      "surprise     0.641     0.760     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.666     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.144728 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.722994  [ 1200/ 4873]\n",
      "loss: 0.929463  [ 2400/ 4873]\n",
      "loss: 0.886404  [ 3600/ 4873]\n",
      "loss: 0.704287  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.811     0.571    105\n",
      " disgust     0.652     0.757     0.716    109\n",
      "    fear     0.652     0.606     0.500    80\n",
      "   happy     0.652     0.573     0.679    81\n",
      " neutral     0.652     0.816     0.738    84\n",
      "     sad     0.652     0.481     0.736    87\n",
      "surprise     0.652     0.629     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.668     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.160560 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.649708  [ 1200/ 4873]\n",
      "loss: 1.231641  [ 2400/ 4873]\n",
      "loss: 1.293400  [ 3600/ 4873]\n",
      "loss: 1.222452  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.708     0.600    105\n",
      " disgust     0.651     0.743     0.688    109\n",
      "    fear     0.651     0.531     0.425    80\n",
      "   happy     0.651     0.675     0.691    81\n",
      " neutral     0.651     0.800     0.714    84\n",
      "     sad     0.651     0.496     0.759    87\n",
      "surprise     0.651     0.662     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.659     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.166018 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 1.083375  [ 1200/ 4873]\n",
      "loss: 0.905824  [ 2400/ 4873]\n",
      "loss: 1.082199  [ 3600/ 4873]\n",
      "loss: 1.006898  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.791     0.648    105\n",
      " disgust     0.636     0.846     0.606    109\n",
      "    fear     0.636     0.500     0.550    80\n",
      "   happy     0.636     0.486     0.654    81\n",
      " neutral     0.636     0.779     0.714    84\n",
      "     sad     0.636     0.521     0.701    87\n",
      "surprise     0.636     0.655     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.654     0.634    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.143332 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.538638  [ 1200/ 4873]\n",
      "loss: 0.852438  [ 2400/ 4873]\n",
      "loss: 0.968599  [ 3600/ 4873]\n",
      "loss: 0.990334  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.697     0.657    105\n",
      " disgust     0.657     0.850     0.624    109\n",
      "    fear     0.657     0.600     0.525    80\n",
      "   happy     0.657     0.602     0.728    81\n",
      " neutral     0.657     0.670     0.750    84\n",
      "     sad     0.657     0.592     0.667    87\n",
      "surprise     0.657     0.592     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.658     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.151426 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.783900  [ 1200/ 4873]\n",
      "loss: 1.295362  [ 2400/ 4873]\n",
      "loss: 1.234200  [ 3600/ 4873]\n",
      "loss: 0.884794  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.722     0.619    105\n",
      " disgust     0.639     0.819     0.624    109\n",
      "    fear     0.639     0.582     0.487    80\n",
      "   happy     0.639     0.591     0.642    81\n",
      " neutral     0.639     0.653     0.762    84\n",
      "     sad     0.639     0.488     0.713    87\n",
      "surprise     0.639     0.702     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.651     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.139878 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.812227  [ 1200/ 4873]\n",
      "loss: 1.129517  [ 2400/ 4873]\n",
      "loss: 0.818688  [ 3600/ 4873]\n",
      "loss: 0.774777  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.816     0.590    105\n",
      " disgust     0.652     0.784     0.697    109\n",
      "    fear     0.652     0.500     0.537    80\n",
      "   happy     0.652     0.566     0.691    81\n",
      " neutral     0.652     0.819     0.702    84\n",
      "     sad     0.652     0.508     0.747    87\n",
      "surprise     0.652     0.712     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.672     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.109531 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.416382  [ 1200/ 4873]\n",
      "loss: 1.021468  [ 2400/ 4873]\n",
      "loss: 1.435441  [ 3600/ 4873]\n",
      "loss: 1.224890  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.667     0.648    105\n",
      " disgust     0.654     0.703     0.716    109\n",
      "    fear     0.654     0.609     0.487    80\n",
      "   happy     0.654     0.703     0.642    81\n",
      " neutral     0.654     0.787     0.750    84\n",
      "     sad     0.654     0.481     0.724    87\n",
      "surprise     0.654     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.671     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.138272 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 1.053373  [ 1200/ 4873]\n",
      "loss: 1.480992  [ 2400/ 4873]\n",
      "loss: 0.771983  [ 3600/ 4873]\n",
      "loss: 1.078302  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.618     0.625     0.667    105\n",
      " disgust     0.618     0.795     0.606    109\n",
      "    fear     0.618     0.660     0.388    80\n",
      "   happy     0.618     0.662     0.580    81\n",
      " neutral     0.618     0.657     0.798    84\n",
      "     sad     0.618     0.470     0.632    87\n",
      "surprise     0.618     0.526     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.618     0.628     0.616    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.226136 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.762669  [ 1200/ 4873]\n",
      "loss: 1.016929  [ 2400/ 4873]\n",
      "loss: 0.692707  [ 3600/ 4873]\n",
      "loss: 0.840690  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.699     0.619    105\n",
      " disgust     0.654     0.755     0.706    109\n",
      "    fear     0.654     0.548     0.500    80\n",
      "   happy     0.654     0.583     0.691    81\n",
      " neutral     0.654     0.753     0.726    84\n",
      "     sad     0.654     0.563     0.667    87\n",
      "surprise     0.654     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.654     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.145445 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 1.166944  [ 1200/ 4873]\n",
      "loss: 1.067197  [ 2400/ 4873]\n",
      "loss: 0.802996  [ 3600/ 4873]\n",
      "loss: 1.066150  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.634     0.685     0.600    105\n",
      " disgust     0.634     0.703     0.716    109\n",
      "    fear     0.634     0.525     0.525    80\n",
      "   happy     0.634     0.681     0.580    81\n",
      " neutral     0.634     0.741     0.714    84\n",
      "     sad     0.634     0.489     0.736    87\n",
      "surprise     0.634     0.717     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.634     0.649     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1.156271 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.995442  [ 1200/ 4873]\n",
      "loss: 0.614606  [ 2400/ 4873]\n",
      "loss: 0.838517  [ 3600/ 4873]\n",
      "loss: 0.571197  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.785     0.590    105\n",
      " disgust     0.638     0.785     0.670    109\n",
      "    fear     0.638     0.526     0.512    80\n",
      "   happy     0.638     0.583     0.605    81\n",
      " neutral     0.638     0.718     0.726    84\n",
      "     sad     0.638     0.484     0.713    87\n",
      "surprise     0.638     0.651     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.647     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.160605 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.599424  [ 1200/ 4873]\n",
      "loss: 0.836408  [ 2400/ 4873]\n",
      "loss: 1.033000  [ 3600/ 4873]\n",
      "loss: 0.769890  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.723     0.648    105\n",
      " disgust     0.651     0.713     0.706    109\n",
      "    fear     0.651     0.582     0.487    80\n",
      "   happy     0.651     0.701     0.580    81\n",
      " neutral     0.651     0.735     0.726    84\n",
      "     sad     0.651     0.479     0.770    87\n",
      "surprise     0.651     0.745     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.668     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.140357 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.487849  [ 1200/ 4873]\n",
      "loss: 0.590517  [ 2400/ 4873]\n",
      "loss: 0.657150  [ 3600/ 4873]\n",
      "loss: 0.765747  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.670     0.695    105\n",
      " disgust     0.662     0.757     0.716    109\n",
      "    fear     0.662     0.679     0.475    80\n",
      "   happy     0.662     0.671     0.630    81\n",
      " neutral     0.662     0.674     0.762    84\n",
      "     sad     0.662     0.508     0.724    87\n",
      "surprise     0.662     0.787     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.678     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.131508 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.794649  [ 1200/ 4873]\n",
      "loss: 0.750169  [ 2400/ 4873]\n",
      "loss: 1.215739  [ 3600/ 4873]\n",
      "loss: 0.946690  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.623     0.776     0.629    105\n",
      " disgust     0.623     0.810     0.587    109\n",
      "    fear     0.623     0.547     0.512    80\n",
      "   happy     0.623     0.596     0.691    81\n",
      " neutral     0.623     0.633     0.738    84\n",
      "     sad     0.623     0.426     0.632    87\n",
      "surprise     0.623     0.720     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.623     0.644     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.164917 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 1.048972  [ 1200/ 4873]\n",
      "loss: 0.438859  [ 2400/ 4873]\n",
      "loss: 1.083862  [ 3600/ 4873]\n",
      "loss: 0.863589  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.702     0.629    105\n",
      " disgust     0.639     0.773     0.688    109\n",
      "    fear     0.639     0.569     0.512    80\n",
      "   happy     0.639     0.549     0.556    81\n",
      " neutral     0.639     0.792     0.726    84\n",
      "     sad     0.639     0.500     0.713    87\n",
      "surprise     0.639     0.625     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.644     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.101873 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 1.033238  [ 1200/ 4873]\n",
      "loss: 0.661303  [ 2400/ 4873]\n",
      "loss: 1.249435  [ 3600/ 4873]\n",
      "loss: 0.996405  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.716     0.648    105\n",
      " disgust     0.646     0.748     0.706    109\n",
      "    fear     0.646     0.532     0.525    80\n",
      "   happy     0.646     0.634     0.642    81\n",
      " neutral     0.646     0.744     0.726    84\n",
      "     sad     0.646     0.484     0.690    87\n",
      "surprise     0.646     0.756     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.659     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.147364 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.534118  [ 1200/ 4873]\n",
      "loss: 0.823573  [ 2400/ 4873]\n",
      "loss: 1.013360  [ 3600/ 4873]\n",
      "loss: 0.572019  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.733     0.629    105\n",
      " disgust     0.644     0.753     0.670    109\n",
      "    fear     0.644     0.614     0.438    80\n",
      "   happy     0.644     0.634     0.642    81\n",
      " neutral     0.644     0.756     0.702    84\n",
      "     sad     0.644     0.454     0.793    87\n",
      "surprise     0.644     0.722     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.667     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.166123 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.730526  [ 1200/ 4873]\n",
      "loss: 0.356444  [ 2400/ 4873]\n",
      "loss: 0.466704  [ 3600/ 4873]\n",
      "loss: 0.447937  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.813     0.581    105\n",
      " disgust     0.656     0.689     0.752    109\n",
      "    fear     0.656     0.590     0.450    80\n",
      "   happy     0.656     0.667     0.642    81\n",
      " neutral     0.656     0.795     0.738    84\n",
      "     sad     0.656     0.471     0.736    87\n",
      "surprise     0.656     0.683     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.672     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.122206 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 1.031516  [ 1200/ 4873]\n",
      "loss: 0.970264  [ 2400/ 4873]\n",
      "loss: 1.087669  [ 3600/ 4873]\n",
      "loss: 0.730872  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.670     0.619    105\n",
      " disgust     0.649     0.745     0.725    109\n",
      "    fear     0.649     0.600     0.412    80\n",
      "   happy     0.649     0.602     0.654    81\n",
      " neutral     0.649     0.773     0.690    84\n",
      "     sad     0.649     0.523     0.770    87\n",
      "surprise     0.649     0.672     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.655     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.149205 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 1.327545  [ 1200/ 4873]\n",
      "loss: 1.254249  [ 2400/ 4873]\n",
      "loss: 0.858504  [ 3600/ 4873]\n",
      "loss: 0.576146  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.639     0.590    105\n",
      " disgust     0.649     0.748     0.706    109\n",
      "    fear     0.649     0.544     0.463    80\n",
      "   happy     0.649     0.703     0.642    81\n",
      " neutral     0.649     0.756     0.738    84\n",
      "     sad     0.649     0.533     0.747    87\n",
      "surprise     0.649     0.641     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.652     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.124971 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.486414  [ 1200/ 4873]\n",
      "loss: 0.898685  [ 2400/ 4873]\n",
      "loss: 0.608709  [ 3600/ 4873]\n",
      "loss: 0.894462  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.774     0.619    105\n",
      " disgust     0.657     0.747     0.679    109\n",
      "    fear     0.657     0.505     0.613    80\n",
      "   happy     0.657     0.571     0.691    81\n",
      " neutral     0.657     0.851     0.679    84\n",
      "     sad     0.657     0.578     0.724    87\n",
      "surprise     0.657     0.661     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.670     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.113750 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.517553  [ 1200/ 4873]\n",
      "loss: 0.735173  [ 2400/ 4873]\n",
      "loss: 0.656117  [ 3600/ 4873]\n",
      "loss: 0.855540  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.729     0.667    105\n",
      " disgust     0.633     0.784     0.697    109\n",
      "    fear     0.633     0.506     0.537    80\n",
      "   happy     0.633     0.549     0.691    81\n",
      " neutral     0.633     0.695     0.679    84\n",
      "     sad     0.633     0.513     0.701    87\n",
      "surprise     0.633     0.793     0.359    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.653     0.619    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.195353 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.603990  [ 1200/ 4873]\n",
      "loss: 1.105980  [ 2400/ 4873]\n",
      "loss: 0.679106  [ 3600/ 4873]\n",
      "loss: 0.676854  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.721     0.590    105\n",
      " disgust     0.646     0.704     0.633    109\n",
      "    fear     0.646     0.539     0.512    80\n",
      "   happy     0.646     0.573     0.679    81\n",
      " neutral     0.646     0.759     0.714    84\n",
      "     sad     0.646     0.557     0.736    87\n",
      "surprise     0.646     0.717     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.653     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.175354 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.681353  [ 1200/ 4873]\n",
      "loss: 1.395906  [ 2400/ 4873]\n",
      "loss: 0.869488  [ 3600/ 4873]\n",
      "loss: 1.287856  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.695     0.629    105\n",
      " disgust     0.669     0.769     0.642    109\n",
      "    fear     0.669     0.574     0.438    80\n",
      "   happy     0.669     0.644     0.716    81\n",
      " neutral     0.669     0.810     0.762    84\n",
      "     sad     0.669     0.539     0.793    87\n",
      "surprise     0.669     0.697     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.675     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.131259 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep233_acc_67.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep200_acc_67\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep233_acc_67\"! Old accuracy: 66.6, new accuracy: 66.9\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 1.083047  [ 1200/ 4873]\n",
      "loss: 1.038808  [ 2400/ 4873]\n",
      "loss: 0.894760  [ 3600/ 4873]\n",
      "loss: 0.820807  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.808     0.600    105\n",
      " disgust     0.664     0.781     0.688    109\n",
      "    fear     0.664     0.500     0.575    80\n",
      "   happy     0.664     0.602     0.728    81\n",
      " neutral     0.664     0.778     0.750    84\n",
      "     sad     0.664     0.553     0.655    87\n",
      "surprise     0.664     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.671     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.133829 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.900575  [ 1200/ 4873]\n",
      "loss: 1.722346  [ 2400/ 4873]\n",
      "loss: 0.797322  [ 3600/ 4873]\n",
      "loss: 1.225194  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.707     0.619    105\n",
      " disgust     0.651     0.768     0.670    109\n",
      "    fear     0.651     0.581     0.537    80\n",
      "   happy     0.651     0.624     0.654    81\n",
      " neutral     0.651     0.738     0.738    84\n",
      "     sad     0.651     0.492     0.724    87\n",
      "surprise     0.651     0.731     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.663     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.114065 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 1.145959  [ 1200/ 4873]\n",
      "loss: 0.620362  [ 2400/ 4873]\n",
      "loss: 1.162537  [ 3600/ 4873]\n",
      "loss: 0.632733  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.753     0.638    105\n",
      " disgust     0.652     0.767     0.725    109\n",
      "    fear     0.652     0.562     0.512    80\n",
      "   happy     0.652     0.589     0.654    81\n",
      " neutral     0.652     0.670     0.702    84\n",
      "     sad     0.652     0.513     0.690    87\n",
      "surprise     0.652     0.780     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.662     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.111315 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 1.229476  [ 1200/ 4873]\n",
      "loss: 0.820997  [ 2400/ 4873]\n",
      "loss: 0.901344  [ 3600/ 4873]\n",
      "loss: 0.812427  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.695     0.695    105\n",
      " disgust     0.657     0.753     0.670    109\n",
      "    fear     0.657     0.528     0.475    80\n",
      "   happy     0.657     0.662     0.630    81\n",
      " neutral     0.657     0.782     0.726    84\n",
      "     sad     0.657     0.512     0.747    87\n",
      "surprise     0.657     0.741     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.668     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.064386 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.803178  [ 1200/ 4873]\n",
      "loss: 1.006086  [ 2400/ 4873]\n",
      "loss: 1.158030  [ 3600/ 4873]\n",
      "loss: 1.181402  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.765     0.590    105\n",
      " disgust     0.646     0.780     0.651    109\n",
      "    fear     0.646     0.577     0.512    80\n",
      "   happy     0.646     0.571     0.691    81\n",
      " neutral     0.646     0.718     0.726    84\n",
      "     sad     0.646     0.489     0.759    87\n",
      "surprise     0.646     0.755     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.665     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.139443 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.989217  [ 1200/ 4873]\n",
      "loss: 0.559482  [ 2400/ 4873]\n",
      "loss: 0.921380  [ 3600/ 4873]\n",
      "loss: 0.777640  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.765     0.590    105\n",
      " disgust     0.648     0.882     0.688    109\n",
      "    fear     0.648     0.465     0.588    80\n",
      "   happy     0.648     0.611     0.679    81\n",
      " neutral     0.648     0.707     0.690    84\n",
      "     sad     0.648     0.518     0.667    87\n",
      "surprise     0.648     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.661     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.174848 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 1.003448  [ 1200/ 4873]\n",
      "loss: 0.928850  [ 2400/ 4873]\n",
      "loss: 0.241065  [ 3600/ 4873]\n",
      "loss: 0.617554  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.677     0.638    105\n",
      " disgust     0.649     0.778     0.642    109\n",
      "    fear     0.649     0.644     0.475    80\n",
      "   happy     0.649     0.585     0.679    81\n",
      " neutral     0.649     0.644     0.774    84\n",
      "     sad     0.649     0.602     0.713    87\n",
      "surprise     0.649     0.609     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.648     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.117757 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.390565  [ 1200/ 4873]\n",
      "loss: 0.555200  [ 2400/ 4873]\n",
      "loss: 1.387991  [ 3600/ 4873]\n",
      "loss: 0.915082  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.705     0.638    105\n",
      " disgust     0.631     0.784     0.697    109\n",
      "    fear     0.631     0.539     0.512    80\n",
      "   happy     0.631     0.604     0.716    81\n",
      " neutral     0.631     0.618     0.750    84\n",
      "     sad     0.631     0.509     0.655    87\n",
      "surprise     0.631     0.719     0.359    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.640     0.618    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.172314 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.876978  [ 1200/ 4873]\n",
      "loss: 1.193498  [ 2400/ 4873]\n",
      "loss: 0.444364  [ 3600/ 4873]\n",
      "loss: 1.274341  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.645     0.657    105\n",
      " disgust     0.649     0.710     0.697    109\n",
      "    fear     0.649     0.585     0.475    80\n",
      "   happy     0.649     0.707     0.654    81\n",
      " neutral     0.649     0.693     0.726    84\n",
      "     sad     0.649     0.496     0.667    87\n",
      "surprise     0.649     0.804     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.663     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.170646 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 1.182082  [ 1200/ 4873]\n",
      "loss: 0.706901  [ 2400/ 4873]\n",
      "loss: 0.780908  [ 3600/ 4873]\n",
      "loss: 0.995097  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.753     0.610    105\n",
      " disgust     0.649     0.818     0.661    109\n",
      "    fear     0.649     0.597     0.463    80\n",
      "   happy     0.649     0.558     0.654    81\n",
      " neutral     0.649     0.667     0.786    84\n",
      "     sad     0.649     0.516     0.747    87\n",
      "surprise     0.649     0.709     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.660     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.105979 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.853516  [ 1200/ 4873]\n",
      "loss: 0.574941  [ 2400/ 4873]\n",
      "loss: 0.447344  [ 3600/ 4873]\n",
      "loss: 0.668252  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.695     0.629    105\n",
      " disgust     0.666     0.815     0.688    109\n",
      "    fear     0.666     0.568     0.525    80\n",
      "   happy     0.666     0.667     0.716    81\n",
      " neutral     0.666     0.769     0.714    84\n",
      "     sad     0.666     0.504     0.759    87\n",
      "surprise     0.666     0.736     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.679     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.052788 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.946088  [ 1200/ 4873]\n",
      "loss: 0.959406  [ 2400/ 4873]\n",
      "loss: 0.987651  [ 3600/ 4873]\n",
      "loss: 0.921903  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.708     0.648    105\n",
      " disgust     0.651     0.800     0.661    109\n",
      "    fear     0.651     0.582     0.487    80\n",
      "   happy     0.651     0.544     0.691    81\n",
      " neutral     0.651     0.694     0.702    84\n",
      "     sad     0.651     0.549     0.713    87\n",
      "surprise     0.651     0.732     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.658     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.102123 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.832071  [ 1200/ 4873]\n",
      "loss: 0.938642  [ 2400/ 4873]\n",
      "loss: 0.408049  [ 3600/ 4873]\n",
      "loss: 0.889535  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.747     0.648    105\n",
      " disgust     0.666     0.755     0.706    109\n",
      "    fear     0.666     0.517     0.562    80\n",
      "   happy     0.666     0.620     0.704    81\n",
      " neutral     0.666     0.847     0.726    84\n",
      "     sad     0.666     0.589     0.644    87\n",
      "surprise     0.666     0.592     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.667     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.127763 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 1.330199  [ 1200/ 4873]\n",
      "loss: 0.862769  [ 2400/ 4873]\n",
      "loss: 0.897399  [ 3600/ 4873]\n",
      "loss: 0.558858  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.750     0.629    105\n",
      " disgust     0.659     0.828     0.706    109\n",
      "    fear     0.659     0.533     0.500    80\n",
      "   happy     0.659     0.589     0.654    81\n",
      " neutral     0.659     0.670     0.750    84\n",
      "     sad     0.659     0.534     0.724    87\n",
      "surprise     0.659     0.769     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.668     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.105145 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.372913  [ 1200/ 4873]\n",
      "loss: 1.106475  [ 2400/ 4873]\n",
      "loss: 0.572974  [ 3600/ 4873]\n",
      "loss: 0.635012  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.693     0.667    105\n",
      " disgust     0.662     0.758     0.688    109\n",
      "    fear     0.662     0.597     0.500    80\n",
      "   happy     0.662     0.566     0.741    81\n",
      " neutral     0.662     0.652     0.714    84\n",
      "     sad     0.662     0.637     0.667    87\n",
      "surprise     0.662     0.759     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.666     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.104781 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 1.192751  [ 1200/ 4873]\n",
      "loss: 0.580336  [ 2400/ 4873]\n",
      "loss: 0.690169  [ 3600/ 4873]\n",
      "loss: 0.901062  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.630     0.738     0.590    105\n",
      " disgust     0.630     0.784     0.633    109\n",
      "    fear     0.630     0.452     0.588    80\n",
      "   happy     0.630     0.513     0.753    81\n",
      " neutral     0.630     0.770     0.679    84\n",
      "     sad     0.630     0.578     0.598    87\n",
      "surprise     0.630     0.706     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.630     0.649     0.629    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.255854 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.755404  [ 1200/ 4873]\n",
      "loss: 0.685008  [ 2400/ 4873]\n",
      "loss: 0.987784  [ 3600/ 4873]\n",
      "loss: 0.947684  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.681     0.610    105\n",
      " disgust     0.677     0.780     0.716    109\n",
      "    fear     0.677     0.566     0.537    80\n",
      "   happy     0.677     0.617     0.716    81\n",
      " neutral     0.677     0.840     0.750    84\n",
      "     sad     0.677     0.586     0.747    87\n",
      "surprise     0.677     0.700     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.681     0.676    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.044231 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep250_acc_68.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep233_acc_67\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep250_acc_68\"! Old accuracy: 66.9, new accuracy: 67.7\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.756975  [ 1200/ 4873]\n",
      "loss: 0.353045  [ 2400/ 4873]\n",
      "loss: 0.363244  [ 3600/ 4873]\n",
      "loss: 1.696292  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.731     0.648    105\n",
      " disgust     0.649     0.664     0.761    109\n",
      "    fear     0.649     0.583     0.525    80\n",
      "   happy     0.649     0.658     0.642    81\n",
      " neutral     0.649     0.784     0.690    84\n",
      "     sad     0.649     0.500     0.747    87\n",
      "surprise     0.649     0.757     0.438    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.668     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.142801 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.806162  [ 1200/ 4873]\n",
      "loss: 1.238464  [ 2400/ 4873]\n",
      "loss: 1.762331  [ 3600/ 4873]\n",
      "loss: 1.714402  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.744     0.581    105\n",
      " disgust     0.648     0.784     0.734    109\n",
      "    fear     0.648     0.452     0.525    80\n",
      "   happy     0.648     0.544     0.691    81\n",
      " neutral     0.648     0.845     0.714    84\n",
      "     sad     0.648     0.540     0.701    87\n",
      "surprise     0.648     0.761     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.667     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.142054 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 1.023712  [ 1200/ 4873]\n",
      "loss: 0.474426  [ 2400/ 4873]\n",
      "loss: 1.348903  [ 3600/ 4873]\n",
      "loss: 0.852333  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.750     0.571    105\n",
      " disgust     0.651     0.787     0.642    109\n",
      "    fear     0.651     0.554     0.575    80\n",
      "   happy     0.651     0.576     0.654    81\n",
      " neutral     0.651     0.733     0.750    84\n",
      "     sad     0.651     0.553     0.724    87\n",
      "surprise     0.651     0.636     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.655     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.079473 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.325629  [ 1200/ 4873]\n",
      "loss: 1.091209  [ 2400/ 4873]\n",
      "loss: 1.355304  [ 3600/ 4873]\n",
      "loss: 0.888637  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.716     0.648    105\n",
      " disgust     0.659     0.779     0.679    109\n",
      "    fear     0.659     0.592     0.525    80\n",
      "   happy     0.659     0.633     0.704    81\n",
      " neutral     0.659     0.699     0.690    84\n",
      "     sad     0.659     0.555     0.701    87\n",
      "surprise     0.659     0.636     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.658     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.112020 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.537629  [ 1200/ 4873]\n",
      "loss: 0.629354  [ 2400/ 4873]\n",
      "loss: 0.710853  [ 3600/ 4873]\n",
      "loss: 0.795964  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.750     0.571    105\n",
      " disgust     0.638     0.698     0.743    109\n",
      "    fear     0.638     0.500     0.537    80\n",
      "   happy     0.638     0.557     0.667    81\n",
      " neutral     0.638     0.757     0.667    84\n",
      "     sad     0.638     0.558     0.667    87\n",
      "surprise     0.638     0.698     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.645     0.633    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.200628 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.731512  [ 1200/ 4873]\n",
      "loss: 1.071909  [ 2400/ 4873]\n",
      "loss: 0.472960  [ 3600/ 4873]\n",
      "loss: 0.587006  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.645     0.676    105\n",
      " disgust     0.661     0.800     0.697    109\n",
      "    fear     0.661     0.600     0.487    80\n",
      "   happy     0.661     0.697     0.654    81\n",
      " neutral     0.661     0.769     0.714    84\n",
      "     sad     0.661     0.471     0.759    87\n",
      "surprise     0.661     0.826     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.687     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.113160 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 1.313991  [ 1200/ 4873]\n",
      "loss: 0.861607  [ 2400/ 4873]\n",
      "loss: 0.859446  [ 3600/ 4873]\n",
      "loss: 0.582569  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.708     0.648    105\n",
      " disgust     0.644     0.727     0.661    109\n",
      "    fear     0.644     0.519     0.500    80\n",
      "   happy     0.644     0.553     0.704    81\n",
      " neutral     0.644     0.829     0.690    84\n",
      "     sad     0.644     0.546     0.678    87\n",
      "surprise     0.644     0.684     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.653     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.156689 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 1.247440  [ 1200/ 4873]\n",
      "loss: 0.578783  [ 2400/ 4873]\n",
      "loss: 0.355657  [ 3600/ 4873]\n",
      "loss: 0.855129  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.795     0.590    105\n",
      " disgust     0.644     0.859     0.615    109\n",
      "    fear     0.644     0.636     0.525    80\n",
      "   happy     0.644     0.524     0.679    81\n",
      " neutral     0.644     0.611     0.786    84\n",
      "     sad     0.644     0.553     0.724    87\n",
      "surprise     0.644     0.623     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.657     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.183992 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.724793  [ 1200/ 4873]\n",
      "loss: 0.478044  [ 2400/ 4873]\n",
      "loss: 0.786973  [ 3600/ 4873]\n",
      "loss: 0.856710  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.664     0.676    105\n",
      " disgust     0.661     0.738     0.697    109\n",
      "    fear     0.661     0.603     0.438    80\n",
      "   happy     0.661     0.700     0.691    81\n",
      " neutral     0.661     0.747     0.738    84\n",
      "     sad     0.661     0.548     0.724    87\n",
      "surprise     0.661     0.625     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.661     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.133244 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 1.302464  [ 1200/ 4873]\n",
      "loss: 0.792600  [ 2400/ 4873]\n",
      "loss: 0.752978  [ 3600/ 4873]\n",
      "loss: 0.553633  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.793     0.619    105\n",
      " disgust     0.662     0.717     0.743    109\n",
      "    fear     0.662     0.627     0.525    80\n",
      "   happy     0.662     0.561     0.679    81\n",
      " neutral     0.662     0.759     0.714    84\n",
      "     sad     0.662     0.520     0.736    87\n",
      "surprise     0.662     0.771     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.678     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.102189 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.649671  [ 1200/ 4873]\n",
      "loss: 1.461438  [ 2400/ 4873]\n",
      "loss: 0.800419  [ 3600/ 4873]\n",
      "loss: 0.784217  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.592     0.676    105\n",
      " disgust     0.636     0.692     0.679    109\n",
      "    fear     0.636     0.661     0.487    80\n",
      "   happy     0.636     0.634     0.642    81\n",
      " neutral     0.636     0.729     0.738    84\n",
      "     sad     0.636     0.518     0.667    87\n",
      "surprise     0.636     0.711     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.648     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.127779 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.899897  [ 1200/ 4873]\n",
      "loss: 1.218680  [ 2400/ 4873]\n",
      "loss: 0.988481  [ 3600/ 4873]\n",
      "loss: 0.515516  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.698     0.638    105\n",
      " disgust     0.648     0.755     0.679    109\n",
      "    fear     0.648     0.607     0.463    80\n",
      "   happy     0.648     0.514     0.691    81\n",
      " neutral     0.648     0.721     0.738    84\n",
      "     sad     0.648     0.584     0.678    87\n",
      "surprise     0.648     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.651     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.140730 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.895038  [ 1200/ 4873]\n",
      "loss: 0.833073  [ 2400/ 4873]\n",
      "loss: 0.859598  [ 3600/ 4873]\n",
      "loss: 0.875644  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.816     0.590    105\n",
      " disgust     0.636     0.826     0.651    109\n",
      "    fear     0.636     0.516     0.600    80\n",
      "   happy     0.636     0.456     0.704    81\n",
      " neutral     0.636     0.685     0.750    84\n",
      "     sad     0.636     0.582     0.655    87\n",
      "surprise     0.636     0.750     0.469    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.661     0.631    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.135733 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.698027  [ 1200/ 4873]\n",
      "loss: 0.399560  [ 2400/ 4873]\n",
      "loss: 0.986053  [ 3600/ 4873]\n",
      "loss: 0.725289  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.608     0.695    105\n",
      " disgust     0.643     0.717     0.651    109\n",
      "    fear     0.643     0.711     0.400    80\n",
      "   happy     0.643     0.684     0.642    81\n",
      " neutral     0.643     0.623     0.786    84\n",
      "     sad     0.643     0.523     0.667    87\n",
      "surprise     0.643     0.755     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.660     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.136454 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.419096  [ 1200/ 4873]\n",
      "loss: 0.613488  [ 2400/ 4873]\n",
      "loss: 0.990548  [ 3600/ 4873]\n",
      "loss: 1.926436  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.681     0.590    105\n",
      " disgust     0.648     0.773     0.688    109\n",
      "    fear     0.648     0.587     0.463    80\n",
      "   happy     0.648     0.557     0.728    81\n",
      " neutral     0.648     0.709     0.726    84\n",
      "     sad     0.648     0.540     0.701    87\n",
      "surprise     0.648     0.741     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.655     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.108893 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.729985  [ 1200/ 4873]\n",
      "loss: 0.615122  [ 2400/ 4873]\n",
      "loss: 0.542776  [ 3600/ 4873]\n",
      "loss: 0.561013  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.684     0.638    105\n",
      " disgust     0.667     0.811     0.706    109\n",
      "    fear     0.667     0.564     0.550    80\n",
      "   happy     0.667     0.621     0.667    81\n",
      " neutral     0.667     0.768     0.750    84\n",
      "     sad     0.667     0.555     0.701    87\n",
      "surprise     0.667     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.669     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.070605 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.469869  [ 1200/ 4873]\n",
      "loss: 0.528485  [ 2400/ 4873]\n",
      "loss: 0.882362  [ 3600/ 4873]\n",
      "loss: 0.999775  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.699     0.686    105\n",
      " disgust     0.666     0.798     0.688    109\n",
      "    fear     0.666     0.644     0.475    80\n",
      "   happy     0.666     0.581     0.667    81\n",
      " neutral     0.666     0.663     0.774    84\n",
      "     sad     0.666     0.588     0.690    87\n",
      "surprise     0.666     0.689     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.666     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.075065 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.938533  [ 1200/ 4873]\n",
      "loss: 1.058480  [ 2400/ 4873]\n",
      "loss: 0.725401  [ 3600/ 4873]\n",
      "loss: 0.583825  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.711     0.610    105\n",
      " disgust     0.657     0.765     0.688    109\n",
      "    fear     0.657     0.547     0.512    80\n",
      "   happy     0.657     0.509     0.728    81\n",
      " neutral     0.657     0.818     0.750    84\n",
      "     sad     0.657     0.629     0.701    87\n",
      "surprise     0.657     0.667     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.664     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.104904 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.934762  [ 1200/ 4873]\n",
      "loss: 0.471008  [ 2400/ 4873]\n",
      "loss: 0.896090  [ 3600/ 4873]\n",
      "loss: 0.936383  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.711     0.610    105\n",
      " disgust     0.661     0.735     0.688    109\n",
      "    fear     0.661     0.581     0.537    80\n",
      "   happy     0.661     0.576     0.654    81\n",
      " neutral     0.661     0.797     0.750    84\n",
      "     sad     0.661     0.545     0.770    87\n",
      "surprise     0.661     0.760     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.672     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.081737 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 1.031140  [ 1200/ 4873]\n",
      "loss: 0.456173  [ 2400/ 4873]\n",
      "loss: 1.232859  [ 3600/ 4873]\n",
      "loss: 0.558771  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.693     0.581    105\n",
      " disgust     0.651     0.783     0.661    109\n",
      "    fear     0.651     0.551     0.475    80\n",
      "   happy     0.651     0.629     0.691    81\n",
      " neutral     0.651     0.640     0.762    84\n",
      "     sad     0.651     0.559     0.713    87\n",
      "surprise     0.651     0.721     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.654     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.157758 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.931417  [ 1200/ 4873]\n",
      "loss: 0.546688  [ 2400/ 4873]\n",
      "loss: 0.739514  [ 3600/ 4873]\n",
      "loss: 0.919944  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.703     0.610    105\n",
      " disgust     0.639     0.772     0.651    109\n",
      "    fear     0.639     0.434     0.537    80\n",
      "   happy     0.639     0.515     0.654    81\n",
      " neutral     0.639     0.836     0.726    84\n",
      "     sad     0.639     0.622     0.701    87\n",
      "surprise     0.639     0.685     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.652     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.216630 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.601609  [ 1200/ 4873]\n",
      "loss: 0.983130  [ 2400/ 4873]\n",
      "loss: 0.466203  [ 3600/ 4873]\n",
      "loss: 0.898291  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.636     0.667    105\n",
      " disgust     0.652     0.761     0.642    109\n",
      "    fear     0.652     0.567     0.475    80\n",
      "   happy     0.652     0.639     0.654    81\n",
      " neutral     0.652     0.729     0.738    84\n",
      "     sad     0.652     0.578     0.724    87\n",
      "surprise     0.652     0.656     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.652     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.144991 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 1.186924  [ 1200/ 4873]\n",
      "loss: 0.495310  [ 2400/ 4873]\n",
      "loss: 0.893781  [ 3600/ 4873]\n",
      "loss: 0.157986  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.690     0.657    105\n",
      " disgust     0.651     0.644     0.697    109\n",
      "    fear     0.651     0.524     0.550    80\n",
      "   happy     0.651     0.576     0.654    81\n",
      " neutral     0.651     0.857     0.714    84\n",
      "     sad     0.651     0.598     0.701    87\n",
      "surprise     0.651     0.773     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.666     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.176859 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.993385  [ 1200/ 4873]\n",
      "loss: 0.933476  [ 2400/ 4873]\n",
      "loss: 0.767774  [ 3600/ 4873]\n",
      "loss: 0.819109  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.630     0.726     0.657    105\n",
      " disgust     0.630     0.752     0.697    109\n",
      "    fear     0.630     0.535     0.475    80\n",
      "   happy     0.630     0.491     0.654    81\n",
      " neutral     0.630     0.767     0.667    84\n",
      "     sad     0.630     0.518     0.655    87\n",
      "surprise     0.630     0.673     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.630     0.638     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.146107 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.817745  [ 1200/ 4873]\n",
      "loss: 0.328156  [ 2400/ 4873]\n",
      "loss: 1.393669  [ 3600/ 4873]\n",
      "loss: 0.543641  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.667     0.629    105\n",
      " disgust     0.641     0.750     0.688    109\n",
      "    fear     0.641     0.529     0.450    80\n",
      "   happy     0.641     0.658     0.642    81\n",
      " neutral     0.641     0.689     0.738    84\n",
      "     sad     0.641     0.504     0.724    87\n",
      "surprise     0.641     0.755     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.650     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.125749 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.718310  [ 1200/ 4873]\n",
      "loss: 0.184321  [ 2400/ 4873]\n",
      "loss: 1.319263  [ 3600/ 4873]\n",
      "loss: 0.463424  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.785     0.590    105\n",
      " disgust     0.667     0.726     0.752    109\n",
      "    fear     0.667     0.567     0.475    80\n",
      "   happy     0.667     0.578     0.778    81\n",
      " neutral     0.667     0.881     0.702    84\n",
      "     sad     0.667     0.538     0.736    87\n",
      "surprise     0.667     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.681     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.116531 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.761833  [ 1200/ 4873]\n",
      "loss: 0.552667  [ 2400/ 4873]\n",
      "loss: 0.465463  [ 3600/ 4873]\n",
      "loss: 0.929281  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.775     0.590    105\n",
      " disgust     0.656     0.675     0.725    109\n",
      "    fear     0.656     0.560     0.525    80\n",
      "   happy     0.656     0.566     0.691    81\n",
      " neutral     0.656     0.843     0.702    84\n",
      "     sad     0.656     0.561     0.690    87\n",
      "surprise     0.656     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.665     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.132515 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.657049  [ 1200/ 4873]\n",
      "loss: 0.671255  [ 2400/ 4873]\n",
      "loss: 0.610396  [ 3600/ 4873]\n",
      "loss: 1.450563  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.656     0.600    105\n",
      " disgust     0.639     0.708     0.688    109\n",
      "    fear     0.639     0.519     0.512    80\n",
      "   happy     0.639     0.581     0.667    81\n",
      " neutral     0.639     0.667     0.738    84\n",
      "     sad     0.639     0.615     0.644    87\n",
      "surprise     0.639     0.750     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.642     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.146541 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.688518  [ 1200/ 4873]\n",
      "loss: 0.275113  [ 2400/ 4873]\n",
      "loss: 0.575069  [ 3600/ 4873]\n",
      "loss: 0.588189  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.789     0.571    105\n",
      " disgust     0.643     0.802     0.596    109\n",
      "    fear     0.643     0.480     0.588    80\n",
      "   happy     0.643     0.528     0.704    81\n",
      " neutral     0.643     0.700     0.750    84\n",
      "     sad     0.643     0.608     0.678    87\n",
      "surprise     0.643     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.656     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.200102 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.416057  [ 1200/ 4873]\n",
      "loss: 1.416355  [ 2400/ 4873]\n",
      "loss: 0.718399  [ 3600/ 4873]\n",
      "loss: 0.635224  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.782     0.581    105\n",
      " disgust     0.646     0.789     0.688    109\n",
      "    fear     0.646     0.535     0.475    80\n",
      "   happy     0.646     0.547     0.716    81\n",
      " neutral     0.646     0.727     0.762    84\n",
      "     sad     0.646     0.517     0.713    87\n",
      "surprise     0.646     0.692     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.656     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.105042 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 1.306112  [ 1200/ 4873]\n",
      "loss: 0.951504  [ 2400/ 4873]\n",
      "loss: 0.824278  [ 3600/ 4873]\n",
      "loss: 0.677387  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.764     0.648    105\n",
      " disgust     0.674     0.748     0.706    109\n",
      "    fear     0.674     0.569     0.512    80\n",
      "   happy     0.674     0.578     0.778    81\n",
      " neutral     0.674     0.772     0.726    84\n",
      "     sad     0.674     0.578     0.724    87\n",
      "surprise     0.674     0.776     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.684     0.670    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.109263 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.806424  [ 1200/ 4873]\n",
      "loss: 0.462660  [ 2400/ 4873]\n",
      "loss: 0.827088  [ 3600/ 4873]\n",
      "loss: 0.748630  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.701     0.648    105\n",
      " disgust     0.677     0.736     0.716    109\n",
      "    fear     0.677     0.603     0.512    80\n",
      "   happy     0.677     0.592     0.753    81\n",
      " neutral     0.677     0.829     0.750    84\n",
      "     sad     0.677     0.596     0.713    87\n",
      "surprise     0.677     0.714     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.682     0.674    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.096700 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.619770  [ 1200/ 4873]\n",
      "loss: 0.897647  [ 2400/ 4873]\n",
      "loss: 0.850549  [ 3600/ 4873]\n",
      "loss: 0.620501  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.793     0.619    105\n",
      " disgust     0.661     0.714     0.688    109\n",
      "    fear     0.661     0.618     0.525    80\n",
      "   happy     0.661     0.573     0.728    81\n",
      " neutral     0.661     0.773     0.690    84\n",
      "     sad     0.661     0.589     0.724    87\n",
      "surprise     0.661     0.586     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.664     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.117746 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.664865  [ 1200/ 4873]\n",
      "loss: 0.797058  [ 2400/ 4873]\n",
      "loss: 1.567813  [ 3600/ 4873]\n",
      "loss: 0.882257  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.698     0.705    105\n",
      " disgust     0.649     0.742     0.661    109\n",
      "    fear     0.649     0.574     0.438    80\n",
      "   happy     0.649     0.585     0.593    81\n",
      " neutral     0.649     0.708     0.750    84\n",
      "     sad     0.649     0.560     0.747    87\n",
      "surprise     0.649     0.661     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.647     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.085320 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.506776  [ 1200/ 4873]\n",
      "loss: 1.437301  [ 2400/ 4873]\n",
      "loss: 0.441578  [ 3600/ 4873]\n",
      "loss: 0.594495  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.636     0.714    105\n",
      " disgust     0.664     0.683     0.651    109\n",
      "    fear     0.664     0.698     0.463    80\n",
      "   happy     0.664     0.624     0.716    81\n",
      " neutral     0.664     0.692     0.750    84\n",
      "     sad     0.664     0.632     0.690    87\n",
      "surprise     0.664     0.732     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.671     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.155094 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.484989  [ 1200/ 4873]\n",
      "loss: 0.880277  [ 2400/ 4873]\n",
      "loss: 0.524051  [ 3600/ 4873]\n",
      "loss: 1.106963  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.725     0.629    105\n",
      " disgust     0.643     0.758     0.633    109\n",
      "    fear     0.643     0.578     0.463    80\n",
      "   happy     0.643     0.570     0.704    81\n",
      " neutral     0.643     0.667     0.738    84\n",
      "     sad     0.643     0.549     0.713    87\n",
      "surprise     0.643     0.672     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.646     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.140811 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.764390  [ 1200/ 4873]\n",
      "loss: 0.809098  [ 2400/ 4873]\n",
      "loss: 0.795977  [ 3600/ 4873]\n",
      "loss: 1.205653  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.765     0.619    105\n",
      " disgust     0.661     0.692     0.743    109\n",
      "    fear     0.661     0.553     0.525    80\n",
      "   happy     0.661     0.587     0.667    81\n",
      " neutral     0.661     0.759     0.714    84\n",
      "     sad     0.661     0.596     0.678    87\n",
      "surprise     0.661     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.661     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.106801 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.472067  [ 1200/ 4873]\n",
      "loss: 0.449337  [ 2400/ 4873]\n",
      "loss: 0.946533  [ 3600/ 4873]\n",
      "loss: 0.374642  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.768     0.600    105\n",
      " disgust     0.656     0.800     0.624    109\n",
      "    fear     0.656     0.517     0.562    80\n",
      "   happy     0.656     0.520     0.815    81\n",
      " neutral     0.656     0.805     0.738    84\n",
      "     sad     0.656     0.648     0.678    87\n",
      "surprise     0.656     0.607     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.666     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.134957 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 1.156333  [ 1200/ 4873]\n",
      "loss: 0.861129  [ 2400/ 4873]\n",
      "loss: 0.497086  [ 3600/ 4873]\n",
      "loss: 0.724492  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.736     0.610    105\n",
      " disgust     0.657     0.755     0.734    109\n",
      "    fear     0.657     0.526     0.512    80\n",
      "   happy     0.657     0.578     0.728    81\n",
      " neutral     0.657     0.803     0.726    84\n",
      "     sad     0.657     0.554     0.713    87\n",
      "surprise     0.657     0.694     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.664     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.105754 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 1.180686  [ 1200/ 4873]\n",
      "loss: 0.867953  [ 2400/ 4873]\n",
      "loss: 0.541691  [ 3600/ 4873]\n",
      "loss: 0.531887  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.729     0.667    105\n",
      " disgust     0.677     0.747     0.679    109\n",
      "    fear     0.677     0.506     0.562    80\n",
      "   happy     0.677     0.653     0.790    81\n",
      " neutral     0.677     0.882     0.714    84\n",
      "     sad     0.677     0.562     0.678    87\n",
      "surprise     0.677     0.745     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.689     0.676    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.072827 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.688132  [ 1200/ 4873]\n",
      "loss: 0.219168  [ 2400/ 4873]\n",
      "loss: 0.620721  [ 3600/ 4873]\n",
      "loss: 1.168359  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.744     0.638    105\n",
      " disgust     0.652     0.795     0.642    109\n",
      "    fear     0.652     0.494     0.512    80\n",
      "   happy     0.652     0.554     0.691    81\n",
      " neutral     0.652     0.750     0.750    84\n",
      "     sad     0.652     0.577     0.690    87\n",
      "surprise     0.652     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.657     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.105857 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.696162  [ 1200/ 4873]\n",
      "loss: 1.008079  [ 2400/ 4873]\n",
      "loss: 0.905530  [ 3600/ 4873]\n",
      "loss: 0.526676  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.694     0.648    105\n",
      " disgust     0.648     0.766     0.661    109\n",
      "    fear     0.648     0.474     0.450    80\n",
      "   happy     0.648     0.604     0.753    81\n",
      " neutral     0.648     0.685     0.726    84\n",
      "     sad     0.648     0.622     0.586    87\n",
      "surprise     0.648     0.657     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.643     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.145675 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 1.139726  [ 1200/ 4873]\n",
      "loss: 0.579767  [ 2400/ 4873]\n",
      "loss: 0.708649  [ 3600/ 4873]\n",
      "loss: 0.582051  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.707     0.619    105\n",
      " disgust     0.659     0.822     0.679    109\n",
      "    fear     0.659     0.588     0.500    80\n",
      "   happy     0.659     0.536     0.741    81\n",
      " neutral     0.659     0.663     0.774    84\n",
      "     sad     0.659     0.647     0.632    87\n",
      "surprise     0.659     0.662     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.661     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.137291 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.647235  [ 1200/ 4873]\n",
      "loss: 1.079704  [ 2400/ 4873]\n",
      "loss: 0.630959  [ 3600/ 4873]\n",
      "loss: 0.450023  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.694     0.648    105\n",
      " disgust     0.652     0.870     0.615    109\n",
      "    fear     0.652     0.573     0.537    80\n",
      "   happy     0.652     0.536     0.827    81\n",
      " neutral     0.652     0.721     0.738    84\n",
      "     sad     0.652     0.577     0.644    87\n",
      "surprise     0.652     0.673     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.664     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.138204 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.517516  [ 1200/ 4873]\n",
      "loss: 0.941454  [ 2400/ 4873]\n",
      "loss: 0.761304  [ 3600/ 4873]\n",
      "loss: 0.830921  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.791     0.648    105\n",
      " disgust     0.656     0.757     0.716    109\n",
      "    fear     0.656     0.562     0.512    80\n",
      "   happy     0.656     0.604     0.679    81\n",
      " neutral     0.656     0.612     0.750    84\n",
      "     sad     0.656     0.542     0.667    87\n",
      "surprise     0.656     0.787     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.665     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.123825 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 1.050135  [ 1200/ 4873]\n",
      "loss: 1.172605  [ 2400/ 4873]\n",
      "loss: 0.358902  [ 3600/ 4873]\n",
      "loss: 0.324230  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.707     0.619    105\n",
      " disgust     0.651     0.763     0.679    109\n",
      "    fear     0.651     0.645     0.500    80\n",
      "   happy     0.651     0.674     0.716    81\n",
      " neutral     0.651     0.579     0.738    84\n",
      "     sad     0.651     0.639     0.609    87\n",
      "surprise     0.651     0.542     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.650     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.221077 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.563662  [ 1200/ 4873]\n",
      "loss: 0.787549  [ 2400/ 4873]\n",
      "loss: 0.530671  [ 3600/ 4873]\n",
      "loss: 1.183099  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.759     0.629    105\n",
      " disgust     0.666     0.765     0.716    109\n",
      "    fear     0.666     0.511     0.588    80\n",
      "   happy     0.666     0.589     0.691    81\n",
      " neutral     0.666     0.744     0.726    84\n",
      "     sad     0.666     0.606     0.724    87\n",
      "surprise     0.666     0.729     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.672     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.123732 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.933465  [ 1200/ 4873]\n",
      "loss: 0.256630  [ 2400/ 4873]\n",
      "loss: 0.559099  [ 3600/ 4873]\n",
      "loss: 0.774087  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.825     0.629    105\n",
      " disgust     0.652     0.798     0.615    109\n",
      "    fear     0.652     0.506     0.562    80\n",
      "   happy     0.652     0.569     0.716    81\n",
      " neutral     0.652     0.792     0.726    84\n",
      "     sad     0.652     0.525     0.724    87\n",
      "surprise     0.652     0.655     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.667     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.131555 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.446055  [ 1200/ 4873]\n",
      "loss: 0.981469  [ 2400/ 4873]\n",
      "loss: 0.513862  [ 3600/ 4873]\n",
      "loss: 0.542494  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.722     0.667    105\n",
      " disgust     0.667     0.675     0.761    109\n",
      "    fear     0.667     0.647     0.550    80\n",
      "   happy     0.667     0.667     0.593    81\n",
      " neutral     0.667     0.790     0.762    84\n",
      "     sad     0.667     0.517     0.713    87\n",
      "surprise     0.667     0.735     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.679     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.162284 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.470468  [ 1200/ 4873]\n",
      "loss: 0.930721  [ 2400/ 4873]\n",
      "loss: 1.404261  [ 3600/ 4873]\n",
      "loss: 0.286253  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.738     0.562    105\n",
      " disgust     0.654     0.755     0.679    109\n",
      "    fear     0.654     0.540     0.588    80\n",
      "   happy     0.654     0.607     0.667    81\n",
      " neutral     0.654     0.733     0.750    84\n",
      "     sad     0.654     0.575     0.701    87\n",
      "surprise     0.654     0.641     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.655     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.131973 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.611767  [ 1200/ 4873]\n",
      "loss: 0.458674  [ 2400/ 4873]\n",
      "loss: 0.768471  [ 3600/ 4873]\n",
      "loss: 0.545022  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.721     0.590    105\n",
      " disgust     0.656     0.679     0.661    109\n",
      "    fear     0.656     0.612     0.512    80\n",
      "   happy     0.656     0.652     0.741    81\n",
      " neutral     0.656     0.691     0.774    84\n",
      "     sad     0.656     0.628     0.678    87\n",
      "surprise     0.656     0.577     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.652     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.210949 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.592234  [ 1200/ 4873]\n",
      "loss: 0.623335  [ 2400/ 4873]\n",
      "loss: 1.158659  [ 3600/ 4873]\n",
      "loss: 0.960174  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.775     0.657    105\n",
      " disgust     0.657     0.738     0.725    109\n",
      "    fear     0.657     0.519     0.512    80\n",
      "   happy     0.657     0.559     0.704    81\n",
      " neutral     0.657     0.811     0.714    84\n",
      "     sad     0.657     0.550     0.701    87\n",
      "surprise     0.657     0.708     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.666     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.131285 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.776467  [ 1200/ 4873]\n",
      "loss: 0.926195  [ 2400/ 4873]\n",
      "loss: 0.449863  [ 3600/ 4873]\n",
      "loss: 0.894864  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.745     0.667    105\n",
      " disgust     0.661     0.736     0.743    109\n",
      "    fear     0.661     0.494     0.500    80\n",
      "   happy     0.661     0.626     0.704    81\n",
      " neutral     0.661     0.812     0.667    84\n",
      "     sad     0.661     0.538     0.724    87\n",
      "surprise     0.661     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.672     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.155980 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.533530  [ 1200/ 4873]\n",
      "loss: 0.663142  [ 2400/ 4873]\n",
      "loss: 0.826000  [ 3600/ 4873]\n",
      "loss: 0.401849  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.756     0.619    105\n",
      " disgust     0.654     0.735     0.688    109\n",
      "    fear     0.654     0.594     0.512    80\n",
      "   happy     0.654     0.611     0.716    81\n",
      " neutral     0.654     0.700     0.750    84\n",
      "     sad     0.654     0.545     0.632    87\n",
      "surprise     0.654     0.627     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.652     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.128765 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.630784  [ 1200/ 4873]\n",
      "loss: 1.236393  [ 2400/ 4873]\n",
      "loss: 1.205104  [ 3600/ 4873]\n",
      "loss: 0.507382  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.739     0.648    105\n",
      " disgust     0.659     0.709     0.716    109\n",
      "    fear     0.659     0.642     0.537    80\n",
      "   happy     0.659     0.602     0.654    81\n",
      " neutral     0.659     0.677     0.750    84\n",
      "     sad     0.659     0.526     0.701    87\n",
      "surprise     0.659     0.818     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.673     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.098785 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.345334  [ 1200/ 4873]\n",
      "loss: 1.043139  [ 2400/ 4873]\n",
      "loss: 0.455052  [ 3600/ 4873]\n",
      "loss: 1.193800  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.683     0.657    105\n",
      " disgust     0.657     0.734     0.633    109\n",
      "    fear     0.657     0.566     0.537    80\n",
      "   happy     0.657     0.558     0.778    81\n",
      " neutral     0.657     0.738     0.738    84\n",
      "     sad     0.657     0.655     0.632    87\n",
      "surprise     0.657     0.690     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.660     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.174931 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.749236  [ 1200/ 4873]\n",
      "loss: 1.122831  [ 2400/ 4873]\n",
      "loss: 0.350122  [ 3600/ 4873]\n",
      "loss: 0.732738  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.778     0.600    105\n",
      " disgust     0.651     0.721     0.688    109\n",
      "    fear     0.651     0.561     0.575    80\n",
      "   happy     0.651     0.561     0.679    81\n",
      " neutral     0.651     0.735     0.726    84\n",
      "     sad     0.651     0.547     0.667    87\n",
      "surprise     0.651     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.657     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.169559 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.505294  [ 1200/ 4873]\n",
      "loss: 0.705763  [ 2400/ 4873]\n",
      "loss: 0.760122  [ 3600/ 4873]\n",
      "loss: 0.631670  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.733     0.600    105\n",
      " disgust     0.641     0.721     0.688    109\n",
      "    fear     0.641     0.585     0.475    80\n",
      "   happy     0.641     0.562     0.667    81\n",
      " neutral     0.641     0.649     0.750    84\n",
      "     sad     0.641     0.550     0.701    87\n",
      "surprise     0.641     0.725     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.646     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.145905 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.690799  [ 1200/ 4873]\n",
      "loss: 0.830756  [ 2400/ 4873]\n",
      "loss: 0.737886  [ 3600/ 4873]\n",
      "loss: 0.559442  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.694     0.648    105\n",
      " disgust     0.666     0.708     0.624    109\n",
      "    fear     0.666     0.689     0.525    80\n",
      "   happy     0.666     0.621     0.728    81\n",
      " neutral     0.666     0.714     0.774    84\n",
      "     sad     0.666     0.594     0.724    87\n",
      "surprise     0.666     0.651     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.667     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.134920 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.361710  [ 1200/ 4873]\n",
      "loss: 0.865060  [ 2400/ 4873]\n",
      "loss: 0.569329  [ 3600/ 4873]\n",
      "loss: 0.459795  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.663     0.619    105\n",
      " disgust     0.638     0.696     0.651    109\n",
      "    fear     0.638     0.550     0.550    80\n",
      "   happy     0.638     0.652     0.716    81\n",
      " neutral     0.638     0.740     0.679    84\n",
      "     sad     0.638     0.513     0.690    87\n",
      "surprise     0.638     0.723     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.648     0.634    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.202824 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 1.083123  [ 1200/ 4873]\n",
      "loss: 0.314707  [ 2400/ 4873]\n",
      "loss: 0.788987  [ 3600/ 4873]\n",
      "loss: 0.490407  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.628     0.741     0.600    105\n",
      " disgust     0.628     0.681     0.706    109\n",
      "    fear     0.628     0.531     0.537    80\n",
      "   happy     0.628     0.539     0.593    81\n",
      " neutral     0.628     0.649     0.750    84\n",
      "     sad     0.628     0.580     0.586    87\n",
      "surprise     0.628     0.667     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.628     0.627     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.173164 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.843085  [ 1200/ 4873]\n",
      "loss: 1.014809  [ 2400/ 4873]\n",
      "loss: 0.795934  [ 3600/ 4873]\n",
      "loss: 0.559895  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.684     0.619    105\n",
      " disgust     0.649     0.735     0.688    109\n",
      "    fear     0.649     0.600     0.525    80\n",
      "   happy     0.649     0.504     0.716    81\n",
      " neutral     0.649     0.841     0.690    84\n",
      "     sad     0.649     0.598     0.701    87\n",
      "surprise     0.649     0.649     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.659     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.116755 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.679980  [ 1200/ 4873]\n",
      "loss: 0.694454  [ 2400/ 4873]\n",
      "loss: 0.498520  [ 3600/ 4873]\n",
      "loss: 1.083038  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.701     0.648    105\n",
      " disgust     0.662     0.758     0.633    109\n",
      "    fear     0.662     0.656     0.500    80\n",
      "   happy     0.662     0.546     0.728    81\n",
      " neutral     0.662     0.724     0.750    84\n",
      "     sad     0.662     0.589     0.724    87\n",
      "surprise     0.662     0.712     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.669     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.101643 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.438457  [ 1200/ 4873]\n",
      "loss: 0.619535  [ 2400/ 4873]\n",
      "loss: 1.966879  [ 3600/ 4873]\n",
      "loss: 0.923847  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.648     0.648    105\n",
      " disgust     0.643     0.675     0.725    109\n",
      "    fear     0.643     0.631     0.512    80\n",
      "   happy     0.643     0.662     0.630    81\n",
      " neutral     0.643     0.685     0.750    84\n",
      "     sad     0.643     0.513     0.667    87\n",
      "surprise     0.643     0.780     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.656     0.633    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.157426 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 1.189779  [ 1200/ 4873]\n",
      "loss: 0.528541  [ 2400/ 4873]\n",
      "loss: 0.751893  [ 3600/ 4873]\n",
      "loss: 1.814806  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.835     0.629    105\n",
      " disgust     0.657     0.721     0.688    109\n",
      "    fear     0.657     0.571     0.550    80\n",
      "   happy     0.657     0.580     0.630    81\n",
      " neutral     0.657     0.733     0.786    84\n",
      "     sad     0.657     0.541     0.690    87\n",
      "surprise     0.657     0.639     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.660     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.132521 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.420090  [ 1200/ 4873]\n",
      "loss: 0.604611  [ 2400/ 4873]\n",
      "loss: 1.349258  [ 3600/ 4873]\n",
      "loss: 0.595712  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.756     0.619    105\n",
      " disgust     0.659     0.726     0.706    109\n",
      "    fear     0.659     0.560     0.588    80\n",
      "   happy     0.659     0.522     0.728    81\n",
      " neutral     0.659     0.819     0.702    84\n",
      "     sad     0.659     0.573     0.724    87\n",
      "surprise     0.659     0.821     0.500    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.682     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.150019 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.544026  [ 1200/ 4873]\n",
      "loss: 0.869449  [ 2400/ 4873]\n",
      "loss: 0.787577  [ 3600/ 4873]\n",
      "loss: 0.721843  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.723     0.648    105\n",
      " disgust     0.672     0.716     0.716    109\n",
      "    fear     0.672     0.633     0.475    80\n",
      "   happy     0.672     0.634     0.728    81\n",
      " neutral     0.672     0.775     0.738    84\n",
      "     sad     0.672     0.565     0.747    87\n",
      "surprise     0.672     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.675     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.121916 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 1.064289  [ 1200/ 4873]\n",
      "loss: 0.515725  [ 2400/ 4873]\n",
      "loss: 0.553899  [ 3600/ 4873]\n",
      "loss: 1.096026  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.697     0.724    105\n",
      " disgust     0.667     0.694     0.688    109\n",
      "    fear     0.667     0.585     0.475    80\n",
      "   happy     0.667     0.648     0.704    81\n",
      " neutral     0.667     0.787     0.750    84\n",
      "     sad     0.667     0.573     0.632    87\n",
      "surprise     0.667     0.672     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.665     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.130003 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.867996  [ 1200/ 4873]\n",
      "loss: 0.756981  [ 2400/ 4873]\n",
      "loss: 0.402612  [ 3600/ 4873]\n",
      "loss: 0.558433  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.702     0.629    105\n",
      " disgust     0.672     0.623     0.743    109\n",
      "    fear     0.672     0.755     0.500    80\n",
      "   happy     0.672     0.707     0.654    81\n",
      " neutral     0.672     0.805     0.738    84\n",
      "     sad     0.672     0.512     0.759    87\n",
      "surprise     0.672     0.808     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.702     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.114254 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.345231  [ 1200/ 4873]\n",
      "loss: 0.589991  [ 2400/ 4873]\n",
      "loss: 0.608610  [ 3600/ 4873]\n",
      "loss: 0.904889  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.783     0.686    105\n",
      " disgust     0.661     0.722     0.716    109\n",
      "    fear     0.661     0.554     0.450    80\n",
      "   happy     0.661     0.541     0.654    81\n",
      " neutral     0.661     0.808     0.750    84\n",
      "     sad     0.661     0.549     0.770    87\n",
      "surprise     0.661     0.723     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.669     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.121893 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.709703  [ 1200/ 4873]\n",
      "loss: 0.580544  [ 2400/ 4873]\n",
      "loss: 0.792020  [ 3600/ 4873]\n",
      "loss: 0.931445  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.736     0.610    105\n",
      " disgust     0.661     0.748     0.734    109\n",
      "    fear     0.661     0.549     0.562    80\n",
      "   happy     0.661     0.553     0.704    81\n",
      " neutral     0.661     0.795     0.738    84\n",
      "     sad     0.661     0.571     0.690    87\n",
      "surprise     0.661     0.729     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.669     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.136576 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.391556  [ 1200/ 4873]\n",
      "loss: 0.783843  [ 2400/ 4873]\n",
      "loss: 1.131533  [ 3600/ 4873]\n",
      "loss: 1.468123  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.783     0.619    105\n",
      " disgust     0.638     0.788     0.716    109\n",
      "    fear     0.638     0.519     0.512    80\n",
      "   happy     0.638     0.496     0.704    81\n",
      " neutral     0.638     0.744     0.726    84\n",
      "     sad     0.638     0.520     0.747    87\n",
      "surprise     0.638     0.815     0.344    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.666     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.136120 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.444853  [ 1200/ 4873]\n",
      "loss: 0.789463  [ 2400/ 4873]\n",
      "loss: 0.502056  [ 3600/ 4873]\n",
      "loss: 0.427475  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.765     0.619    105\n",
      " disgust     0.669     0.717     0.743    109\n",
      "    fear     0.669     0.603     0.475    80\n",
      "   happy     0.669     0.557     0.790    81\n",
      " neutral     0.669     0.824     0.726    84\n",
      "     sad     0.669     0.562     0.724    87\n",
      "surprise     0.669     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.683     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.115170 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 1.173999  [ 1200/ 4873]\n",
      "loss: 0.553041  [ 2400/ 4873]\n",
      "loss: 1.266590  [ 3600/ 4873]\n",
      "loss: 0.483703  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.778     0.600    105\n",
      " disgust     0.670     0.755     0.679    109\n",
      "    fear     0.670     0.603     0.512    80\n",
      "   happy     0.670     0.520     0.790    81\n",
      " neutral     0.670     0.787     0.750    84\n",
      "     sad     0.670     0.626     0.713    87\n",
      "surprise     0.670     0.689     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.680     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.140844 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.885076  [ 1200/ 4873]\n",
      "loss: 0.892269  [ 2400/ 4873]\n",
      "loss: 0.354431  [ 3600/ 4873]\n",
      "loss: 0.648315  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.673     0.705    105\n",
      " disgust     0.659     0.738     0.725    109\n",
      "    fear     0.659     0.487     0.463    80\n",
      "   happy     0.659     0.616     0.654    81\n",
      " neutral     0.659     0.824     0.726    84\n",
      "     sad     0.659     0.552     0.667    87\n",
      "surprise     0.659     0.769     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.666     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.133347 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 1.149903  [ 1200/ 4873]\n",
      "loss: 0.509781  [ 2400/ 4873]\n",
      "loss: 0.926217  [ 3600/ 4873]\n",
      "loss: 0.740250  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.667     0.629    105\n",
      " disgust     0.649     0.782     0.624    109\n",
      "    fear     0.649     0.576     0.475    80\n",
      "   happy     0.649     0.647     0.679    81\n",
      " neutral     0.649     0.680     0.833    84\n",
      "     sad     0.649     0.598     0.598    87\n",
      "surprise     0.649     0.566     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.645     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.199973 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.584622  [ 1200/ 4873]\n",
      "loss: 1.137499  [ 2400/ 4873]\n",
      "loss: 0.604412  [ 3600/ 4873]\n",
      "loss: 0.392697  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.753     0.610    105\n",
      " disgust     0.662     0.763     0.679    109\n",
      "    fear     0.662     0.611     0.550    80\n",
      "   happy     0.662     0.557     0.728    81\n",
      " neutral     0.662     0.680     0.786    84\n",
      "     sad     0.662     0.613     0.655    87\n",
      "surprise     0.662     0.667     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.663     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.183633 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.494022  [ 1200/ 4873]\n",
      "loss: 0.753044  [ 2400/ 4873]\n",
      "loss: 0.791330  [ 3600/ 4873]\n",
      "loss: 0.794092  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.719     0.657    105\n",
      " disgust     0.664     0.768     0.670    109\n",
      "    fear     0.664     0.524     0.537    80\n",
      "   happy     0.664     0.516     0.802    81\n",
      " neutral     0.664     0.857     0.714    84\n",
      "     sad     0.664     0.655     0.655    87\n",
      "surprise     0.664     0.704     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.678     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.127398 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.610249  [ 1200/ 4873]\n",
      "loss: 0.838742  [ 2400/ 4873]\n",
      "loss: 0.593331  [ 3600/ 4873]\n",
      "loss: 0.355081  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.762     0.581    105\n",
      " disgust     0.664     0.770     0.706    109\n",
      "    fear     0.664     0.569     0.512    80\n",
      "   happy     0.664     0.532     0.728    81\n",
      " neutral     0.664     0.808     0.702    84\n",
      "     sad     0.664     0.602     0.747    87\n",
      "surprise     0.664     0.652     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.671     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.152156 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.709475  [ 1200/ 4873]\n",
      "loss: 0.896277  [ 2400/ 4873]\n",
      "loss: 0.327465  [ 3600/ 4873]\n",
      "loss: 0.813736  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.793     0.619    105\n",
      " disgust     0.648     0.680     0.761    109\n",
      "    fear     0.648     0.465     0.588    80\n",
      "   happy     0.648     0.505     0.630    81\n",
      " neutral     0.648     0.983     0.679    84\n",
      "     sad     0.648     0.565     0.747    87\n",
      "surprise     0.648     0.871     0.422    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.695     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.251153 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.457905  [ 1200/ 4873]\n",
      "loss: 0.555991  [ 2400/ 4873]\n",
      "loss: 0.632920  [ 3600/ 4873]\n",
      "loss: 0.365973  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.670     0.695    105\n",
      " disgust     0.680     0.773     0.688    109\n",
      "    fear     0.680     0.661     0.487    80\n",
      "   happy     0.680     0.590     0.765    81\n",
      " neutral     0.680     0.896     0.714    84\n",
      "     sad     0.680     0.596     0.747    87\n",
      "surprise     0.680     0.641     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.690     0.677    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.161063 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep331_acc_68.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep250_acc_68\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep331_acc_68\"! Old accuracy: 67.7, new accuracy: 68.0\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.670735  [ 1200/ 4873]\n",
      "loss: 0.604759  [ 2400/ 4873]\n",
      "loss: 0.671099  [ 3600/ 4873]\n",
      "loss: 0.595999  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.699     0.686    105\n",
      " disgust     0.657     0.740     0.706    109\n",
      "    fear     0.657     0.619     0.487    80\n",
      "   happy     0.657     0.573     0.728    81\n",
      " neutral     0.657     0.649     0.726    84\n",
      "     sad     0.657     0.600     0.586    87\n",
      "surprise     0.657     0.724     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.658     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.144266 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.757376  [ 1200/ 4873]\n",
      "loss: 0.331719  [ 2400/ 4873]\n",
      "loss: 0.876822  [ 3600/ 4873]\n",
      "loss: 0.466256  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.731     0.648    105\n",
      " disgust     0.666     0.736     0.716    109\n",
      "    fear     0.666     0.656     0.500    80\n",
      "   happy     0.666     0.678     0.753    81\n",
      " neutral     0.666     0.648     0.810    84\n",
      "     sad     0.666     0.545     0.621    87\n",
      "surprise     0.666     0.661     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.665     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.139093 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.567238  [ 1200/ 4873]\n",
      "loss: 0.609319  [ 2400/ 4873]\n",
      "loss: 0.273661  [ 3600/ 4873]\n",
      "loss: 0.552777  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.717     0.629    105\n",
      " disgust     0.661     0.740     0.651    109\n",
      "    fear     0.661     0.661     0.463    80\n",
      "   happy     0.661     0.566     0.741    81\n",
      " neutral     0.661     0.717     0.786    84\n",
      "     sad     0.661     0.560     0.701    87\n",
      "surprise     0.661     0.712     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.668     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.143063 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 1.432238  [ 1200/ 4873]\n",
      "loss: 0.709848  [ 2400/ 4873]\n",
      "loss: 0.189563  [ 3600/ 4873]\n",
      "loss: 0.979366  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.782     0.581    105\n",
      " disgust     0.638     0.779     0.679    109\n",
      "    fear     0.638     0.545     0.525    80\n",
      "   happy     0.638     0.476     0.741    81\n",
      " neutral     0.638     0.753     0.726    84\n",
      "     sad     0.638     0.558     0.667    87\n",
      "surprise     0.638     0.673     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.652     0.633    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.169003 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 1.279478  [ 1200/ 4873]\n",
      "loss: 0.479720  [ 2400/ 4873]\n",
      "loss: 1.081316  [ 3600/ 4873]\n",
      "loss: 0.518473  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.721     0.590    105\n",
      " disgust     0.667     0.714     0.688    109\n",
      "    fear     0.667     0.617     0.463    80\n",
      "   happy     0.667     0.598     0.790    81\n",
      " neutral     0.667     0.778     0.750    84\n",
      "     sad     0.667     0.637     0.667    87\n",
      "surprise     0.667     0.600     0.750    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.666     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.169719 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 1.125017  [ 1200/ 4873]\n",
      "loss: 0.649563  [ 2400/ 4873]\n",
      "loss: 0.379704  [ 3600/ 4873]\n",
      "loss: 0.919075  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.714     0.619    105\n",
      " disgust     0.659     0.740     0.651    109\n",
      "    fear     0.659     0.679     0.450    80\n",
      "   happy     0.659     0.594     0.741    81\n",
      " neutral     0.659     0.722     0.774    84\n",
      "     sad     0.659     0.606     0.690    87\n",
      "surprise     0.659     0.562     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.660     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.188489 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.207630  [ 1200/ 4873]\n",
      "loss: 0.270193  [ 2400/ 4873]\n",
      "loss: 0.549854  [ 3600/ 4873]\n",
      "loss: 0.386515  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.703     0.676    105\n",
      " disgust     0.675     0.730     0.743    109\n",
      "    fear     0.675     0.673     0.463    80\n",
      "   happy     0.675     0.536     0.728    81\n",
      " neutral     0.675     0.736     0.762    84\n",
      "     sad     0.675     0.630     0.667    87\n",
      "surprise     0.675     0.778     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.684     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.131026 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.921878  [ 1200/ 4873]\n",
      "loss: 0.543477  [ 2400/ 4873]\n",
      "loss: 0.519292  [ 3600/ 4873]\n",
      "loss: 0.536885  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.652     0.695    105\n",
      " disgust     0.670     0.714     0.688    109\n",
      "    fear     0.670     0.660     0.438    80\n",
      "   happy     0.670     0.589     0.778    81\n",
      " neutral     0.670     0.816     0.738    84\n",
      "     sad     0.670     0.592     0.667    87\n",
      "surprise     0.670     0.729     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.679     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.166970 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.355763  [ 1200/ 4873]\n",
      "loss: 1.733995  [ 2400/ 4873]\n",
      "loss: 0.836529  [ 3600/ 4873]\n",
      "loss: 0.645776  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.744     0.610    105\n",
      " disgust     0.654     0.748     0.706    109\n",
      "    fear     0.654     0.552     0.463    80\n",
      "   happy     0.654     0.504     0.753    81\n",
      " neutral     0.654     0.747     0.774    84\n",
      "     sad     0.654     0.615     0.644    87\n",
      "surprise     0.654     0.709     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.660     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.132340 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.688514  [ 1200/ 4873]\n",
      "loss: 0.554960  [ 2400/ 4873]\n",
      "loss: 0.447454  [ 3600/ 4873]\n",
      "loss: 1.178442  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.787     0.600    105\n",
      " disgust     0.680     0.740     0.679    109\n",
      "    fear     0.680     0.614     0.537    80\n",
      "   happy     0.680     0.586     0.840    81\n",
      " neutral     0.680     0.797     0.750    84\n",
      "     sad     0.680     0.670     0.701    87\n",
      "surprise     0.680     0.581     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.682     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.142683 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.349256  [ 1200/ 4873]\n",
      "loss: 0.771857  [ 2400/ 4873]\n",
      "loss: 0.548628  [ 3600/ 4873]\n",
      "loss: 0.919251  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.753     0.638    105\n",
      " disgust     0.677     0.713     0.706    109\n",
      "    fear     0.677     0.577     0.562    80\n",
      "   happy     0.677     0.575     0.753    81\n",
      " neutral     0.677     0.847     0.726    84\n",
      "     sad     0.677     0.610     0.701    87\n",
      "surprise     0.677     0.719     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.685     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.132637 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.845672  [ 1200/ 4873]\n",
      "loss: 0.293513  [ 2400/ 4873]\n",
      "loss: 0.384638  [ 3600/ 4873]\n",
      "loss: 0.328206  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.779     0.638    105\n",
      " disgust     0.670     0.719     0.752    109\n",
      "    fear     0.670     0.547     0.512    80\n",
      "   happy     0.670     0.604     0.716    81\n",
      " neutral     0.670     0.759     0.714    84\n",
      "     sad     0.670     0.567     0.678    87\n",
      "surprise     0.670     0.750     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.675     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.209318 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.723811  [ 1200/ 4873]\n",
      "loss: 0.257417  [ 2400/ 4873]\n",
      "loss: 0.510656  [ 3600/ 4873]\n",
      "loss: 0.871517  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.742     0.657    105\n",
      " disgust     0.664     0.750     0.688    109\n",
      "    fear     0.664     0.527     0.487    80\n",
      "   happy     0.664     0.579     0.765    81\n",
      " neutral     0.664     0.756     0.738    84\n",
      "     sad     0.664     0.584     0.678    87\n",
      "surprise     0.664     0.736     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.668     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.143729 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.447602  [ 1200/ 4873]\n",
      "loss: 0.919907  [ 2400/ 4873]\n",
      "loss: 0.993775  [ 3600/ 4873]\n",
      "loss: 0.647042  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.720     0.638    105\n",
      " disgust     0.656     0.745     0.670    109\n",
      "    fear     0.656     0.586     0.512    80\n",
      "   happy     0.656     0.537     0.716    81\n",
      " neutral     0.656     0.775     0.738    84\n",
      "     sad     0.656     0.545     0.690    87\n",
      "surprise     0.656     0.765     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.668     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.129883 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.780443  [ 1200/ 4873]\n",
      "loss: 1.068347  [ 2400/ 4873]\n",
      "loss: 0.765229  [ 3600/ 4873]\n",
      "loss: 0.597943  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.776     0.629    105\n",
      " disgust     0.669     0.743     0.716    109\n",
      "    fear     0.669     0.587     0.463    80\n",
      "   happy     0.669     0.567     0.679    81\n",
      " neutral     0.669     0.762     0.762    84\n",
      "     sad     0.669     0.575     0.793    87\n",
      "surprise     0.669     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.672     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.167907 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.728088  [ 1200/ 4873]\n",
      "loss: 0.267791  [ 2400/ 4873]\n",
      "loss: 0.619305  [ 3600/ 4873]\n",
      "loss: 0.556317  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.676     0.657    105\n",
      " disgust     0.646     0.752     0.725    109\n",
      "    fear     0.646     0.673     0.438    80\n",
      "   happy     0.646     0.556     0.617    81\n",
      " neutral     0.646     0.649     0.750    84\n",
      "     sad     0.646     0.571     0.690    87\n",
      "surprise     0.646     0.644     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.646     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.218435 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 1.688964  [ 1200/ 4873]\n",
      "loss: 0.834477  [ 2400/ 4873]\n",
      "loss: 0.370855  [ 3600/ 4873]\n",
      "loss: 0.466073  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.793     0.619    105\n",
      " disgust     0.649     0.760     0.697    109\n",
      "    fear     0.649     0.698     0.463    80\n",
      "   happy     0.649     0.522     0.741    81\n",
      " neutral     0.649     0.625     0.774    84\n",
      "     sad     0.649     0.579     0.632    87\n",
      "surprise     0.649     0.623     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.657     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.180150 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.927962  [ 1200/ 4873]\n",
      "loss: 0.449046  [ 2400/ 4873]\n",
      "loss: 0.714591  [ 3600/ 4873]\n",
      "loss: 0.569752  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.696     0.610    105\n",
      " disgust     0.651     0.762     0.706    109\n",
      "    fear     0.651     0.600     0.487    80\n",
      "   happy     0.651     0.646     0.654    81\n",
      " neutral     0.651     0.607     0.774    84\n",
      "     sad     0.651     0.546     0.678    87\n",
      "surprise     0.651     0.727     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.655     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.229971 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.485461  [ 1200/ 4873]\n",
      "loss: 1.236653  [ 2400/ 4873]\n",
      "loss: 0.573268  [ 3600/ 4873]\n",
      "loss: 0.784186  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.695     0.629    105\n",
      " disgust     0.669     0.798     0.615    109\n",
      "    fear     0.669     0.561     0.575    80\n",
      "   happy     0.669     0.529     0.790    81\n",
      " neutral     0.669     0.797     0.750    84\n",
      "     sad     0.669     0.674     0.690    87\n",
      "surprise     0.669     0.700     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.679     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.206969 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.528253  [ 1200/ 4873]\n",
      "loss: 0.486205  [ 2400/ 4873]\n",
      "loss: 0.349473  [ 3600/ 4873]\n",
      "loss: 0.562050  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.739     0.619    105\n",
      " disgust     0.654     0.743     0.716    109\n",
      "    fear     0.654     0.615     0.500    80\n",
      "   happy     0.654     0.586     0.716    81\n",
      " neutral     0.654     0.595     0.821    84\n",
      "     sad     0.654     0.570     0.609    87\n",
      "surprise     0.654     0.818     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.667     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.157312 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.820056  [ 1200/ 4873]\n",
      "loss: 0.351478  [ 2400/ 4873]\n",
      "loss: 0.894135  [ 3600/ 4873]\n",
      "loss: 0.375781  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.687     0.648    105\n",
      " disgust     0.659     0.703     0.716    109\n",
      "    fear     0.659     0.569     0.512    80\n",
      "   happy     0.659     0.588     0.741    81\n",
      " neutral     0.659     0.765     0.738    84\n",
      "     sad     0.659     0.591     0.632    87\n",
      "surprise     0.659     0.731     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.662     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.137916 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.430885  [ 1200/ 4873]\n",
      "loss: 0.474716  [ 2400/ 4873]\n",
      "loss: 0.656377  [ 3600/ 4873]\n",
      "loss: 0.535748  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.813     0.581    105\n",
      " disgust     0.656     0.661     0.734    109\n",
      "    fear     0.656     0.551     0.537    80\n",
      "   happy     0.656     0.596     0.765    81\n",
      " neutral     0.656     0.853     0.690    84\n",
      "     sad     0.656     0.504     0.678    87\n",
      "surprise     0.656     0.787     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.681     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.237988 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.542522  [ 1200/ 4873]\n",
      "loss: 0.429187  [ 2400/ 4873]\n",
      "loss: 1.004943  [ 3600/ 4873]\n",
      "loss: 0.720256  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.694     0.648    105\n",
      " disgust     0.666     0.679     0.679    109\n",
      "    fear     0.666     0.656     0.500    80\n",
      "   happy     0.666     0.608     0.728    81\n",
      " neutral     0.666     0.778     0.750    84\n",
      "     sad     0.666     0.587     0.701    87\n",
      "surprise     0.666     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.669     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.202668 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.532665  [ 1200/ 4873]\n",
      "loss: 0.570919  [ 2400/ 4873]\n",
      "loss: 0.261545  [ 3600/ 4873]\n",
      "loss: 0.653446  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.689     0.695    105\n",
      " disgust     0.674     0.689     0.651    109\n",
      "    fear     0.674     0.696     0.487    80\n",
      "   happy     0.674     0.568     0.778    81\n",
      " neutral     0.674     0.719     0.762    84\n",
      "     sad     0.674     0.667     0.690    87\n",
      "surprise     0.674     0.745     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.682     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.139062 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.563370  [ 1200/ 4873]\n",
      "loss: 0.885639  [ 2400/ 4873]\n",
      "loss: 0.681869  [ 3600/ 4873]\n",
      "loss: 0.364579  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.744     0.552    105\n",
      " disgust     0.670     0.757     0.716    109\n",
      "    fear     0.670     0.600     0.525    80\n",
      "   happy     0.670     0.675     0.691    81\n",
      " neutral     0.670     0.605     0.821    84\n",
      "     sad     0.670     0.657     0.770    87\n",
      "surprise     0.670     0.650     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.670     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.150828 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.744379  [ 1200/ 4873]\n",
      "loss: 0.886928  [ 2400/ 4873]\n",
      "loss: 0.896805  [ 3600/ 4873]\n",
      "loss: 0.560396  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.685     0.835     0.629    105\n",
      " disgust     0.685     0.729     0.716    109\n",
      "    fear     0.685     0.720     0.450    80\n",
      "   happy     0.685     0.562     0.728    81\n",
      " neutral     0.685     0.696     0.845    84\n",
      "     sad     0.685     0.614     0.805    87\n",
      "surprise     0.685     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.685     0.696     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 1.133085 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep357_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep331_acc_68\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep357_acc_69\"! Old accuracy: 68.0, new accuracy: 68.5\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.419646  [ 1200/ 4873]\n",
      "loss: 0.335381  [ 2400/ 4873]\n",
      "loss: 1.152891  [ 3600/ 4873]\n",
      "loss: 0.511462  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.735     0.581    105\n",
      " disgust     0.659     0.672     0.752    109\n",
      "    fear     0.659     0.600     0.487    80\n",
      "   happy     0.659     0.585     0.765    81\n",
      " neutral     0.659     0.813     0.726    84\n",
      "     sad     0.659     0.596     0.678    87\n",
      "surprise     0.659     0.633     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.662     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.196037 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.575405  [ 1200/ 4873]\n",
      "loss: 0.907233  [ 2400/ 4873]\n",
      "loss: 0.572083  [ 3600/ 4873]\n",
      "loss: 0.124798  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.769     0.571    105\n",
      " disgust     0.654     0.776     0.697    109\n",
      "    fear     0.654     0.609     0.525    80\n",
      "   happy     0.654     0.500     0.716    81\n",
      " neutral     0.654     0.703     0.762    84\n",
      "     sad     0.654     0.667     0.713    87\n",
      "surprise     0.654     0.569     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.656     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.175902 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.581850  [ 1200/ 4873]\n",
      "loss: 0.280960  [ 2400/ 4873]\n",
      "loss: 0.475613  [ 3600/ 4873]\n",
      "loss: 1.081347  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.747     0.619    105\n",
      " disgust     0.662     0.786     0.706    109\n",
      "    fear     0.662     0.541     0.500    80\n",
      "   happy     0.662     0.579     0.765    81\n",
      " neutral     0.662     0.670     0.774    84\n",
      "     sad     0.662     0.614     0.621    87\n",
      "surprise     0.662     0.695     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.662     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.141283 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.912500  [ 1200/ 4873]\n",
      "loss: 0.750169  [ 2400/ 4873]\n",
      "loss: 0.572045  [ 3600/ 4873]\n",
      "loss: 0.527022  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.759     0.600    105\n",
      " disgust     0.662     0.771     0.679    109\n",
      "    fear     0.662     0.632     0.450    80\n",
      "   happy     0.662     0.534     0.765    81\n",
      " neutral     0.662     0.710     0.786    84\n",
      "     sad     0.662     0.612     0.690    87\n",
      "surprise     0.662     0.642     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.666     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.192653 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.303499  [ 1200/ 4873]\n",
      "loss: 0.441127  [ 2400/ 4873]\n",
      "loss: 0.299054  [ 3600/ 4873]\n",
      "loss: 1.090735  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.821     0.610    105\n",
      " disgust     0.641     0.708     0.734    109\n",
      "    fear     0.641     0.537     0.450    80\n",
      "   happy     0.641     0.514     0.667    81\n",
      " neutral     0.641     0.674     0.738    84\n",
      "     sad     0.641     0.563     0.770    87\n",
      "surprise     0.641     0.778     0.438    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.656     0.629    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.208905 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.966287  [ 1200/ 4873]\n",
      "loss: 0.642011  [ 2400/ 4873]\n",
      "loss: 0.441927  [ 3600/ 4873]\n",
      "loss: 0.612579  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.759     0.600    105\n",
      " disgust     0.649     0.733     0.706    109\n",
      "    fear     0.649     0.550     0.550    80\n",
      "   happy     0.649     0.566     0.691    81\n",
      " neutral     0.649     0.667     0.762    84\n",
      "     sad     0.649     0.600     0.655    87\n",
      "surprise     0.649     0.673     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.650     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.157778 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.509152  [ 1200/ 4873]\n",
      "loss: 0.612366  [ 2400/ 4873]\n",
      "loss: 0.563221  [ 3600/ 4873]\n",
      "loss: 0.275802  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.810     0.610    105\n",
      " disgust     0.648     0.702     0.734    109\n",
      "    fear     0.648     0.537     0.550    80\n",
      "   happy     0.648     0.483     0.716    81\n",
      " neutral     0.648     0.721     0.738    84\n",
      "     sad     0.648     0.635     0.701    87\n",
      "surprise     0.648     0.788     0.406    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.668     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.184773 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.262265  [ 1200/ 4873]\n",
      "loss: 0.447551  [ 2400/ 4873]\n",
      "loss: 0.884046  [ 3600/ 4873]\n",
      "loss: 0.797227  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.753     0.638    105\n",
      " disgust     0.648     0.734     0.734    109\n",
      "    fear     0.648     0.569     0.463    80\n",
      "   happy     0.648     0.519     0.691    81\n",
      " neutral     0.648     0.622     0.726    84\n",
      "     sad     0.648     0.615     0.644    87\n",
      "surprise     0.648     0.760     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.653     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.135159 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.572938  [ 1200/ 4873]\n",
      "loss: 0.598638  [ 2400/ 4873]\n",
      "loss: 0.484395  [ 3600/ 4873]\n",
      "loss: 1.482429  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.702     0.629    105\n",
      " disgust     0.662     0.702     0.734    109\n",
      "    fear     0.662     0.575     0.525    80\n",
      "   happy     0.662     0.583     0.691    81\n",
      " neutral     0.662     0.756     0.738    84\n",
      "     sad     0.662     0.587     0.701    87\n",
      "surprise     0.662     0.787     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.670     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.163061 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.936307  [ 1200/ 4873]\n",
      "loss: 1.575156  [ 2400/ 4873]\n",
      "loss: 0.896426  [ 3600/ 4873]\n",
      "loss: 0.618286  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.765     0.590    105\n",
      " disgust     0.657     0.740     0.706    109\n",
      "    fear     0.657     0.543     0.550    80\n",
      "   happy     0.657     0.492     0.765    81\n",
      " neutral     0.657     0.797     0.750    84\n",
      "     sad     0.657     0.679     0.655    87\n",
      "surprise     0.657     0.655     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.667     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.188936 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.426640  [ 1200/ 4873]\n",
      "loss: 0.814297  [ 2400/ 4873]\n",
      "loss: 0.591412  [ 3600/ 4873]\n",
      "loss: 0.283349  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.740     0.543    105\n",
      " disgust     0.657     0.683     0.752    109\n",
      "    fear     0.657     0.563     0.500    80\n",
      "   happy     0.657     0.526     0.753    81\n",
      " neutral     0.657     0.838     0.738    84\n",
      "     sad     0.657     0.636     0.644    87\n",
      "surprise     0.657     0.672     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.666     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.200937 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.345813  [ 1200/ 4873]\n",
      "loss: 0.700653  [ 2400/ 4873]\n",
      "loss: 0.328438  [ 3600/ 4873]\n",
      "loss: 0.418557  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.723     0.571    105\n",
      " disgust     0.646     0.706     0.706    109\n",
      "    fear     0.646     0.521     0.463    80\n",
      "   happy     0.646     0.543     0.704    81\n",
      " neutral     0.646     0.775     0.738    84\n",
      "     sad     0.646     0.566     0.690    87\n",
      "surprise     0.646     0.732     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.652     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.163235 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.808168  [ 1200/ 4873]\n",
      "loss: 0.921805  [ 2400/ 4873]\n",
      "loss: 0.749303  [ 3600/ 4873]\n",
      "loss: 0.558889  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.853     0.610    105\n",
      " disgust     0.669     0.762     0.706    109\n",
      "    fear     0.669     0.562     0.562    80\n",
      "   happy     0.669     0.536     0.741    81\n",
      " neutral     0.669     0.660     0.786    84\n",
      "     sad     0.669     0.678     0.678    87\n",
      "surprise     0.669     0.673     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.675     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.205299 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.444350  [ 1200/ 4873]\n",
      "loss: 0.315039  [ 2400/ 4873]\n",
      "loss: 1.023622  [ 3600/ 4873]\n",
      "loss: 0.626638  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.719     0.610    105\n",
      " disgust     0.664     0.721     0.734    109\n",
      "    fear     0.664     0.551     0.537    80\n",
      "   happy     0.664     0.566     0.691    81\n",
      " neutral     0.664     0.680     0.786    84\n",
      "     sad     0.664     0.678     0.701    87\n",
      "surprise     0.664     0.761     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.668     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.120423 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.376041  [ 1200/ 4873]\n",
      "loss: 0.347139  [ 2400/ 4873]\n",
      "loss: 0.681396  [ 3600/ 4873]\n",
      "loss: 0.507215  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.737     0.667    105\n",
      " disgust     0.662     0.741     0.734    109\n",
      "    fear     0.662     0.617     0.463    80\n",
      "   happy     0.662     0.545     0.679    81\n",
      " neutral     0.662     0.726     0.726    84\n",
      "     sad     0.662     0.590     0.678    87\n",
      "surprise     0.662     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.662     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.130521 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.401605  [ 1200/ 4873]\n",
      "loss: 0.486201  [ 2400/ 4873]\n",
      "loss: 0.831304  [ 3600/ 4873]\n",
      "loss: 1.018896  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.728     0.638    105\n",
      " disgust     0.652     0.743     0.743    109\n",
      "    fear     0.652     0.607     0.425    80\n",
      "   happy     0.652     0.538     0.691    81\n",
      " neutral     0.652     0.772     0.726    84\n",
      "     sad     0.652     0.521     0.701    87\n",
      "surprise     0.652     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.661     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.138455 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.823218  [ 1200/ 4873]\n",
      "loss: 0.674882  [ 2400/ 4873]\n",
      "loss: 0.332812  [ 3600/ 4873]\n",
      "loss: 0.696779  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.776     0.629    105\n",
      " disgust     0.662     0.675     0.761    109\n",
      "    fear     0.662     0.541     0.500    80\n",
      "   happy     0.662     0.589     0.654    81\n",
      " neutral     0.662     0.921     0.690    84\n",
      "     sad     0.662     0.524     0.747    87\n",
      "surprise     0.662     0.765     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.684     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.170170 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.511124  [ 1200/ 4873]\n",
      "loss: 1.256664  [ 2400/ 4873]\n",
      "loss: 0.732365  [ 3600/ 4873]\n",
      "loss: 0.133721  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.692     0.600    105\n",
      " disgust     0.669     0.779     0.679    109\n",
      "    fear     0.669     0.554     0.512    80\n",
      "   happy     0.669     0.545     0.815    81\n",
      " neutral     0.669     0.818     0.750    84\n",
      "     sad     0.669     0.619     0.747    87\n",
      "surprise     0.669     0.766     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.682     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.140186 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.947813  [ 1200/ 4873]\n",
      "loss: 0.743986  [ 2400/ 4873]\n",
      "loss: 0.534183  [ 3600/ 4873]\n",
      "loss: 0.389057  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.744     0.581    105\n",
      " disgust     0.654     0.796     0.679    109\n",
      "    fear     0.654     0.556     0.500    80\n",
      "   happy     0.654     0.474     0.790    81\n",
      " neutral     0.654     0.861     0.738    84\n",
      "     sad     0.654     0.588     0.690    87\n",
      "surprise     0.654     0.704     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.675     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.192333 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.659733  [ 1200/ 4873]\n",
      "loss: 0.436724  [ 2400/ 4873]\n",
      "loss: 0.549218  [ 3600/ 4873]\n",
      "loss: 0.529312  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.775     0.657    105\n",
      " disgust     0.670     0.712     0.725    109\n",
      "    fear     0.670     0.597     0.537    80\n",
      "   happy     0.670     0.613     0.704    81\n",
      " neutral     0.670     0.744     0.762    84\n",
      "     sad     0.670     0.542     0.736    87\n",
      "surprise     0.670     0.805     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.684     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.117068 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.330207  [ 1200/ 4873]\n",
      "loss: 0.513983  [ 2400/ 4873]\n",
      "loss: 0.485683  [ 3600/ 4873]\n",
      "loss: 0.852580  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.707     0.619    105\n",
      " disgust     0.666     0.758     0.688    109\n",
      "    fear     0.666     0.571     0.500    80\n",
      "   happy     0.666     0.508     0.815    81\n",
      " neutral     0.666     0.870     0.714    84\n",
      "     sad     0.666     0.667     0.644    87\n",
      "surprise     0.666     0.667     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.678     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.157390 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.747423  [ 1200/ 4873]\n",
      "loss: 0.371245  [ 2400/ 4873]\n",
      "loss: 0.532572  [ 3600/ 4873]\n",
      "loss: 0.626236  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.699     0.619    105\n",
      " disgust     0.664     0.791     0.661    109\n",
      "    fear     0.664     0.636     0.438    80\n",
      "   happy     0.664     0.644     0.716    81\n",
      " neutral     0.664     0.744     0.798    84\n",
      "     sad     0.664     0.517     0.701    87\n",
      "surprise     0.664     0.644     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.668     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.157861 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.777338  [ 1200/ 4873]\n",
      "loss: 0.444004  [ 2400/ 4873]\n",
      "loss: 0.295009  [ 3600/ 4873]\n",
      "loss: 0.472332  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.692     0.686    105\n",
      " disgust     0.661     0.669     0.725    109\n",
      "    fear     0.661     0.614     0.438    80\n",
      "   happy     0.661     0.667     0.667    81\n",
      " neutral     0.661     0.781     0.679    84\n",
      "     sad     0.661     0.533     0.747    87\n",
      "surprise     0.661     0.745     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.672     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.219284 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.718708  [ 1200/ 4873]\n",
      "loss: 0.727740  [ 2400/ 4873]\n",
      "loss: 0.761605  [ 3600/ 4873]\n",
      "loss: 0.772487  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.768     0.600    105\n",
      " disgust     0.651     0.768     0.670    109\n",
      "    fear     0.651     0.567     0.475    80\n",
      "   happy     0.651     0.468     0.815    81\n",
      " neutral     0.651     0.762     0.762    84\n",
      "     sad     0.651     0.625     0.632    87\n",
      "surprise     0.651     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.668     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.245286 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.951063  [ 1200/ 4873]\n",
      "loss: 0.646263  [ 2400/ 4873]\n",
      "loss: 0.633439  [ 3600/ 4873]\n",
      "loss: 0.887070  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.765     0.590    105\n",
      " disgust     0.654     0.694     0.771    109\n",
      "    fear     0.654     0.567     0.475    80\n",
      "   happy     0.654     0.598     0.679    81\n",
      " neutral     0.654     0.700     0.750    84\n",
      "     sad     0.654     0.555     0.701    87\n",
      "surprise     0.654     0.735     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.659     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.183811 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.741676  [ 1200/ 4873]\n",
      "loss: 1.618193  [ 2400/ 4873]\n",
      "loss: 0.414397  [ 3600/ 4873]\n",
      "loss: 0.800961  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.747     0.676    105\n",
      " disgust     0.659     0.696     0.716    109\n",
      "    fear     0.659     0.623     0.475    80\n",
      "   happy     0.659     0.514     0.691    81\n",
      " neutral     0.659     0.803     0.726    84\n",
      "     sad     0.659     0.585     0.793    87\n",
      "surprise     0.659     0.744     0.453    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.673     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.139944 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.582928  [ 1200/ 4873]\n",
      "loss: 0.670465  [ 2400/ 4873]\n",
      "loss: 0.242597  [ 3600/ 4873]\n",
      "loss: 0.554187  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.753     0.610    105\n",
      " disgust     0.667     0.695     0.752    109\n",
      "    fear     0.667     0.606     0.500    80\n",
      "   happy     0.667     0.663     0.728    81\n",
      " neutral     0.667     0.701     0.726    84\n",
      "     sad     0.667     0.526     0.701    87\n",
      "surprise     0.667     0.816     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.680     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.146854 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.252358  [ 1200/ 4873]\n",
      "loss: 0.491705  [ 2400/ 4873]\n",
      "loss: 0.511219  [ 3600/ 4873]\n",
      "loss: 0.972743  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.711     0.610    105\n",
      " disgust     0.669     0.722     0.716    109\n",
      "    fear     0.669     0.582     0.487    80\n",
      "   happy     0.669     0.596     0.691    81\n",
      " neutral     0.669     0.725     0.786    84\n",
      "     sad     0.669     0.610     0.736    87\n",
      "surprise     0.669     0.745     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.670     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.078589 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.586725  [ 1200/ 4873]\n",
      "loss: 0.848764  [ 2400/ 4873]\n",
      "loss: 0.581235  [ 3600/ 4873]\n",
      "loss: 1.327458  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.727     0.610    105\n",
      " disgust     0.641     0.689     0.651    109\n",
      "    fear     0.641     0.617     0.463    80\n",
      "   happy     0.641     0.560     0.753    81\n",
      " neutral     0.641     0.738     0.738    84\n",
      "     sad     0.641     0.582     0.655    87\n",
      "surprise     0.641     0.574     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.641     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.217444 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.576180  [ 1200/ 4873]\n",
      "loss: 0.517465  [ 2400/ 4873]\n",
      "loss: 0.484158  [ 3600/ 4873]\n",
      "loss: 0.829447  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.639     0.590    105\n",
      " disgust     0.652     0.708     0.688    109\n",
      "    fear     0.652     0.648     0.438    80\n",
      "   happy     0.652     0.577     0.741    81\n",
      " neutral     0.652     0.765     0.738    84\n",
      "     sad     0.652     0.561     0.736    87\n",
      "surprise     0.652     0.741     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.663     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.148498 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.502551  [ 1200/ 4873]\n",
      "loss: 0.626012  [ 2400/ 4873]\n",
      "loss: 0.501902  [ 3600/ 4873]\n",
      "loss: 0.578899  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.779     0.638    105\n",
      " disgust     0.661     0.712     0.679    109\n",
      "    fear     0.661     0.563     0.500    80\n",
      "   happy     0.661     0.518     0.704    81\n",
      " neutral     0.661     0.840     0.750    84\n",
      "     sad     0.661     0.558     0.724    87\n",
      "surprise     0.661     0.765     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.676     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.176661 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.609562  [ 1200/ 4873]\n",
      "loss: 0.565727  [ 2400/ 4873]\n",
      "loss: 0.876220  [ 3600/ 4873]\n",
      "loss: 0.467879  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.704     0.657    105\n",
      " disgust     0.674     0.708     0.688    109\n",
      "    fear     0.674     0.603     0.512    80\n",
      "   happy     0.674     0.573     0.679    81\n",
      " neutral     0.674     0.780     0.762    84\n",
      "     sad     0.674     0.602     0.747    87\n",
      "surprise     0.674     0.808     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.683     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.181907 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 1.143774  [ 1200/ 4873]\n",
      "loss: 0.241484  [ 2400/ 4873]\n",
      "loss: 0.776891  [ 3600/ 4873]\n",
      "loss: 0.425192  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.714     0.619    105\n",
      " disgust     0.651     0.731     0.697    109\n",
      "    fear     0.651     0.554     0.512    80\n",
      "   happy     0.651     0.529     0.679    81\n",
      " neutral     0.651     0.790     0.762    84\n",
      "     sad     0.651     0.575     0.701    87\n",
      "surprise     0.651     0.700     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.656     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.160465 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.915284  [ 1200/ 4873]\n",
      "loss: 0.695125  [ 2400/ 4873]\n",
      "loss: 0.495023  [ 3600/ 4873]\n",
      "loss: 0.203166  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.780     0.610    105\n",
      " disgust     0.657     0.725     0.725    109\n",
      "    fear     0.657     0.606     0.500    80\n",
      "   happy     0.657     0.562     0.667    81\n",
      " neutral     0.657     0.733     0.786    84\n",
      "     sad     0.657     0.528     0.747    87\n",
      "surprise     0.657     0.750     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.669     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.132621 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.540971  [ 1200/ 4873]\n",
      "loss: 0.865629  [ 2400/ 4873]\n",
      "loss: 0.097762  [ 3600/ 4873]\n",
      "loss: 0.641432  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.772     0.581    105\n",
      " disgust     0.652     0.772     0.716    109\n",
      "    fear     0.652     0.545     0.525    80\n",
      "   happy     0.652     0.465     0.741    81\n",
      " neutral     0.652     0.795     0.738    84\n",
      "     sad     0.652     0.608     0.713    87\n",
      "surprise     0.652     0.750     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.673     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.100462 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.132585  [ 1200/ 4873]\n",
      "loss: 0.441427  [ 2400/ 4873]\n",
      "loss: 0.340224  [ 3600/ 4873]\n",
      "loss: 0.802765  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.747     0.590    105\n",
      " disgust     0.661     0.664     0.743    109\n",
      "    fear     0.661     0.592     0.525    80\n",
      "   happy     0.661     0.577     0.741    81\n",
      " neutral     0.661     0.772     0.726    84\n",
      "     sad     0.661     0.652     0.667    87\n",
      "surprise     0.661     0.629     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.662     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.171738 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.202216  [ 1200/ 4873]\n",
      "loss: 0.535771  [ 2400/ 4873]\n",
      "loss: 1.039953  [ 3600/ 4873]\n",
      "loss: 0.742159  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.727     0.610    105\n",
      " disgust     0.672     0.736     0.716    109\n",
      "    fear     0.672     0.606     0.537    80\n",
      "   happy     0.672     0.521     0.765    81\n",
      " neutral     0.672     0.871     0.726    84\n",
      "     sad     0.672     0.649     0.724    87\n",
      "surprise     0.672     0.661     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.682     0.670    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.187492 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.458005  [ 1200/ 4873]\n",
      "loss: 0.771966  [ 2400/ 4873]\n",
      "loss: 0.307806  [ 3600/ 4873]\n",
      "loss: 0.347902  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.774     0.619    105\n",
      " disgust     0.649     0.683     0.771    109\n",
      "    fear     0.649     0.500     0.487    80\n",
      "   happy     0.649     0.542     0.642    81\n",
      " neutral     0.649     0.792     0.726    84\n",
      "     sad     0.649     0.567     0.678    87\n",
      "surprise     0.649     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.658     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.214895 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.974525  [ 1200/ 4873]\n",
      "loss: 0.306126  [ 2400/ 4873]\n",
      "loss: 0.466089  [ 3600/ 4873]\n",
      "loss: 0.672862  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.681     0.590    105\n",
      " disgust     0.648     0.702     0.734    109\n",
      "    fear     0.648     0.528     0.475    80\n",
      "   happy     0.648     0.520     0.654    81\n",
      " neutral     0.648     0.753     0.762    84\n",
      "     sad     0.648     0.606     0.724    87\n",
      "surprise     0.648     0.833     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.660     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.164120 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.982272  [ 1200/ 4873]\n",
      "loss: 0.349759  [ 2400/ 4873]\n",
      "loss: 0.490053  [ 3600/ 4873]\n",
      "loss: 0.869181  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.759     0.629    105\n",
      " disgust     0.675     0.672     0.734    109\n",
      "    fear     0.675     0.600     0.487    80\n",
      "   happy     0.675     0.590     0.765    81\n",
      " neutral     0.675     0.849     0.738    84\n",
      "     sad     0.675     0.554     0.713    87\n",
      "surprise     0.675     0.837     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.694     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.174520 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.660294  [ 1200/ 4873]\n",
      "loss: 0.644708  [ 2400/ 4873]\n",
      "loss: 0.548665  [ 3600/ 4873]\n",
      "loss: 0.725598  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.694     0.648    105\n",
      " disgust     0.672     0.672     0.716    109\n",
      "    fear     0.672     0.691     0.475    80\n",
      "   happy     0.672     0.612     0.741    81\n",
      " neutral     0.672     0.744     0.762    84\n",
      "     sad     0.672     0.583     0.724    87\n",
      "surprise     0.672     0.796     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.685     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.157812 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.998965  [ 1200/ 4873]\n",
      "loss: 0.560584  [ 2400/ 4873]\n",
      "loss: 1.031308  [ 3600/ 4873]\n",
      "loss: 0.991599  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.774     0.619    105\n",
      " disgust     0.675     0.718     0.771    109\n",
      "    fear     0.675     0.667     0.500    80\n",
      "   happy     0.675     0.562     0.728    81\n",
      " neutral     0.675     0.775     0.738    84\n",
      "     sad     0.675     0.563     0.667    87\n",
      "surprise     0.675     0.721     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.683     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.094748 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.991362  [ 1200/ 4873]\n",
      "loss: 0.485122  [ 2400/ 4873]\n",
      "loss: 0.563034  [ 3600/ 4873]\n",
      "loss: 0.265299  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.710     0.676    105\n",
      " disgust     0.651     0.681     0.743    109\n",
      "    fear     0.651     0.636     0.438    80\n",
      "   happy     0.651     0.686     0.593    81\n",
      " neutral     0.651     0.747     0.702    84\n",
      "     sad     0.651     0.489     0.747    87\n",
      "surprise     0.651     0.704     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.665     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.229105 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.496015  [ 1200/ 4873]\n",
      "loss: 1.254998  [ 2400/ 4873]\n",
      "loss: 0.597786  [ 3600/ 4873]\n",
      "loss: 0.927232  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.689     0.676    105\n",
      " disgust     0.670     0.667     0.734    109\n",
      "    fear     0.670     0.636     0.438    80\n",
      "   happy     0.670     0.617     0.716    81\n",
      " neutral     0.670     0.805     0.738    84\n",
      "     sad     0.670     0.608     0.713    87\n",
      "surprise     0.670     0.695     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.674     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.175071 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.484009  [ 1200/ 4873]\n",
      "loss: 0.631946  [ 2400/ 4873]\n",
      "loss: 0.558005  [ 3600/ 4873]\n",
      "loss: 0.737033  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.744     0.638    105\n",
      " disgust     0.670     0.661     0.734    109\n",
      "    fear     0.670     0.586     0.512    80\n",
      "   happy     0.670     0.574     0.716    81\n",
      " neutral     0.670     0.759     0.786    84\n",
      "     sad     0.670     0.655     0.632    87\n",
      "surprise     0.670     0.737     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.674     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.162685 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.727165  [ 1200/ 4873]\n",
      "loss: 0.558573  [ 2400/ 4873]\n",
      "loss: 0.449076  [ 3600/ 4873]\n",
      "loss: 0.135948  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.782     0.648    105\n",
      " disgust     0.664     0.704     0.743    109\n",
      "    fear     0.664     0.582     0.575    80\n",
      "   happy     0.664     0.544     0.691    81\n",
      " neutral     0.664     0.772     0.726    84\n",
      "     sad     0.664     0.588     0.655    87\n",
      "surprise     0.664     0.720     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.670     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.232860 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.286318  [ 1200/ 4873]\n",
      "loss: 0.503042  [ 2400/ 4873]\n",
      "loss: 0.413578  [ 3600/ 4873]\n",
      "loss: 0.767572  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.744     0.638    105\n",
      " disgust     0.652     0.758     0.688    109\n",
      "    fear     0.652     0.597     0.463    80\n",
      "   happy     0.652     0.539     0.679    81\n",
      " neutral     0.652     0.640     0.762    84\n",
      "     sad     0.652     0.594     0.690    87\n",
      "surprise     0.652     0.714     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.655     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.198395 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.846500  [ 1200/ 4873]\n",
      "loss: 0.749430  [ 2400/ 4873]\n",
      "loss: 0.620750  [ 3600/ 4873]\n",
      "loss: 0.660755  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.677     0.619    105\n",
      " disgust     0.648     0.733     0.679    109\n",
      "    fear     0.648     0.692     0.450    80\n",
      "   happy     0.648     0.574     0.716    81\n",
      " neutral     0.648     0.688     0.762    84\n",
      "     sad     0.648     0.560     0.644    87\n",
      "surprise     0.648     0.627     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.650     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.250263 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.834942  [ 1200/ 4873]\n",
      "loss: 0.446735  [ 2400/ 4873]\n",
      "loss: 0.710466  [ 3600/ 4873]\n",
      "loss: 0.312900  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.720     0.638    105\n",
      " disgust     0.649     0.732     0.651    109\n",
      "    fear     0.649     0.500     0.487    80\n",
      "   happy     0.649     0.592     0.716    81\n",
      " neutral     0.649     0.729     0.738    84\n",
      "     sad     0.649     0.571     0.690    87\n",
      "surprise     0.649     0.722     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.652     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.252671 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.752963  [ 1200/ 4873]\n",
      "loss: 0.977307  [ 2400/ 4873]\n",
      "loss: 0.559829  [ 3600/ 4873]\n",
      "loss: 0.497492  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.782     0.581    105\n",
      " disgust     0.669     0.802     0.706    109\n",
      "    fear     0.669     0.639     0.487    80\n",
      "   happy     0.669     0.481     0.802    81\n",
      " neutral     0.669     0.756     0.774    84\n",
      "     sad     0.669     0.653     0.713    87\n",
      "surprise     0.669     0.661     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.682     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.279508 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.522515  [ 1200/ 4873]\n",
      "loss: 0.548933  [ 2400/ 4873]\n",
      "loss: 0.967618  [ 3600/ 4873]\n",
      "loss: 0.370337  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.756     0.648    105\n",
      " disgust     0.669     0.731     0.697    109\n",
      "    fear     0.669     0.632     0.450    80\n",
      "   happy     0.669     0.545     0.753    81\n",
      " neutral     0.669     0.765     0.774    84\n",
      "     sad     0.669     0.606     0.690    87\n",
      "surprise     0.669     0.667     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.671     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.195011 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.768996  [ 1200/ 4873]\n",
      "loss: 0.505330  [ 2400/ 4873]\n",
      "loss: 0.502662  [ 3600/ 4873]\n",
      "loss: 0.232060  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.670     0.695    105\n",
      " disgust     0.672     0.745     0.697    109\n",
      "    fear     0.672     0.673     0.463    80\n",
      "   happy     0.672     0.647     0.679    81\n",
      " neutral     0.672     0.660     0.762    84\n",
      "     sad     0.672     0.629     0.701    87\n",
      "surprise     0.672     0.677     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.671     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.217486 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.301939  [ 1200/ 4873]\n",
      "loss: 0.430953  [ 2400/ 4873]\n",
      "loss: 0.904750  [ 3600/ 4873]\n",
      "loss: 0.694011  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.637     0.752    105\n",
      " disgust     0.674     0.722     0.716    109\n",
      "    fear     0.674     0.636     0.438    80\n",
      "   happy     0.674     0.687     0.704    81\n",
      " neutral     0.674     0.811     0.714    84\n",
      "     sad     0.674     0.571     0.690    87\n",
      "surprise     0.674     0.689     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.679     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.195442 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.258649  [ 1200/ 4873]\n",
      "loss: 0.569870  [ 2400/ 4873]\n",
      "loss: 0.129284  [ 3600/ 4873]\n",
      "loss: 0.312577  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.714     0.619    105\n",
      " disgust     0.659     0.785     0.670    109\n",
      "    fear     0.659     0.554     0.512    80\n",
      "   happy     0.659     0.508     0.741    81\n",
      " neutral     0.659     0.827     0.738    84\n",
      "     sad     0.659     0.574     0.713    87\n",
      "surprise     0.659     0.765     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.675     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.191586 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.631198  [ 1200/ 4873]\n",
      "loss: 1.112440  [ 2400/ 4873]\n",
      "loss: 0.186306  [ 3600/ 4873]\n",
      "loss: 0.733716  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.721     0.590    105\n",
      " disgust     0.666     0.724     0.697    109\n",
      "    fear     0.666     0.600     0.487    80\n",
      "   happy     0.666     0.541     0.815    81\n",
      " neutral     0.666     0.762     0.762    84\n",
      "     sad     0.666     0.659     0.690    87\n",
      "surprise     0.666     0.684     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.670     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.260433 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.244487  [ 1200/ 4873]\n",
      "loss: 0.475671  [ 2400/ 4873]\n",
      "loss: 0.377510  [ 3600/ 4873]\n",
      "loss: 0.550298  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.768     0.600    105\n",
      " disgust     0.644     0.689     0.752    109\n",
      "    fear     0.644     0.592     0.525    80\n",
      "   happy     0.644     0.600     0.630    81\n",
      " neutral     0.644     0.638     0.798    84\n",
      "     sad     0.644     0.567     0.632    87\n",
      "surprise     0.644     0.647     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.643     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.215798 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.829006  [ 1200/ 4873]\n",
      "loss: 0.425598  [ 2400/ 4873]\n",
      "loss: 0.719556  [ 3600/ 4873]\n",
      "loss: 0.310324  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.787     0.600    105\n",
      " disgust     0.651     0.672     0.734    109\n",
      "    fear     0.651     0.573     0.537    80\n",
      "   happy     0.651     0.538     0.704    81\n",
      " neutral     0.651     0.808     0.702    84\n",
      "     sad     0.651     0.517     0.690    87\n",
      "surprise     0.651     0.854     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.679     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.241423 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.408829  [ 1200/ 4873]\n",
      "loss: 0.263517  [ 2400/ 4873]\n",
      "loss: 0.544673  [ 3600/ 4873]\n",
      "loss: 0.201663  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.730     0.619    105\n",
      " disgust     0.656     0.787     0.679    109\n",
      "    fear     0.656     0.512     0.550    80\n",
      "   happy     0.656     0.517     0.753    81\n",
      " neutral     0.656     0.790     0.762    84\n",
      "     sad     0.656     0.618     0.632    87\n",
      "surprise     0.656     0.698     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.665     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.162287 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.377849  [ 1200/ 4873]\n",
      "loss: 0.397101  [ 2400/ 4873]\n",
      "loss: 0.591194  [ 3600/ 4873]\n",
      "loss: 0.616650  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.711     0.562    105\n",
      " disgust     0.651     0.712     0.679    109\n",
      "    fear     0.651     0.619     0.487    80\n",
      "   happy     0.651     0.504     0.741    81\n",
      " neutral     0.651     0.822     0.714    84\n",
      "     sad     0.651     0.596     0.747    87\n",
      "surprise     0.651     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.663     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.195722 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.753606  [ 1200/ 4873]\n",
      "loss: 0.662500  [ 2400/ 4873]\n",
      "loss: 1.082955  [ 3600/ 4873]\n",
      "loss: 0.238961  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.708     0.648    105\n",
      " disgust     0.670     0.672     0.789    109\n",
      "    fear     0.670     0.580     0.500    80\n",
      "   happy     0.670     0.602     0.654    81\n",
      " neutral     0.670     0.861     0.738    84\n",
      "     sad     0.670     0.577     0.736    87\n",
      "surprise     0.670     0.783     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.683     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.217850 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.854376  [ 1200/ 4873]\n",
      "loss: 0.360849  [ 2400/ 4873]\n",
      "loss: 0.952484  [ 3600/ 4873]\n",
      "loss: 0.148013  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.663     0.638    105\n",
      " disgust     0.651     0.681     0.706    109\n",
      "    fear     0.651     0.652     0.537    80\n",
      "   happy     0.651     0.576     0.654    81\n",
      " neutral     0.651     0.756     0.738    84\n",
      "     sad     0.651     0.575     0.701    87\n",
      "surprise     0.651     0.680     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.655     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.172154 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.545857  [ 1200/ 4873]\n",
      "loss: 0.171325  [ 2400/ 4873]\n",
      "loss: 0.379079  [ 3600/ 4873]\n",
      "loss: 0.572654  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.778     0.600    105\n",
      " disgust     0.669     0.747     0.679    109\n",
      "    fear     0.669     0.625     0.562    80\n",
      "   happy     0.669     0.504     0.741    81\n",
      " neutral     0.669     0.797     0.750    84\n",
      "     sad     0.669     0.594     0.724    87\n",
      "surprise     0.669     0.741     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.684     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.160662 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.502157  [ 1200/ 4873]\n",
      "loss: 0.657605  [ 2400/ 4873]\n",
      "loss: 0.636841  [ 3600/ 4873]\n",
      "loss: 0.541929  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.744     0.638    105\n",
      " disgust     0.666     0.743     0.743    109\n",
      "    fear     0.666     0.625     0.438    80\n",
      "   happy     0.666     0.591     0.679    81\n",
      " neutral     0.666     0.680     0.786    84\n",
      "     sad     0.666     0.578     0.724    87\n",
      "surprise     0.666     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.666     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.184750 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.694113  [ 1200/ 4873]\n",
      "loss: 0.631072  [ 2400/ 4873]\n",
      "loss: 0.489652  [ 3600/ 4873]\n",
      "loss: 0.672498  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.776     0.562    105\n",
      " disgust     0.656     0.778     0.706    109\n",
      "    fear     0.656     0.577     0.562    80\n",
      "   happy     0.656     0.488     0.753    81\n",
      " neutral     0.656     0.813     0.726    84\n",
      "     sad     0.656     0.573     0.678    87\n",
      "surprise     0.656     0.704     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.673     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.192144 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.861267  [ 1200/ 4873]\n",
      "loss: 0.278155  [ 2400/ 4873]\n",
      "loss: 0.525221  [ 3600/ 4873]\n",
      "loss: 1.195785  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.688     0.610    105\n",
      " disgust     0.646     0.783     0.661    109\n",
      "    fear     0.646     0.613     0.475    80\n",
      "   happy     0.646     0.581     0.667    81\n",
      " neutral     0.646     0.616     0.726    84\n",
      "     sad     0.646     0.581     0.701    87\n",
      "surprise     0.646     0.667     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.647     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.201459 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.946765  [ 1200/ 4873]\n",
      "loss: 0.638801  [ 2400/ 4873]\n",
      "loss: 0.311108  [ 3600/ 4873]\n",
      "loss: 0.769574  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.725     0.629    105\n",
      " disgust     0.646     0.737     0.642    109\n",
      "    fear     0.646     0.593     0.438    80\n",
      "   happy     0.646     0.563     0.716    81\n",
      " neutral     0.646     0.689     0.738    84\n",
      "     sad     0.646     0.602     0.713    87\n",
      "surprise     0.646     0.594     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.643     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.235255 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.191750  [ 1200/ 4873]\n",
      "loss: 0.787820  [ 2400/ 4873]\n",
      "loss: 0.821403  [ 3600/ 4873]\n",
      "loss: 0.442698  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.716     0.648    105\n",
      " disgust     0.670     0.755     0.706    109\n",
      "    fear     0.670     0.600     0.525    80\n",
      "   happy     0.670     0.587     0.790    81\n",
      " neutral     0.670     0.769     0.714    84\n",
      "     sad     0.670     0.609     0.644    87\n",
      "surprise     0.670     0.656     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.670     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.254238 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.706089  [ 1200/ 4873]\n",
      "loss: 0.615629  [ 2400/ 4873]\n",
      "loss: 0.731936  [ 3600/ 4873]\n",
      "loss: 0.628084  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.736     0.638    105\n",
      " disgust     0.662     0.743     0.688    109\n",
      "    fear     0.662     0.639     0.487    80\n",
      "   happy     0.662     0.529     0.778    81\n",
      " neutral     0.662     0.756     0.738    84\n",
      "     sad     0.662     0.611     0.632    87\n",
      "surprise     0.662     0.652     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.667     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.288136 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 1.662894  [ 1200/ 4873]\n",
      "loss: 0.451869  [ 2400/ 4873]\n",
      "loss: 0.744776  [ 3600/ 4873]\n",
      "loss: 0.678452  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.716     0.648    105\n",
      " disgust     0.659     0.743     0.688    109\n",
      "    fear     0.659     0.633     0.475    80\n",
      "   happy     0.659     0.566     0.691    81\n",
      " neutral     0.659     0.604     0.762    84\n",
      "     sad     0.659     0.630     0.724    87\n",
      "surprise     0.659     0.776     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.667     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.228624 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.716172  [ 1200/ 4873]\n",
      "loss: 0.701501  [ 2400/ 4873]\n",
      "loss: 0.582884  [ 3600/ 4873]\n",
      "loss: 1.046352  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.694     0.648    105\n",
      " disgust     0.657     0.695     0.752    109\n",
      "    fear     0.657     0.627     0.463    80\n",
      "   happy     0.657     0.631     0.654    81\n",
      " neutral     0.657     0.701     0.726    84\n",
      "     sad     0.657     0.553     0.724    87\n",
      "surprise     0.657     0.740     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.663     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.242216 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.689193  [ 1200/ 4873]\n",
      "loss: 1.097406  [ 2400/ 4873]\n",
      "loss: 0.418562  [ 3600/ 4873]\n",
      "loss: 0.888785  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.724     0.600    105\n",
      " disgust     0.666     0.725     0.725    109\n",
      "    fear     0.666     0.580     0.500    80\n",
      "   happy     0.666     0.530     0.753    81\n",
      " neutral     0.666     0.803     0.726    84\n",
      "     sad     0.666     0.638     0.690    87\n",
      "surprise     0.666     0.700     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.671     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.278275 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.588546  [ 1200/ 4873]\n",
      "loss: 0.937663  [ 2400/ 4873]\n",
      "loss: 0.577335  [ 3600/ 4873]\n",
      "loss: 0.506892  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.685     0.600    105\n",
      " disgust     0.644     0.703     0.651    109\n",
      "    fear     0.644     0.581     0.450    80\n",
      "   happy     0.644     0.504     0.802    81\n",
      " neutral     0.644     0.724     0.750    84\n",
      "     sad     0.644     0.701     0.621    87\n",
      "surprise     0.644     0.661     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.651     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.309538 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.617175  [ 1200/ 4873]\n",
      "loss: 0.759279  [ 2400/ 4873]\n",
      "loss: 0.614267  [ 3600/ 4873]\n",
      "loss: 0.806975  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.670     0.638    105\n",
      " disgust     0.669     0.699     0.725    109\n",
      "    fear     0.669     0.644     0.475    80\n",
      "   happy     0.669     0.583     0.691    81\n",
      " neutral     0.669     0.753     0.762    84\n",
      "     sad     0.669     0.610     0.701    87\n",
      "surprise     0.669     0.754     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.673     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.230534 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.683103  [ 1200/ 4873]\n",
      "loss: 0.308896  [ 2400/ 4873]\n",
      "loss: 0.938987  [ 3600/ 4873]\n",
      "loss: 0.842948  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.691     0.638    105\n",
      " disgust     0.675     0.706     0.706    109\n",
      "    fear     0.675     0.631     0.512    80\n",
      "   happy     0.675     0.549     0.827    81\n",
      " neutral     0.675     0.810     0.762    84\n",
      "     sad     0.675     0.659     0.667    87\n",
      "surprise     0.675     0.760     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.687     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.201170 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.911003  [ 1200/ 4873]\n",
      "loss: 0.765299  [ 2400/ 4873]\n",
      "loss: 1.289541  [ 3600/ 4873]\n",
      "loss: 0.402762  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.720     0.638    105\n",
      " disgust     0.667     0.778     0.706    109\n",
      "    fear     0.667     0.625     0.438    80\n",
      "   happy     0.667     0.564     0.704    81\n",
      " neutral     0.667     0.719     0.762    84\n",
      "     sad     0.667     0.592     0.701    87\n",
      "surprise     0.667     0.667     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.667     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.225609 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.456859  [ 1200/ 4873]\n",
      "loss: 0.237816  [ 2400/ 4873]\n",
      "loss: 0.869057  [ 3600/ 4873]\n",
      "loss: 0.558689  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.753     0.667    105\n",
      " disgust     0.651     0.716     0.670    109\n",
      "    fear     0.651     0.586     0.512    80\n",
      "   happy     0.651     0.521     0.778    81\n",
      " neutral     0.651     0.776     0.702    84\n",
      "     sad     0.651     0.567     0.632    87\n",
      "surprise     0.651     0.706     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.661     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.230650 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.659441  [ 1200/ 4873]\n",
      "loss: 0.354212  [ 2400/ 4873]\n",
      "loss: 0.437326  [ 3600/ 4873]\n",
      "loss: 0.937653  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.789     0.533    105\n",
      " disgust     0.646     0.738     0.697    109\n",
      "    fear     0.646     0.493     0.450    80\n",
      "   happy     0.646     0.528     0.691    81\n",
      " neutral     0.646     0.776     0.786    84\n",
      "     sad     0.646     0.566     0.736    87\n",
      "surprise     0.646     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.653     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.247515 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.358962  [ 1200/ 4873]\n",
      "loss: 0.517592  [ 2400/ 4873]\n",
      "loss: 0.245050  [ 3600/ 4873]\n",
      "loss: 0.323544  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.730     0.619    105\n",
      " disgust     0.648     0.757     0.716    109\n",
      "    fear     0.648     0.581     0.450    80\n",
      "   happy     0.648     0.518     0.704    81\n",
      " neutral     0.648     0.741     0.750    84\n",
      "     sad     0.648     0.517     0.701    87\n",
      "surprise     0.648     0.814     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.666     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.159803 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.335113  [ 1200/ 4873]\n",
      "loss: 0.665834  [ 2400/ 4873]\n",
      "loss: 0.445410  [ 3600/ 4873]\n",
      "loss: 0.628750  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.753     0.610    105\n",
      " disgust     0.661     0.675     0.706    109\n",
      "    fear     0.661     0.725     0.463    80\n",
      "   happy     0.661     0.570     0.704    81\n",
      " neutral     0.661     0.691     0.774    84\n",
      "     sad     0.661     0.559     0.759    87\n",
      "surprise     0.661     0.771     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.678     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.137528 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.873659  [ 1200/ 4873]\n",
      "loss: 1.319843  [ 2400/ 4873]\n",
      "loss: 0.712240  [ 3600/ 4873]\n",
      "loss: 0.411511  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.788     0.638    105\n",
      " disgust     0.657     0.701     0.688    109\n",
      "    fear     0.657     0.580     0.500    80\n",
      "   happy     0.657     0.509     0.716    81\n",
      " neutral     0.657     0.775     0.738    84\n",
      "     sad     0.657     0.606     0.690    87\n",
      "surprise     0.657     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.665     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.104631 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.757706  [ 1200/ 4873]\n",
      "loss: 0.730554  [ 2400/ 4873]\n",
      "loss: 0.706001  [ 3600/ 4873]\n",
      "loss: 0.377198  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.778     0.600    105\n",
      " disgust     0.659     0.686     0.743    109\n",
      "    fear     0.659     0.708     0.425    80\n",
      "   happy     0.659     0.544     0.765    81\n",
      " neutral     0.659     0.831     0.702    84\n",
      "     sad     0.659     0.562     0.678    87\n",
      "surprise     0.659     0.603     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.673     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.269152 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.831664  [ 1200/ 4873]\n",
      "loss: 0.350073  [ 2400/ 4873]\n",
      "loss: 0.414918  [ 3600/ 4873]\n",
      "loss: 0.480112  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.768     0.600    105\n",
      " disgust     0.656     0.789     0.688    109\n",
      "    fear     0.656     0.656     0.500    80\n",
      "   happy     0.656     0.475     0.716    81\n",
      " neutral     0.656     0.759     0.750    84\n",
      "     sad     0.656     0.636     0.644    87\n",
      "surprise     0.656     0.570     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.665     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.243954 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.127813  [ 1200/ 4873]\n",
      "loss: 0.351287  [ 2400/ 4873]\n",
      "loss: 0.305981  [ 3600/ 4873]\n",
      "loss: 0.466203  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.745     0.667    105\n",
      " disgust     0.667     0.743     0.716    109\n",
      "    fear     0.667     0.627     0.463    80\n",
      "   happy     0.667     0.560     0.753    81\n",
      " neutral     0.667     0.773     0.690    84\n",
      "     sad     0.667     0.583     0.690    87\n",
      "surprise     0.667     0.662     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.670     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.257529 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.432699  [ 1200/ 4873]\n",
      "loss: 0.264481  [ 2400/ 4873]\n",
      "loss: 0.937879  [ 3600/ 4873]\n",
      "loss: 0.146634  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.677     0.638    105\n",
      " disgust     0.664     0.747     0.679    109\n",
      "    fear     0.664     0.632     0.450    80\n",
      "   happy     0.664     0.513     0.741    81\n",
      " neutral     0.664     0.824     0.726    84\n",
      "     sad     0.664     0.653     0.736    87\n",
      "surprise     0.664     0.652     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.671     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.167629 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.637046  [ 1200/ 4873]\n",
      "loss: 0.444267  [ 2400/ 4873]\n",
      "loss: 0.963593  [ 3600/ 4873]\n",
      "loss: 0.793779  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.802     0.619    105\n",
      " disgust     0.690     0.741     0.734    109\n",
      "    fear     0.690     0.627     0.525    80\n",
      "   happy     0.690     0.537     0.802    81\n",
      " neutral     0.690     0.836     0.726    84\n",
      "     sad     0.690     0.638     0.770    87\n",
      "surprise     0.690     0.745     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.704     0.688    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.205693 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep442_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep357_acc_69\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep442_acc_69\"! Old accuracy: 68.5, new accuracy: 69.0\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.585836  [ 1200/ 4873]\n",
      "loss: 0.673677  [ 2400/ 4873]\n",
      "loss: 0.451789  [ 3600/ 4873]\n",
      "loss: 0.311291  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.697     0.657    105\n",
      " disgust     0.656     0.721     0.688    109\n",
      "    fear     0.656     0.636     0.438    80\n",
      "   happy     0.656     0.540     0.667    81\n",
      " neutral     0.656     0.808     0.750    84\n",
      "     sad     0.656     0.540     0.782    87\n",
      "surprise     0.656     0.750     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.670     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.167715 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 1.071938  [ 1200/ 4873]\n",
      "loss: 0.600996  [ 2400/ 4873]\n",
      "loss: 0.538260  [ 3600/ 4873]\n",
      "loss: 0.457162  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.747     0.590    105\n",
      " disgust     0.659     0.650     0.734    109\n",
      "    fear     0.659     0.623     0.475    80\n",
      "   happy     0.659     0.517     0.753    81\n",
      " neutral     0.659     0.805     0.738    84\n",
      "     sad     0.659     0.615     0.678    87\n",
      "surprise     0.659     0.769     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.675     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.233303 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.670762  [ 1200/ 4873]\n",
      "loss: 0.472905  [ 2400/ 4873]\n",
      "loss: 0.296416  [ 3600/ 4873]\n",
      "loss: 0.401302  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.694     0.648    105\n",
      " disgust     0.677     0.765     0.688    109\n",
      "    fear     0.677     0.656     0.500    80\n",
      "   happy     0.677     0.574     0.765    81\n",
      " neutral     0.677     0.795     0.738    84\n",
      "     sad     0.677     0.628     0.678    87\n",
      "surprise     0.677     0.644     0.734    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.679     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.185734 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.428872  [ 1200/ 4873]\n",
      "loss: 0.574623  [ 2400/ 4873]\n",
      "loss: 0.287135  [ 3600/ 4873]\n",
      "loss: 0.434041  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.726     0.657    105\n",
      " disgust     0.662     0.772     0.651    109\n",
      "    fear     0.662     0.686     0.438    80\n",
      "   happy     0.662     0.528     0.691    81\n",
      " neutral     0.662     0.741     0.750    84\n",
      "     sad     0.662     0.571     0.736    87\n",
      "surprise     0.662     0.667     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.670     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.224137 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.735236  [ 1200/ 4873]\n",
      "loss: 0.458443  [ 2400/ 4873]\n",
      "loss: 0.207860  [ 3600/ 4873]\n",
      "loss: 0.478602  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.714     0.619    105\n",
      " disgust     0.667     0.762     0.706    109\n",
      "    fear     0.667     0.594     0.512    80\n",
      "   happy     0.667     0.531     0.741    81\n",
      " neutral     0.667     0.805     0.738    84\n",
      "     sad     0.667     0.606     0.724    87\n",
      "surprise     0.667     0.709     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.675     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.211740 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.555575  [ 1200/ 4873]\n",
      "loss: 0.385261  [ 2400/ 4873]\n",
      "loss: 1.107206  [ 3600/ 4873]\n",
      "loss: 0.384196  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.738     0.590    105\n",
      " disgust     0.651     0.787     0.642    109\n",
      "    fear     0.651     0.586     0.512    80\n",
      "   happy     0.651     0.462     0.741    81\n",
      " neutral     0.651     0.810     0.762    84\n",
      "     sad     0.651     0.590     0.713    87\n",
      "surprise     0.651     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.670     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.189529 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 1.366289  [ 1200/ 4873]\n",
      "loss: 0.364765  [ 2400/ 4873]\n",
      "loss: 0.733926  [ 3600/ 4873]\n",
      "loss: 0.472378  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.738     0.562    105\n",
      " disgust     0.646     0.743     0.716    109\n",
      "    fear     0.646     0.684     0.487    80\n",
      "   happy     0.646     0.488     0.741    81\n",
      " neutral     0.646     0.849     0.738    84\n",
      "     sad     0.646     0.512     0.724    87\n",
      "surprise     0.646     0.673     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.670     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.193775 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.597050  [ 1200/ 4873]\n",
      "loss: 0.736017  [ 2400/ 4873]\n",
      "loss: 0.442299  [ 3600/ 4873]\n",
      "loss: 0.391747  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.710     0.629    105\n",
      " disgust     0.644     0.629     0.761    109\n",
      "    fear     0.644     0.547     0.438    80\n",
      "   happy     0.644     0.552     0.654    81\n",
      " neutral     0.644     0.753     0.726    84\n",
      "     sad     0.644     0.583     0.644    87\n",
      "surprise     0.644     0.812     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.655     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.274190 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.431752  [ 1200/ 4873]\n",
      "loss: 0.706399  [ 2400/ 4873]\n",
      "loss: 0.877816  [ 3600/ 4873]\n",
      "loss: 0.833983  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.725     0.629    105\n",
      " disgust     0.662     0.723     0.670    109\n",
      "    fear     0.662     0.629     0.550    80\n",
      "   happy     0.662     0.550     0.753    81\n",
      " neutral     0.662     0.771     0.762    84\n",
      "     sad     0.662     0.565     0.701    87\n",
      "surprise     0.662     0.761     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.675     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.277772 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.519382  [ 1200/ 4873]\n",
      "loss: 0.686965  [ 2400/ 4873]\n",
      "loss: 0.466304  [ 3600/ 4873]\n",
      "loss: 0.282090  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.733     0.705    105\n",
      " disgust     0.682     0.678     0.734    109\n",
      "    fear     0.682     0.704     0.475    80\n",
      "   happy     0.682     0.651     0.691    81\n",
      " neutral     0.682     0.667     0.762    84\n",
      "     sad     0.682     0.619     0.747    87\n",
      "surprise     0.682     0.780     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.690     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.215113 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.531436  [ 1200/ 4873]\n",
      "loss: 0.253280  [ 2400/ 4873]\n",
      "loss: 0.425075  [ 3600/ 4873]\n",
      "loss: 0.487342  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.784     0.552    105\n",
      " disgust     0.659     0.727     0.734    109\n",
      "    fear     0.659     0.594     0.512    80\n",
      "   happy     0.659     0.513     0.716    81\n",
      " neutral     0.659     0.769     0.714    84\n",
      "     sad     0.659     0.535     0.701    87\n",
      "surprise     0.659     0.846     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.681     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.239312 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.105798  [ 1200/ 4873]\n",
      "loss: 0.979790  [ 2400/ 4873]\n",
      "loss: 0.303254  [ 3600/ 4873]\n",
      "loss: 0.738991  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.744     0.638    105\n",
      " disgust     0.652     0.731     0.697    109\n",
      "    fear     0.652     0.607     0.425    80\n",
      "   happy     0.652     0.559     0.704    81\n",
      " neutral     0.652     0.728     0.702    84\n",
      "     sad     0.652     0.524     0.759    87\n",
      "surprise     0.652     0.765     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.665     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.293804 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.457730  [ 1200/ 4873]\n",
      "loss: 0.571254  [ 2400/ 4873]\n",
      "loss: 0.419469  [ 3600/ 4873]\n",
      "loss: 0.435966  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.710     0.629    105\n",
      " disgust     0.661     0.786     0.706    109\n",
      "    fear     0.661     0.554     0.512    80\n",
      "   happy     0.661     0.579     0.679    81\n",
      " neutral     0.661     0.756     0.738    84\n",
      "     sad     0.661     0.559     0.713    87\n",
      "surprise     0.661     0.702     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.664     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.215550 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.834543  [ 1200/ 4873]\n",
      "loss: 1.083979  [ 2400/ 4873]\n",
      "loss: 0.866110  [ 3600/ 4873]\n",
      "loss: 0.826883  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.684     0.638    105\n",
      " disgust     0.654     0.698     0.743    109\n",
      "    fear     0.654     0.649     0.463    80\n",
      "   happy     0.654     0.555     0.753    81\n",
      " neutral     0.654     0.776     0.702    84\n",
      "     sad     0.654     0.583     0.644    87\n",
      "surprise     0.654     0.667     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.659     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.265213 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.160039  [ 1200/ 4873]\n",
      "loss: 0.702756  [ 2400/ 4873]\n",
      "loss: 0.583830  [ 3600/ 4873]\n",
      "loss: 0.789281  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.793     0.619    105\n",
      " disgust     0.672     0.757     0.743    109\n",
      "    fear     0.672     0.615     0.500    80\n",
      "   happy     0.672     0.528     0.691    81\n",
      " neutral     0.672     0.736     0.762    84\n",
      "     sad     0.672     0.600     0.724    87\n",
      "surprise     0.672     0.707     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.677     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.173814 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.709526  [ 1200/ 4873]\n",
      "loss: 0.544985  [ 2400/ 4873]\n",
      "loss: 0.155339  [ 3600/ 4873]\n",
      "loss: 0.752468  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.765     0.619    105\n",
      " disgust     0.651     0.753     0.670    109\n",
      "    fear     0.651     0.649     0.463    80\n",
      "   happy     0.651     0.537     0.716    81\n",
      " neutral     0.651     0.677     0.798    84\n",
      "     sad     0.651     0.558     0.724    87\n",
      "surprise     0.651     0.667     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.658     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.289853 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.694969  [ 1200/ 4873]\n",
      "loss: 0.320125  [ 2400/ 4873]\n",
      "loss: 0.557269  [ 3600/ 4873]\n",
      "loss: 0.555081  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.713     0.686    105\n",
      " disgust     0.657     0.745     0.670    109\n",
      "    fear     0.657     0.593     0.438    80\n",
      "   happy     0.657     0.509     0.667    81\n",
      " neutral     0.657     0.719     0.762    84\n",
      "     sad     0.657     0.694     0.678    87\n",
      "surprise     0.657     0.611     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.655     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.234199 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.709579  [ 1200/ 4873]\n",
      "loss: 0.860177  [ 2400/ 4873]\n",
      "loss: 0.666458  [ 3600/ 4873]\n",
      "loss: 0.477959  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.786     0.629    105\n",
      " disgust     0.654     0.779     0.615    109\n",
      "    fear     0.654     0.554     0.512    80\n",
      "   happy     0.654     0.513     0.741    81\n",
      " neutral     0.654     0.808     0.750    84\n",
      "     sad     0.654     0.583     0.724    87\n",
      "surprise     0.654     0.619     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.663     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.248998 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.536830  [ 1200/ 4873]\n",
      "loss: 0.460490  [ 2400/ 4873]\n",
      "loss: 0.917586  [ 3600/ 4873]\n",
      "loss: 0.351602  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.767     0.657    105\n",
      " disgust     0.648     0.755     0.679    109\n",
      "    fear     0.648     0.660     0.438    80\n",
      "   happy     0.648     0.514     0.667    81\n",
      " neutral     0.648     0.674     0.762    84\n",
      "     sad     0.648     0.554     0.713    87\n",
      "surprise     0.648     0.649     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.653     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.223172 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.672367  [ 1200/ 4873]\n",
      "loss: 0.848086  [ 2400/ 4873]\n",
      "loss: 0.606048  [ 3600/ 4873]\n",
      "loss: 0.611112  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.774     0.619    105\n",
      " disgust     0.662     0.796     0.679    109\n",
      "    fear     0.662     0.638     0.463    80\n",
      "   happy     0.662     0.420     0.778    81\n",
      " neutral     0.662     0.808     0.750    84\n",
      "     sad     0.662     0.677     0.747    87\n",
      "surprise     0.662     0.725     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.691     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.144866 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.506613  [ 1200/ 4873]\n",
      "loss: 0.327597  [ 2400/ 4873]\n",
      "loss: 0.391549  [ 3600/ 4873]\n",
      "loss: 0.485110  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.721     0.590    105\n",
      " disgust     0.687     0.740     0.706    109\n",
      "    fear     0.687     0.667     0.525    80\n",
      "   happy     0.687     0.588     0.827    81\n",
      " neutral     0.687     0.785     0.738    84\n",
      "     sad     0.687     0.690     0.690    87\n",
      "surprise     0.687     0.636     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.690     0.692    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.294829 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.381242  [ 1200/ 4873]\n",
      "loss: 0.957473  [ 2400/ 4873]\n",
      "loss: 0.320698  [ 3600/ 4873]\n",
      "loss: 0.355449  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.772     0.581    105\n",
      " disgust     0.649     0.796     0.679    109\n",
      "    fear     0.649     0.613     0.475    80\n",
      "   happy     0.649     0.484     0.753    81\n",
      " neutral     0.649     0.685     0.750    84\n",
      "     sad     0.649     0.619     0.747    87\n",
      "surprise     0.649     0.642     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.659     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.228007 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.440806  [ 1200/ 4873]\n",
      "loss: 0.870954  [ 2400/ 4873]\n",
      "loss: 0.577334  [ 3600/ 4873]\n",
      "loss: 0.511881  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.736     0.610    105\n",
      " disgust     0.674     0.750     0.661    109\n",
      "    fear     0.674     0.717     0.475    80\n",
      "   happy     0.674     0.524     0.802    81\n",
      " neutral     0.674     0.713     0.798    84\n",
      "     sad     0.674     0.690     0.690    87\n",
      "surprise     0.674     0.652     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.683     0.677    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.183814 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 1.158826  [ 1200/ 4873]\n",
      "loss: 0.315160  [ 2400/ 4873]\n",
      "loss: 0.532064  [ 3600/ 4873]\n",
      "loss: 0.598842  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.673     0.667    105\n",
      " disgust     0.674     0.772     0.651    109\n",
      "    fear     0.674     0.655     0.475    80\n",
      "   happy     0.674     0.600     0.778    81\n",
      " neutral     0.674     0.779     0.714    84\n",
      "     sad     0.674     0.593     0.736    87\n",
      "surprise     0.674     0.682     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.679     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.280647 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 1.094710  [ 1200/ 4873]\n",
      "loss: 0.982223  [ 2400/ 4873]\n",
      "loss: 0.565530  [ 3600/ 4873]\n",
      "loss: 1.179477  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.724     0.600    105\n",
      " disgust     0.654     0.750     0.716    109\n",
      "    fear     0.654     0.597     0.500    80\n",
      "   happy     0.654     0.531     0.642    81\n",
      " neutral     0.654     0.703     0.762    84\n",
      "     sad     0.654     0.576     0.782    87\n",
      "surprise     0.654     0.756     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.662     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.232304 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.814936  [ 1200/ 4873]\n",
      "loss: 0.478410  [ 2400/ 4873]\n",
      "loss: 1.138158  [ 3600/ 4873]\n",
      "loss: 0.293411  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.797     0.600    105\n",
      " disgust     0.648     0.777     0.734    109\n",
      "    fear     0.648     0.518     0.550    80\n",
      "   happy     0.648     0.475     0.691    81\n",
      " neutral     0.648     0.766     0.702    84\n",
      "     sad     0.648     0.571     0.690    87\n",
      "surprise     0.648     0.767     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.667     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.337003 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.371596  [ 1200/ 4873]\n",
      "loss: 1.519285  [ 2400/ 4873]\n",
      "loss: 0.825059  [ 3600/ 4873]\n",
      "loss: 0.254667  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.744     0.610    105\n",
      " disgust     0.667     0.762     0.734    109\n",
      "    fear     0.667     0.639     0.487    80\n",
      "   happy     0.667     0.477     0.753    81\n",
      " neutral     0.667     0.782     0.726    84\n",
      "     sad     0.667     0.613     0.747    87\n",
      "surprise     0.667     0.804     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.689     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.218012 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.252069  [ 1200/ 4873]\n",
      "loss: 0.263818  [ 2400/ 4873]\n",
      "loss: 0.261743  [ 3600/ 4873]\n",
      "loss: 1.102880  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.790     0.610    105\n",
      " disgust     0.662     0.724     0.771    109\n",
      "    fear     0.662     0.648     0.438    80\n",
      "   happy     0.662     0.554     0.691    81\n",
      " neutral     0.662     0.811     0.714    84\n",
      "     sad     0.662     0.553     0.724    87\n",
      "surprise     0.662     0.600     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.669     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.256291 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.720141  [ 1200/ 4873]\n",
      "loss: 1.290251  [ 2400/ 4873]\n",
      "loss: 1.103586  [ 3600/ 4873]\n",
      "loss: 0.738633  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.717     0.629    105\n",
      " disgust     0.667     0.794     0.706    109\n",
      "    fear     0.667     0.614     0.438    80\n",
      "   happy     0.667     0.516     0.802    81\n",
      " neutral     0.667     0.765     0.738    84\n",
      "     sad     0.667     0.642     0.701    87\n",
      "surprise     0.667     0.661     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.673     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.250953 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.792257  [ 1200/ 4873]\n",
      "loss: 0.897936  [ 2400/ 4873]\n",
      "loss: 0.847145  [ 3600/ 4873]\n",
      "loss: 0.325340  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.787     0.562    105\n",
      " disgust     0.648     0.703     0.716    109\n",
      "    fear     0.648     0.532     0.512    80\n",
      "   happy     0.648     0.480     0.741    81\n",
      " neutral     0.648     0.800     0.714    84\n",
      "     sad     0.648     0.592     0.701    87\n",
      "surprise     0.648     0.818     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.673     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.248496 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.529342  [ 1200/ 4873]\n",
      "loss: 0.711652  [ 2400/ 4873]\n",
      "loss: 0.595205  [ 3600/ 4873]\n",
      "loss: 0.398048  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.628     0.762     0.610    105\n",
      " disgust     0.628     0.750     0.661    109\n",
      "    fear     0.628     0.569     0.412    80\n",
      "   happy     0.628     0.514     0.667    81\n",
      " neutral     0.628     0.663     0.750    84\n",
      "     sad     0.628     0.531     0.690    87\n",
      "surprise     0.628     0.627     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.628     0.631     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.255649 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.741359  [ 1200/ 4873]\n",
      "loss: 0.262101  [ 2400/ 4873]\n",
      "loss: 0.157402  [ 3600/ 4873]\n",
      "loss: 0.451127  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.750     0.629    105\n",
      " disgust     0.664     0.717     0.743    109\n",
      "    fear     0.664     0.597     0.463    80\n",
      "   happy     0.664     0.580     0.716    81\n",
      " neutral     0.664     0.692     0.750    84\n",
      "     sad     0.664     0.604     0.701    87\n",
      "surprise     0.664     0.709     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.664     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.218334 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.448090  [ 1200/ 4873]\n",
      "loss: 0.653290  [ 2400/ 4873]\n",
      "loss: 0.600813  [ 3600/ 4873]\n",
      "loss: 0.229619  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.729     0.590    105\n",
      " disgust     0.670     0.743     0.716    109\n",
      "    fear     0.670     0.629     0.487    80\n",
      "   happy     0.670     0.600     0.741    81\n",
      " neutral     0.670     0.768     0.750    84\n",
      "     sad     0.670     0.602     0.713    87\n",
      "surprise     0.670     0.616     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.670     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.239958 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.388295  [ 1200/ 4873]\n",
      "loss: 1.201466  [ 2400/ 4873]\n",
      "loss: 0.350355  [ 3600/ 4873]\n",
      "loss: 0.851891  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.722     0.619    105\n",
      " disgust     0.657     0.784     0.734    109\n",
      "    fear     0.657     0.679     0.450    80\n",
      "   happy     0.657     0.559     0.704    81\n",
      " neutral     0.657     0.677     0.774    84\n",
      "     sad     0.657     0.553     0.655    87\n",
      "surprise     0.657     0.641     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.659     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.268731 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.146071  [ 1200/ 4873]\n",
      "loss: 0.651245  [ 2400/ 4873]\n",
      "loss: 0.605252  [ 3600/ 4873]\n",
      "loss: 0.255579  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.766     0.562    105\n",
      " disgust     0.652     0.720     0.780    109\n",
      "    fear     0.652     0.514     0.463    80\n",
      "   happy     0.652     0.530     0.753    81\n",
      " neutral     0.652     0.827     0.738    84\n",
      "     sad     0.652     0.593     0.621    87\n",
      "surprise     0.652     0.645     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.657     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.223567 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.361484  [ 1200/ 4873]\n",
      "loss: 0.675954  [ 2400/ 4873]\n",
      "loss: 0.387055  [ 3600/ 4873]\n",
      "loss: 1.106275  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.630     0.738     0.590    105\n",
      " disgust     0.630     0.763     0.679    109\n",
      "    fear     0.630     0.508     0.412    80\n",
      "   happy     0.630     0.495     0.667    81\n",
      " neutral     0.630     0.735     0.726    84\n",
      "     sad     0.630     0.525     0.713    87\n",
      "surprise     0.630     0.704     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.630     0.638     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.240571 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.356525  [ 1200/ 4873]\n",
      "loss: 0.315929  [ 2400/ 4873]\n",
      "loss: 0.332140  [ 3600/ 4873]\n",
      "loss: 0.594962  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.762     0.610    105\n",
      " disgust     0.654     0.701     0.752    109\n",
      "    fear     0.654     0.569     0.463    80\n",
      "   happy     0.654     0.535     0.667    81\n",
      " neutral     0.654     0.744     0.762    84\n",
      "     sad     0.654     0.613     0.655    87\n",
      "surprise     0.654     0.641     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.652     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.254148 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.645875  [ 1200/ 4873]\n",
      "loss: 0.186827  [ 2400/ 4873]\n",
      "loss: 0.790092  [ 3600/ 4873]\n",
      "loss: 0.563469  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.765     0.619    105\n",
      " disgust     0.670     0.755     0.706    109\n",
      "    fear     0.670     0.600     0.525    80\n",
      "   happy     0.670     0.504     0.741    81\n",
      " neutral     0.670     0.725     0.786    84\n",
      "     sad     0.670     0.734     0.667    87\n",
      "surprise     0.670     0.641     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.675     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.282163 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.639696  [ 1200/ 4873]\n",
      "loss: 0.357818  [ 2400/ 4873]\n",
      "loss: 0.468286  [ 3600/ 4873]\n",
      "loss: 0.277052  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.704     0.657    105\n",
      " disgust     0.659     0.785     0.670    109\n",
      "    fear     0.659     0.706     0.450    80\n",
      "   happy     0.659     0.568     0.667    81\n",
      " neutral     0.659     0.667     0.762    84\n",
      "     sad     0.659     0.600     0.724    87\n",
      "surprise     0.659     0.597     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.661     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.243954 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.873888  [ 1200/ 4873]\n",
      "loss: 0.782938  [ 2400/ 4873]\n",
      "loss: 0.554381  [ 3600/ 4873]\n",
      "loss: 1.164010  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.630     0.718     0.581    105\n",
      " disgust     0.630     0.740     0.679    109\n",
      "    fear     0.630     0.552     0.463    80\n",
      "   happy     0.630     0.446     0.716    81\n",
      " neutral     0.630     0.735     0.726    84\n",
      "     sad     0.630     0.606     0.655    87\n",
      "surprise     0.630     0.706     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.630     0.643     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.354741 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.877417  [ 1200/ 4873]\n",
      "loss: 0.631950  [ 2400/ 4873]\n",
      "loss: 1.084399  [ 3600/ 4873]\n",
      "loss: 0.140716  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.817     0.552    105\n",
      " disgust     0.641     0.694     0.688    109\n",
      "    fear     0.641     0.533     0.500    80\n",
      "   happy     0.641     0.479     0.716    81\n",
      " neutral     0.641     0.700     0.750    84\n",
      "     sad     0.641     0.651     0.644    87\n",
      "surprise     0.641     0.695     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.653     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.281372 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.594270  [ 1200/ 4873]\n",
      "loss: 0.366399  [ 2400/ 4873]\n",
      "loss: 0.955307  [ 3600/ 4873]\n",
      "loss: 0.740966  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.778     0.600    105\n",
      " disgust     0.649     0.720     0.706    109\n",
      "    fear     0.649     0.583     0.438    80\n",
      "   happy     0.649     0.504     0.704    81\n",
      " neutral     0.649     0.705     0.738    84\n",
      "     sad     0.649     0.619     0.690    87\n",
      "surprise     0.649     0.656     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.652     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.253924 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 1.071767  [ 1200/ 4873]\n",
      "loss: 0.244162  [ 2400/ 4873]\n",
      "loss: 1.331556  [ 3600/ 4873]\n",
      "loss: 0.810557  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.739     0.619    105\n",
      " disgust     0.662     0.788     0.716    109\n",
      "    fear     0.662     0.554     0.512    80\n",
      "   happy     0.662     0.500     0.716    81\n",
      " neutral     0.662     0.859     0.726    84\n",
      "     sad     0.662     0.573     0.724    87\n",
      "surprise     0.662     0.731     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.678     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.260995 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.403676  [ 1200/ 4873]\n",
      "loss: 0.994065  [ 2400/ 4873]\n",
      "loss: 0.578505  [ 3600/ 4873]\n",
      "loss: 0.588768  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.764     0.648    105\n",
      " disgust     0.664     0.762     0.706    109\n",
      "    fear     0.664     0.581     0.450    80\n",
      "   happy     0.664     0.517     0.741    81\n",
      " neutral     0.664     0.765     0.738    84\n",
      "     sad     0.664     0.583     0.690    87\n",
      "surprise     0.664     0.724     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.671     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.250070 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.437648  [ 1200/ 4873]\n",
      "loss: 0.132743  [ 2400/ 4873]\n",
      "loss: 0.400838  [ 3600/ 4873]\n",
      "loss: 0.303368  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.798     0.638    105\n",
      " disgust     0.684     0.780     0.780    109\n",
      "    fear     0.684     0.560     0.525    80\n",
      "   happy     0.684     0.561     0.790    81\n",
      " neutral     0.684     0.824     0.726    84\n",
      "     sad     0.684     0.615     0.770    87\n",
      "surprise     0.684     0.689     0.484    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.690     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.247632 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.171252  [ 1200/ 4873]\n",
      "loss: 0.814521  [ 2400/ 4873]\n",
      "loss: 0.372377  [ 3600/ 4873]\n",
      "loss: 0.287462  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.717     0.629    105\n",
      " disgust     0.657     0.721     0.688    109\n",
      "    fear     0.657     0.562     0.450    80\n",
      "   happy     0.657     0.556     0.741    81\n",
      " neutral     0.657     0.713     0.738    84\n",
      "     sad     0.657     0.648     0.678    87\n",
      "surprise     0.657     0.672     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.656     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.293894 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.164194  [ 1200/ 4873]\n",
      "loss: 0.355812  [ 2400/ 4873]\n",
      "loss: 0.667306  [ 3600/ 4873]\n",
      "loss: 0.782213  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.690     0.657    105\n",
      " disgust     0.662     0.752     0.752    109\n",
      "    fear     0.662     0.630     0.425    80\n",
      "   happy     0.662     0.592     0.716    81\n",
      " neutral     0.662     0.646     0.762    84\n",
      "     sad     0.662     0.596     0.678    87\n",
      "surprise     0.662     0.745     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.664     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.231861 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.570575  [ 1200/ 4873]\n",
      "loss: 0.712834  [ 2400/ 4873]\n",
      "loss: 0.184245  [ 3600/ 4873]\n",
      "loss: 0.598535  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.756     0.590    105\n",
      " disgust     0.674     0.700     0.771    109\n",
      "    fear     0.674     0.655     0.450    80\n",
      "   happy     0.674     0.524     0.815    81\n",
      " neutral     0.674     0.750     0.750    84\n",
      "     sad     0.674     0.685     0.701    87\n",
      "surprise     0.674     0.722     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.685     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.281338 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.306150  [ 1200/ 4873]\n",
      "loss: 0.172117  [ 2400/ 4873]\n",
      "loss: 0.855663  [ 3600/ 4873]\n",
      "loss: 0.614685  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.769     0.667    105\n",
      " disgust     0.684     0.762     0.734    109\n",
      "    fear     0.684     0.597     0.463    80\n",
      "   happy     0.684     0.598     0.790    81\n",
      " neutral     0.684     0.688     0.786    84\n",
      "     sad     0.684     0.625     0.690    87\n",
      "surprise     0.684     0.755     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.685     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.247836 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.580443  [ 1200/ 4873]\n",
      "loss: 0.080743  [ 2400/ 4873]\n",
      "loss: 0.729149  [ 3600/ 4873]\n",
      "loss: 0.345252  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.742     0.657    105\n",
      " disgust     0.672     0.765     0.716    109\n",
      "    fear     0.672     0.660     0.412    80\n",
      "   happy     0.672     0.492     0.778    81\n",
      " neutral     0.672     0.823     0.774    84\n",
      "     sad     0.672     0.620     0.713    87\n",
      "surprise     0.672     0.690     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.684     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.204350 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.670700  [ 1200/ 4873]\n",
      "loss: 0.634772  [ 2400/ 4873]\n",
      "loss: 0.727874  [ 3600/ 4873]\n",
      "loss: 0.360771  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.846     0.629    105\n",
      " disgust     0.648     0.769     0.734    109\n",
      "    fear     0.648     0.571     0.450    80\n",
      "   happy     0.648     0.470     0.667    81\n",
      " neutral     0.648     0.610     0.762    84\n",
      "     sad     0.648     0.622     0.701    87\n",
      "surprise     0.648     0.723     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.659     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.294657 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.749511  [ 1200/ 4873]\n",
      "loss: 0.731310  [ 2400/ 4873]\n",
      "loss: 0.115340  [ 3600/ 4873]\n",
      "loss: 0.568310  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.800     0.610    105\n",
      " disgust     0.670     0.691     0.780    109\n",
      "    fear     0.670     0.559     0.475    80\n",
      "   happy     0.670     0.545     0.753    81\n",
      " neutral     0.670     0.780     0.762    84\n",
      "     sad     0.670     0.652     0.667    87\n",
      "surprise     0.670     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.675     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.216913 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.323643  [ 1200/ 4873]\n",
      "loss: 0.424167  [ 2400/ 4873]\n",
      "loss: 0.269353  [ 3600/ 4873]\n",
      "loss: 0.826098  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.782     0.581    105\n",
      " disgust     0.672     0.792     0.697    109\n",
      "    fear     0.672     0.600     0.525    80\n",
      "   happy     0.672     0.530     0.765    81\n",
      " neutral     0.672     0.705     0.798    84\n",
      "     sad     0.672     0.645     0.690    87\n",
      "surprise     0.672     0.689     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.678     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.302517 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.327885  [ 1200/ 4873]\n",
      "loss: 0.275028  [ 2400/ 4873]\n",
      "loss: 0.735683  [ 3600/ 4873]\n",
      "loss: 0.572574  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.798     0.638    105\n",
      " disgust     0.662     0.745     0.752    109\n",
      "    fear     0.662     0.567     0.425    80\n",
      "   happy     0.662     0.518     0.728    81\n",
      " neutral     0.662     0.722     0.774    84\n",
      "     sad     0.662     0.598     0.632    87\n",
      "surprise     0.662     0.700     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.664     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.300150 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.601623  [ 1200/ 4873]\n",
      "loss: 0.157330  [ 2400/ 4873]\n",
      "loss: 0.198456  [ 3600/ 4873]\n",
      "loss: 0.636621  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.818     0.600    105\n",
      " disgust     0.646     0.768     0.697    109\n",
      "    fear     0.646     0.521     0.475    80\n",
      "   happy     0.646     0.500     0.691    81\n",
      " neutral     0.646     0.691     0.798    84\n",
      "     sad     0.646     0.560     0.701    87\n",
      "surprise     0.646     0.767     0.516    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.661     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.299104 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.824817  [ 1200/ 4873]\n",
      "loss: 0.294473  [ 2400/ 4873]\n",
      "loss: 0.571338  [ 3600/ 4873]\n",
      "loss: 1.281433  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.753     0.638    105\n",
      " disgust     0.677     0.736     0.716    109\n",
      "    fear     0.677     0.591     0.487    80\n",
      "   happy     0.677     0.577     0.741    81\n",
      " neutral     0.677     0.705     0.798    84\n",
      "     sad     0.677     0.660     0.736    87\n",
      "surprise     0.677     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.677     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.249865 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.483398  [ 1200/ 4873]\n",
      "loss: 0.585537  [ 2400/ 4873]\n",
      "loss: 0.336522  [ 3600/ 4873]\n",
      "loss: 1.209793  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.734     0.657    105\n",
      " disgust     0.667     0.776     0.697    109\n",
      "    fear     0.667     0.672     0.487    80\n",
      "   happy     0.667     0.496     0.728    81\n",
      " neutral     0.667     0.739     0.774    84\n",
      "     sad     0.667     0.685     0.701    87\n",
      "surprise     0.667     0.594     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.671     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.280085 \n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "loss: 0.870480  [ 1200/ 4873]\n",
      "loss: 0.336671  [ 2400/ 4873]\n",
      "loss: 0.612424  [ 3600/ 4873]\n",
      "loss: 0.313561  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.753     0.667    105\n",
      " disgust     0.669     0.743     0.716    109\n",
      "    fear     0.669     0.632     0.450    80\n",
      "   happy     0.669     0.643     0.667    81\n",
      " neutral     0.669     0.642     0.833    84\n",
      "     sad     0.669     0.620     0.655    87\n",
      "surprise     0.669     0.614     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.664     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.237023 \n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "loss: 0.385014  [ 1200/ 4873]\n",
      "loss: 0.905993  [ 2400/ 4873]\n",
      "loss: 0.780418  [ 3600/ 4873]\n",
      "loss: 0.434586  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.692     0.600    105\n",
      " disgust     0.646     0.710     0.697    109\n",
      "    fear     0.646     0.607     0.425    80\n",
      "   happy     0.646     0.532     0.716    81\n",
      " neutral     0.646     0.780     0.762    84\n",
      "     sad     0.646     0.551     0.678    87\n",
      "surprise     0.646     0.690     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.652     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.307262 \n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "loss: 0.354871  [ 1200/ 4873]\n",
      "loss: 0.335366  [ 2400/ 4873]\n",
      "loss: 0.469827  [ 3600/ 4873]\n",
      "loss: 0.840965  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.800     0.571    105\n",
      " disgust     0.633     0.791     0.661    109\n",
      "    fear     0.633     0.506     0.525    80\n",
      "   happy     0.633     0.447     0.728    81\n",
      " neutral     0.633     0.753     0.726    84\n",
      "     sad     0.633     0.586     0.667    87\n",
      "surprise     0.633     0.694     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.654     0.630    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.345903 \n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "loss: 0.400730  [ 1200/ 4873]\n",
      "loss: 0.562420  [ 2400/ 4873]\n",
      "loss: 0.450150  [ 3600/ 4873]\n",
      "loss: 0.676738  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.700     0.667    105\n",
      " disgust     0.652     0.759     0.752    109\n",
      "    fear     0.652     0.673     0.412    80\n",
      "   happy     0.652     0.553     0.704    81\n",
      " neutral     0.652     0.714     0.714    84\n",
      "     sad     0.652     0.556     0.690    87\n",
      "surprise     0.652     0.621     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.654     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.240294 \n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "loss: 0.392962  [ 1200/ 4873]\n",
      "loss: 0.945454  [ 2400/ 4873]\n",
      "loss: 0.430373  [ 3600/ 4873]\n",
      "loss: 0.289147  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.819     0.648    105\n",
      " disgust     0.656     0.729     0.642    109\n",
      "    fear     0.656     0.660     0.412    80\n",
      "   happy     0.656     0.541     0.654    81\n",
      " neutral     0.656     0.677     0.798    84\n",
      "     sad     0.656     0.550     0.759    87\n",
      "surprise     0.656     0.672     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.664     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.246654 \n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "loss: 0.281800  [ 1200/ 4873]\n",
      "loss: 0.381478  [ 2400/ 4873]\n",
      "loss: 0.415003  [ 3600/ 4873]\n",
      "loss: 0.512979  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.785     0.590    105\n",
      " disgust     0.669     0.750     0.716    109\n",
      "    fear     0.669     0.684     0.487    80\n",
      "   happy     0.669     0.531     0.741    81\n",
      " neutral     0.669     0.783     0.774    84\n",
      "     sad     0.669     0.594     0.724    87\n",
      "surprise     0.669     0.603     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.676     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.260682 \n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "loss: 0.188700  [ 1200/ 4873]\n",
      "loss: 0.396662  [ 2400/ 4873]\n",
      "loss: 0.276418  [ 3600/ 4873]\n",
      "loss: 1.061626  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.759     0.600    105\n",
      " disgust     0.662     0.726     0.706    109\n",
      "    fear     0.662     0.597     0.463    80\n",
      "   happy     0.662     0.538     0.704    81\n",
      " neutral     0.662     0.790     0.762    84\n",
      "     sad     0.662     0.610     0.736    87\n",
      "surprise     0.662     0.627     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.664     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.210703 \n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "loss: 0.281357  [ 1200/ 4873]\n",
      "loss: 0.258626  [ 2400/ 4873]\n",
      "loss: 0.858296  [ 3600/ 4873]\n",
      "loss: 0.604633  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.753     0.581    105\n",
      " disgust     0.670     0.766     0.752    109\n",
      "    fear     0.670     0.623     0.475    80\n",
      "   happy     0.670     0.557     0.728    81\n",
      " neutral     0.670     0.840     0.750    84\n",
      "     sad     0.670     0.535     0.793    87\n",
      "surprise     0.670     0.725     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.686     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.215012 \n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "loss: 0.329163  [ 1200/ 4873]\n",
      "loss: 0.570487  [ 2400/ 4873]\n",
      "loss: 0.976904  [ 3600/ 4873]\n",
      "loss: 0.660148  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.737     0.695    105\n",
      " disgust     0.670     0.765     0.716    109\n",
      "    fear     0.670     0.593     0.400    80\n",
      "   happy     0.670     0.528     0.704    81\n",
      " neutral     0.670     0.699     0.774    84\n",
      "     sad     0.670     0.646     0.736    87\n",
      "surprise     0.670     0.727     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.671     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.244766 \n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "loss: 0.381937  [ 1200/ 4873]\n",
      "loss: 0.478271  [ 2400/ 4873]\n",
      "loss: 0.457656  [ 3600/ 4873]\n",
      "loss: 0.681602  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.833     0.667    105\n",
      " disgust     0.687     0.714     0.734    109\n",
      "    fear     0.687     0.549     0.562    80\n",
      "   happy     0.687     0.583     0.741    81\n",
      " neutral     0.687     0.808     0.750    84\n",
      "     sad     0.687     0.641     0.759    87\n",
      "surprise     0.687     0.729     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.694     0.680    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.292349 \n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "loss: 0.700132  [ 1200/ 4873]\n",
      "loss: 0.260502  [ 2400/ 4873]\n",
      "loss: 0.607997  [ 3600/ 4873]\n",
      "loss: 1.014980  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.800     0.533    105\n",
      " disgust     0.662     0.760     0.725    109\n",
      "    fear     0.662     0.557     0.550    80\n",
      "   happy     0.662     0.474     0.790    81\n",
      " neutral     0.662     0.840     0.750    84\n",
      "     sad     0.662     0.714     0.690    87\n",
      "surprise     0.662     0.603     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.678     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.329337 \n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "loss: 0.213527  [ 1200/ 4873]\n",
      "loss: 0.871364  [ 2400/ 4873]\n",
      "loss: 0.673592  [ 3600/ 4873]\n",
      "loss: 0.361098  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.797     0.600    105\n",
      " disgust     0.656     0.713     0.706    109\n",
      "    fear     0.656     0.556     0.500    80\n",
      "   happy     0.656     0.512     0.765    81\n",
      " neutral     0.656     0.759     0.750    84\n",
      "     sad     0.656     0.625     0.632    87\n",
      "surprise     0.656     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.663     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.304498 \n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "loss: 0.501158  [ 1200/ 4873]\n",
      "loss: 0.285312  [ 2400/ 4873]\n",
      "loss: 0.513434  [ 3600/ 4873]\n",
      "loss: 0.595121  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.710     0.676    105\n",
      " disgust     0.664     0.604     0.771    109\n",
      "    fear     0.664     0.639     0.487    80\n",
      "   happy     0.664     0.583     0.691    81\n",
      " neutral     0.664     0.855     0.702    84\n",
      "     sad     0.664     0.632     0.632    87\n",
      "surprise     0.664     0.707     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.676     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.317032 \n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "loss: 0.804266  [ 1200/ 4873]\n",
      "loss: 0.881630  [ 2400/ 4873]\n",
      "loss: 0.322587  [ 3600/ 4873]\n",
      "loss: 0.954882  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.744     0.610    105\n",
      " disgust     0.651     0.713     0.661    109\n",
      "    fear     0.651     0.585     0.475    80\n",
      "   happy     0.651     0.541     0.728    81\n",
      " neutral     0.651     0.722     0.774    84\n",
      "     sad     0.651     0.691     0.644    87\n",
      "surprise     0.651     0.551     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.650     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.303562 \n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "loss: 0.463263  [ 1200/ 4873]\n",
      "loss: 0.682922  [ 2400/ 4873]\n",
      "loss: 0.979564  [ 3600/ 4873]\n",
      "loss: 0.116385  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.780     0.610    105\n",
      " disgust     0.662     0.707     0.752    109\n",
      "    fear     0.662     0.562     0.512    80\n",
      "   happy     0.662     0.513     0.728    81\n",
      " neutral     0.662     0.836     0.726    84\n",
      "     sad     0.662     0.573     0.678    87\n",
      "surprise     0.662     0.792     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.680     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.331626 \n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "loss: 0.235741  [ 1200/ 4873]\n",
      "loss: 0.484441  [ 2400/ 4873]\n",
      "loss: 0.304912  [ 3600/ 4873]\n",
      "loss: 1.147740  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.750     0.600    105\n",
      " disgust     0.649     0.717     0.743    109\n",
      "    fear     0.649     0.536     0.463    80\n",
      "   happy     0.649     0.514     0.704    81\n",
      " neutral     0.649     0.792     0.726    84\n",
      "     sad     0.649     0.567     0.632    87\n",
      "surprise     0.649     0.712     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.655     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.316858 \n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "loss: 0.756611  [ 1200/ 4873]\n",
      "loss: 0.498793  [ 2400/ 4873]\n",
      "loss: 0.431279  [ 3600/ 4873]\n",
      "loss: 0.317922  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.797     0.562    105\n",
      " disgust     0.651     0.684     0.734    109\n",
      "    fear     0.651     0.553     0.525    80\n",
      "   happy     0.651     0.530     0.753    81\n",
      " neutral     0.651     0.817     0.690    84\n",
      "     sad     0.651     0.578     0.678    87\n",
      "surprise     0.651     0.691     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.664     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.304049 \n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "loss: 0.885494  [ 1200/ 4873]\n",
      "loss: 0.350647  [ 2400/ 4873]\n",
      "loss: 1.023872  [ 3600/ 4873]\n",
      "loss: 1.120895  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.800     0.610    105\n",
      " disgust     0.664     0.706     0.706    109\n",
      "    fear     0.664     0.639     0.487    80\n",
      "   happy     0.664     0.553     0.704    81\n",
      " neutral     0.664     0.787     0.750    84\n",
      "     sad     0.664     0.556     0.747    87\n",
      "surprise     0.664     0.667     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.673     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.278097 \n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "loss: 0.875038  [ 1200/ 4873]\n",
      "loss: 0.319970  [ 2400/ 4873]\n",
      "loss: 0.804037  [ 3600/ 4873]\n",
      "loss: 0.290554  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.758     0.657    105\n",
      " disgust     0.664     0.700     0.706    109\n",
      "    fear     0.664     0.562     0.450    80\n",
      "   happy     0.664     0.640     0.704    81\n",
      " neutral     0.664     0.674     0.738    84\n",
      "     sad     0.664     0.555     0.701    87\n",
      "surprise     0.664     0.796     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.669     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.318843 \n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "loss: 0.658446  [ 1200/ 4873]\n",
      "loss: 0.330549  [ 2400/ 4873]\n",
      "loss: 0.395672  [ 3600/ 4873]\n",
      "loss: 0.319121  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.790     0.610    105\n",
      " disgust     0.662     0.739     0.752    109\n",
      "    fear     0.662     0.557     0.487    80\n",
      "   happy     0.662     0.500     0.728    81\n",
      " neutral     0.662     0.787     0.702    84\n",
      "     sad     0.662     0.585     0.713    87\n",
      "surprise     0.662     0.796     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.679     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.227294 \n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "loss: 0.318919  [ 1200/ 4873]\n",
      "loss: 0.555054  [ 2400/ 4873]\n",
      "loss: 0.295237  [ 3600/ 4873]\n",
      "loss: 0.440805  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.823     0.619    105\n",
      " disgust     0.667     0.702     0.734    109\n",
      "    fear     0.667     0.613     0.475    80\n",
      "   happy     0.667     0.527     0.716    81\n",
      " neutral     0.667     0.718     0.726    84\n",
      "     sad     0.667     0.612     0.724    87\n",
      "surprise     0.667     0.737     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.676     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.295652 \n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "loss: 0.564087  [ 1200/ 4873]\n",
      "loss: 0.311525  [ 2400/ 4873]\n",
      "loss: 0.288006  [ 3600/ 4873]\n",
      "loss: 0.682591  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.707     0.667    105\n",
      " disgust     0.662     0.699     0.725    109\n",
      "    fear     0.662     0.661     0.463    80\n",
      "   happy     0.662     0.589     0.691    81\n",
      " neutral     0.662     0.721     0.738    84\n",
      "     sad     0.662     0.553     0.724    87\n",
      "surprise     0.662     0.787     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.674     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.226234 \n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "loss: 1.079198  [ 1200/ 4873]\n",
      "loss: 0.366729  [ 2400/ 4873]\n",
      "loss: 1.407821  [ 3600/ 4873]\n",
      "loss: 0.596404  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.680     0.648    105\n",
      " disgust     0.669     0.724     0.697    109\n",
      "    fear     0.669     0.646     0.525    80\n",
      "   happy     0.669     0.542     0.716    81\n",
      " neutral     0.669     0.742     0.786    84\n",
      "     sad     0.669     0.667     0.736    87\n",
      "surprise     0.669     0.708     0.531    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.673     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.247369 \n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "loss: 0.416007  [ 1200/ 4873]\n",
      "loss: 0.565314  [ 2400/ 4873]\n",
      "loss: 0.675488  [ 3600/ 4873]\n",
      "loss: 0.345017  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.696     0.676    105\n",
      " disgust     0.672     0.717     0.743    109\n",
      "    fear     0.672     0.654     0.425    80\n",
      "   happy     0.672     0.629     0.691    81\n",
      " neutral     0.672     0.703     0.762    84\n",
      "     sad     0.672     0.596     0.678    87\n",
      "surprise     0.672     0.703     0.703    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.671     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.266252 \n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "loss: 0.763608  [ 1200/ 4873]\n",
      "loss: 1.337930  [ 2400/ 4873]\n",
      "loss: 0.882234  [ 3600/ 4873]\n",
      "loss: 0.449528  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.719     0.657    105\n",
      " disgust     0.666     0.653     0.743    109\n",
      "    fear     0.666     0.667     0.475    80\n",
      "   happy     0.666     0.598     0.679    81\n",
      " neutral     0.666     0.730     0.774    84\n",
      "     sad     0.666     0.611     0.667    87\n",
      "surprise     0.666     0.702     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.668     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.327863 \n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "loss: 0.353298  [ 1200/ 4873]\n",
      "loss: 0.301137  [ 2400/ 4873]\n",
      "loss: 0.608994  [ 3600/ 4873]\n",
      "loss: 0.342711  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.767     0.629    105\n",
      " disgust     0.661     0.722     0.716    109\n",
      "    fear     0.661     0.561     0.463    80\n",
      "   happy     0.661     0.550     0.753    81\n",
      " neutral     0.661     0.787     0.702    84\n",
      "     sad     0.661     0.570     0.701    87\n",
      "surprise     0.661     0.719     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.668     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.320937 \n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "loss: 1.108155  [ 1200/ 4873]\n",
      "loss: 0.555147  [ 2400/ 4873]\n",
      "loss: 0.322413  [ 3600/ 4873]\n",
      "loss: 0.111843  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.780     0.610    105\n",
      " disgust     0.666     0.731     0.725    109\n",
      "    fear     0.666     0.603     0.512    80\n",
      "   happy     0.666     0.518     0.728    81\n",
      " neutral     0.666     0.766     0.702    84\n",
      "     sad     0.666     0.607     0.747    87\n",
      "surprise     0.666     0.722     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.675     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.232557 \n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "loss: 0.779501  [ 1200/ 4873]\n",
      "loss: 0.574162  [ 2400/ 4873]\n",
      "loss: 0.624637  [ 3600/ 4873]\n",
      "loss: 0.215297  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.739     0.648    105\n",
      " disgust     0.670     0.717     0.743    109\n",
      "    fear     0.670     0.520     0.487    80\n",
      "   happy     0.670     0.590     0.728    81\n",
      " neutral     0.670     0.792     0.726    84\n",
      "     sad     0.670     0.598     0.701    87\n",
      "surprise     0.670     0.784     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.677     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.303358 \n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "loss: 0.332359  [ 1200/ 4873]\n",
      "loss: 0.951241  [ 2400/ 4873]\n",
      "loss: 0.591748  [ 3600/ 4873]\n",
      "loss: 0.466755  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.750     0.629    105\n",
      " disgust     0.672     0.718     0.771    109\n",
      "    fear     0.672     0.610     0.450    80\n",
      "   happy     0.672     0.564     0.765    81\n",
      " neutral     0.672     0.838     0.738    84\n",
      "     sad     0.672     0.585     0.713    87\n",
      "surprise     0.672     0.679     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.678     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.280191 \n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "loss: 0.591134  [ 1200/ 4873]\n",
      "loss: 0.161659  [ 2400/ 4873]\n",
      "loss: 0.158521  [ 3600/ 4873]\n",
      "loss: 0.235249  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.747     0.619    105\n",
      " disgust     0.669     0.711     0.743    109\n",
      "    fear     0.669     0.575     0.525    80\n",
      "   happy     0.669     0.592     0.716    81\n",
      " neutral     0.669     0.833     0.714    84\n",
      "     sad     0.669     0.542     0.747    87\n",
      "surprise     0.669     0.804     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.686     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.320764 \n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "loss: 0.360852  [ 1200/ 4873]\n",
      "loss: 0.182708  [ 2400/ 4873]\n",
      "loss: 0.578895  [ 3600/ 4873]\n",
      "loss: 0.350621  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.797     0.600    105\n",
      " disgust     0.670     0.769     0.734    109\n",
      "    fear     0.670     0.534     0.487    80\n",
      "   happy     0.670     0.552     0.716    81\n",
      " neutral     0.670     0.765     0.738    84\n",
      "     sad     0.670     0.591     0.747    87\n",
      "surprise     0.670     0.724     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.676     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.280258 \n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "loss: 0.345795  [ 1200/ 4873]\n",
      "loss: 0.228244  [ 2400/ 4873]\n",
      "loss: 0.738940  [ 3600/ 4873]\n",
      "loss: 0.171475  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.788     0.638    105\n",
      " disgust     0.646     0.695     0.670    109\n",
      "    fear     0.646     0.589     0.537    80\n",
      "   happy     0.646     0.549     0.691    81\n",
      " neutral     0.646     0.756     0.702    84\n",
      "     sad     0.646     0.521     0.701    87\n",
      "surprise     0.646     0.700     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.657     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.304739 \n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "loss: 0.467782  [ 1200/ 4873]\n",
      "loss: 0.418953  [ 2400/ 4873]\n",
      "loss: 0.890751  [ 3600/ 4873]\n",
      "loss: 0.597973  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.716     0.648    105\n",
      " disgust     0.667     0.740     0.706    109\n",
      "    fear     0.667     0.603     0.475    80\n",
      "   happy     0.667     0.602     0.765    81\n",
      " neutral     0.667     0.792     0.726    84\n",
      "     sad     0.667     0.594     0.655    87\n",
      "surprise     0.667     0.611     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.665     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.301107 \n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "loss: 0.857812  [ 1200/ 4873]\n",
      "loss: 0.799416  [ 2400/ 4873]\n",
      "loss: 0.227392  [ 3600/ 4873]\n",
      "loss: 0.724414  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.720     0.638    105\n",
      " disgust     0.652     0.741     0.761    109\n",
      "    fear     0.652     0.590     0.450    80\n",
      "   happy     0.652     0.556     0.617    81\n",
      " neutral     0.652     0.762     0.762    84\n",
      "     sad     0.652     0.538     0.724    87\n",
      "surprise     0.652     0.660     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.653     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.294292 \n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "loss: 0.253682  [ 1200/ 4873]\n",
      "loss: 0.634783  [ 2400/ 4873]\n",
      "loss: 0.412344  [ 3600/ 4873]\n",
      "loss: 0.439318  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.696     0.610    105\n",
      " disgust     0.657     0.780     0.651    109\n",
      "    fear     0.657     0.674     0.388    80\n",
      "   happy     0.657     0.539     0.765    81\n",
      " neutral     0.657     0.720     0.798    84\n",
      "     sad     0.657     0.579     0.713    87\n",
      "surprise     0.657     0.667     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.665     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.343772 \n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "loss: 0.336999  [ 1200/ 4873]\n",
      "loss: 0.663113  [ 2400/ 4873]\n",
      "loss: 0.618921  [ 3600/ 4873]\n",
      "loss: 0.654255  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.768     0.600    105\n",
      " disgust     0.657     0.694     0.706    109\n",
      "    fear     0.657     0.543     0.475    80\n",
      "   happy     0.657     0.525     0.765    81\n",
      " neutral     0.657     0.833     0.714    84\n",
      "     sad     0.657     0.663     0.632    87\n",
      "surprise     0.657     0.622     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.664     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.380533 \n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "loss: 0.390313  [ 1200/ 4873]\n",
      "loss: 0.287022  [ 2400/ 4873]\n",
      "loss: 0.268960  [ 3600/ 4873]\n",
      "loss: 0.329969  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.733     0.600    105\n",
      " disgust     0.669     0.726     0.752    109\n",
      "    fear     0.669     0.636     0.525    80\n",
      "   happy     0.669     0.611     0.716    81\n",
      " neutral     0.669     0.696     0.762    84\n",
      "     sad     0.669     0.602     0.713    87\n",
      "surprise     0.669     0.673     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.668     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.234933 \n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "loss: 0.536858  [ 1200/ 4873]\n",
      "loss: 0.268266  [ 2400/ 4873]\n",
      "loss: 0.561119  [ 3600/ 4873]\n",
      "loss: 0.771404  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.747     0.619    105\n",
      " disgust     0.675     0.696     0.734    109\n",
      "    fear     0.675     0.561     0.463    80\n",
      "   happy     0.675     0.598     0.679    81\n",
      " neutral     0.675     0.790     0.762    84\n",
      "     sad     0.675     0.631     0.805    87\n",
      "surprise     0.675     0.707     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.676     0.672    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.328610 \n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "loss: 0.573164  [ 1200/ 4873]\n",
      "loss: 1.043723  [ 2400/ 4873]\n",
      "loss: 0.845556  [ 3600/ 4873]\n",
      "loss: 0.777038  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.723     0.571    105\n",
      " disgust     0.670     0.708     0.734    109\n",
      "    fear     0.670     0.645     0.500    80\n",
      "   happy     0.670     0.541     0.815    81\n",
      " neutral     0.670     0.853     0.690    84\n",
      "     sad     0.670     0.650     0.770    87\n",
      "surprise     0.670     0.644     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.681     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.324628 \n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "loss: 0.159220  [ 1200/ 4873]\n",
      "loss: 0.761478  [ 2400/ 4873]\n",
      "loss: 0.521779  [ 3600/ 4873]\n",
      "loss: 0.891717  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.756     0.619    105\n",
      " disgust     0.680     0.817     0.697    109\n",
      "    fear     0.680     0.577     0.512    80\n",
      "   happy     0.680     0.562     0.778    81\n",
      " neutral     0.680     0.756     0.738    84\n",
      "     sad     0.680     0.699     0.667    87\n",
      "surprise     0.680     0.602     0.781    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.681     0.685    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.370663 \n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "loss: 0.397672  [ 1200/ 4873]\n",
      "loss: 0.448681  [ 2400/ 4873]\n",
      "loss: 0.505518  [ 3600/ 4873]\n",
      "loss: 0.834385  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.747     0.648    105\n",
      " disgust     0.656     0.714     0.734    109\n",
      "    fear     0.656     0.623     0.412    80\n",
      "   happy     0.656     0.564     0.654    81\n",
      " neutral     0.656     0.795     0.738    84\n",
      "     sad     0.656     0.519     0.770    87\n",
      "surprise     0.656     0.698     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.666     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.276646 \n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "loss: 0.321002  [ 1200/ 4873]\n",
      "loss: 0.645416  [ 2400/ 4873]\n",
      "loss: 0.465401  [ 3600/ 4873]\n",
      "loss: 0.541220  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.808     0.600    105\n",
      " disgust     0.672     0.735     0.761    109\n",
      "    fear     0.672     0.590     0.450    80\n",
      "   happy     0.672     0.496     0.765    81\n",
      " neutral     0.672     0.910     0.726    84\n",
      "     sad     0.672     0.610     0.701    87\n",
      "surprise     0.672     0.667     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.688     0.670    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.303364 \n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "loss: 0.542868  [ 1200/ 4873]\n",
      "loss: 0.674180  [ 2400/ 4873]\n",
      "loss: 1.071695  [ 3600/ 4873]\n",
      "loss: 0.388963  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.762     0.610    105\n",
      " disgust     0.675     0.762     0.706    109\n",
      "    fear     0.675     0.603     0.550    80\n",
      "   happy     0.675     0.538     0.778    81\n",
      " neutral     0.675     0.818     0.750    84\n",
      "     sad     0.675     0.612     0.690    87\n",
      "surprise     0.675     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.683     0.675    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.258185 \n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "loss: 0.643703  [ 1200/ 4873]\n",
      "loss: 0.317451  [ 2400/ 4873]\n",
      "loss: 0.763457  [ 3600/ 4873]\n",
      "loss: 0.258833  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.657     0.657    105\n",
      " disgust     0.690     0.682     0.807    109\n",
      "    fear     0.690     0.739     0.425    80\n",
      "   happy     0.690     0.716     0.716    81\n",
      " neutral     0.690     0.770     0.798    84\n",
      "     sad     0.690     0.583     0.770    87\n",
      "surprise     0.690     0.809     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.708     0.681    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.214729 \n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "loss: 0.429877  [ 1200/ 4873]\n",
      "loss: 0.769853  [ 2400/ 4873]\n",
      "loss: 0.211298  [ 3600/ 4873]\n",
      "loss: 0.486784  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.767     0.629    105\n",
      " disgust     0.667     0.767     0.725    109\n",
      "    fear     0.667     0.535     0.475    80\n",
      "   happy     0.667     0.541     0.728    81\n",
      " neutral     0.667     0.910     0.726    84\n",
      "     sad     0.667     0.552     0.793    87\n",
      "surprise     0.667     0.714     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.684     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.204452 \n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "loss: 0.315610  [ 1200/ 4873]\n",
      "loss: 0.839834  [ 2400/ 4873]\n",
      "loss: 0.317376  [ 3600/ 4873]\n",
      "loss: 0.984587  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.829     0.600    105\n",
      " disgust     0.670     0.716     0.761    109\n",
      "    fear     0.670     0.567     0.475    80\n",
      "   happy     0.670     0.583     0.741    81\n",
      " neutral     0.670     0.716     0.750    84\n",
      "     sad     0.670     0.575     0.747    87\n",
      "surprise     0.670     0.787     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.682     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.249694 \n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "loss: 0.839910  [ 1200/ 4873]\n",
      "loss: 1.359382  [ 2400/ 4873]\n",
      "loss: 0.846041  [ 3600/ 4873]\n",
      "loss: 0.179770  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.773     0.648    105\n",
      " disgust     0.679     0.733     0.780    109\n",
      "    fear     0.679     0.642     0.425    80\n",
      "   happy     0.679     0.541     0.728    81\n",
      " neutral     0.679     0.838     0.738    84\n",
      "     sad     0.679     0.583     0.770    87\n",
      "surprise     0.679     0.709     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.688     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.276536 \n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "loss: 0.490341  [ 1200/ 4873]\n",
      "loss: 0.394011  [ 2400/ 4873]\n",
      "loss: 0.281344  [ 3600/ 4873]\n",
      "loss: 0.527012  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.736     0.610    105\n",
      " disgust     0.670     0.798     0.725    109\n",
      "    fear     0.670     0.603     0.512    80\n",
      "   happy     0.670     0.522     0.741    81\n",
      " neutral     0.670     0.780     0.762    84\n",
      "     sad     0.670     0.598     0.736    87\n",
      "surprise     0.670     0.712     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.678     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.260155 \n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "loss: 0.510467  [ 1200/ 4873]\n",
      "loss: 0.609845  [ 2400/ 4873]\n",
      "loss: 0.580506  [ 3600/ 4873]\n",
      "loss: 0.179042  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.762     0.581    105\n",
      " disgust     0.669     0.701     0.752    109\n",
      "    fear     0.669     0.589     0.537    80\n",
      "   happy     0.669     0.583     0.741    81\n",
      " neutral     0.669     0.797     0.702    84\n",
      "     sad     0.669     0.594     0.724    87\n",
      "surprise     0.669     0.702     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.675     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.307554 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "loss: 0.554920  [ 1200/ 4873]\n",
      "loss: 0.292055  [ 2400/ 4873]\n",
      "loss: 0.281564  [ 3600/ 4873]\n",
      "loss: 0.463342  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.805     0.590    105\n",
      " disgust     0.666     0.732     0.752    109\n",
      "    fear     0.666     0.606     0.500    80\n",
      "   happy     0.666     0.526     0.741    81\n",
      " neutral     0.666     0.787     0.750    84\n",
      "     sad     0.666     0.553     0.724    87\n",
      "surprise     0.666     0.766     0.562    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.682     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.267451 \n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "loss: 0.353475  [ 1200/ 4873]\n",
      "loss: 0.435347  [ 2400/ 4873]\n",
      "loss: 0.641392  [ 3600/ 4873]\n",
      "loss: 0.087255  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.703     0.610    105\n",
      " disgust     0.682     0.763     0.679    109\n",
      "    fear     0.682     0.633     0.475    80\n",
      "   happy     0.682     0.574     0.815    81\n",
      " neutral     0.682     0.827     0.738    84\n",
      "     sad     0.682     0.685     0.724    87\n",
      "surprise     0.682     0.613     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.685     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.370291 \n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "loss: 1.009312  [ 1200/ 4873]\n",
      "loss: 0.384490  [ 2400/ 4873]\n",
      "loss: 0.591658  [ 3600/ 4873]\n",
      "loss: 0.175340  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.719     0.657    105\n",
      " disgust     0.669     0.783     0.661    109\n",
      "    fear     0.669     0.556     0.500    80\n",
      "   happy     0.669     0.539     0.765    81\n",
      " neutral     0.669     0.790     0.762    84\n",
      "     sad     0.669     0.638     0.690    87\n",
      "surprise     0.669     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.673     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.300573 \n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "loss: 0.421685  [ 1200/ 4873]\n",
      "loss: 0.557306  [ 2400/ 4873]\n",
      "loss: 0.485860  [ 3600/ 4873]\n",
      "loss: 1.224294  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.739     0.619    105\n",
      " disgust     0.646     0.843     0.688    109\n",
      "    fear     0.646     0.586     0.425    80\n",
      "   happy     0.646     0.513     0.716    81\n",
      " neutral     0.646     0.586     0.774    84\n",
      "     sad     0.646     0.606     0.690    87\n",
      "surprise     0.646     0.712     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.655     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.320338 \n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "loss: 0.292018  [ 1200/ 4873]\n",
      "loss: 0.357737  [ 2400/ 4873]\n",
      "loss: 0.173709  [ 3600/ 4873]\n",
      "loss: 0.067600  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.741     0.600    105\n",
      " disgust     0.666     0.704     0.697    109\n",
      "    fear     0.666     0.625     0.438    80\n",
      "   happy     0.666     0.523     0.716    81\n",
      " neutral     0.666     0.793     0.774    84\n",
      "     sad     0.666     0.600     0.793    87\n",
      "surprise     0.666     0.755     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.677     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.323165 \n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "loss: 0.204313  [ 1200/ 4873]\n",
      "loss: 0.352588  [ 2400/ 4873]\n",
      "loss: 0.184959  [ 3600/ 4873]\n",
      "loss: 0.248311  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.759     0.629    105\n",
      " disgust     0.667     0.748     0.706    109\n",
      "    fear     0.667     0.552     0.463    80\n",
      "   happy     0.667     0.521     0.778    81\n",
      " neutral     0.667     0.724     0.750    84\n",
      "     sad     0.667     0.670     0.724    87\n",
      "surprise     0.667     0.745     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.674     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.263984 \n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "loss: 0.224254  [ 1200/ 4873]\n",
      "loss: 0.558920  [ 2400/ 4873]\n",
      "loss: 0.785552  [ 3600/ 4873]\n",
      "loss: 0.502911  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.732     0.676    105\n",
      " disgust     0.680     0.714     0.734    109\n",
      "    fear     0.680     0.654     0.425    80\n",
      "   happy     0.680     0.625     0.741    81\n",
      " neutral     0.680     0.727     0.762    84\n",
      "     sad     0.680     0.615     0.736    87\n",
      "surprise     0.680     0.689     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.679     0.676    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.275692 \n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "loss: 0.599321  [ 1200/ 4873]\n",
      "loss: 0.309637  [ 2400/ 4873]\n",
      "loss: 0.095672  [ 3600/ 4873]\n",
      "loss: 0.323899  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.716     0.695    105\n",
      " disgust     0.672     0.696     0.716    109\n",
      "    fear     0.672     0.588     0.500    80\n",
      "   happy     0.672     0.629     0.691    81\n",
      " neutral     0.672     0.707     0.774    84\n",
      "     sad     0.672     0.604     0.667    87\n",
      "surprise     0.672     0.784     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.675     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.267999 \n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n",
      "loss: 0.272840  [ 1200/ 4873]\n",
      "loss: 0.357773  [ 2400/ 4873]\n",
      "loss: 0.304393  [ 3600/ 4873]\n",
      "loss: 0.718678  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.741     0.600    105\n",
      " disgust     0.657     0.696     0.716    109\n",
      "    fear     0.657     0.625     0.500    80\n",
      "   happy     0.657     0.528     0.802    81\n",
      " neutral     0.657     0.797     0.702    84\n",
      "     sad     0.657     0.598     0.632    87\n",
      "surprise     0.657     0.683     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.667     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.361628 \n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "loss: 0.225170  [ 1200/ 4873]\n",
      "loss: 0.141322  [ 2400/ 4873]\n",
      "loss: 0.102003  [ 3600/ 4873]\n",
      "loss: 0.540536  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.739     0.619    105\n",
      " disgust     0.679     0.755     0.734    109\n",
      "    fear     0.679     0.594     0.475    80\n",
      "   happy     0.679     0.589     0.778    81\n",
      " neutral     0.679     0.708     0.750    84\n",
      "     sad     0.679     0.674     0.713    87\n",
      "surprise     0.679     0.672     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.676     0.677    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.267895 \n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "loss: 0.130247  [ 1200/ 4873]\n",
      "loss: 1.491128  [ 2400/ 4873]\n",
      "loss: 0.534347  [ 3600/ 4873]\n",
      "loss: 0.307709  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.782     0.648    105\n",
      " disgust     0.679     0.792     0.734    109\n",
      "    fear     0.679     0.649     0.463    80\n",
      "   happy     0.679     0.536     0.741    81\n",
      " neutral     0.679     0.756     0.774    84\n",
      "     sad     0.679     0.604     0.701    87\n",
      "surprise     0.679     0.652     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.681     0.676    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.236293 \n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "loss: 0.500439  [ 1200/ 4873]\n",
      "loss: 0.312670  [ 2400/ 4873]\n",
      "loss: 0.864698  [ 3600/ 4873]\n",
      "loss: 0.104124  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.767     0.629    105\n",
      " disgust     0.687     0.719     0.752    109\n",
      "    fear     0.687     0.619     0.487    80\n",
      "   happy     0.687     0.581     0.753    81\n",
      " neutral     0.687     0.698     0.798    84\n",
      "     sad     0.687     0.670     0.724    87\n",
      "surprise     0.687     0.788     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.692     0.683    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.245217 \n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "loss: 0.403857  [ 1200/ 4873]\n",
      "loss: 0.344143  [ 2400/ 4873]\n",
      "loss: 0.814610  [ 3600/ 4873]\n",
      "loss: 0.273849  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.707     0.619    105\n",
      " disgust     0.675     0.747     0.679    109\n",
      "    fear     0.675     0.576     0.475    80\n",
      "   happy     0.675     0.614     0.765    81\n",
      " neutral     0.675     0.765     0.738    84\n",
      "     sad     0.675     0.633     0.713    87\n",
      "surprise     0.675     0.671     0.766    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.673     0.679    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.243271 \n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "loss: 0.391885  [ 1200/ 4873]\n",
      "loss: 0.103194  [ 2400/ 4873]\n",
      "loss: 0.343967  [ 3600/ 4873]\n",
      "loss: 0.544798  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.818     0.600    105\n",
      " disgust     0.657     0.733     0.706    109\n",
      "    fear     0.657     0.542     0.562    80\n",
      "   happy     0.657     0.480     0.741    81\n",
      " neutral     0.657     0.833     0.774    84\n",
      "     sad     0.657     0.643     0.724    87\n",
      "surprise     0.657     0.636     0.438    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.669     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.254605 \n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "loss: 0.224280  [ 1200/ 4873]\n",
      "loss: 0.114414  [ 2400/ 4873]\n",
      "loss: 0.389704  [ 3600/ 4873]\n",
      "loss: 0.481790  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.692     0.717     0.724    105\n",
      " disgust     0.692     0.752     0.725    109\n",
      "    fear     0.692     0.650     0.487    80\n",
      "   happy     0.692     0.555     0.753    81\n",
      " neutral     0.692     0.812     0.774    84\n",
      "     sad     0.692     0.690     0.690    87\n",
      "surprise     0.692     0.677     0.656    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.692     0.693     0.687    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 1.218628 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep563_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep442_acc_69\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments_final//Run_Nr_0/dimred/emo_reco_best_ep563_acc_69\"! Old accuracy: 69.0, new accuracy: 69.2\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "loss: 0.299940  [ 1200/ 4873]\n",
      "loss: 0.489785  [ 2400/ 4873]\n",
      "loss: 0.214081  [ 3600/ 4873]\n",
      "loss: 0.741873  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.780     0.610    105\n",
      " disgust     0.666     0.770     0.706    109\n",
      "    fear     0.666     0.627     0.525    80\n",
      "   happy     0.666     0.486     0.642    81\n",
      " neutral     0.666     0.812     0.774    84\n",
      "     sad     0.666     0.588     0.770    87\n",
      "surprise     0.666     0.650     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.673     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.289593 \n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "loss: 0.462231  [ 1200/ 4873]\n",
      "loss: 0.314248  [ 2400/ 4873]\n",
      "loss: 0.311341  [ 3600/ 4873]\n",
      "loss: 0.742425  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.802     0.619    105\n",
      " disgust     0.666     0.705     0.725    109\n",
      "    fear     0.666     0.526     0.500    80\n",
      "   happy     0.666     0.569     0.765    81\n",
      " neutral     0.666     0.797     0.702    84\n",
      "     sad     0.666     0.655     0.632    87\n",
      "surprise     0.666     0.622     0.719    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.668     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.425048 \n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "loss: 0.402313  [ 1200/ 4873]\n",
      "loss: 0.208158  [ 2400/ 4873]\n",
      "loss: 0.280320  [ 3600/ 4873]\n",
      "loss: 0.330841  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.793     0.619    105\n",
      " disgust     0.667     0.667     0.752    109\n",
      "    fear     0.667     0.613     0.475    80\n",
      "   happy     0.667     0.553     0.778    81\n",
      " neutral     0.667     0.762     0.762    84\n",
      "     sad     0.667     0.655     0.632    87\n",
      "surprise     0.667     0.656     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.671     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.355212 \n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "loss: 0.458497  [ 1200/ 4873]\n",
      "loss: 0.555829  [ 2400/ 4873]\n",
      "loss: 0.077297  [ 3600/ 4873]\n",
      "loss: 0.085719  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.721     0.590    105\n",
      " disgust     0.633     0.714     0.734    109\n",
      "    fear     0.633     0.571     0.450    80\n",
      "   happy     0.633     0.523     0.716    81\n",
      " neutral     0.633     0.765     0.738    84\n",
      "     sad     0.633     0.582     0.655    87\n",
      "surprise     0.633     0.525     0.484    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.629     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.292369 \n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "loss: 0.244943  [ 1200/ 4873]\n",
      "loss: 0.361178  [ 2400/ 4873]\n",
      "loss: 0.604607  [ 3600/ 4873]\n",
      "loss: 0.601035  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.747     0.562    105\n",
      " disgust     0.648     0.738     0.697    109\n",
      "    fear     0.648     0.545     0.525    80\n",
      "   happy     0.648     0.514     0.704    81\n",
      " neutral     0.648     0.756     0.738    84\n",
      "     sad     0.648     0.596     0.678    87\n",
      "surprise     0.648     0.678     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.653     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.348031 \n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "loss: 0.465495  [ 1200/ 4873]\n",
      "loss: 0.624519  [ 2400/ 4873]\n",
      "loss: 0.440501  [ 3600/ 4873]\n",
      "loss: 0.620775  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.660     0.648    105\n",
      " disgust     0.661     0.798     0.651    109\n",
      "    fear     0.661     0.667     0.475    80\n",
      "   happy     0.661     0.590     0.765    81\n",
      " neutral     0.661     0.716     0.750    84\n",
      "     sad     0.661     0.622     0.701    87\n",
      "surprise     0.661     0.571     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.661     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.289759 \n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "loss: 0.154477  [ 1200/ 4873]\n",
      "loss: 0.079356  [ 2400/ 4873]\n",
      "loss: 0.224388  [ 3600/ 4873]\n",
      "loss: 0.671143  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.793     0.619    105\n",
      " disgust     0.670     0.732     0.752    109\n",
      "    fear     0.670     0.607     0.463    80\n",
      "   happy     0.670     0.531     0.741    81\n",
      " neutral     0.670     0.759     0.750    84\n",
      "     sad     0.670     0.617     0.667    87\n",
      "surprise     0.670     0.677     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.674     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.261219 \n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "loss: 0.304020  [ 1200/ 4873]\n",
      "loss: 0.418397  [ 2400/ 4873]\n",
      "loss: 0.699326  [ 3600/ 4873]\n",
      "loss: 0.481928  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.782     0.581    105\n",
      " disgust     0.661     0.701     0.752    109\n",
      "    fear     0.661     0.567     0.475    80\n",
      "   happy     0.661     0.541     0.741    81\n",
      " neutral     0.661     0.747     0.738    84\n",
      "     sad     0.661     0.625     0.690    87\n",
      "surprise     0.661     0.690     0.625    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.665     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.394518 \n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "loss: 0.144344  [ 1200/ 4873]\n",
      "loss: 0.810347  [ 2400/ 4873]\n",
      "loss: 0.316169  [ 3600/ 4873]\n",
      "loss: 0.415213  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.750     0.600    105\n",
      " disgust     0.659     0.745     0.725    109\n",
      "    fear     0.659     0.643     0.450    80\n",
      "   happy     0.659     0.512     0.765    81\n",
      " neutral     0.659     0.735     0.726    84\n",
      "     sad     0.659     0.588     0.690    87\n",
      "surprise     0.659     0.707     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.669     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.277419 \n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "loss: 0.423940  [ 1200/ 4873]\n",
      "loss: 0.153746  [ 2400/ 4873]\n",
      "loss: 0.277278  [ 3600/ 4873]\n",
      "loss: 0.408988  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.783     0.619    105\n",
      " disgust     0.680     0.764     0.743    109\n",
      "    fear     0.680     0.645     0.500    80\n",
      "   happy     0.680     0.521     0.765    81\n",
      " neutral     0.680     0.787     0.750    84\n",
      "     sad     0.680     0.625     0.747    87\n",
      "surprise     0.680     0.696     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.689     0.676    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.320672 \n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "loss: 0.564331  [ 1200/ 4873]\n",
      "loss: 0.428459  [ 2400/ 4873]\n",
      "loss: 0.270139  [ 3600/ 4873]\n",
      "loss: 0.391289  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.780     0.610    105\n",
      " disgust     0.662     0.703     0.761    109\n",
      "    fear     0.662     0.548     0.425    80\n",
      "   happy     0.662     0.565     0.753    81\n",
      " neutral     0.662     0.822     0.714    84\n",
      "     sad     0.662     0.546     0.747    87\n",
      "surprise     0.662     0.771     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.677     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.300007 \n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "loss: 0.320697  [ 1200/ 4873]\n",
      "loss: 0.771948  [ 2400/ 4873]\n",
      "loss: 0.948891  [ 3600/ 4873]\n",
      "loss: 0.482437  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.737     0.667    105\n",
      " disgust     0.667     0.727     0.734    109\n",
      "    fear     0.667     0.600     0.450    80\n",
      "   happy     0.667     0.568     0.667    81\n",
      " neutral     0.667     0.771     0.762    84\n",
      "     sad     0.667     0.586     0.747    87\n",
      "surprise     0.667     0.679     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.667     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.291429 \n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "loss: 0.151371  [ 1200/ 4873]\n",
      "loss: 0.314847  [ 2400/ 4873]\n",
      "loss: 0.453245  [ 3600/ 4873]\n",
      "loss: 0.516418  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.692     0.686    105\n",
      " disgust     0.666     0.798     0.688    109\n",
      "    fear     0.666     0.625     0.438    80\n",
      "   happy     0.666     0.514     0.704    81\n",
      " neutral     0.666     0.725     0.786    84\n",
      "     sad     0.666     0.690     0.690    87\n",
      "surprise     0.666     0.612     0.641    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.665     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.271120 \n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "loss: 0.782660  [ 1200/ 4873]\n",
      "loss: 0.312180  [ 2400/ 4873]\n",
      "loss: 0.571444  [ 3600/ 4873]\n",
      "loss: 0.632480  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.797     0.600    105\n",
      " disgust     0.669     0.768     0.697    109\n",
      "    fear     0.669     0.590     0.450    80\n",
      "   happy     0.669     0.500     0.778    81\n",
      " neutral     0.669     0.765     0.774    84\n",
      "     sad     0.669     0.614     0.713    87\n",
      "surprise     0.669     0.729     0.672    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.680     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.392687 \n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "loss: 0.492865  [ 1200/ 4873]\n",
      "loss: 0.337589  [ 2400/ 4873]\n",
      "loss: 0.285051  [ 3600/ 4873]\n",
      "loss: 0.503872  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.739     0.619    105\n",
      " disgust     0.651     0.743     0.716    109\n",
      "    fear     0.651     0.556     0.438    80\n",
      "   happy     0.651     0.496     0.765    81\n",
      " neutral     0.651     0.753     0.762    84\n",
      "     sad     0.651     0.659     0.621    87\n",
      "surprise     0.651     0.629     0.609    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.653     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.328707 \n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "loss: 0.574803  [ 1200/ 4873]\n",
      "loss: 0.267894  [ 2400/ 4873]\n",
      "loss: 0.830635  [ 3600/ 4873]\n",
      "loss: 0.294433  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.734     0.657    105\n",
      " disgust     0.674     0.685     0.679    109\n",
      "    fear     0.674     0.621     0.450    80\n",
      "   happy     0.674     0.579     0.765    81\n",
      " neutral     0.674     0.800     0.762    84\n",
      "     sad     0.674     0.639     0.713    87\n",
      "surprise     0.674     0.667     0.688    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.675     0.673    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.419561 \n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "loss: 0.741819  [ 1200/ 4873]\n",
      "loss: 0.131144  [ 2400/ 4873]\n",
      "loss: 0.333314  [ 3600/ 4873]\n",
      "loss: 0.324756  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.775     0.590    105\n",
      " disgust     0.654     0.717     0.743    109\n",
      "    fear     0.654     0.569     0.463    80\n",
      "   happy     0.654     0.509     0.716    81\n",
      " neutral     0.654     0.747     0.738    84\n",
      "     sad     0.654     0.587     0.701    87\n",
      "surprise     0.654     0.745     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.664     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.366122 \n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "loss: 0.227376  [ 1200/ 4873]\n",
      "loss: 0.169266  [ 2400/ 4873]\n",
      "loss: 0.295054  [ 3600/ 4873]\n",
      "loss: 0.410258  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.744     0.581    105\n",
      " disgust     0.657     0.760     0.670    109\n",
      "    fear     0.657     0.667     0.525    80\n",
      "   happy     0.657     0.466     0.753    81\n",
      " neutral     0.657     0.780     0.762    84\n",
      "     sad     0.657     0.602     0.713    87\n",
      "surprise     0.657     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.677     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.296449 \n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "loss: 0.690212  [ 1200/ 4873]\n",
      "loss: 0.185269  [ 2400/ 4873]\n",
      "loss: 0.533824  [ 3600/ 4873]\n",
      "loss: 0.263975  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.747     0.562    105\n",
      " disgust     0.638     0.664     0.725    109\n",
      "    fear     0.638     0.582     0.487    80\n",
      "   happy     0.638     0.552     0.716    81\n",
      " neutral     0.638     0.735     0.726    84\n",
      "     sad     0.638     0.529     0.632    87\n",
      "surprise     0.638     0.717     0.594    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.647     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.375873 \n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "loss: 0.307614  [ 1200/ 4873]\n",
      "loss: 0.561337  [ 2400/ 4873]\n",
      "loss: 0.291947  [ 3600/ 4873]\n",
      "loss: 0.648838  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.780     0.610    105\n",
      " disgust     0.662     0.762     0.706    109\n",
      "    fear     0.662     0.633     0.475    80\n",
      "   happy     0.662     0.536     0.728    81\n",
      " neutral     0.662     0.649     0.857    84\n",
      "     sad     0.662     0.628     0.678    87\n",
      "surprise     0.662     0.673     0.547    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.666     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.371375 \n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "loss: 0.299619  [ 1200/ 4873]\n",
      "loss: 0.437416  [ 2400/ 4873]\n",
      "loss: 0.303591  [ 3600/ 4873]\n",
      "loss: 0.329541  [ 4800/ 4873]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.816     0.590    105\n",
      " disgust     0.656     0.738     0.725    109\n",
      "    fear     0.656     0.577     0.512    80\n",
      "   happy     0.656     0.491     0.691    81\n",
      " neutral     0.656     0.691     0.798    84\n",
      "     sad     0.656     0.604     0.667    87\n",
      "surprise     0.656     0.755     0.578    64\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.668     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.306133 \n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mexp_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_em\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:42\u001B[0m, in \u001B[0;36mExperimentsTrainer.train_em\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#highest_acc_c, higest_epoch_c, higest_true_c, higest_pred_c = self.run_conv_model_test(lr, self.epochs_per_model, trail)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m## generate Report\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m#SSGenModelTrainer.genAndSaveEvaluation(f\"{self.models_dir}/Run_Nr_{trail}/conv\", higest_true_c, higest_pred_c, highest_acc_c, higest_epoch_c, \"Convolutional\", self.dataset.encoded_dataset.label_list)\u001B[39;00m\n\u001B[1;32m     41\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m---> 42\u001B[0m highest_acc_dr, higest_epoch_dr, higest_true_dr, higest_pred_dr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_dim_red_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs_per_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrail\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# generate Report\u001B[39;00m\n\u001B[1;32m     44\u001B[0m SSGenModelTrainer\u001B[38;5;241m.\u001B[39mgenAndSaveEvaluation(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrail\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/dimred\u001B[39m\u001B[38;5;124m\"\u001B[39m, higest_true_dr, higest_pred_dr, highest_acc_dr, higest_epoch_dr, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimension Reduced\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mencoded_dataset\u001B[38;5;241m.\u001B[39mlabel_list)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:64\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_dim_red_model_test\u001B[0;34m(self, lr, epochs, current_run)\u001B[0m\n\u001B[1;32m     62\u001B[0m model \u001B[38;5;241m=\u001B[39m SSDimRedModel(num_emotions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list))\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     63\u001B[0m save_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_run\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/dimred/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:85\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_model_test\u001B[0;34m(self, lr, epochs, model, save_dir, bs)\u001B[0m\n\u001B[1;32m     77\u001B[0m valDs, testDs \u001B[38;5;241m=\u001B[39m train_val_dataset(testDs, val_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m     78\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SSGenModelTrainer(lr\u001B[38;5;241m=\u001B[39mlr, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model\u001B[38;5;241m=\u001B[39mmodel, train_dataset\u001B[38;5;241m=\u001B[39mtrainDS,\n\u001B[1;32m     79\u001B[0m                             eval_dataset\u001B[38;5;241m=\u001B[39mtestDs,\n\u001B[1;32m     80\u001B[0m                             device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, labelList\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     83\u001B[0m                             save_highest_acc_min_acc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_highest_acc_min_acc,\n\u001B[1;32m     84\u001B[0m                             model_path \u001B[38;5;241m=\u001B[39m save_dir, regularize_dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregularize_dims)\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:81\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# this is the trainloop\u001B[39;00m\n\u001B[0;32m---> 81\u001B[0m epoch_train_losses \u001B[38;5;241m=\u001B[39m epoch_train_losses \u001B[38;5;241m+\u001B[39m  [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# --------------------- testloop and evaluation- ---------------\u001B[39;00m\n\u001B[1;32m     84\u001B[0m acc, true, preds, loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loop(test_dataloader, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:116\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer, epoch)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(X) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 116\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;66;03m# Compute prediction and loss\u001B[39;00m\n\u001B[1;32m    118\u001B[0m z \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 10:00:38.227854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 10:00:38.701976: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 10:00:38.702028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 10:00:38.702032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 267178 samples and validating with randomly splitted 14063 samples\n",
      "0: soundstream total loss: 545563.781, soundstream recon loss: 0.603 | discr (scale 1) loss: 2.001 | discr (scale 0.5) loss: 2.002 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to ../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32\n",
      "0: saving model to ../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32\n",
      "1: soundstream total loss: 660274.273, soundstream recon loss: 0.702 | discr (scale 1) loss: 1.989 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 1.997\n",
      "2: soundstream total loss: 526542.078, soundstream recon loss: 0.547 | discr (scale 1) loss: 1.966 | discr (scale 0.5) loss: 1.990 | discr (scale 0.25) loss: 2.001\n",
      "3: soundstream total loss: 158701.781, soundstream recon loss: 0.173 | discr (scale 1) loss: 1.958 | discr (scale 0.5) loss: 1.994 | discr (scale 0.25) loss: 2.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 37\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#soundstream.load(\"../content/soundstream/verision0.12.1/00_10000_1e-4_bs6_gae8_dml320-32/soundstream.42500.pt\")\u001B[39;00m\n\u001B[1;32m     20\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SoundStreamTrainer(\n\u001B[1;32m     21\u001B[0m     soundstream,\n\u001B[1;32m     22\u001B[0m     folder \u001B[38;5;241m=\u001B[39m  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/home/ckwdani/Music/libri\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m#standard lr 3e-4,\u001B[39;00m\n\u001B[1;32m     35\u001B[0m )\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m---> 37\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:455\u001B[0m, in \u001B[0;36mSoundStreamTrainer.train\u001B[0;34m(self, log_fn)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m, log_fn \u001B[38;5;241m=\u001B[39m noop):\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_train_steps:\n\u001B[0;32m--> 455\u001B[0m         logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m         log_fn(logs)\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining complete\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:357\u001B[0m, in \u001B[0;36mSoundStreamTrainer.train_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    354\u001B[0m     multiscale_discr_optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad_accum_every):\n\u001B[0;32m--> 357\u001B[0m     wave, \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdl_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m     wave \u001B[38;5;241m=\u001B[39m wave\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    360\u001B[0m     discr_losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoundstream(\n\u001B[1;32m    361\u001B[0m         wave,\n\u001B[1;32m    362\u001B[0m         apply_grad_penalty \u001B[38;5;241m=\u001B[39m apply_grad_penalty,\n\u001B[1;32m    363\u001B[0m         return_discr_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    364\u001B[0m         return_discr_losses_separately \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    365\u001B[0m     )\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/trainer.py:72\u001B[0m, in \u001B[0;36mcycle\u001B[0;34m(dl)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcycle\u001B[39m(dl):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 72\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dl:\n\u001B[1;32m     73\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/accelerate/data_loader.py:383\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    382\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m send_to_device(current_batch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 383\u001B[0m next_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m current_batch\n\u001B[1;32m    385\u001B[0m current_batch \u001B[38;5;241m=\u001B[39m next_batch\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/audiolm_pytorch/data.py:67\u001B[0m, in \u001B[0;36mSoundDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     65\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfiles[idx]\n\u001B[0;32m---> 67\u001B[0m     data, sample_hz \u001B[38;5;241m=\u001B[39m \u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m data\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mone of your audio file (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) is empty. please remove it from your folder\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;66;03m# the audio has more than 1 channel, convert to mono\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:222\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _fallback_load_fileobj(filepath, frame_offset, num_frames, normalize, channels_first, \u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    221\u001B[0m     filepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(filepath)\n\u001B[0;32m--> 222\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorchaudio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msox_io_load_audio_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\n\u001B[1;32m    224\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_ops.py:442\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "#\n",
    "# import numpy as np\n",
    "# from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "#\n",
    "# soundstream = SoundStream(\n",
    "#\n",
    "#     codebook_size = 2048,\n",
    "#     rq_num_quantizers = 12,\n",
    "#     attn_window_size = 128,       # local attention receptive field at bottleneck\n",
    "#     attn_depth = 3\n",
    "#     #target_sample_hz=16000\n",
    "# )\n",
    "#\n",
    "# #soundstream.load(\"../content/soundstream/verision0.12.1/00_10000_1e-4_bs6_gae8_dml320-32/soundstream.42500.pt\")\n",
    "#\n",
    "# trainer = SoundStreamTrainer(\n",
    "#     soundstream,\n",
    "#     folder =  '/home/ckwdani/Music/libri',\n",
    "#     #'/home/ckwdani/Music/train-clean-100',\n",
    "#     batch_size = 6,\n",
    "#     grad_accum_every = 8,         # effective batch size of 32\n",
    "#     data_max_length = 320 * 32,\n",
    "#     save_results_every=1000,\n",
    "#     num_train_steps = 60001,\n",
    "#     save_model_every= 500,\n",
    "#     results_folder = '../content/soundstream/verision0.12.1/RQ12_CBS_2048_attnD_3/00_10000_1e-4_bs6_gae8_dml320-32',\n",
    "#     #lr=1e-4\n",
    "# #    lr = 5e-5,\n",
    "#     #lr = 5e-5\n",
    "#     #standard lr 3e-4,\n",
    "# ).cuda()\n",
    "#\n",
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
