{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!python --version\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "#from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions defined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ndef scale_minmax(X, min=0.0, max=1.0):\\n    X_std = (X - X.min()) / (X.max() - X.min())\\n    X_scaled = X_std * (max - min) + min\\n    return X_scaled\\n\\ndef spectrogram_image(y, sr, out, hop_length, n_mels):\\n    # use log-melspectrogram\\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\\n                                            n_fft=hop_length*2, hop_length=hop_length)\\n    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\\n\\n    # min-max scale to fit inside 8-bit range\\n    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\\n    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\\n    img = 255-img # invert. make black==more energy\\n\\n    # save as PNG\\n    #plt.axis(\\'off\\')\\n    plt.imsave(\"spec.png\",img)\\n    plt.imshow(img)\\n\\ndef createSpectrogramm(path, label):\\n    data, sr = librosa.load(path)\\n    fourier = librosa.stft(data)\\n    fourierdb = librosa.amplitude_to_db(abs(fourier))\\n    librosa.display.specshow(fourierdb, sr=sr, x_axis=\\'time\\', y_axis=\\'hz\\')\\n\\ndef createMelSpectrogramm(path, label):\\n    data, sr = librosa.load(path, sr=22050, offset=0, duration=1)\\n    melSpec = librosa.feature.melspectrogram(data, sr, n_mels=192, n_fft=1024, hop_length=260)\\n\\n    # plt.figure(figsize=(10, 4))\\n    # plt.title(label, size=20)\\n    librosa.display.specshow(melSpec, sr=sr, x_axis=\\'time\\', y_axis=\\'hz\\')\\n    # plt.colorbar()\\n'"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def load_custom_dataset():\n",
    "    paths = []\n",
    "    testpaths = []\n",
    "    testlabels = []\n",
    "    terminator = 'D:/Uni/19.Master/Daten/terminator.wav'\n",
    "    print(sys.executable)\n",
    "    emotions = []\n",
    "    # for dirname, _, filenames in os.walk('Daten/TESS Toronto emotional speech set data'):\n",
    "    # D:\\Uni\\19.Master\\DATEN\n",
    "    for dirname, _, filenames in os.walk('../tess'):\n",
    "        for filename in filenames:\n",
    "            label = filename.split('_')[-1]\n",
    "            label = label.split('.')[0]\n",
    "            if (label != 'neutral'):\n",
    "                emotions.append(label.lower())\n",
    "                paths.append(os.path.join(dirname, filename))\n",
    "    for dirname, _, filenames in os.walk('../stimuli_intensitätsmorphs'):\n",
    "        for filename in filenames:\n",
    "\n",
    "            intens = filename.split('_')[-2]\n",
    "            emot = filename.split('_')[1]\n",
    "            label = emot\n",
    "            match label:\n",
    "                case 'ang':\n",
    "                    label = 'angry'\n",
    "                case 'dis':\n",
    "                    label = 'disgust'\n",
    "                case 'fea':\n",
    "                    label = 'fear'\n",
    "                case 'hap':\n",
    "                    label = 'happy'\n",
    "                case 'sad':\n",
    "                    label = 'sad'\n",
    "                case 'sur':\n",
    "                    label = 'ps'\n",
    "            if (emot != 'ple'):\n",
    "                testpaths.append(os.path.join(dirname, filename))\n",
    "                testlabels.append(label.lower())\n",
    "    com_labels = testlabels + emotions\n",
    "    com_paths = testpaths + paths\n",
    "    print(testlabels)\n",
    "    print(testpaths)\n",
    "    print('Dataset is loaded')\n",
    "    return paths, emotions, testpaths, testlabels\n",
    "\"\"\"\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def spectrogram_image(y, sr, out, hop_length, n_mels):\n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
    "                                            n_fft=hop_length*2, hop_length=hop_length)\n",
    "    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\n",
    "    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "\n",
    "    # save as PNG\n",
    "    #plt.axis('off')\n",
    "    plt.imsave(\"spec.png\",img)\n",
    "    plt.imshow(img)\n",
    "\n",
    "def createSpectrogramm(path, label):\n",
    "    data, sr = librosa.load(path)\n",
    "    fourier = librosa.stft(data)\n",
    "    fourierdb = librosa.amplitude_to_db(abs(fourier))\n",
    "    librosa.display.specshow(fourierdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "\n",
    "def createMelSpectrogramm(path, label):\n",
    "    data, sr = librosa.load(path, sr=22050, offset=0, duration=1)\n",
    "    melSpec = librosa.feature.melspectrogram(data, sr, n_mels=192, n_fft=1024, hop_length=260)\n",
    "\n",
    "    # plt.figure(figsize=(10, 4))\n",
    "    # plt.title(label, size=20)\n",
    "    librosa.display.specshow(melSpec, sr=sr, x_axis='time', y_axis='hz')\n",
    "    # plt.colorbar()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\Scripts\\python.exe\n",
      "['angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'angry', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'disgust', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps', 'ps']\n",
      "['../stimuli_intensitätsmorphs\\\\nf01_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_fea_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sad_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf01_sur_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_fea_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sad_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf02_sur_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_fea_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sad_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf03_sur_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_dis_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_fea_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sad_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nf04_sur_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_fea_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sad_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm01_sur_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_fea_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sad_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm02_sur_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_fea_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sad_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm03_sur_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_ang_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_dis_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_fea_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w01_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w01_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w01_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w01_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_hap_w05_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w02_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w02_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w02_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w02_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w03_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w03_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w03_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w03_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w05_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w05_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w05_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sad_w05_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w01_c_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w01_c_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w01_c_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w01_c_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w02_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w02_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w02_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w02_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w03_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w03_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w03_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w03_o_75_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w05_o_100_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w05_o_25_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w05_o_50_70dB.wav', '../stimuli_intensitätsmorphs\\\\nm04_sur_w05_o_75_70dB.wav']\n",
      "Dataset is loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  path emotion\n0    ../stimuli_intensitätsmorphs\\nf01_ang_w01_o_10...   angry\n1    ../stimuli_intensitätsmorphs\\nf01_ang_w01_o_25...   angry\n2    ../stimuli_intensitätsmorphs\\nf01_ang_w01_o_50...   angry\n3    ../stimuli_intensitätsmorphs\\nf01_ang_w01_o_75...   angry\n4    ../stimuli_intensitätsmorphs\\nf01_ang_w02_o_10...   angry\n..                                                 ...     ...\n763  ../stimuli_intensitätsmorphs\\nm04_sur_w03_o_75...      ps\n764  ../stimuli_intensitätsmorphs\\nm04_sur_w05_o_10...      ps\n765  ../stimuli_intensitätsmorphs\\nm04_sur_w05_o_25...      ps\n766  ../stimuli_intensitätsmorphs\\nm04_sur_w05_o_50...      ps\n767  ../stimuli_intensitätsmorphs\\nm04_sur_w05_o_75...      ps\n\n[768 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../stimuli_intensitätsmorphs\\nf01_ang_w01_o_10...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../stimuli_intensitätsmorphs\\nf01_ang_w01_o_25...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../stimuli_intensitätsmorphs\\nf01_ang_w01_o_50...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../stimuli_intensitätsmorphs\\nf01_ang_w01_o_75...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../stimuli_intensitätsmorphs\\nf01_ang_w02_o_10...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>../stimuli_intensitätsmorphs\\nm04_sur_w03_o_75...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>../stimuli_intensitätsmorphs\\nm04_sur_w05_o_10...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>../stimuli_intensitätsmorphs\\nm04_sur_w05_o_25...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>../stimuli_intensitätsmorphs\\nm04_sur_w05_o_50...</td>\n      <td>ps</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>../stimuli_intensitätsmorphs\\nm04_sur_w05_o_75...</td>\n      <td>ps</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpaths, trainlabels, testpaths, testlabels = load_custom_dataset()\n",
    "\n",
    "###create dataframes for training and testing###\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF[\"path\"] = trainpaths\n",
    "trainDF[\"emotion\"] = trainlabels\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF[\"path\"] = testpaths\n",
    "testDF[\"emotion\"] = testlabels\n",
    "\n",
    "\n",
    "testDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\npath = trainDF[\"speech\"][0]\\n# settings\\nhop_length = 302 # number of samples per time-step in spectrogram\\nn_mels = 192 # number of bins in spectrogram. Height of image\\ntime_steps = 192 # number of time-steps. Width of image\\n\\n# load audio. Using example from librosa\\ny, sr = librosa.load(path, offset=0.1, duration=1, sr=99100)\\nout = \\'out.png\\'\\n\\n# extract a fixed length window\\nstart_sample = 0 # starting at beginning\\nlength_samples = time_steps*hop_length\\nwindow = y[start_sample:start_sample+length_samples]\\n\\n# convert to PNG\\nspectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\\n\\n#train_spectros = trainDF[\"speech\"].apply(lambda x: spectrogram_image(x))\\n'"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = trainDF[\"speech\"][0]\n",
    "# settings\n",
    "hop_length = 302 # number of samples per time-step in spectrogram\n",
    "n_mels = 192 # number of bins in spectrogram. Height of image\n",
    "time_steps = 192 # number of time-steps. Width of image\n",
    "\n",
    "# load audio. Using example from librosa\n",
    "y, sr = librosa.load(path, offset=0.1, duration=1, sr=99100)\n",
    "out = 'out.png'\n",
    "\n",
    "# extract a fixed length window\n",
    "start_sample = 0 # starting at beginning\n",
    "length_samples = time_steps*hop_length\n",
    "window = y[start_sample:start_sample+length_samples]\n",
    "\n",
    "# convert to PNG\n",
    "spectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "#train_spectros = trainDF[\"speech\"].apply(lambda x: spectrogram_image(x))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\\n'"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchvision.models import AlexNet_Weights\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\"\"\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# load dummy dataset and read soundfiles\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\n# tokenize\\ninput_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\\n\\n# retrieve logits\\nlogits = model(input_values).logits\\n\\n# take argmax and decode\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\\n'"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import wer\n",
    "import torch\n",
    "\"\"\"\n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ny = librosa.load(sr=16000, path=testDF[\"speech\"][0])\\n\\n\\ninput_values = processor(y[0], return_tensors=\"pt\",  sampling_rate=16000).input_values  # Batch size 1\\n\\n# retrieve logits\\nlogits = model(input_values).logits\\n\\n# take argmax and decode\\npredicted_ids = torch.argmax(logits, dim=-1)\\ntranscription = processor.batch_decode(predicted_ids)\\n'"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "y = librosa.load(sr=16000, path=testDF[\"speech\"][0])\n",
    "\n",
    "\n",
    "input_values = processor(y[0], return_tensors=\"pt\",  sampling_rate=16000).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\ntrain_audios=[]\\nfor x in testDF[\"speech\"]:\\n    audio=librosa.load(path=x, sr=16000)\\n    train_audios.append(audio[0])\\ntrain_audios\\n'"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_audios=[]\n",
    "for x in testDF[\"speech\"]:\n",
    "    audio=librosa.load(path=x, sr=16000)\n",
    "    train_audios.append(audio[0])\n",
    "train_audios\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "save_path=\"/masterarbeit_programming/notebooks/content/data\"\n",
    "train_df, test_df=train_test_split(trainDF, test_size=0.2, random_state=101, stratify=trainDF[\"emotion\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 2)\n",
      "(480, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-235da878ddab3f84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/tonib/.cache/huggingface/datasets/csv/default-235da878ddab3f84/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6d55397f4ee4d9f941caf76ab614322"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f209ab7c1434f43972271fde80f9ccc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "def5e7ff69844dd7bd53c18d98714cf6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/tonib/.cache/huggingface/datasets/csv/default-235da878ddab3f84/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06a96b6a747842659b98c14856ef20a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'emotion'],\n",
      "    num_rows: 1920\n",
      "})\n",
      "Dataset({\n",
      "    features: ['path', 'emotion'],\n",
      "    num_rows: 480\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading the created dataset using datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/masterarbeit_programming/notebooks/content/data/train.csv\",\n",
    "    \"validation\": \"/masterarbeit_programming/notebooks/content/data/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "train_df = dataset[\"train\"]\n",
    "test_df = dataset[\"validation\"]\n",
    "\n",
    "print(train_df)\n",
    "print(test_df)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "# specifiy input and output column\n",
    "input_colum=\"path\"\n",
    "output_column=\"emotion\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Classes: ['angry', 'disgust', 'fear', 'happy', 'ps', 'sad']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#distinguish labels\n",
    "class_list = train_df.unique(output_column)\n",
    "class_list.sort()\n",
    "num_class = len(class_list)\n",
    "\n",
    "print(f\"{num_class} Classes: {class_list}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "\n",
    "model_name_path =  \"facebook/wav2vec2-base-960h\"\n",
    "pooling_mode = \"mean\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angry\",\n",
      "    \"1\": \"disgust\",\n",
      "    \"2\": \"fear\",\n",
      "    \"3\": \"happy\",\n",
      "    \"4\": \"ps\",\n",
      "    \"5\": \"sad\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"angry\": 0,\n",
      "    \"disgust\": 1,\n",
      "    \"fear\": 2,\n",
      "    \"happy\": 3,\n",
      "    \"ps\": 4,\n",
      "    \"sad\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_path,\n",
    "    num_labels=num_class,\n",
    "    label2id={label: i for i, label in enumerate(class_list)},\n",
    "    id2label={i: label for i, label in enumerate(class_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\"\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"finetuning_task\": \"wav2vec2_clf\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angry\",\n",
      "    \"1\": \"disgust\",\n",
      "    \"2\": \"fear\",\n",
      "    \"3\": \"happy\",\n",
      "    \"4\": \"ps\",\n",
      "    \"5\": \"sad\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"angry\": 0,\n",
      "    \"disgust\": 1,\n",
      "    \"fear\": 2,\n",
      "    \"happy\": 3,\n",
      "    \"ps\": 4,\n",
      "    \"sad\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooling_mode\": \"mean\",\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\vocab.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\tokenizer_config.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "print(config)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_path)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"Target sample rate: {target_sampling_rate}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchaudio\n",
    "# print(torchaudio.__version__)\n",
    "# print(torch.__version__)\n",
    "# torchaudio.load(train_df[0]['path']['path'])\n",
    "# train_df[0]['path']['path']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import soundfile\n",
    "\n",
    "def speech_file_to_array(speech_path):\n",
    "\n",
    "    speech_array, sampling_rate = torchaudio.load(speech_path, normalize=True)\n",
    "    print('test1')\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    print('test2')\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    print('test3')\n",
    "    return speech\n",
    "\n",
    "def speech_file_to_array_librosa(speech_path):\n",
    "    speech_array, sampling_rate = librosa.load(speech_path)\n",
    "    resampler = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=target_sampling_rate).squeeze()\n",
    "\n",
    "    return resampler\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list = [ speech_file_to_array_librosa(speech_path) for speech_path in examples[input_colum]]\n",
    "    target_list = [ label_to_id(label, class_list) for label in examples[output_column]]\n",
    "    attention_mask_list= [numpy.ones(shape=sf.shape) for sf in speech_list]\n",
    "\n",
    "    result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    result[\"labels\"] = list(target_list)\n",
    "    result[\"attention_mask\"] = list(attention_mask_list)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "### resample\n",
    "#from datasets import Audio\n",
    "#train_df = train_df.cast_column(\"path\", Audio(sampling_rate=target_sampling_rate))\n",
    "\n",
    "#test_df = test_df.cast_column(\"path\", Audio(sampling_rate=target_sampling_rate))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffe104e070ed43dc8333a7ecbeba5437"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\feature_extraction_utils.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b00afe140668463fbc0610a4341d7c7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = train_df.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    #num_proc=4\n",
    ")\n",
    "test_df = test_df.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    #num_proc=4\n",
    ")\n",
    "\n",
    "#test1 = speech_file_to_array(test_df[0]['path'])\n",
    "from datasets import Audio\n",
    "# train_df = train_df.cast_column('path', Audio(sampling_rate=16000))\n",
    "# test_df = test_df.cast_column('path', Audio(sampling_rate=16000))\n",
    "#test2 = speech_file_to_array_librosa(test_df[0]['path'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "{'path': '../tess\\\\OAF_Pleasant_surprise\\\\OAF_chief_ps.wav',\n 'emotion': 'ps',\n 'input_values': [0.00012689562572631985,\n  0.0058380174450576305,\n  0.00578723568469286,\n  0.0047672828659415245,\n  0.003909342456609011,\n  0.004343182779848576,\n  0.005542287603020668,\n  0.005779862869530916,\n  0.00740136718377471,\n  0.010440682992339134,\n  0.01527947373688221,\n  0.017540067434310913,\n  0.018711376935243607,\n  0.022364282980561256,\n  0.024427887052297592,\n  0.021836576983332634,\n  0.024900106713175774,\n  0.02727128192782402,\n  0.028622902929782867,\n  0.032586876302957535,\n  0.032769132405519485,\n  0.03493119776248932,\n  0.03769749402999878,\n  0.03610454499721527,\n  0.033735666424036026,\n  0.03351058065891266,\n  0.034070588648319244,\n  0.03399619087576866,\n  0.03080596588551998,\n  0.0325588583946228,\n  0.03685218468308449,\n  0.03479083254933357,\n  0.03440998122096062,\n  0.034398630261421204,\n  0.03036915697157383,\n  0.028516311198472977,\n  0.02781406044960022,\n  0.029108155518770218,\n  0.03065369464457035,\n  0.031687382608652115,\n  0.03204840421676636,\n  0.02777428925037384,\n  0.03123040311038494,\n  0.02993754856288433,\n  0.024973856285214424,\n  0.02843865379691124,\n  0.02605784684419632,\n  0.02805631421506405,\n  0.030956843867897987,\n  0.028829023241996765,\n  0.0310817938297987,\n  0.032202817499637604,\n  0.03314539045095444,\n  0.03235435485839844,\n  0.030702585354447365,\n  0.03384310007095337,\n  0.03479340299963951,\n  0.035482462495565414,\n  0.036020368337631226,\n  0.03919949010014534,\n  0.04227164760231972,\n  0.04205935075879097,\n  0.04322126880288124,\n  0.046012409031391144,\n  0.04887356981635094,\n  0.04240291565656662,\n  0.043471187353134155,\n  0.04326096177101135,\n  0.04255214333534241,\n  0.04868961125612259,\n  0.048347488045692444,\n  0.05019086226820946,\n  0.04818720370531082,\n  0.04797530919313431,\n  0.04975244402885437,\n  0.04938862472772598,\n  0.04875076934695244,\n  0.051042340695858,\n  0.04978084936738014,\n  0.050602301955223083,\n  0.05400051921606064,\n  0.050889752805233,\n  0.050974275916814804,\n  0.04913269728422165,\n  0.051537156105041504,\n  0.04852232336997986,\n  0.04851889610290527,\n  0.05117719992995262,\n  0.04933353513479233,\n  0.052225615829229355,\n  0.04925767332315445,\n  0.050947390496730804,\n  0.04844849184155464,\n  0.04836641997098923,\n  0.05209142342209816,\n  0.05083760991692543,\n  0.05118202045559883,\n  0.05306060239672661,\n  0.05572989583015442,\n  0.054213885217905045,\n  0.05537112429738045,\n  0.05289033055305481,\n  0.055212732404470444,\n  0.05605677142739296,\n  0.05655105784535408,\n  0.0618089959025383,\n  0.05983640253543854,\n  0.06046860292553902,\n  0.0632939487695694,\n  0.06640923023223877,\n  0.06481557339429855,\n  0.06314793229103088,\n  0.06467149406671524,\n  0.06726278364658356,\n  0.06891226023435593,\n  0.06999731808900833,\n  0.06835576891899109,\n  0.06813827902078629,\n  0.06999539583921432,\n  0.06638678908348083,\n  0.06886568665504456,\n  0.07024352997541428,\n  0.06851764768362045,\n  0.07204101234674454,\n  0.06981373578310013,\n  0.067803293466568,\n  0.06998839229345322,\n  0.06915673613548279,\n  0.06943214684724808,\n  0.07033561915159225,\n  0.06838446110486984,\n  0.07038530707359314,\n  0.07434578239917755,\n  0.07079422473907471,\n  0.07540711760520935,\n  0.07106631249189377,\n  0.06839832663536072,\n  0.07386288791894913,\n  0.07026313245296478,\n  0.07753673195838928,\n  0.07278233021497726,\n  0.07157064229249954,\n  0.07691527903079987,\n  0.07553986459970474,\n  0.08005481213331223,\n  0.07827526330947876,\n  0.07604070752859116,\n  0.07898663729429245,\n  0.08042477071285248,\n  0.08123111724853516,\n  0.08244357258081436,\n  0.08068326115608215,\n  0.08286349475383759,\n  0.08070828765630722,\n  0.08263374119997025,\n  0.08436643332242966,\n  0.08099851757287979,\n  0.0877641886472702,\n  0.07973650097846985,\n  0.07866346836090088,\n  0.0840599536895752,\n  0.08273809403181076,\n  0.08516985923051834,\n  0.07934075593948364,\n  0.08191341906785965,\n  0.07916940003633499,\n  0.080269955098629,\n  0.08489569276571274,\n  0.08223597705364227,\n  0.08401095122098923,\n  0.08078610897064209,\n  0.07949661463499069,\n  0.08290769904851913,\n  0.08683054149150848,\n  0.08313965052366257,\n  0.08509892225265503,\n  0.08602973818778992,\n  0.08935746550559998,\n  0.09069687873125076,\n  0.08522851765155792,\n  0.0901724323630333,\n  0.08771976083517075,\n  0.09208071231842041,\n  0.09071709960699081,\n  0.09065843373537064,\n  0.09278880804777145,\n  0.08994635194540024,\n  0.09670008718967438,\n  0.09316989779472351,\n  0.09391669183969498,\n  0.09200900793075562,\n  0.08925434947013855,\n  0.08973497897386551,\n  0.0932101160287857,\n  0.0933241918683052,\n  0.09395690262317657,\n  0.09571243077516556,\n  0.0932803675532341,\n  0.09765292704105377,\n  0.0928986594080925,\n  0.09682214260101318,\n  0.0940530002117157,\n  0.09410172700881958,\n  0.09805546700954437,\n  0.0948662981390953,\n  0.09841759502887726,\n  0.09693220257759094,\n  0.10133473575115204,\n  0.09915672987699509,\n  0.10125281661748886,\n  0.10100002586841583,\n  0.101677805185318,\n  0.10853753238916397,\n  0.10116969794034958,\n  0.10230295360088348,\n  0.10256437957286835,\n  0.10483378916978836,\n  0.10546967387199402,\n  0.10758794844150543,\n  0.10988838225603104,\n  0.10729417204856873,\n  0.10867893695831299,\n  0.10895346850156784,\n  0.11122775822877884,\n  0.1078585758805275,\n  0.11183279007673264,\n  0.10954708606004715,\n  0.11364530771970749,\n  0.11572927236557007,\n  0.11484256386756897,\n  0.115094393491745,\n  0.11057430505752563,\n  0.11602858453989029,\n  0.11775638908147812,\n  0.11851876974105835,\n  0.11599057167768478,\n  0.12201665341854095,\n  0.1195167824625969,\n  0.11975604295730591,\n  0.11297966539859772,\n  0.11480686813592911,\n  0.1200198382139206,\n  0.11770087480545044,\n  0.12622042000293732,\n  0.11627403646707535,\n  0.12371387332677841,\n  0.1208164244890213,\n  0.1197182759642601,\n  0.12056788802146912,\n  0.11622945964336395,\n  0.12213217467069626,\n  0.12074926495552063,\n  0.1262626051902771,\n  0.12420961260795593,\n  0.12592561542987823,\n  0.12318136543035507,\n  0.12099896371364594,\n  0.12197784334421158,\n  0.12254117429256439,\n  0.1270454078912735,\n  0.12442771345376968,\n  0.12893924117088318,\n  0.1301543116569519,\n  0.13046018779277802,\n  0.13205131888389587,\n  0.126246839761734,\n  0.12611804902553558,\n  0.12948739528656006,\n  0.13006645441055298,\n  0.12959839403629303,\n  0.1331041306257248,\n  0.12645332515239716,\n  0.13523966073989868,\n  0.1394614428281784,\n  0.13571065664291382,\n  0.1408160775899887,\n  0.13134467601776123,\n  0.1390707641839981,\n  0.13382992148399353,\n  0.13375888764858246,\n  0.1425040364265442,\n  0.14160814881324768,\n  0.14321672916412354,\n  0.1406640112400055,\n  0.14360089600086212,\n  0.14943547546863556,\n  0.15160654485225677,\n  0.14926594495773315,\n  0.14889201521873474,\n  0.14406001567840576,\n  0.15272392332553864,\n  0.15636087954044342,\n  0.15484918653964996,\n  0.14997850358486176,\n  0.14709624648094177,\n  0.15787573158740997,\n  0.1586867868900299,\n  0.15501554310321808,\n  0.15115109086036682,\n  0.15548042953014374,\n  0.14851529896259308,\n  0.15031929314136505,\n  0.1588670015335083,\n  0.15508471429347992,\n  0.16176649928092957,\n  0.15466170012950897,\n  0.1563316136598587,\n  0.16057056188583374,\n  0.1557103395462036,\n  0.15668147802352905,\n  0.1477043330669403,\n  0.15099431574344635,\n  0.14620308578014374,\n  0.15040859580039978,\n  0.15574388206005096,\n  0.15091891586780548,\n  0.15658022463321686,\n  0.1443800926208496,\n  0.14489446580410004,\n  0.14734520018100739,\n  0.1441795825958252,\n  0.1486988067626953,\n  0.15141214430332184,\n  0.15280787646770477,\n  0.16186776757240295,\n  0.15884919464588165,\n  0.15610270202159882,\n  0.159040167927742,\n  0.15583226084709167,\n  0.16333076357841492,\n  0.15545037388801575,\n  0.154418483376503,\n  0.15413299202919006,\n  0.1624854952096939,\n  0.17325353622436523,\n  0.17354661226272583,\n  0.1771637201309204,\n  0.17574599385261536,\n  0.16683943569660187,\n  0.16665877401828766,\n  0.17598611116409302,\n  0.17032480239868164,\n  0.17796458303928375,\n  0.1718597114086151,\n  0.1866462528705597,\n  0.19099374115467072,\n  0.17946763336658478,\n  0.1777936965227127,\n  0.16097088158130646,\n  0.1692081093788147,\n  0.16615191102027893,\n  0.16729138791561127,\n  0.17224998772144318,\n  0.18102556467056274,\n  0.18310269713401794,\n  0.18350599706172943,\n  0.16708719730377197,\n  0.1643534153699875,\n  0.1647956818342209,\n  0.15046946704387665,\n  0.16381864249706268,\n  0.16312654316425323,\n  0.182851180434227,\n  0.17789556086063385,\n  0.17319846153259277,\n  0.17581281065940857,\n  0.17560909688472748,\n  0.18127359449863434,\n  0.1699543297290802,\n  0.16961121559143066,\n  0.18787944316864014,\n  0.2021126002073288,\n  0.19696056842803955,\n  0.19765983521938324,\n  0.19003885984420776,\n  0.18781068921089172,\n  0.17271409928798676,\n  0.18556684255599976,\n  0.19895672798156738,\n  0.19513148069381714,\n  0.19943960011005402,\n  0.1963842362165451,\n  0.2072170525789261,\n  0.2119002640247345,\n  0.19907373189926147,\n  0.1840927004814148,\n  0.20273812115192413,\n  0.210819810628891,\n  0.23965108394622803,\n  0.23434628546237946,\n  0.22233405709266663,\n  0.23398657143115997,\n  0.21150465309619904,\n  0.21906644105911255,\n  0.2000473439693451,\n  0.20295493304729462,\n  0.2170618623495102,\n  0.20333334803581238,\n  0.21804161369800568,\n  0.2340509444475174,\n  0.21524253487586975,\n  0.20752398669719696,\n  0.19523508846759796,\n  0.17452536523342133,\n  0.18445871770381927,\n  0.18386581540107727,\n  0.1993607133626938,\n  0.18713612854480743,\n  0.17290516197681427,\n  0.18790166079998016,\n  0.18228624761104584,\n  0.16896595060825348,\n  0.14312060177326202,\n  0.1363779902458191,\n  0.161915123462677,\n  0.161973237991333,\n  0.15220825374126434,\n  0.16214591264724731,\n  0.16216717660427094,\n  0.15151257812976837,\n  0.1397772580385208,\n  0.1473119556903839,\n  0.14211085438728333,\n  0.15079936385154724,\n  0.15629926323890686,\n  0.15099497139453888,\n  0.17362917959690094,\n  0.1712663769721985,\n  0.1738394945859909,\n  0.180470809340477,\n  0.17816442251205444,\n  0.17384183406829834,\n  0.17510226368904114,\n  0.19667606055736542,\n  0.19681787490844727,\n  0.20000965893268585,\n  0.21444997191429138,\n  0.19825905561447144,\n  0.20163482427597046,\n  0.1984083354473114,\n  0.2072565108537674,\n  0.21892032027244568,\n  0.20412983000278473,\n  0.2004070281982422,\n  0.19633202254772186,\n  0.19751301407814026,\n  0.21065136790275574,\n  0.2089998573064804,\n  0.18729378283023834,\n  0.2072768211364746,\n  0.19211047887802124,\n  0.19273361563682556,\n  0.18571121990680695,\n  0.17527547478675842,\n  0.18487879633903503,\n  0.1548231542110443,\n  0.16470573842525482,\n  0.18598824739456177,\n  0.20329545438289642,\n  0.17731782793998718,\n  0.15616734325885773,\n  0.1751430779695511,\n  0.1949501484632492,\n  0.16721880435943604,\n  0.17626594007015228,\n  0.16405677795410156,\n  0.15100373327732086,\n  0.20140308141708374,\n  0.18990357220172882,\n  0.20508041977882385,\n  0.17632170021533966,\n  0.15537774562835693,\n  0.1849638968706131,\n  0.19052910804748535,\n  0.1772887259721756,\n  0.1643608659505844,\n  0.16805100440979004,\n  0.20071373879909515,\n  0.1971094012260437,\n  0.18459713459014893,\n  0.18355311453342438,\n  0.1599375456571579,\n  0.21205353736877441,\n  0.18497790396213531,\n  0.16912740468978882,\n  0.1804972141981125,\n  0.18169113993644714,\n  0.22173599898815155,\n  0.1965801864862442,\n  0.16840772330760956,\n  0.17587615549564362,\n  0.20588919520378113,\n  0.1970968246459961,\n  0.1788892149925232,\n  0.17927058041095734,\n  0.18607661128044128,\n  0.19064190983772278,\n  0.19207729399204254,\n  0.18922637403011322,\n  0.19772155582904816,\n  0.21951842308044434,\n  0.19960090517997742,\n  0.1818905919790268,\n  0.19796276092529297,\n  0.1930553913116455,\n  0.22779124975204468,\n  0.21198458969593048,\n  0.15651817619800568,\n  0.18799981474876404,\n  0.20680512487888336,\n  0.23060773313045502,\n  0.1860116869211197,\n  0.16588251292705536,\n  0.19745223224163055,\n  0.2252134382724762,\n  0.17496170103549957,\n  0.14107948541641235,\n  0.20280079543590546,\n  0.20611131191253662,\n  0.22701355814933777,\n  0.16052865982055664,\n  0.17775607109069824,\n  0.22661225497722626,\n  0.192051962018013,\n  0.1460929960012436,\n  0.14735884964466095,\n  0.20159287750720978,\n  0.20543526113033295,\n  0.1854063868522644,\n  0.11928517371416092,\n  0.2010936588048935,\n  0.2512224316596985,\n  0.2115238457918167,\n  0.13602595031261444,\n  0.14194881916046143,\n  0.21325020492076874,\n  0.19392336905002594,\n  0.21647648513317108,\n  0.16122908890247345,\n  0.2027532309293747,\n  0.22189383208751678,\n  0.1805492788553238,\n  0.17927558720111847,\n  0.18155765533447266,\n  0.19185377657413483,\n  0.15719054639339447,\n  0.1720515936613083,\n  0.20556646585464478,\n  0.26090800762176514,\n  0.2067386358976364,\n  0.15947620570659637,\n  0.16843202710151672,\n  0.19522756338119507,\n  0.22057582437992096,\n  0.19079038500785828,\n  0.16763785481452942,\n  0.19808053970336914,\n  0.21830934286117554,\n  0.20162470638751984,\n  0.2081701010465622,\n  0.191751629114151,\n  0.1904454529285431,\n  0.15336187183856964,\n  0.18587958812713623,\n  0.17588329315185547,\n  0.2269810140132904,\n  0.2933681011199951,\n  0.16156810522079468,\n  0.1704317033290863,\n  0.2838069498538971,\n  0.2530786097049713,\n  0.22066304087638855,\n  0.23206079006195068,\n  0.1915290504693985,\n  0.23663613200187683,\n  0.2566339671611786,\n  0.28373295068740845,\n  0.27336180210113525,\n  0.19423291087150574,\n  0.20531988143920898,\n  0.27314049005508423,\n  0.27018609642982483,\n  0.2156018316745758,\n  0.2523818016052246,\n  0.26365986466407776,\n  0.2654469609260559,\n  0.2723323106765747,\n  0.25053954124450684,\n  0.2808894217014313,\n  0.27366700768470764,\n  0.18727834522724152,\n  0.2121214121580124,\n  0.2849031984806061,\n  0.27578189969062805,\n  0.20070727169513702,\n  0.1564953774213791,\n  0.18628838658332825,\n  0.2721024453639984,\n  0.2436356097459793,\n  0.1101643294095993,\n  0.15229648351669312,\n  0.17861135303974152,\n  0.21567155420780182,\n  0.2506108582019806,\n  0.16249001026153564,\n  0.12475386261940002,\n  0.15399883687496185,\n  0.18342754244804382,\n  0.19793421030044556,\n  0.1706319898366928,\n  0.13169008493423462,\n  0.1256256103515625,\n  0.095185287296772,\n  0.15800465643405914,\n  0.20136670768260956,\n  0.14828917384147644,\n  0.1053573414683342,\n  0.07797443866729736,\n  0.0820341631770134,\n  0.1604529470205307,\n  0.16852858662605286,\n  0.0868837758898735,\n  0.06377090513706207,\n  0.07339578866958618,\n  0.1589009314775467,\n  0.07045771926641464,\n  0.10673072189092636,\n  0.11062687635421753,\n  0.016003157943487167,\n  0.12658679485321045,\n  0.09390560537576675,\n  0.1640610694885254,\n  0.13307109475135803,\n  0.04252356290817261,\n  0.13241398334503174,\n  0.1708710491657257,\n  0.15425017476081848,\n  0.14411619305610657,\n  0.14666485786437988,\n  0.21208256483078003,\n  0.2102787345647812,\n  0.1700250655412674,\n  0.20964117348194122,\n  0.2144712507724762,\n  0.2620565891265869,\n  0.16710256040096283,\n  0.18139584362506866,\n  0.26197677850723267,\n  0.24239671230316162,\n  0.2520885765552521,\n  0.189466655254364,\n  0.19425946474075317,\n  0.3053194284439087,\n  0.28908631205558777,\n  0.13071680068969727,\n  0.24759681522846222,\n  0.2530600428581238,\n  0.2531977593898773,\n  0.28833937644958496,\n  0.13094383478164673,\n  0.265979140996933,\n  0.2557368576526642,\n  0.19350238144397736,\n  0.19290634989738464,\n  0.19101370871067047,\n  0.30208685994148254,\n  0.18355056643486023,\n  0.12207876145839691,\n  0.22815032303333282,\n  0.23971500992774963,\n  0.23605762422084808,\n  0.1427856981754303,\n  0.09819092601537704,\n  0.19019071757793427,\n  0.10862921178340912,\n  0.19720648229122162,\n  0.1529330462217331,\n  0.04495469853281975,\n  0.1587637960910797,\n  0.10475287586450577,\n  0.12817132472991943,\n  0.19106952846050262,\n  0.12554870545864105,\n  0.04189230501651764,\n  0.18365412950515747,\n  0.1899919956922531,\n  0.16199496388435364,\n  0.14516301453113556,\n  0.10955115407705307,\n  0.26754331588745117,\n  0.16960524022579193,\n  0.05689992383122444,\n  0.1617862433195114,\n  0.3261617124080658,\n  0.1722591072320938,\n  0.0946250781416893,\n  0.20169255137443542,\n  0.2661352753639221,\n  0.3588397800922394,\n  0.18640846014022827,\n  0.14233411848545074,\n  0.2697069048881531,\n  0.38733479380607605,\n  0.23149842023849487,\n  0.1668820083141327,\n  0.24216914176940918,\n  0.31248700618743896,\n  0.3611847758293152,\n  0.1881421059370041,\n  0.22667230665683746,\n  0.28674742579460144,\n  0.27953580021858215,\n  0.20253640413284302,\n  0.2850998342037201,\n  0.2856612801551819,\n  0.23929241299629211,\n  0.29373812675476074,\n  0.16752126812934875,\n  0.14465734362602234,\n  0.30180904269218445,\n  0.22591684758663177,\n  0.1554403156042099,\n  0.10008074343204498,\n  0.04298824816942215,\n  0.37809884548187256,\n  0.07781403511762619,\n  0.11319471150636673,\n  0.25997456908226013,\n  -0.06675796955823898,\n  0.19243308901786804,\n  0.17438814043998718,\n  0.07862742990255356,\n  0.16971801221370697,\n  0.0998997762799263,\n  0.1380174458026886,\n  0.11106329411268234,\n  0.06987228244543076,\n  0.1989331841468811,\n  0.05073559656739235,\n  -0.02466607466340065,\n  0.08928356319665909,\n  0.09410528093576431,\n  0.041151318699121475,\n  0.0482080802321434,\n  0.06304848939180374,\n  -0.08634315431118011,\n  0.0603288933634758,\n  0.04568880423903465,\n  -0.06550514698028564,\n  0.006714192219078541,\n  0.0018506264314055443,\n  0.13323751091957092,\n  -0.054965730756521225,\n  -0.06440145522356033,\n  0.057916246354579926,\n  0.0202677883207798,\n  0.10786118358373642,\n  -0.1756419688463211,\n  -0.03185394033789635,\n  0.23303793370723724,\n  -0.04542538896203041,\n  -0.06731046736240387,\n  0.03326089307665825,\n  0.10161314159631729,\n  0.007187204901129007,\n  -0.041852451860904694,\n  0.13799454271793365,\n  0.09653475880622864,\n  0.08143351972103119,\n  0.17326264083385468,\n  0.10277515649795532,\n  0.16423456370830536,\n  0.24756111204624176,\n  0.14046119153499603,\n  0.17206993699073792,\n  0.10996832698583603,\n  0.17802493274211884,\n  0.305418461561203,\n  0.18573379516601562,\n  0.13631291687488556,\n  0.19038525223731995,\n  0.1954462230205536,\n  0.10047372430562973,\n  0.21199531853199005,\n  0.21946461498737335,\n  0.2211315929889679,\n  0.19021716713905334,\n  0.03378908336162567,\n  0.26574796438217163,\n  0.4116191565990448,\n  0.0649474710226059,\n  -0.0018937676213681698,\n  0.2857989966869354,\n  0.2512943744659424,\n  0.24220725893974304,\n  0.17971469461917877,\n  0.216583251953125,\n  0.3444229066371918,\n  0.23749291896820068,\n  0.20153161883354187,\n  0.20834843814373016,\n  0.311097115278244,\n  0.3833671808242798,\n  0.14780688285827637,\n  0.1430518478155136,\n  0.44584792852401733,\n  0.17286570370197296,\n  0.1836579144001007,\n  0.11222544312477112,\n  0.12338225543498993,\n  0.35961857438087463,\n  -0.031972791999578476,\n  0.0665363147854805,\n  0.12051090598106384,\n  0.3044376075267792,\n  0.18306706845760345,\n  -0.14352688193321228,\n  0.18425790965557098,\n  0.17663121223449707,\n  -0.06698565930128098,\n  0.07636520266532898,\n  0.12112727761268616,\n  -0.09001550823450089,\n  0.23360784351825714,\n  0.03245619311928749,\n  -0.04636303335428238,\n  0.21815423667430878,\n  0.018771328032016754,\n  0.18474598228931427,\n  0.02281522937119007,\n  -0.011927706189453602,\n  0.17297910153865814,\n  0.04160306230187416,\n  0.15449145436286926,\n  0.08205354958772659,\n  -0.1117778941988945,\n  -0.11672943085432053,\n  0.15865112841129303,\n  0.10742507874965668,\n  -0.2244589477777481,\n  0.09632813185453415,\n  0.11664127558469772,\n  0.08010794222354889,\n  0.04168061912059784,\n  0.043481118977069855,\n  0.05187727138400078,\n  -0.09647415578365326,\n  0.159025639295578,\n  -0.001309222192503512,\n  0.10818047821521759,\n  0.0873953327536583,\n  -0.06020043417811394,\n  0.1255112588405609,\n  0.12258858233690262,\n  0.08032718300819397,\n  0.015988755971193314,\n  0.12406206130981445,\n  -0.019770391285419464,\n  0.11136449128389359,\n  0.10314074903726578,\n  -0.031213197857141495,\n  0.2537635862827301,\n  0.061027105897665024,\n  0.052099257707595825,\n  0.15019617974758148,\n  0.1768161654472351,\n  0.13037537038326263,\n  0.06416507065296173,\n  0.21282000839710236,\n  0.13394051790237427,\n  0.22673115134239197,\n  0.19984813034534454,\n  0.05265069752931595,\n  0.0016461400082334876,\n  0.23099729418754578,\n  0.23530197143554688,\n  0.09268296509981155,\n  0.12455190718173981,\n  0.01513657160103321,\n  0.20925520360469818,\n  0.1873946338891983,\n  0.11462797224521637,\n  0.03358649089932442,\n  0.21636557579040527,\n  0.3852701783180237,\n  -0.0417323112487793,\n  0.17025326192378998,\n  0.40254753828048706,\n  -0.00420666579157114,\n  0.18467038869857788,\n  0.27481934428215027,\n  0.07805348932743073,\n  0.3515224754810333,\n  0.18630099296569824,\n  0.1426696628332138,\n  0.3075868487358093,\n  0.02382562682032585,\n  0.005027327220886946,\n  0.20675300061702728,\n  0.33755946159362793,\n  0.12915916740894318,\n  -0.1424492597579956,\n  0.31215330958366394,\n  0.4046880602836609,\n  -0.027953295037150383,\n  -0.0247801523655653,\n  0.04708503559231758,\n  0.16770848631858826,\n  0.33789756894111633,\n  -0.11602143198251724,\n  -0.19518627226352692,\n  0.4390271306037903,\n  0.09454482793807983,\n  -0.2024681270122528,\n  0.1857856661081314,\n  0.08293720334768295,\n  -0.09335443377494812,\n  0.047092780470848083,\n  0.1467866152524948,\n  0.2739361524581909,\n  0.13169828057289124,\n  -0.041771698743104935,\n  0.06369972229003906,\n  0.18534715473651886,\n  0.11387783288955688,\n  -0.21570739150047302,\n  0.18655219674110413,\n  0.311752051115036,\n  -0.09601759910583496,\n  -0.06386344134807587,\n  -0.037033844739198685,\n  0.29004478454589844,\n  0.009495205245912075,\n  -0.3246642053127289,\n  0.31420624256134033,\n  0.23764362931251526,\n  -0.13851045072078705,\n  0.08655097335577011,\n  0.026158837601542473,\n  0.05474604293704033,\n  0.23890157043933868,\n  0.023362385109066963,\n  -0.02615244686603546,\n  0.3092283606529236,\n  0.07467491179704666,\n  -0.1842184066772461,\n  0.07554599642753601,\n  0.07962741702795029,\n  0.14104482531547546,\n  -0.22847554087638855,\n  -0.08111182600259781,\n  0.2907635569572449,\n  0.02911592833697796,\n  -0.22336086630821228,\n  -0.0379079207777977,\n  0.1420103758573532,\n  0.06355471909046173,\n  0.1563468873500824,\n  -0.3552098274230957,\n  0.29047706723213196,\n  0.31848832964897156,\n  -0.3163338601589203,\n  -0.023048309609293938,\n  0.16376648843288422,\n  0.37309780716896057,\n  -0.21177347004413605,\n  -0.0046522356569767,\n  0.23159611225128174,\n  0.2483496218919754,\n  0.03963010758161545,\n  -0.3444077670574188,\n  0.35263919830322266,\n  0.15253980457782745,\n  0.12451007962226868,\n  0.11208973824977875,\n  -0.07417764514684677,\n  0.2672556936740875,\n  0.17169252038002014,\n  -0.16015273332595825,\n  -0.01420173142105341,\n  0.3684484362602234,\n  0.14696988463401794,\n  0.009588937275111675,\n  0.17798581719398499,\n  0.5321884155273438,\n  -0.10643940418958664,\n  0.06282767653465271,\n  0.32277172803878784,\n  -0.19958072900772095,\n  0.07120499759912491,\n  0.16185744106769562,\n  0.25364989042282104,\n  0.012901358306407928,\n  0.0209867712110281,\n  0.46822869777679443,\n  0.08603102713823318,\n  -0.24960677325725555,\n  0.19484968483448029,\n  0.2875262200832367,\n  0.24780449271202087,\n  ...],\n 'labels': 4,\n 'attention_mask': [1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  ...]}"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[9]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "#from datasets import ClassLabel\n",
    "\n",
    "### label 2 id\n",
    "\n",
    "#train_df = train_df.cast_column(output_column, ClassLabel(num_classes=num_class,names=class_list))\n",
    "\n",
    "#test_df = test_df.cast_column(output_column, ClassLabel(num_classes=num_class,names=class_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "{'path': '../tess\\\\OAF_Fear\\\\OAF_choice_fear.wav',\n 'emotion': 'fear',\n 'input_values': [0.004722138401120901,\n  -0.00035432036384008825,\n  0.0010949079878628254,\n  -0.0003840071731247008,\n  -0.0029296313878148794,\n  -0.0010490970453247428,\n  -0.0042106714099645615,\n  -0.003468363545835018,\n  -0.003068897407501936,\n  -0.007702418137341738,\n  -0.007001595105975866,\n  -0.005584786646068096,\n  -0.0098490035161376,\n  -0.010392286814749241,\n  -0.010619891807436943,\n  -0.010391466319561005,\n  -0.008252652361989021,\n  -0.01080710906535387,\n  -0.011785155162215233,\n  -0.013369373977184296,\n  -0.012864617630839348,\n  -0.01156588364392519,\n  -0.013559042476117611,\n  -0.013344797305762768,\n  -0.014262956567108631,\n  -0.015416963025927544,\n  -0.0178836677223444,\n  -0.019587934017181396,\n  -0.01886187121272087,\n  -0.02166689932346344,\n  -0.020970268175005913,\n  -0.020148999989032745,\n  -0.02249022014439106,\n  -0.023697098717093468,\n  -0.024010350927710533,\n  -0.02196562848985195,\n  -0.023354414850473404,\n  -0.026017365977168083,\n  -0.024762241169810295,\n  -0.02593337930738926,\n  -0.0282309390604496,\n  -0.028595805168151855,\n  -0.02974061295390129,\n  -0.028263162821531296,\n  -0.02782205492258072,\n  -0.02976081520318985,\n  -0.02904570661485195,\n  -0.030289093032479286,\n  -0.03449389711022377,\n  -0.03329625353217125,\n  -0.03055497072637081,\n  -0.03262850642204285,\n  -0.03336312621831894,\n  -0.03188183531165123,\n  -0.035351838916540146,\n  -0.0349302738904953,\n  -0.03202066570520401,\n  -0.03519027307629585,\n  -0.035498831421136856,\n  -0.03586110472679138,\n  -0.036973416805267334,\n  -0.03951847180724144,\n  -0.03853566572070122,\n  -0.03874678164720535,\n  -0.04006452485918999,\n  -0.039626918733119965,\n  -0.04203310236334801,\n  -0.040330786257982254,\n  -0.04250440374016762,\n  -0.04000630974769592,\n  -0.04219507426023483,\n  -0.047141484916210175,\n  -0.04378268122673035,\n  -0.04376530647277832,\n  -0.04468093067407608,\n  -0.04790732264518738,\n  -0.04363163188099861,\n  -0.045383770018815994,\n  -0.047532372176647186,\n  -0.04464579001069069,\n  -0.04979978874325752,\n  -0.05023828148841858,\n  -0.051491014659404755,\n  -0.05082555487751961,\n  -0.04946482926607132,\n  -0.05397436022758484,\n  -0.05582556128501892,\n  -0.05670991539955139,\n  -0.057724010199308395,\n  -0.0560385063290596,\n  -0.056394629180431366,\n  -0.05911967530846596,\n  -0.058295171707868576,\n  -0.05893836170434952,\n  -0.061470404267311096,\n  -0.05768866464495659,\n  -0.06315455585718155,\n  -0.06478287279605865,\n  -0.06174362823367119,\n  -0.06468512862920761,\n  -0.06180548295378685,\n  -0.06554289907217026,\n  -0.06314513087272644,\n  -0.06362222135066986,\n  -0.06820127367973328,\n  -0.06563417613506317,\n  -0.06788856536149979,\n  -0.069966159760952,\n  -0.06921680271625519,\n  -0.06967274099588394,\n  -0.07009831815958023,\n  -0.06653643399477005,\n  -0.06703407317399979,\n  -0.0685633048415184,\n  -0.07005970925092697,\n  -0.07151269167661667,\n  -0.07048477232456207,\n  -0.07155580818653107,\n  -0.07103364169597626,\n  -0.07193291187286377,\n  -0.07299822568893433,\n  -0.0696200430393219,\n  -0.07226156443357468,\n  -0.07455278187990189,\n  -0.07162749022245407,\n  -0.0728595033288002,\n  -0.07461617141962051,\n  -0.07498369365930557,\n  -0.07739998400211334,\n  -0.07743015885353088,\n  -0.07690970599651337,\n  -0.077501580119133,\n  -0.07677741348743439,\n  -0.07667037844657898,\n  -0.07502992451190948,\n  -0.080093152821064,\n  -0.08017084747552872,\n  -0.07655797153711319,\n  -0.07845990359783173,\n  -0.07839223742485046,\n  -0.0790453627705574,\n  -0.07786445319652557,\n  -0.079071044921875,\n  -0.07818673551082611,\n  -0.07755286246538162,\n  -0.08037147670984268,\n  -0.07859484106302261,\n  -0.0785166546702385,\n  -0.07835516333580017,\n  -0.07831315696239471,\n  -0.08065486699342728,\n  -0.08141268044710159,\n  -0.08022473752498627,\n  -0.07933211326599121,\n  -0.07880164682865143,\n  -0.07469769567251205,\n  -0.07729091495275497,\n  -0.07964859157800674,\n  -0.07976708561182022,\n  -0.07957348227500916,\n  -0.07949639111757278,\n  -0.08274631202220917,\n  -0.0796869769692421,\n  -0.07881058007478714,\n  -0.0780712217092514,\n  -0.07791043072938919,\n  -0.07982658594846725,\n  -0.08038078993558884,\n  -0.0799013152718544,\n  -0.0821620523929596,\n  -0.08256619423627853,\n  -0.07898104190826416,\n  -0.0780702754855156,\n  -0.0772748589515686,\n  -0.08005411922931671,\n  -0.07884617894887924,\n  -0.07999390363693237,\n  -0.08213728666305542,\n  -0.0808616653084755,\n  -0.07881651073694229,\n  -0.07830771058797836,\n  -0.08106663823127747,\n  -0.07921645790338516,\n  -0.08037389814853668,\n  -0.0809716284275055,\n  -0.08188207447528839,\n  -0.08216454833745956,\n  -0.08134960383176804,\n  -0.08382145315408707,\n  -0.08270227909088135,\n  -0.08088914304971695,\n  -0.08309736847877502,\n  -0.08309108763933182,\n  -0.08319785445928574,\n  -0.08406398445367813,\n  -0.08303960412740707,\n  -0.08246895670890808,\n  -0.08072040230035782,\n  -0.0834728255867958,\n  -0.08535456657409668,\n  -0.08559484034776688,\n  -0.08399283140897751,\n  -0.0842071995139122,\n  -0.08729437738656998,\n  -0.08362948149442673,\n  -0.08571742475032806,\n  -0.08563755452632904,\n  -0.08340185135602951,\n  -0.0866934135556221,\n  -0.08381915092468262,\n  -0.08597474545240402,\n  -0.08207398653030396,\n  -0.08106156438589096,\n  -0.08763918280601501,\n  -0.08468715846538544,\n  -0.08569186180830002,\n  -0.08473484218120575,\n  -0.08135486394166946,\n  -0.08175569772720337,\n  -0.08340023458003998,\n  -0.08322643488645554,\n  -0.08338809758424759,\n  -0.08083625882863998,\n  -0.08340232819318771,\n  -0.08639685809612274,\n  -0.0839182585477829,\n  -0.08484958857297897,\n  -0.08339442312717438,\n  -0.084060437977314,\n  -0.08210448920726776,\n  -0.08161040395498276,\n  -0.08234062790870667,\n  -0.0807780846953392,\n  -0.08182451874017715,\n  -0.08200415223836899,\n  -0.07897461950778961,\n  -0.08005174249410629,\n  -0.0795208066701889,\n  -0.07659905403852463,\n  -0.08037733286619186,\n  -0.07897153496742249,\n  -0.07862494885921478,\n  -0.07676567882299423,\n  -0.07696469128131866,\n  -0.07790528237819672,\n  -0.07594757527112961,\n  -0.07464005798101425,\n  -0.07152128219604492,\n  -0.07437413185834885,\n  -0.07300577312707901,\n  -0.07305170595645905,\n  -0.07296223938465118,\n  -0.06879837065935135,\n  -0.0709415078163147,\n  -0.07201298326253891,\n  -0.07091901451349258,\n  -0.07106015086174011,\n  -0.07143417745828629,\n  -0.06999693065881729,\n  -0.06417661160230637,\n  -0.0663653016090393,\n  -0.06811448931694031,\n  -0.06525324285030365,\n  -0.06578604876995087,\n  -0.0649016723036766,\n  -0.06622482091188431,\n  -0.06341767311096191,\n  -0.06345299631357193,\n  -0.0654740035533905,\n  -0.06494266539812088,\n  -0.06475690007209778,\n  -0.06322009116411209,\n  -0.06072298064827919,\n  -0.061045367270708084,\n  -0.06418442726135254,\n  -0.06283383816480637,\n  -0.061827968806028366,\n  -0.061484746634960175,\n  -0.061511579900979996,\n  -0.06248127296566963,\n  -0.06289492547512054,\n  -0.06309071183204651,\n  -0.06207861751317978,\n  -0.061124708503484726,\n  -0.05872967094182968,\n  -0.05866961553692818,\n  -0.06155496835708618,\n  -0.05953770503401756,\n  -0.06042298302054405,\n  -0.06162388250231743,\n  -0.05848820134997368,\n  -0.059610333293676376,\n  -0.06375285983085632,\n  -0.061480872333049774,\n  -0.06022248789668083,\n  -0.060592200607061386,\n  -0.05897442251443863,\n  -0.061928603798151016,\n  -0.06042861565947533,\n  -0.06049070879817009,\n  -0.06008392572402954,\n  -0.059306155890226364,\n  -0.062390558421611786,\n  -0.06134128198027611,\n  -0.059171512722969055,\n  -0.061317894607782364,\n  -0.06192406266927719,\n  -0.05823897942900658,\n  -0.056886181235313416,\n  -0.05786532163619995,\n  -0.05988401174545288,\n  -0.05970548838376999,\n  -0.0585397370159626,\n  -0.05570288747549057,\n  -0.05674314126372337,\n  -0.05672897771000862,\n  -0.05639307200908661,\n  -0.06080075353384018,\n  -0.057579949498176575,\n  -0.05511186271905899,\n  -0.05546990782022476,\n  -0.05508888512849808,\n  -0.053306613117456436,\n  -0.05356211960315704,\n  -0.05268405005335808,\n  -0.05118066817522049,\n  -0.05451942980289459,\n  -0.053359340876340866,\n  -0.0520038940012455,\n  -0.05128735303878784,\n  -0.05210928991436958,\n  -0.05187283456325531,\n  -0.050698768347501755,\n  -0.04866822808980942,\n  -0.05085700750350952,\n  -0.0540064312517643,\n  -0.04811905696988106,\n  -0.048433978110551834,\n  -0.04787936434149742,\n  -0.04695030674338341,\n  -0.04787902534008026,\n  -0.04739850386977196,\n  -0.048138152807950974,\n  -0.04627310857176781,\n  -0.04446364939212799,\n  -0.04350743815302849,\n  -0.044492680579423904,\n  -0.044174715876579285,\n  -0.04556552320718765,\n  -0.044309042394161224,\n  -0.042656101286411285,\n  -0.043278276920318604,\n  -0.040750280022621155,\n  -0.04106048867106438,\n  -0.04259190708398819,\n  -0.04053874313831329,\n  -0.04120241850614548,\n  -0.041734129190444946,\n  -0.0391937755048275,\n  -0.04138447344303131,\n  -0.04188300669193268,\n  -0.03917941451072693,\n  -0.04040447995066643,\n  -0.04277306795120239,\n  -0.04073469713330269,\n  -0.040982458740472794,\n  -0.039772920310497284,\n  -0.03974428400397301,\n  -0.039241451770067215,\n  -0.03640652820467949,\n  -0.039236560463905334,\n  -0.03888148441910744,\n  -0.0367344468832016,\n  -0.03582669049501419,\n  -0.03916216269135475,\n  -0.03914028778672218,\n  -0.038725003600120544,\n  -0.040945395827293396,\n  -0.0403231643140316,\n  -0.04021235182881355,\n  -0.0384611114859581,\n  -0.038355037569999695,\n  -0.0379592664539814,\n  -0.04004402831196785,\n  -0.04154032841324806,\n  -0.039246030151844025,\n  -0.037446554750204086,\n  -0.03795608878135681,\n  -0.03874444216489792,\n  -0.04075156897306442,\n  -0.04424834996461868,\n  -0.041041649878025055,\n  -0.04134797677397728,\n  -0.04111213982105255,\n  -0.041603971272706985,\n  -0.044283535331487656,\n  -0.041244570165872574,\n  -0.042898084968328476,\n  -0.04178638383746147,\n  -0.041723184287548065,\n  -0.04588562622666359,\n  -0.04624110087752342,\n  -0.046684592962265015,\n  -0.045420434325933456,\n  -0.04492339864373207,\n  -0.044422026723623276,\n  -0.04613327980041504,\n  -0.04793958738446236,\n  -0.046098481863737106,\n  -0.04713065177202225,\n  -0.0462578609585762,\n  -0.05006276071071625,\n  -0.05030044913291931,\n  -0.04669441282749176,\n  -0.0481162928044796,\n  -0.04704256355762482,\n  -0.05019174888730049,\n  -0.04794672876596451,\n  -0.048260290175676346,\n  -0.050031986087560654,\n  -0.04609057307243347,\n  -0.048388704657554626,\n  -0.0461982786655426,\n  -0.04934263229370117,\n  -0.05126996710896492,\n  -0.0501965656876564,\n  -0.05267848074436188,\n  -0.047351446002721786,\n  -0.048614587634801865,\n  -0.049753643572330475,\n  -0.05084947496652603,\n  -0.05202003940939903,\n  -0.05032069608569145,\n  -0.05148022994399071,\n  -0.051376476883888245,\n  -0.05284547060728073,\n  -0.051085080951452255,\n  -0.050435565412044525,\n  -0.0502500981092453,\n  -0.05265995115041733,\n  -0.054433006793260574,\n  -0.05122644081711769,\n  -0.054473258554935455,\n  -0.05349208042025566,\n  -0.05011362209916115,\n  -0.05317823588848114,\n  -0.05499400570988655,\n  -0.05509115010499954,\n  -0.05311090126633644,\n  -0.05336632952094078,\n  -0.054888609796762466,\n  -0.05278532952070236,\n  -0.05146850273013115,\n  -0.05360289290547371,\n  -0.05317708104848862,\n  -0.05218030884861946,\n  -0.05453317612409592,\n  -0.054507993161678314,\n  -0.05345385521650314,\n  -0.05495021864771843,\n  -0.05731740593910217,\n  -0.056624576449394226,\n  -0.05759391188621521,\n  -0.057509321719408035,\n  -0.05704942345619202,\n  -0.058266643434762955,\n  -0.05778946354985237,\n  -0.059227120131254196,\n  -0.05849362537264824,\n  -0.05755625665187836,\n  -0.05917170271277428,\n  -0.05796109512448311,\n  -0.0615011490881443,\n  -0.06512492150068283,\n  -0.062443941831588745,\n  -0.06281987577676773,\n  -0.061823099851608276,\n  -0.06285180896520615,\n  -0.06557457149028778,\n  -0.06274241954088211,\n  -0.06330598145723343,\n  -0.06516287475824356,\n  -0.06281542032957077,\n  -0.06478726863861084,\n  -0.06519302725791931,\n  -0.065952830016613,\n  -0.06870989501476288,\n  -0.06519219279289246,\n  -0.06686002016067505,\n  -0.06941376626491547,\n  -0.06922702491283417,\n  -0.06999397277832031,\n  -0.06732504069805145,\n  -0.06608264893293381,\n  -0.06696011871099472,\n  -0.06928683072328568,\n  -0.06872919946908951,\n  -0.06926627457141876,\n  -0.06855951249599457,\n  -0.0662907212972641,\n  -0.0717024877667427,\n  -0.07164014875888824,\n  -0.07170392572879791,\n  -0.07189221680164337,\n  -0.07014050334692001,\n  -0.07186566293239594,\n  -0.07212458550930023,\n  -0.07182208448648453,\n  -0.07066617161035538,\n  -0.07248614728450775,\n  -0.07237161695957184,\n  -0.07072430104017258,\n  -0.07171117514371872,\n  -0.07477393746376038,\n  -0.07591249793767929,\n  -0.07419048249721527,\n  -0.07552967220544815,\n  -0.07632842659950256,\n  -0.07723385095596313,\n  -0.07848478108644485,\n  -0.07503899186849594,\n  -0.07501428574323654,\n  -0.078717902302742,\n  -0.07548561692237854,\n  -0.07768549025058746,\n  -0.07919406145811081,\n  -0.07809510082006454,\n  -0.08075040578842163,\n  -0.07845184206962585,\n  -0.08022341132164001,\n  -0.07984227687120438,\n  -0.07823993265628815,\n  -0.08056706190109253,\n  -0.08063267916440964,\n  -0.07976389676332474,\n  -0.08226028829813004,\n  -0.0806390792131424,\n  -0.08283640444278717,\n  -0.08679366111755371,\n  -0.08173219859600067,\n  -0.08321090042591095,\n  -0.08223149180412292,\n  -0.0808565691113472,\n  -0.08372683078050613,\n  -0.08427230268716812,\n  -0.08255397528409958,\n  -0.08471260964870453,\n  -0.08595681935548782,\n  -0.08563673496246338,\n  -0.08837790787220001,\n  -0.0865849182009697,\n  -0.08799277991056442,\n  -0.08995579928159714,\n  -0.08869299292564392,\n  -0.088750459253788,\n  -0.08830194920301437,\n  -0.08754803985357285,\n  -0.08926762640476227,\n  -0.08853060752153397,\n  -0.0908355563879013,\n  -0.09001555293798447,\n  -0.0892850011587143,\n  -0.09293057024478912,\n  -0.09053195267915726,\n  -0.09183084219694138,\n  -0.09225466847419739,\n  -0.09253289550542831,\n  -0.09274746477603912,\n  -0.09203095734119415,\n  -0.09306290000677109,\n  -0.09599572420120239,\n  -0.09718949347734451,\n  -0.09659775346517563,\n  -0.09756625443696976,\n  -0.09765937924385071,\n  -0.0971715897321701,\n  -0.09918089210987091,\n  -0.10156239569187164,\n  -0.09604204446077347,\n  -0.10020387172698975,\n  -0.10002144426107407,\n  -0.09748019278049469,\n  -0.1035730242729187,\n  -0.10214999318122864,\n  -0.10323785990476608,\n  -0.10079695284366608,\n  -0.09975854307413101,\n  -0.10240032523870468,\n  -0.10416172444820404,\n  -0.1048448234796524,\n  -0.10514620691537857,\n  -0.10536637902259827,\n  -0.1064455583691597,\n  -0.10883158445358276,\n  -0.10727593302726746,\n  -0.10840439796447754,\n  -0.10982157289981842,\n  -0.10997042804956436,\n  -0.10782277584075928,\n  -0.10800661146640778,\n  -0.11224444210529327,\n  -0.11292694509029388,\n  -0.1124487891793251,\n  -0.11238516867160797,\n  -0.11288382858037949,\n  -0.1137184351682663,\n  -0.11448308080434799,\n  -0.11458225548267365,\n  -0.11299290508031845,\n  -0.11404158174991608,\n  -0.11697293072938919,\n  -0.11792992800474167,\n  -0.1173115000128746,\n  -0.11987969279289246,\n  -0.12130006402730942,\n  -0.11923786997795105,\n  -0.11886143684387207,\n  -0.11694948375225067,\n  -0.1199415773153305,\n  -0.12083809077739716,\n  -0.12048118561506271,\n  -0.1230563297867775,\n  -0.12156394124031067,\n  -0.12247440963983536,\n  -0.12311847507953644,\n  -0.12621630728244781,\n  -0.12493163347244263,\n  -0.12303639948368073,\n  -0.12554316222667694,\n  -0.12500925362110138,\n  -0.1271599531173706,\n  -0.12610629200935364,\n  -0.1284542977809906,\n  -0.12742915749549866,\n  -0.12725605070590973,\n  -0.12908607721328735,\n  -0.12499392777681351,\n  -0.12875671684741974,\n  -0.13132953643798828,\n  -0.12846501171588898,\n  -0.12488210946321487,\n  -0.13161610066890717,\n  -0.12995381653308868,\n  -0.12756425142288208,\n  -0.13190005719661713,\n  -0.12848860025405884,\n  -0.1313176304101944,\n  -0.12980453670024872,\n  -0.13253988325595856,\n  -0.1328485608100891,\n  -0.1291523277759552,\n  -0.13407808542251587,\n  -0.13259905576705933,\n  -0.13116799294948578,\n  -0.13459841907024384,\n  -0.1321658492088318,\n  -0.13091617822647095,\n  -0.13500912487506866,\n  -0.13623955845832825,\n  -0.13414046168327332,\n  -0.13597942888736725,\n  -0.1378750205039978,\n  -0.1366461217403412,\n  -0.14062421023845673,\n  -0.14109723269939423,\n  -0.13830730319023132,\n  -0.1389344185590744,\n  -0.14092309772968292,\n  -0.14006979763507843,\n  -0.14350606501102448,\n  -0.14276184141635895,\n  -0.13925179839134216,\n  -0.14170944690704346,\n  -0.14009572565555573,\n  -0.14191481471061707,\n  -0.14165183901786804,\n  -0.14152947068214417,\n  -0.14148308336734772,\n  -0.14108455181121826,\n  -0.14327821135520935,\n  -0.14345812797546387,\n  -0.1431460678577423,\n  -0.14435212314128876,\n  -0.146293044090271,\n  -0.14619013667106628,\n  -0.1426142305135727,\n  -0.14249983429908752,\n  -0.14720579981803894,\n  -0.1451978236436844,\n  -0.14716193079948425,\n  -0.14753931760787964,\n  -0.1464557945728302,\n  -0.15006688237190247,\n  -0.1528485119342804,\n  -0.15561184287071228,\n  -0.15533457696437836,\n  -0.15337486565113068,\n  -0.15202952921390533,\n  -0.15636102855205536,\n  -0.15654604136943817,\n  -0.1548180729150772,\n  -0.1552511304616928,\n  -0.1562991738319397,\n  -0.15503041446208954,\n  -0.1560804694890976,\n  -0.15730462968349457,\n  -0.15625469386577606,\n  -0.16159303486347198,\n  -0.15637584030628204,\n  -0.15190695226192474,\n  -0.1530890017747879,\n  -0.15404093265533447,\n  -0.15533187985420227,\n  -0.16072462499141693,\n  -0.16249915957450867,\n  -0.1594647765159607,\n  -0.16223369538784027,\n  -0.1600806713104248,\n  -0.16117437183856964,\n  -0.16134075820446014,\n  -0.1611049920320511,\n  -0.16093260049819946,\n  -0.1609136164188385,\n  -0.16105686128139496,\n  -0.1614598035812378,\n  -0.1642993539571762,\n  -0.16537828743457794,\n  -0.1661936640739441,\n  -0.16475790739059448,\n  -0.16398824751377106,\n  -0.16338028013706207,\n  -0.16591903567314148,\n  -0.1638956069946289,\n  -0.16665704548358917,\n  -0.1667184978723526,\n  -0.1614633947610855,\n  -0.16419555246829987,\n  -0.16435730457305908,\n  -0.16353441774845123,\n  -0.16438926756381989,\n  -0.16705632209777832,\n  -0.16522184014320374,\n  -0.16473373770713806,\n  -0.16354253888130188,\n  -0.16722562909126282,\n  -0.16629229485988617,\n  -0.16355520486831665,\n  -0.16698679327964783,\n  -0.16545823216438293,\n  -0.17052094638347626,\n  -0.16947011649608612,\n  -0.16868771612644196,\n  -0.16926391422748566,\n  -0.1727699339389801,\n  -0.17192938923835754,\n  -0.16765707731246948,\n  -0.17260494828224182,\n  -0.17107944190502167,\n  -0.17129099369049072,\n  -0.17218025028705597,\n  -0.1743471622467041,\n  -0.17355284094810486,\n  -0.1732274889945984,\n  -0.17410396039485931,\n  -0.1749567985534668,\n  -0.17512676119804382,\n  -0.17105898261070251,\n  -0.17373695969581604,\n  -0.17278148233890533,\n  -0.17372813820838928,\n  -0.1749321073293686,\n  -0.17689575254917145,\n  -0.18163207173347473,\n  -0.18124333024024963,\n  -0.18181295692920685,\n  -0.1807766556739807,\n  -0.1829647570848465,\n  -0.1825575828552246,\n  -0.18015648424625397,\n  -0.1823229044675827,\n  -0.1833973079919815,\n  -0.1868859827518463,\n  -0.18340525031089783,\n  -0.18420039117336273,\n  -0.18518361449241638,\n  -0.18004971742630005,\n  -0.1847020834684372,\n  -0.1803930401802063,\n  -0.1784912794828415,\n  -0.18056316673755646,\n  -0.1822049468755722,\n  -0.18417468667030334,\n  -0.181447371840477,\n  -0.1870090365409851,\n  -0.1863621026277542,\n  -0.18643803894519806,\n  -0.18949191272258759,\n  -0.189589262008667,\n  -0.19282211363315582,\n  -0.19129112362861633,\n  -0.1931365430355072,\n  -0.19394004344940186,\n  -0.19390884041786194,\n  -0.19635416567325592,\n  -0.1961342990398407,\n  -0.1957722306251526,\n  -0.19256921112537384,\n  -0.1925300657749176,\n  -0.19125840067863464,\n  -0.1941021978855133,\n  -0.19673223793506622,\n  -0.1909332126379013,\n  -0.19337019324302673,\n  -0.1950780749320984,\n  -0.19230352342128754,\n  -0.19195634126663208,\n  -0.19431045651435852,\n  -0.1956067532300949,\n  -0.1931987702846527,\n  -0.19149023294448853,\n  -0.18838946521282196,\n  -0.1895640790462494,\n  -0.18997761607170105,\n  -0.19003000855445862,\n  -0.19359412789344788,\n  -0.19300714135169983,\n  -0.1919132024049759,\n  -0.19340062141418457,\n  -0.19417282938957214,\n  -0.19314152002334595,\n  -0.19334609806537628,\n  -0.1927206963300705,\n  -0.19512426853179932,\n  -0.1942262202501297,\n  -0.19250071048736572,\n  -0.19536219537258148,\n  -0.19932010769844055,\n  -0.19645696878433228,\n  -0.19351957738399506,\n  -0.19871243834495544,\n  -0.1953568309545517,\n  -0.1952415108680725,\n  -0.19582165777683258,\n  -0.195719376206398,\n  -0.1951787918806076,\n  -0.19396187365055084,\n  -0.1932130604982376,\n  -0.1924412101507187,\n  -0.19581811130046844,\n  -0.1972322314977646,\n  -0.19757172465324402,\n  -0.19309653341770172,\n  -0.19368760287761688,\n  -0.1892397701740265,\n  -0.1901790350675583,\n  -0.1913054883480072,\n  -0.18864768743515015,\n  -0.19456177949905396,\n  -0.1914144605398178,\n  -0.19292926788330078,\n  -0.1945621222257614,\n  -0.19551144540309906,\n  -0.19539053738117218,\n  -0.19316457211971283,\n  -0.19223497807979584,\n  -0.18866635859012604,\n  -0.18926142156124115,\n  -0.19172541797161102,\n  -0.19365601241588593,\n  -0.19364608824253082,\n  -0.19674044847488403,\n  -0.19619973003864288,\n  -0.1970076858997345,\n  -0.2002648264169693,\n  -0.2002905160188675,\n  -0.202808678150177,\n  -0.1975194662809372,\n  -0.19955575466156006,\n  -0.20232093334197998,\n  -0.19790631532669067,\n  -0.20232780277729034,\n  -0.20663072168827057,\n  -0.205756276845932,\n  -0.20453454554080963,\n  -0.20592910051345825,\n  -0.20479632914066315,\n  -0.2040899246931076,\n  -0.2040024846792221,\n  -0.203150674700737,\n  -0.20259201526641846,\n  -0.20456573367118835,\n  -0.20192496478557587,\n  -0.20483264327049255,\n  -0.21012672781944275,\n  -0.20710700750350952,\n  -0.21101203560829163,\n  -0.20827996730804443,\n  -0.2064238339662552,\n  -0.20517590641975403,\n  -0.1987830400466919,\n  -0.19812430441379547,\n  -0.20078547298908234,\n  -0.2002401351928711,\n  -0.2002132385969162,\n  -0.2020510584115982,\n  -0.20369170606136322,\n  -0.20339863002300262,\n  -0.1976694017648697,\n  -0.19729068875312805,\n  -0.1952635645866394,\n  -0.19644330441951752,\n  -0.20002779364585876,\n  -0.20259374380111694,\n  -0.2005023956298828,\n  -0.2019623965024948,\n  -0.20823819935321808,\n  -0.20409822463989258,\n  -0.2074938416481018,\n  -0.207305446267128,\n  -0.20603621006011963,\n  -0.2051790952682495,\n  -0.20070788264274597,\n  -0.20206020772457123,\n  -0.2073574811220169,\n  -0.21394212543964386,\n  -0.21065352857112885,\n  -0.21135279536247253,\n  -0.2156650722026825,\n  -0.21123935282230377,\n  -0.20934785902500153,\n  -0.21143993735313416,\n  -0.21350246667861938,\n  -0.21102704107761383,\n  -0.2059161514043808,\n  -0.20819872617721558,\n  -0.2126154750585556,\n  -0.21626272797584534,\n  -0.21448193490505219,\n  -0.2124967724084854,\n  -0.20930199325084686,\n  -0.20482765138149261,\n  -0.20508691668510437,\n  -0.20306946337223053,\n  -0.2061515897512436,\n  -0.20700931549072266,\n  -0.20663972198963165,\n  -0.20500841736793518,\n  -0.2065964639186859,\n  -0.21268434822559357,\n  -0.21353191137313843,\n  -0.21240784227848053,\n  -0.20622871816158295,\n  -0.20391078293323517,\n  -0.20304542779922485,\n  -0.2000129520893097,\n  -0.2023458182811737,\n  -0.21166731417179108,\n  -0.21532897651195526,\n  -0.2126011848449707,\n  -0.2103366255760193,\n  -0.20960137248039246,\n  -0.2088075876235962,\n  -0.2029941827058792,\n  -0.19717375934123993,\n  -0.1981656402349472,\n  -0.20570749044418335,\n  -0.20297430455684662,\n  -0.20192836225032806,\n  -0.20516981184482574,\n  -0.20622879266738892,\n  -0.20795337855815887,\n  -0.20124337077140808,\n  -0.19653339684009552,\n  -0.1960778832435608,\n  -0.1918196678161621,\n  -0.19281058013439178,\n  -0.19928668439388275,\n  -0.20279206335544586,\n  -0.2070871740579605,\n  -0.20545488595962524,\n  -0.20025020837783813,\n  -0.1985241025686264,\n  -0.2047661691904068,\n  -0.20673993229866028,\n  -0.2021203190088272,\n  -0.20297828316688538,\n  -0.20178088545799255,\n  -0.2054407298564911,\n  -0.21012897789478302,\n  -0.21005111932754517,\n  -0.2163442075252533,\n  -0.22228814661502838,\n  -0.21730297803878784,\n  -0.21031884849071503,\n  -0.20849387347698212,\n  -0.2101345807313919,\n  -0.2088998556137085,\n  -0.20742520689964294,\n  -0.20816265046596527,\n  -0.20665432512760162,\n  ...],\n 'labels': 2,\n 'attention_mask': [1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  ...]}"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "is_regression = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = numpy.squeeze(preds) if is_regression else numpy.argmax(preds, axis=1)\n",
    "\n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(numpy.float32).mean().item()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\tonib/.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h\\snapshots\\22aad52d435eb6dbaf354bdad9b0da84ce7d6156\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    model_name_path,\n",
    "    config=config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "#print(is_apex_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import apex\n",
    "from apex import amp\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/masterarbeit_programming/notebooks/content\",\n",
    "    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1.0,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    label_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"],\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install apex\n",
    "#!pip install -v --no-cache-dir   ../apex\n",
    "num_class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\n",
      "Cuda version 11.8.1\n",
      "Numba version: 0.56.4\n",
      "Numpy version: 1.23.4\n",
      "Torch version:  1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "###import\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import cuda\n",
    "import sys\n",
    "import numba\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Cuda version\", cuda.__version__)\n",
    "print(\"Numba version:\", numba.__version__)\n",
    "print(\"Numpy version:\", numpy.__version__)\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import apex\n",
    "print(\"cuda\",torch.cuda.is_available())\n",
    "transformers.utils.is_apex_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        #if self.amp.use_amp:\n",
    "        if self.use_cuda_amp:\n",
    "            print('*************************************************** OK cudaAmop*********************************************')\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            print('*************************************************** USING CPU *********************************************')\n",
    "            print('*************************************************** USING CPU *********************************************')\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            print('*************************************************** OKacc *********************************************')\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        #if self.use_amp:\n",
    "        if self.use_cuda_amp:\n",
    "            print('*************************************************** OK cuda *********************************************')\n",
    "            self.scaler.scale(loss).backward()\n",
    "        #elif self.use_apex:\n",
    "        elif self.use_apex:\n",
    "            print('*************************************************** USING APEX *********************************************')\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            print('*************************************************** USING DEEPSPEED *********************************************')\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            print('*************************************************** USING CPU (loss backward)*********************************************')\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=test_df,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "print(trainer.label_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "X:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 90766470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/240 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-10\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-10\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-10\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-10\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-20\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-20\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-20\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-20\\preprocessor_config.json\n",
      "Deleting older checkpoint [\\masterarbeit_programming\\notebooks\\content\\checkpoint-210] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-30\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-30\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-30\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-30\\preprocessor_config.json\n",
      "Deleting older checkpoint [\\masterarbeit_programming\\notebooks\\content\\checkpoint-10] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-40\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-40\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-40\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-40\\preprocessor_config.json\n",
      "Deleting older checkpoint [\\masterarbeit_programming\\notebooks\\content\\checkpoint-20] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-50\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-50\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-50\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-50\\preprocessor_config.json\n",
      "Deleting older checkpoint [\\masterarbeit_programming\\notebooks\\content\\checkpoint-30] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-60\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-60\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-60\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-60\\preprocessor_config.json\n",
      "Deleting older checkpoint [\\masterarbeit_programming\\notebooks\\content\\checkpoint-40] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /masterarbeit_programming/notebooks/content\\checkpoint-70\n",
      "Configuration saved in /masterarbeit_programming/notebooks/content\\checkpoint-70\\config.json\n",
      "Model weights saved in /masterarbeit_programming/notebooks/content\\checkpoint-70\\pytorch_model.bin\n",
      "Feature extractor saved in /masterarbeit_programming/notebooks/content\\checkpoint-70\\preprocessor_config.json\n",
      "Deleting older checkpoint [\\masterarbeit_programming\\notebooks\\content\\checkpoint-50] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n",
      "*************************************************** OK cudaAmop*********************************************\n",
      "*************************************************** OKacc *********************************************\n",
      "*************************************************** OK cuda *********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, path. If emotion, path are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 480\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [224], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:1501\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1498\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1500\u001B[0m )\n\u001B[1;32m-> 1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1503\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:1826\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1823\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[0;32m   1824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m-> 1826\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1827\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1828\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:2089\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2083\u001B[0m             metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(\n\u001B[0;32m   2084\u001B[0m                 eval_dataset\u001B[38;5;241m=\u001B[39meval_dataset,\n\u001B[0;32m   2085\u001B[0m                 ignore_keys\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[0;32m   2086\u001B[0m                 metric_key_prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00meval_dataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2087\u001B[0m             )\n\u001B[0;32m   2088\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2089\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2090\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_report_to_hp_search(trial, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step, metrics)\n\u001B[0;32m   2092\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:2796\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   2793\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   2795\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[1;32m-> 2796\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2797\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2799\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[0;32m   2800\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[0;32m   2801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   2802\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2803\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2804\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2806\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[0;32m   2807\u001B[0m output\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[0;32m   2808\u001B[0m     speed_metrics(\n\u001B[0;32m   2809\u001B[0m         metric_key_prefix,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2813\u001B[0m     )\n\u001B[0;32m   2814\u001B[0m )\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:2974\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   2971\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m observed_batch_size\n\u001B[0;32m   2973\u001B[0m \u001B[38;5;66;03m# Prediction step\u001B[39;00m\n\u001B[1;32m-> 2974\u001B[0m loss, logits, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprediction_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2975\u001B[0m inputs_decode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_input(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_inputs_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2977\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_tpu_available():\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\trainer.py:3227\u001B[0m, in \u001B[0;36mTrainer.prediction_step\u001B[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001B[0m\n\u001B[0;32m   3225\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   3226\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[1;32m-> 3227\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m   3228\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m   3229\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m outputs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ignore_keys)\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [195], line 72\u001B[0m, in \u001B[0;36mWav2Vec2ForSpeechClassification.forward\u001B[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     64\u001B[0m         input_values,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     69\u001B[0m         labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     70\u001B[0m ):\n\u001B[0;32m     71\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m---> 72\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwav2vec2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     80\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerged_strategy(hidden_states, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooling_mode)\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1311\u001B[0m, in \u001B[0;36mWav2Vec2Model.forward\u001B[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1306\u001B[0m hidden_states, extract_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_projection(extract_features)\n\u001B[0;32m   1307\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mask_hidden_states(\n\u001B[0;32m   1308\u001B[0m     hidden_states, mask_time_indices\u001B[38;5;241m=\u001B[39mmask_time_indices, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask\n\u001B[0;32m   1309\u001B[0m )\n\u001B[1;32m-> 1311\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1314\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1315\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1317\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1319\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madapter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:794\u001B[0m, in \u001B[0;36mWav2Vec2Encoder.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    788\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    789\u001B[0m             create_custom_forward(layer),\n\u001B[0;32m    790\u001B[0m             hidden_states,\n\u001B[0;32m    791\u001B[0m             attention_mask,\n\u001B[0;32m    792\u001B[0m         )\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 794\u001B[0m         layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    797\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    799\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m skip_the_layer:\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:675\u001B[0m, in \u001B[0;36mWav2Vec2EncoderLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states, attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, output_attentions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    674\u001B[0m     attn_residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m--> 675\u001B[0m     hidden_states, attn_weights, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    678\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[0;32m    679\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m attn_residual \u001B[38;5;241m+\u001B[39m hidden_states\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mX:\\masterarbeit_programming\\venv\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:571\u001B[0m, in \u001B[0;36mWav2Vec2Attention.forward\u001B[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001B[0m\n\u001B[0;32m    568\u001B[0m     past_key_value \u001B[38;5;241m=\u001B[39m (key_states, value_states)\n\u001B[0;32m    570\u001B[0m proj_shape \u001B[38;5;241m=\u001B[39m (bsz \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[1;32m--> 571\u001B[0m query_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbsz\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mproj_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    572\u001B[0m key_states \u001B[38;5;241m=\u001B[39m key_states\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m*\u001B[39mproj_shape)\n\u001B[0;32m    573\u001B[0m value_states \u001B[38;5;241m=\u001B[39m value_states\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m*\u001B[39mproj_shape)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# save_path=\"/masterarbeit_programming/notebooks/content/data\"\n",
    "# trainReal_df, testReal_df=train_test_split(trainDF, test_size=0.2, random_state=101, stratify=trainDF[\"label\"])\n",
    "#\n",
    "# trainRealDf=trainReal_df.reset_index(drop=True)\n",
    "# testRealDf=testReal_df.reset_index(drop=True)\n",
    "#\n",
    "# testRealDf.to_csv(f\"{save_path}/testReal.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the created dataset using datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "data_files = {\n",
    "    \"test\": \"/masterarbeit_programming/notebooks/content/data/test.csv\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "\n",
    "test_dfReal = dataset[\"test\"]\n",
    "\n",
    "\n",
    "print(test_dfReal)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device:{device}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name_path =  \"facebook/wav2vec2-base-960h\"\n",
    "config = AutoConfig.from_pretrained(model_name_path,)\n",
    "processor= Wav2Vec2Processor.from_pretrained(model_name_path)\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_path).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(batch):\n",
    "    features = processor(batch[\"input_values\"], sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = features.input_values.to(device)\n",
    "    #attention_mask = features.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values,\n",
    "                       #attention_mask=attention_mask\n",
    "                       ).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    batch[\"predicted\"] = pred_ids\n",
    "    return batch\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    speech_array = speech_array.squeeze().numpy()\n",
    "    speech_array = librosa.resample(numpy.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n",
    "\n",
    "    batch[\"speech\"] = speech_array\n",
    "    return batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df = test_df.map(speech_file_to_array_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dfReal.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = test_df.map(predict, batched=True, batch_size=8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "import IPython.display as idp\n",
    "\n",
    "idp.Audio(data=result[4][\"input_values\"], rate=target_sampling_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_names = [config.id2label[i] for i in range(config.num_labels)]\n",
    "label_names"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
