{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from network_models.soundsream_models_and_utils.ss_wrapper_trainer_experiments import ExperimentsTrainer\n",
    "import torch\n",
    "from network_models.soundsream_models_and_utils.ss_encoded_dataset import ss_encoded_dataset_full\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 6\n",
    "models_dir = \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1\"\n",
    "trials_per_model_type = 4\n",
    "epochs_per_model = 350\n",
    "save_highest_acc_min_acc = 0.6\n",
    "start_lr = 1e-4\n",
    "lr_quotient = 2\n",
    "gc.collect()\n",
    "data_set= ss_encoded_dataset_full(\n",
    "    csvPath=\"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/data/allEncodings_version0_12_1.pkl\", device=\"cuda\")\n",
    "\n",
    "exp_trainer = ExperimentsTrainer(dataset=data_set, device=device, models_dir=models_dir, batch_size=batch_size, trials_per_model_type=trials_per_model_type,\n",
    "                   epochs_per_model=epochs_per_model, start_lr=start_lr, lr_quotient=lr_quotient, save_highest_acc_min_acc=save_highest_acc_min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.089200  [    0/ 5482]\n",
      "loss: 2.050035  [  600/ 5482]\n",
      "loss: 1.723720  [ 1200/ 5482]\n",
      "loss: 1.995438  [ 1800/ 5482]\n",
      "loss: 2.039797  [ 2400/ 5482]\n",
      "loss: 2.038765  [ 3000/ 5482]\n",
      "loss: 1.793455  [ 3600/ 5482]\n",
      "loss: 1.837098  [ 4200/ 5482]\n",
      "loss: 2.071159  [ 4800/ 5482]\n",
      "loss: 1.955495  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.152     0.000     0.000    99\n",
      " disgust     0.152     0.000     0.000    107\n",
      "    fear     0.152     0.138     0.988    80\n",
      "   happy     0.152     0.000     0.000    77\n",
      " neutral     0.152     0.000     0.000    95\n",
      "     sad     0.152     0.000     0.000    91\n",
      "surprise     0.152     0.378     0.230    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.152     0.074     0.174    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 1.949685 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.892137  [    0/ 5482]\n",
      "loss: 2.033224  [  600/ 5482]\n",
      "loss: 1.778705  [ 1200/ 5482]\n",
      "loss: 1.801993  [ 1800/ 5482]\n",
      "loss: 1.964651  [ 2400/ 5482]\n",
      "loss: 1.909403  [ 3000/ 5482]\n",
      "loss: 2.185667  [ 3600/ 5482]\n",
      "loss: 1.776528  [ 4200/ 5482]\n",
      "loss: 2.010459  [ 4800/ 5482]\n",
      "loss: 1.903945  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.185     0.000     0.000    99\n",
      " disgust     0.185     0.000     0.000    107\n",
      "    fear     0.185     0.143     0.900    80\n",
      "   happy     0.185     0.000     0.000    77\n",
      " neutral     0.185     0.950     0.200    95\n",
      "     sad     0.185     0.000     0.000    91\n",
      "surprise     0.185     0.259     0.361    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.185     0.193     0.209    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 18.5%, Avg loss: 1.919077 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.656606  [    0/ 5482]\n",
      "loss: 1.958051  [  600/ 5482]\n",
      "loss: 1.988252  [ 1200/ 5482]\n",
      "loss: 1.953928  [ 1800/ 5482]\n",
      "loss: 1.798233  [ 2400/ 5482]\n",
      "loss: 1.798305  [ 3000/ 5482]\n",
      "loss: 1.932670  [ 3600/ 5482]\n",
      "loss: 1.795288  [ 4200/ 5482]\n",
      "loss: 1.896032  [ 4800/ 5482]\n",
      "loss: 1.859666  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.218     0.000     0.000    99\n",
      " disgust     0.218     0.000     0.000    107\n",
      "    fear     0.218     0.156     0.975    80\n",
      "   happy     0.218     0.000     0.000    77\n",
      " neutral     0.218     0.783     0.379    95\n",
      "     sad     0.218     0.000     0.000    91\n",
      "surprise     0.218     0.302     0.311    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.218     0.177     0.238    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.8%, Avg loss: 1.894890 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.975539  [    0/ 5482]\n",
      "loss: 1.977803  [  600/ 5482]\n",
      "loss: 1.955583  [ 1200/ 5482]\n",
      "loss: 2.012638  [ 1800/ 5482]\n",
      "loss: 2.020905  [ 2400/ 5482]\n",
      "loss: 1.910082  [ 3000/ 5482]\n",
      "loss: 1.896238  [ 3600/ 5482]\n",
      "loss: 2.070818  [ 4200/ 5482]\n",
      "loss: 1.698068  [ 4800/ 5482]\n",
      "loss: 1.849361  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.233     0.000     0.000    99\n",
      " disgust     0.233     0.000     0.000    107\n",
      "    fear     0.233     0.152     0.863    80\n",
      "   happy     0.233     0.000     0.000    77\n",
      " neutral     0.233     0.852     0.484    95\n",
      "     sad     0.233     0.000     0.000    91\n",
      "surprise     0.233     0.262     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.233     0.181     0.256    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 1.876977 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.940729  [    0/ 5482]\n",
      "loss: 1.976842  [  600/ 5482]\n",
      "loss: 1.849138  [ 1200/ 5482]\n",
      "loss: 2.121607  [ 1800/ 5482]\n",
      "loss: 1.745131  [ 2400/ 5482]\n",
      "loss: 1.825298  [ 3000/ 5482]\n",
      "loss: 1.927322  [ 3600/ 5482]\n",
      "loss: 1.751984  [ 4200/ 5482]\n",
      "loss: 1.756335  [ 4800/ 5482]\n",
      "loss: 1.854417  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.246     0.000     0.000    99\n",
      " disgust     0.246     0.000     0.000    107\n",
      "    fear     0.246     0.155     0.875    80\n",
      "   happy     0.246     0.000     0.000    77\n",
      " neutral     0.246     0.855     0.558    95\n",
      "     sad     0.246     0.000     0.000    91\n",
      "surprise     0.246     0.281     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.246     0.184     0.268    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 1.862440 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.831857  [    0/ 5482]\n",
      "loss: 1.781706  [  600/ 5482]\n",
      "loss: 1.676977  [ 1200/ 5482]\n",
      "loss: 1.879043  [ 1800/ 5482]\n",
      "loss: 1.882091  [ 2400/ 5482]\n",
      "loss: 1.773545  [ 3000/ 5482]\n",
      "loss: 1.706487  [ 3600/ 5482]\n",
      "loss: 1.780835  [ 4200/ 5482]\n",
      "loss: 2.011094  [ 4800/ 5482]\n",
      "loss: 1.646735  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.246     0.000     0.000    99\n",
      " disgust     0.246     0.000     0.000    107\n",
      "    fear     0.246     0.159     0.963    80\n",
      "   happy     0.246     0.000     0.000    77\n",
      " neutral     0.246     0.909     0.526    95\n",
      "     sad     0.246     0.000     0.000    91\n",
      "surprise     0.246     0.319     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.246     0.198     0.267    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 1.853700 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.843104  [    0/ 5482]\n",
      "loss: 1.827055  [  600/ 5482]\n",
      "loss: 1.876760  [ 1200/ 5482]\n",
      "loss: 1.609737  [ 1800/ 5482]\n",
      "loss: 1.776051  [ 2400/ 5482]\n",
      "loss: 1.539520  [ 3000/ 5482]\n",
      "loss: 2.024233  [ 3600/ 5482]\n",
      "loss: 1.830526  [ 4200/ 5482]\n",
      "loss: 1.776283  [ 4800/ 5482]\n",
      "loss: 1.666516  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.244     0.000     0.000    99\n",
      " disgust     0.244     0.000     0.000    107\n",
      "    fear     0.244     0.149     0.800    80\n",
      "   happy     0.244     0.000     0.000    77\n",
      " neutral     0.244     0.871     0.568    95\n",
      "     sad     0.244     0.000     0.000    91\n",
      "surprise     0.244     0.261     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.244     0.183     0.268    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.4%, Avg loss: 1.832549 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.756569  [    0/ 5482]\n",
      "loss: 1.818625  [  600/ 5482]\n",
      "loss: 1.749701  [ 1200/ 5482]\n",
      "loss: 1.720319  [ 1800/ 5482]\n",
      "loss: 1.919784  [ 2400/ 5482]\n",
      "loss: 1.980815  [ 3000/ 5482]\n",
      "loss: 1.805675  [ 3600/ 5482]\n",
      "loss: 1.860829  [ 4200/ 5482]\n",
      "loss: 1.550949  [ 4800/ 5482]\n",
      "loss: 1.722703  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.256     0.000     0.000    99\n",
      " disgust     0.256     0.000     0.000    107\n",
      "    fear     0.256     0.151     0.725    80\n",
      "   happy     0.256     0.353     0.156    77\n",
      " neutral     0.256     0.846     0.579    95\n",
      "     sad     0.256     0.000     0.000    91\n",
      "surprise     0.256     0.246     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.256     0.228     0.281    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 1.821361 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.739420  [    0/ 5482]\n",
      "loss: 1.698137  [  600/ 5482]\n",
      "loss: 1.702882  [ 1200/ 5482]\n",
      "loss: 1.896325  [ 1800/ 5482]\n",
      "loss: 1.794212  [ 2400/ 5482]\n",
      "loss: 1.777360  [ 3000/ 5482]\n",
      "loss: 1.861820  [ 3600/ 5482]\n",
      "loss: 1.834186  [ 4200/ 5482]\n",
      "loss: 1.876572  [ 4800/ 5482]\n",
      "loss: 1.884949  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.272     0.000     0.000    99\n",
      " disgust     0.272     0.000     0.000    107\n",
      "    fear     0.272     0.165     0.675    80\n",
      "   happy     0.272     0.274     0.221    77\n",
      " neutral     0.272     0.838     0.600    95\n",
      "     sad     0.272     0.000     0.000    91\n",
      "surprise     0.272     0.248     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.272     0.218     0.303    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 27.2%, Avg loss: 1.813189 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.768728  [    0/ 5482]\n",
      "loss: 1.767378  [  600/ 5482]\n",
      "loss: 1.734273  [ 1200/ 5482]\n",
      "loss: 2.011214  [ 1800/ 5482]\n",
      "loss: 1.825293  [ 2400/ 5482]\n",
      "loss: 1.711674  [ 3000/ 5482]\n",
      "loss: 1.821012  [ 3600/ 5482]\n",
      "loss: 1.775050  [ 4200/ 5482]\n",
      "loss: 1.637872  [ 4800/ 5482]\n",
      "loss: 1.683220  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.282     0.000     0.000    99\n",
      " disgust     0.282     0.000     0.000    107\n",
      "    fear     0.282     0.167     0.625    80\n",
      "   happy     0.282     0.276     0.273    77\n",
      " neutral     0.282     0.803     0.600    95\n",
      "     sad     0.282     0.000     0.000    91\n",
      "surprise     0.282     0.268     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.282     0.216     0.317    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 1.804845 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.961111  [    0/ 5482]\n",
      "loss: 1.825055  [  600/ 5482]\n",
      "loss: 1.771733  [ 1200/ 5482]\n",
      "loss: 1.898329  [ 1800/ 5482]\n",
      "loss: 1.934177  [ 2400/ 5482]\n",
      "loss: 1.805059  [ 3000/ 5482]\n",
      "loss: 1.580897  [ 3600/ 5482]\n",
      "loss: 1.755904  [ 4200/ 5482]\n",
      "loss: 1.699169  [ 4800/ 5482]\n",
      "loss: 1.707939  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.284     0.000     0.000    99\n",
      " disgust     0.284     0.000     0.000    107\n",
      "    fear     0.284     0.176     0.688    80\n",
      "   happy     0.284     0.292     0.338    77\n",
      " neutral     0.284     0.873     0.579    95\n",
      "     sad     0.284     0.000     0.000    91\n",
      "surprise     0.284     0.255     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.284     0.228     0.316    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.4%, Avg loss: 1.789738 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.870539  [    0/ 5482]\n",
      "loss: 1.799637  [  600/ 5482]\n",
      "loss: 1.680586  [ 1200/ 5482]\n",
      "loss: 1.745369  [ 1800/ 5482]\n",
      "loss: 1.977617  [ 2400/ 5482]\n",
      "loss: 1.658853  [ 3000/ 5482]\n",
      "loss: 1.763387  [ 3600/ 5482]\n",
      "loss: 1.664553  [ 4200/ 5482]\n",
      "loss: 1.844414  [ 4800/ 5482]\n",
      "loss: 1.872428  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.298     0.000     0.000    99\n",
      " disgust     0.298     0.000     0.000    107\n",
      "    fear     0.298     0.193     0.738    80\n",
      "   happy     0.298     0.268     0.390    77\n",
      " neutral     0.298     0.864     0.600    95\n",
      "     sad     0.298     0.000     0.000    91\n",
      "surprise     0.298     0.286     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.298     0.230     0.331    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 29.8%, Avg loss: 1.779207 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.888898  [    0/ 5482]\n",
      "loss: 1.586767  [  600/ 5482]\n",
      "loss: 1.795308  [ 1200/ 5482]\n",
      "loss: 1.904268  [ 1800/ 5482]\n",
      "loss: 1.802226  [ 2400/ 5482]\n",
      "loss: 1.901107  [ 3000/ 5482]\n",
      "loss: 1.741549  [ 3600/ 5482]\n",
      "loss: 1.586866  [ 4200/ 5482]\n",
      "loss: 1.743451  [ 4800/ 5482]\n",
      "loss: 1.809612  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.302     0.000     0.000    99\n",
      " disgust     0.302     0.000     0.000    107\n",
      "    fear     0.302     0.191     0.600    80\n",
      "   happy     0.302     0.267     0.364    77\n",
      " neutral     0.302     0.744     0.674    95\n",
      "     sad     0.302     0.000     0.000    91\n",
      "surprise     0.302     0.262     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.302     0.209     0.337    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 1.776603 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.849453  [    0/ 5482]\n",
      "loss: 1.711820  [  600/ 5482]\n",
      "loss: 1.831830  [ 1200/ 5482]\n",
      "loss: 1.772449  [ 1800/ 5482]\n",
      "loss: 1.648754  [ 2400/ 5482]\n",
      "loss: 1.760833  [ 3000/ 5482]\n",
      "loss: 1.773991  [ 3600/ 5482]\n",
      "loss: 1.673561  [ 4200/ 5482]\n",
      "loss: 1.615489  [ 4800/ 5482]\n",
      "loss: 1.873476  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.316     0.000     0.000    99\n",
      " disgust     0.316     1.000     0.037    107\n",
      "    fear     0.316     0.214     0.688    80\n",
      "   happy     0.316     0.260     0.416    77\n",
      " neutral     0.316     0.797     0.663    95\n",
      "     sad     0.316     0.000     0.000    91\n",
      "surprise     0.316     0.265     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.316     0.362     0.349    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 1.760861 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.239960  [    0/ 5482]\n",
      "loss: 1.699371  [  600/ 5482]\n",
      "loss: 1.623773  [ 1200/ 5482]\n",
      "loss: 1.645965  [ 1800/ 5482]\n",
      "loss: 1.697903  [ 2400/ 5482]\n",
      "loss: 1.916747  [ 3000/ 5482]\n",
      "loss: 1.859843  [ 3600/ 5482]\n",
      "loss: 1.545727  [ 4200/ 5482]\n",
      "loss: 1.738684  [ 4800/ 5482]\n",
      "loss: 1.595733  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.331     0.000     0.000    99\n",
      " disgust     0.331     1.000     0.140    107\n",
      "    fear     0.331     0.214     0.662    80\n",
      "   happy     0.331     0.270     0.390    77\n",
      " neutral     0.331     0.857     0.632    95\n",
      "     sad     0.331     0.000     0.000    91\n",
      "surprise     0.331     0.265     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.331     0.372     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.753650 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.727855  [    0/ 5482]\n",
      "loss: 1.666244  [  600/ 5482]\n",
      "loss: 1.785160  [ 1200/ 5482]\n",
      "loss: 1.979725  [ 1800/ 5482]\n",
      "loss: 1.662622  [ 2400/ 5482]\n",
      "loss: 1.534113  [ 3000/ 5482]\n",
      "loss: 1.667360  [ 3600/ 5482]\n",
      "loss: 1.749498  [ 4200/ 5482]\n",
      "loss: 1.667463  [ 4800/ 5482]\n",
      "loss: 1.821553  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.351     0.000     0.000    99\n",
      " disgust     0.351     1.000     0.252    107\n",
      "    fear     0.351     0.244     0.675    80\n",
      "   happy     0.351     0.242     0.403    77\n",
      " neutral     0.351     0.833     0.632    95\n",
      "     sad     0.351     0.000     0.000    91\n",
      "surprise     0.351     0.259     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.351     0.368     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 1.743904 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.787487  [    0/ 5482]\n",
      "loss: 1.723055  [  600/ 5482]\n",
      "loss: 1.729633  [ 1200/ 5482]\n",
      "loss: 1.756051  [ 1800/ 5482]\n",
      "loss: 1.599147  [ 2400/ 5482]\n",
      "loss: 1.790919  [ 3000/ 5482]\n",
      "loss: 1.698434  [ 3600/ 5482]\n",
      "loss: 1.752006  [ 4200/ 5482]\n",
      "loss: 1.909735  [ 4800/ 5482]\n",
      "loss: 1.807205  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.362     0.000     0.000    99\n",
      " disgust     0.362     1.000     0.393    107\n",
      "    fear     0.362     0.286     0.475    80\n",
      "   happy     0.362     0.293     0.351    77\n",
      " neutral     0.362     0.681     0.674    95\n",
      "     sad     0.362     0.000     0.000    91\n",
      "surprise     0.362     0.201     0.820    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.362     0.352     0.387    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 1.764510 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.750208  [    0/ 5482]\n",
      "loss: 1.519872  [  600/ 5482]\n",
      "loss: 2.056587  [ 1200/ 5482]\n",
      "loss: 1.735485  [ 1800/ 5482]\n",
      "loss: 1.512516  [ 2400/ 5482]\n",
      "loss: 1.485286  [ 3000/ 5482]\n",
      "loss: 1.687925  [ 3600/ 5482]\n",
      "loss: 1.628057  [ 4200/ 5482]\n",
      "loss: 1.793595  [ 4800/ 5482]\n",
      "loss: 1.633585  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.389     0.000     0.000    99\n",
      " disgust     0.389     0.977     0.393    107\n",
      "    fear     0.389     0.281     0.625    80\n",
      "   happy     0.389     0.252     0.429    77\n",
      " neutral     0.389     0.798     0.705    95\n",
      "     sad     0.389     0.000     0.000    91\n",
      "surprise     0.389     0.259     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.389     0.367     0.413    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.728828 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.914763  [    0/ 5482]\n",
      "loss: 1.826622  [  600/ 5482]\n",
      "loss: 1.730055  [ 1200/ 5482]\n",
      "loss: 1.836602  [ 1800/ 5482]\n",
      "loss: 1.752360  [ 2400/ 5482]\n",
      "loss: 1.713769  [ 3000/ 5482]\n",
      "loss: 1.647547  [ 3600/ 5482]\n",
      "loss: 1.697917  [ 4200/ 5482]\n",
      "loss: 1.672636  [ 4800/ 5482]\n",
      "loss: 1.799261  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.403     0.000     0.000    99\n",
      " disgust     0.403     0.944     0.477    107\n",
      "    fear     0.403     0.304     0.650    80\n",
      "   happy     0.403     0.264     0.429    77\n",
      " neutral     0.403     0.812     0.684    95\n",
      "     sad     0.403     0.000     0.000    91\n",
      "surprise     0.403     0.250     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.403     0.368     0.425    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.721960 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.727923  [    0/ 5482]\n",
      "loss: 1.868686  [  600/ 5482]\n",
      "loss: 1.571429  [ 1200/ 5482]\n",
      "loss: 1.828551  [ 1800/ 5482]\n",
      "loss: 1.503849  [ 2400/ 5482]\n",
      "loss: 1.966706  [ 3000/ 5482]\n",
      "loss: 1.816706  [ 3600/ 5482]\n",
      "loss: 1.848726  [ 4200/ 5482]\n",
      "loss: 1.560850  [ 4800/ 5482]\n",
      "loss: 1.775536  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.416     0.000     0.000    99\n",
      " disgust     0.416     0.851     0.533    107\n",
      "    fear     0.416     0.354     0.575    80\n",
      "   happy     0.416     0.274     0.442    77\n",
      " neutral     0.416     0.747     0.747    95\n",
      "     sad     0.416     0.000     0.000    91\n",
      "surprise     0.416     0.237     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.416     0.352     0.436    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.714357 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.903445  [    0/ 5482]\n",
      "loss: 1.834514  [  600/ 5482]\n",
      "loss: 1.539746  [ 1200/ 5482]\n",
      "loss: 1.594959  [ 1800/ 5482]\n",
      "loss: 1.729336  [ 2400/ 5482]\n",
      "loss: 1.565167  [ 3000/ 5482]\n",
      "loss: 1.758411  [ 3600/ 5482]\n",
      "loss: 1.650699  [ 4200/ 5482]\n",
      "loss: 1.670828  [ 4800/ 5482]\n",
      "loss: 1.593835  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.410     0.000     0.000    99\n",
      " disgust     0.410     0.867     0.486    107\n",
      "    fear     0.410     0.387     0.575    80\n",
      "   happy     0.410     0.282     0.429    77\n",
      " neutral     0.410     0.758     0.758    95\n",
      "     sad     0.410     0.000     0.000    91\n",
      "surprise     0.410     0.215     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.410     0.358     0.431    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.715303 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.981776  [    0/ 5482]\n",
      "loss: 1.802876  [  600/ 5482]\n",
      "loss: 1.797694  [ 1200/ 5482]\n",
      "loss: 1.898626  [ 1800/ 5482]\n",
      "loss: 1.613772  [ 2400/ 5482]\n",
      "loss: 1.549290  [ 3000/ 5482]\n",
      "loss: 1.758553  [ 3600/ 5482]\n",
      "loss: 1.597194  [ 4200/ 5482]\n",
      "loss: 1.739643  [ 4800/ 5482]\n",
      "loss: 1.679643  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.416     0.000     0.000    99\n",
      " disgust     0.416     0.873     0.514    107\n",
      "    fear     0.416     0.345     0.613    80\n",
      "   happy     0.416     0.259     0.494    77\n",
      " neutral     0.416     0.861     0.716    95\n",
      "     sad     0.416     0.000     0.000    91\n",
      "surprise     0.416     0.246     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.416     0.369     0.437    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.697199 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.769638  [    0/ 5482]\n",
      "loss: 1.629286  [  600/ 5482]\n",
      "loss: 1.638531  [ 1200/ 5482]\n",
      "loss: 1.665455  [ 1800/ 5482]\n",
      "loss: 1.606057  [ 2400/ 5482]\n",
      "loss: 1.719916  [ 3000/ 5482]\n",
      "loss: 1.719710  [ 3600/ 5482]\n",
      "loss: 1.704750  [ 4200/ 5482]\n",
      "loss: 1.634318  [ 4800/ 5482]\n",
      "loss: 1.384973  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.434     0.000     0.000    99\n",
      " disgust     0.434     0.753     0.598    107\n",
      "    fear     0.434     0.429     0.600    80\n",
      "   happy     0.434     0.270     0.481    77\n",
      " neutral     0.434     0.742     0.758    95\n",
      "     sad     0.434     0.000     0.000    91\n",
      "surprise     0.434     0.246     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.434     0.349     0.451    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 1.685468 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.617592  [    0/ 5482]\n",
      "loss: 1.821645  [  600/ 5482]\n",
      "loss: 1.721185  [ 1200/ 5482]\n",
      "loss: 1.657993  [ 1800/ 5482]\n",
      "loss: 1.633142  [ 2400/ 5482]\n",
      "loss: 1.614338  [ 3000/ 5482]\n",
      "loss: 1.852883  [ 3600/ 5482]\n",
      "loss: 1.716060  [ 4200/ 5482]\n",
      "loss: 1.694842  [ 4800/ 5482]\n",
      "loss: 1.535217  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.425     0.000     0.000    99\n",
      " disgust     0.425     0.785     0.579    107\n",
      "    fear     0.425     0.398     0.537    80\n",
      "   happy     0.425     0.261     0.481    77\n",
      " neutral     0.425     0.778     0.737    95\n",
      "     sad     0.425     0.000     0.000    91\n",
      "surprise     0.425     0.246     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.425     0.352     0.444    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 1.681229 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.606207  [    0/ 5482]\n",
      "loss: 1.505736  [  600/ 5482]\n",
      "loss: 1.781526  [ 1200/ 5482]\n",
      "loss: 1.665195  [ 1800/ 5482]\n",
      "loss: 1.390966  [ 2400/ 5482]\n",
      "loss: 1.672453  [ 3000/ 5482]\n",
      "loss: 1.772073  [ 3600/ 5482]\n",
      "loss: 1.721878  [ 4200/ 5482]\n",
      "loss: 1.813694  [ 4800/ 5482]\n",
      "loss: 1.549157  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.430     0.000     0.000    99\n",
      " disgust     0.430     0.795     0.617    107\n",
      "    fear     0.430     0.373     0.550    80\n",
      "   happy     0.430     0.255     0.532    77\n",
      " neutral     0.430     0.863     0.726    95\n",
      "     sad     0.430     0.000     0.000    91\n",
      "surprise     0.430     0.250     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.430     0.362     0.445    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 1.670961 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.555632  [    0/ 5482]\n",
      "loss: 1.580880  [  600/ 5482]\n",
      "loss: 1.681795  [ 1200/ 5482]\n",
      "loss: 1.581084  [ 1800/ 5482]\n",
      "loss: 1.584847  [ 2400/ 5482]\n",
      "loss: 1.601118  [ 3000/ 5482]\n",
      "loss: 1.496675  [ 3600/ 5482]\n",
      "loss: 1.616396  [ 4200/ 5482]\n",
      "loss: 1.678584  [ 4800/ 5482]\n",
      "loss: 1.470177  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.441     0.000     0.000    99\n",
      " disgust     0.441     0.747     0.664    107\n",
      "    fear     0.441     0.444     0.500    80\n",
      "   happy     0.441     0.249     0.636    77\n",
      " neutral     0.441     0.774     0.758    95\n",
      "     sad     0.441     0.000     0.000    91\n",
      "surprise     0.441     0.274     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.441     0.356     0.452    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 1.667257 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.622580  [    0/ 5482]\n",
      "loss: 1.548056  [  600/ 5482]\n",
      "loss: 1.805061  [ 1200/ 5482]\n",
      "loss: 1.581436  [ 1800/ 5482]\n",
      "loss: 1.560974  [ 2400/ 5482]\n",
      "loss: 1.402195  [ 3000/ 5482]\n",
      "loss: 1.767952  [ 3600/ 5482]\n",
      "loss: 1.735048  [ 4200/ 5482]\n",
      "loss: 1.778156  [ 4800/ 5482]\n",
      "loss: 1.696596  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.434     0.000     0.000    99\n",
      " disgust     0.434     0.717     0.664    107\n",
      "    fear     0.434     0.506     0.525    80\n",
      "   happy     0.434     0.283     0.442    77\n",
      " neutral     0.434     0.800     0.716    95\n",
      "     sad     0.434     0.000     0.000    91\n",
      "surprise     0.434     0.224     0.820    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.434     0.362     0.452    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 1.673278 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.611414  [    0/ 5482]\n",
      "loss: 1.632058  [  600/ 5482]\n",
      "loss: 1.636856  [ 1200/ 5482]\n",
      "loss: 1.593193  [ 1800/ 5482]\n",
      "loss: 1.634073  [ 2400/ 5482]\n",
      "loss: 1.449870  [ 3000/ 5482]\n",
      "loss: 1.620856  [ 3600/ 5482]\n",
      "loss: 1.544112  [ 4200/ 5482]\n",
      "loss: 1.440281  [ 4800/ 5482]\n",
      "loss: 1.816902  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.420     0.000     0.000    99\n",
      " disgust     0.420     0.788     0.626    107\n",
      "    fear     0.420     0.396     0.500    80\n",
      "   happy     0.420     0.235     0.662    77\n",
      " neutral     0.420     0.829     0.663    95\n",
      "     sad     0.420     0.000     0.000    91\n",
      "surprise     0.420     0.267     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.420     0.359     0.432    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.0%, Avg loss: 1.661855 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.601757  [    0/ 5482]\n",
      "loss: 1.571771  [  600/ 5482]\n",
      "loss: 1.726598  [ 1200/ 5482]\n",
      "loss: 1.544575  [ 1800/ 5482]\n",
      "loss: 1.491030  [ 2400/ 5482]\n",
      "loss: 1.436902  [ 3000/ 5482]\n",
      "loss: 1.601631  [ 3600/ 5482]\n",
      "loss: 1.547838  [ 4200/ 5482]\n",
      "loss: 1.352501  [ 4800/ 5482]\n",
      "loss: 1.592154  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.448     0.000     0.000    99\n",
      " disgust     0.448     0.716     0.729    107\n",
      "    fear     0.448     0.506     0.487    80\n",
      "   happy     0.448     0.251     0.623    77\n",
      " neutral     0.448     0.769     0.737    95\n",
      "     sad     0.448     0.000     0.000    91\n",
      "surprise     0.448     0.268     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.448     0.359     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.643091 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.484860  [    0/ 5482]\n",
      "loss: 1.461125  [  600/ 5482]\n",
      "loss: 1.645518  [ 1200/ 5482]\n",
      "loss: 1.479346  [ 1800/ 5482]\n",
      "loss: 1.454189  [ 2400/ 5482]\n",
      "loss: 1.693390  [ 3000/ 5482]\n",
      "loss: 1.439111  [ 3600/ 5482]\n",
      "loss: 1.508928  [ 4200/ 5482]\n",
      "loss: 1.563764  [ 4800/ 5482]\n",
      "loss: 1.504541  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.459     0.000     0.000    99\n",
      " disgust     0.459     0.737     0.785    107\n",
      "    fear     0.459     0.512     0.525    80\n",
      "   happy     0.459     0.283     0.532    77\n",
      " neutral     0.459     0.807     0.747    95\n",
      "     sad     0.459     0.000     0.000    91\n",
      "surprise     0.459     0.233     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.459     0.367     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 1.632595 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.760757  [    0/ 5482]\n",
      "loss: 1.665638  [  600/ 5482]\n",
      "loss: 1.467810  [ 1200/ 5482]\n",
      "loss: 1.584005  [ 1800/ 5482]\n",
      "loss: 1.450307  [ 2400/ 5482]\n",
      "loss: 1.512857  [ 3000/ 5482]\n",
      "loss: 1.540876  [ 3600/ 5482]\n",
      "loss: 1.850717  [ 4200/ 5482]\n",
      "loss: 1.532867  [ 4800/ 5482]\n",
      "loss: 1.486721  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.456     0.000     0.000    99\n",
      " disgust     0.456     0.741     0.748    107\n",
      "    fear     0.456     0.506     0.537    80\n",
      "   happy     0.456     0.284     0.494    77\n",
      " neutral     0.456     0.791     0.758    95\n",
      "     sad     0.456     0.000     0.000    91\n",
      "surprise     0.456     0.236     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.456     0.365     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.632432 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.447344  [    0/ 5482]\n",
      "loss: 1.575869  [  600/ 5482]\n",
      "loss: 1.426182  [ 1200/ 5482]\n",
      "loss: 1.706792  [ 1800/ 5482]\n",
      "loss: 1.380728  [ 2400/ 5482]\n",
      "loss: 1.829848  [ 3000/ 5482]\n",
      "loss: 1.626270  [ 3600/ 5482]\n",
      "loss: 1.655625  [ 4200/ 5482]\n",
      "loss: 1.700168  [ 4800/ 5482]\n",
      "loss: 1.483472  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.448     0.167     0.010    99\n",
      " disgust     0.448     0.784     0.645    107\n",
      "    fear     0.448     0.460     0.575    80\n",
      "   happy     0.448     0.278     0.545    77\n",
      " neutral     0.448     0.864     0.737    95\n",
      "     sad     0.448     0.000     0.000    91\n",
      "surprise     0.448     0.245     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.448     0.400     0.464    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.622158 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.547940  [    0/ 5482]\n",
      "loss: 1.632684  [  600/ 5482]\n",
      "loss: 1.686658  [ 1200/ 5482]\n",
      "loss: 1.391522  [ 1800/ 5482]\n",
      "loss: 1.616216  [ 2400/ 5482]\n",
      "loss: 1.608408  [ 3000/ 5482]\n",
      "loss: 1.678284  [ 3600/ 5482]\n",
      "loss: 1.605561  [ 4200/ 5482]\n",
      "loss: 1.616337  [ 4800/ 5482]\n",
      "loss: 1.722571  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.469     0.333     0.010    99\n",
      " disgust     0.469     0.697     0.794    107\n",
      "    fear     0.469     0.583     0.525    80\n",
      "   happy     0.469     0.264     0.610    77\n",
      " neutral     0.469     0.824     0.737    95\n",
      "     sad     0.469     0.000     0.000    91\n",
      "surprise     0.469     0.273     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.469     0.425     0.478    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 1.611088 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.364729  [    0/ 5482]\n",
      "loss: 1.472777  [  600/ 5482]\n",
      "loss: 1.453208  [ 1200/ 5482]\n",
      "loss: 1.547232  [ 1800/ 5482]\n",
      "loss: 1.462208  [ 2400/ 5482]\n",
      "loss: 1.534737  [ 3000/ 5482]\n",
      "loss: 1.438071  [ 3600/ 5482]\n",
      "loss: 1.675031  [ 4200/ 5482]\n",
      "loss: 1.485621  [ 4800/ 5482]\n",
      "loss: 1.627730  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.456     0.182     0.020    99\n",
      " disgust     0.456     0.736     0.729    107\n",
      "    fear     0.456     0.532     0.512    80\n",
      "   happy     0.456     0.280     0.584    77\n",
      " neutral     0.456     0.826     0.747    95\n",
      "     sad     0.456     0.000     0.000    91\n",
      "surprise     0.456     0.243     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.456     0.400     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.606442 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.498102  [    0/ 5482]\n",
      "loss: 1.595228  [  600/ 5482]\n",
      "loss: 1.519910  [ 1200/ 5482]\n",
      "loss: 1.632610  [ 1800/ 5482]\n",
      "loss: 1.574348  [ 2400/ 5482]\n",
      "loss: 1.755192  [ 3000/ 5482]\n",
      "loss: 1.717800  [ 3600/ 5482]\n",
      "loss: 1.571802  [ 4200/ 5482]\n",
      "loss: 1.762105  [ 4800/ 5482]\n",
      "loss: 1.546639  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.449     0.091     0.020    99\n",
      " disgust     0.449     0.814     0.654    107\n",
      "    fear     0.449     0.524     0.550    80\n",
      "   happy     0.449     0.317     0.494    77\n",
      " neutral     0.449     0.826     0.747    95\n",
      "     sad     0.449     0.000     0.000    91\n",
      "surprise     0.449     0.231     0.803    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.449     0.400     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 1.612070 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.670664  [    0/ 5482]\n",
      "loss: 1.386782  [  600/ 5482]\n",
      "loss: 1.620025  [ 1200/ 5482]\n",
      "loss: 1.606975  [ 1800/ 5482]\n",
      "loss: 1.641100  [ 2400/ 5482]\n",
      "loss: 1.606804  [ 3000/ 5482]\n",
      "loss: 1.498896  [ 3600/ 5482]\n",
      "loss: 1.676264  [ 4200/ 5482]\n",
      "loss: 1.671268  [ 4800/ 5482]\n",
      "loss: 1.370713  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.456     0.167     0.030    99\n",
      " disgust     0.456     0.771     0.692    107\n",
      "    fear     0.456     0.672     0.487    80\n",
      "   happy     0.456     0.336     0.481    77\n",
      " neutral     0.456     0.745     0.768    95\n",
      "     sad     0.456     0.000     0.000    91\n",
      "surprise     0.456     0.226     0.852    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.456     0.417     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.621749 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.133095  [    0/ 5482]\n",
      "loss: 1.560068  [  600/ 5482]\n",
      "loss: 1.493384  [ 1200/ 5482]\n",
      "loss: 1.506259  [ 1800/ 5482]\n",
      "loss: 1.585845  [ 2400/ 5482]\n",
      "loss: 1.438801  [ 3000/ 5482]\n",
      "loss: 1.429379  [ 3600/ 5482]\n",
      "loss: 1.369613  [ 4200/ 5482]\n",
      "loss: 1.306329  [ 4800/ 5482]\n",
      "loss: 1.431850  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.464     0.231     0.061    99\n",
      " disgust     0.464     0.724     0.785    107\n",
      "    fear     0.464     0.556     0.438    80\n",
      "   happy     0.464     0.287     0.597    77\n",
      " neutral     0.464     0.835     0.747    95\n",
      "     sad     0.464     0.000     0.000    91\n",
      "surprise     0.464     0.256     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.464     0.413     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.582780 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.535337  [    0/ 5482]\n",
      "loss: 1.516255  [  600/ 5482]\n",
      "loss: 1.634913  [ 1200/ 5482]\n",
      "loss: 1.558001  [ 1800/ 5482]\n",
      "loss: 1.402628  [ 2400/ 5482]\n",
      "loss: 1.602005  [ 3000/ 5482]\n",
      "loss: 1.557313  [ 3600/ 5482]\n",
      "loss: 1.364697  [ 4200/ 5482]\n",
      "loss: 1.600287  [ 4800/ 5482]\n",
      "loss: 1.463962  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.417     0.101    99\n",
      " disgust     0.492     0.754     0.804    107\n",
      "    fear     0.492     0.600     0.562    80\n",
      "   happy     0.492     0.280     0.662    77\n",
      " neutral     0.492     0.866     0.747    95\n",
      "     sad     0.492     0.000     0.000    91\n",
      "surprise     0.492     0.278     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.456     0.498    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.576085 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.447622  [    0/ 5482]\n",
      "loss: 1.490951  [  600/ 5482]\n",
      "loss: 1.442390  [ 1200/ 5482]\n",
      "loss: 1.538974  [ 1800/ 5482]\n",
      "loss: 1.603853  [ 2400/ 5482]\n",
      "loss: 1.618841  [ 3000/ 5482]\n",
      "loss: 1.579864  [ 3600/ 5482]\n",
      "loss: 1.290275  [ 4200/ 5482]\n",
      "loss: 1.548504  [ 4800/ 5482]\n",
      "loss: 1.400004  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.475     0.238     0.051    99\n",
      " disgust     0.475     0.737     0.785    107\n",
      "    fear     0.475     0.567     0.475    80\n",
      "   happy     0.475     0.307     0.545    77\n",
      " neutral     0.475     0.743     0.789    95\n",
      "     sad     0.475     0.000     0.000    91\n",
      "surprise     0.475     0.271     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.475     0.409     0.486    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.574231 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.608016  [    0/ 5482]\n",
      "loss: 1.577523  [  600/ 5482]\n",
      "loss: 1.498893  [ 1200/ 5482]\n",
      "loss: 1.481465  [ 1800/ 5482]\n",
      "loss: 1.805157  [ 2400/ 5482]\n",
      "loss: 1.588993  [ 3000/ 5482]\n",
      "loss: 1.402104  [ 3600/ 5482]\n",
      "loss: 1.729429  [ 4200/ 5482]\n",
      "loss: 1.509678  [ 4800/ 5482]\n",
      "loss: 1.589494  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.470     0.160     0.040    99\n",
      " disgust     0.470     0.722     0.729    107\n",
      "    fear     0.470     0.684     0.487    80\n",
      "   happy     0.470     0.342     0.532    77\n",
      " neutral     0.470     0.747     0.779    95\n",
      "     sad     0.470     0.000     0.000    91\n",
      "surprise     0.470     0.254     0.836    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.470     0.416     0.486    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 1.584758 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.494872  [    0/ 5482]\n",
      "loss: 1.398775  [  600/ 5482]\n",
      "loss: 1.610570  [ 1200/ 5482]\n",
      "loss: 1.328140  [ 1800/ 5482]\n",
      "loss: 1.481346  [ 2400/ 5482]\n",
      "loss: 1.466786  [ 3000/ 5482]\n",
      "loss: 1.638857  [ 3600/ 5482]\n",
      "loss: 1.441034  [ 4200/ 5482]\n",
      "loss: 1.438233  [ 4800/ 5482]\n",
      "loss: 1.481988  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.467     0.257     0.091    99\n",
      " disgust     0.467     0.752     0.738    107\n",
      "    fear     0.467     0.603     0.512    80\n",
      "   happy     0.467     0.285     0.610    77\n",
      " neutral     0.467     0.755     0.747    95\n",
      "     sad     0.467     0.000     0.000    91\n",
      "surprise     0.467     0.266     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.467     0.417     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 1.558391 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.322836  [    0/ 5482]\n",
      "loss: 1.383399  [  600/ 5482]\n",
      "loss: 1.460787  [ 1200/ 5482]\n",
      "loss: 1.657129  [ 1800/ 5482]\n",
      "loss: 1.439079  [ 2400/ 5482]\n",
      "loss: 1.328542  [ 3000/ 5482]\n",
      "loss: 1.449964  [ 3600/ 5482]\n",
      "loss: 1.364013  [ 4200/ 5482]\n",
      "loss: 1.472156  [ 4800/ 5482]\n",
      "loss: 1.480022  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.375     0.152    99\n",
      " disgust     0.487     0.746     0.794    107\n",
      "    fear     0.487     0.582     0.487    80\n",
      "   happy     0.487     0.275     0.610    77\n",
      " neutral     0.487     0.864     0.737    95\n",
      "     sad     0.487     0.000     0.000    91\n",
      "surprise     0.487     0.299     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.449     0.493    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.545539 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.410634  [    0/ 5482]\n",
      "loss: 1.693111  [  600/ 5482]\n",
      "loss: 1.521694  [ 1200/ 5482]\n",
      "loss: 1.369443  [ 1800/ 5482]\n",
      "loss: 1.529865  [ 2400/ 5482]\n",
      "loss: 1.415294  [ 3000/ 5482]\n",
      "loss: 1.441395  [ 3600/ 5482]\n",
      "loss: 1.640855  [ 4200/ 5482]\n",
      "loss: 1.308386  [ 4800/ 5482]\n",
      "loss: 1.658538  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.475     0.192    99\n",
      " disgust     0.505     0.739     0.822    107\n",
      "    fear     0.505     0.627     0.525    80\n",
      "   happy     0.505     0.278     0.675    77\n",
      " neutral     0.505     0.843     0.737    95\n",
      "     sad     0.505     0.000     0.000    91\n",
      "surprise     0.505     0.325     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.470     0.508    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.540861 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.294940  [    0/ 5482]\n",
      "loss: 1.350813  [  600/ 5482]\n",
      "loss: 1.429391  [ 1200/ 5482]\n",
      "loss: 1.372990  [ 1800/ 5482]\n",
      "loss: 1.269064  [ 2400/ 5482]\n",
      "loss: 1.497794  [ 3000/ 5482]\n",
      "loss: 1.493905  [ 3600/ 5482]\n",
      "loss: 1.568055  [ 4200/ 5482]\n",
      "loss: 1.295159  [ 4800/ 5482]\n",
      "loss: 1.569173  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.383     0.182    99\n",
      " disgust     0.492     0.718     0.785    107\n",
      "    fear     0.492     0.581     0.450    80\n",
      "   happy     0.492     0.290     0.584    77\n",
      " neutral     0.492     0.847     0.758    95\n",
      "     sad     0.492     0.000     0.000    91\n",
      "surprise     0.492     0.312     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.447     0.500    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.534892 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.534166  [    0/ 5482]\n",
      "loss: 1.211190  [  600/ 5482]\n",
      "loss: 1.441552  [ 1200/ 5482]\n",
      "loss: 1.393584  [ 1800/ 5482]\n",
      "loss: 1.362642  [ 2400/ 5482]\n",
      "loss: 1.268062  [ 3000/ 5482]\n",
      "loss: 1.373955  [ 3600/ 5482]\n",
      "loss: 1.515047  [ 4200/ 5482]\n",
      "loss: 1.245354  [ 4800/ 5482]\n",
      "loss: 1.555881  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.432     0.192    99\n",
      " disgust     0.493     0.702     0.813    107\n",
      "    fear     0.493     0.654     0.425    80\n",
      "   happy     0.493     0.295     0.597    77\n",
      " neutral     0.493     0.740     0.779    95\n",
      "     sad     0.493     0.000     0.000    91\n",
      "surprise     0.493     0.306     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.447     0.497    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.528449 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.671801  [    0/ 5482]\n",
      "loss: 1.325894  [  600/ 5482]\n",
      "loss: 1.324476  [ 1200/ 5482]\n",
      "loss: 1.291299  [ 1800/ 5482]\n",
      "loss: 1.467637  [ 2400/ 5482]\n",
      "loss: 1.464200  [ 3000/ 5482]\n",
      "loss: 1.359261  [ 3600/ 5482]\n",
      "loss: 1.378955  [ 4200/ 5482]\n",
      "loss: 1.304806  [ 4800/ 5482]\n",
      "loss: 1.510317  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.439     0.253    99\n",
      " disgust     0.502     0.725     0.813    107\n",
      "    fear     0.502     0.636     0.438    80\n",
      "   happy     0.502     0.285     0.558    77\n",
      " neutral     0.502     0.765     0.789    95\n",
      "     sad     0.502     0.000     0.000    91\n",
      "surprise     0.502     0.318     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.453     0.503    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.524242 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.503686  [    0/ 5482]\n",
      "loss: 1.615449  [  600/ 5482]\n",
      "loss: 1.463492  [ 1200/ 5482]\n",
      "loss: 1.418629  [ 1800/ 5482]\n",
      "loss: 1.476926  [ 2400/ 5482]\n",
      "loss: 1.437414  [ 3000/ 5482]\n",
      "loss: 1.414105  [ 3600/ 5482]\n",
      "loss: 1.480724  [ 4200/ 5482]\n",
      "loss: 1.459623  [ 4800/ 5482]\n",
      "loss: 1.344609  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.547     0.293    99\n",
      " disgust     0.513     0.736     0.832    107\n",
      "    fear     0.513     0.640     0.400    80\n",
      "   happy     0.513     0.296     0.623    77\n",
      " neutral     0.513     0.745     0.768    95\n",
      "     sad     0.513     0.000     0.000    91\n",
      "surprise     0.513     0.333     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.471     0.515    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.514794 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.265399  [    0/ 5482]\n",
      "loss: 1.365243  [  600/ 5482]\n",
      "loss: 1.261689  [ 1200/ 5482]\n",
      "loss: 1.304218  [ 1800/ 5482]\n",
      "loss: 1.342969  [ 2400/ 5482]\n",
      "loss: 1.693551  [ 3000/ 5482]\n",
      "loss: 1.665855  [ 3600/ 5482]\n",
      "loss: 1.363990  [ 4200/ 5482]\n",
      "loss: 1.348346  [ 4800/ 5482]\n",
      "loss: 1.487918  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.554     0.686     0.485    99\n",
      " disgust     0.554     0.767     0.860    107\n",
      "    fear     0.554     0.673     0.463    80\n",
      "   happy     0.554     0.289     0.649    77\n",
      " neutral     0.554     0.747     0.747    95\n",
      "     sad     0.554     0.000     0.000    91\n",
      "surprise     0.554     0.412     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.554     0.511     0.551    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.508881 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.429101  [    0/ 5482]\n",
      "loss: 1.268660  [  600/ 5482]\n",
      "loss: 1.353068  [ 1200/ 5482]\n",
      "loss: 1.213840  [ 1800/ 5482]\n",
      "loss: 1.381944  [ 2400/ 5482]\n",
      "loss: 1.377128  [ 3000/ 5482]\n",
      "loss: 1.521418  [ 3600/ 5482]\n",
      "loss: 1.275570  [ 4200/ 5482]\n",
      "loss: 1.425840  [ 4800/ 5482]\n",
      "loss: 1.362295  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.556     0.354    99\n",
      " disgust     0.525     0.750     0.757    107\n",
      "    fear     0.525     0.636     0.438    80\n",
      "   happy     0.525     0.310     0.584    77\n",
      " neutral     0.525     0.720     0.811    95\n",
      "     sad     0.525     0.000     0.000    91\n",
      "surprise     0.525     0.356     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.475     0.530    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.507339 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.532584  [    0/ 5482]\n",
      "loss: 1.333253  [  600/ 5482]\n",
      "loss: 1.526066  [ 1200/ 5482]\n",
      "loss: 1.245172  [ 1800/ 5482]\n",
      "loss: 1.335262  [ 2400/ 5482]\n",
      "loss: 1.612872  [ 3000/ 5482]\n",
      "loss: 1.222797  [ 3600/ 5482]\n",
      "loss: 1.336114  [ 4200/ 5482]\n",
      "loss: 1.234692  [ 4800/ 5482]\n",
      "loss: 1.241534  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.536     0.542     0.455    99\n",
      " disgust     0.536     0.798     0.738    107\n",
      "    fear     0.536     0.672     0.487    80\n",
      "   happy     0.536     0.321     0.558    77\n",
      " neutral     0.536     0.768     0.768    95\n",
      "     sad     0.536     0.000     0.000    91\n",
      "surprise     0.536     0.340     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.536     0.492     0.542    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.511319 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.233688  [    0/ 5482]\n",
      "loss: 1.281192  [  600/ 5482]\n",
      "loss: 1.662691  [ 1200/ 5482]\n",
      "loss: 1.384660  [ 1800/ 5482]\n",
      "loss: 1.302873  [ 2400/ 5482]\n",
      "loss: 1.120463  [ 3000/ 5482]\n",
      "loss: 1.460958  [ 3600/ 5482]\n",
      "loss: 1.214272  [ 4200/ 5482]\n",
      "loss: 1.422205  [ 4800/ 5482]\n",
      "loss: 1.291020  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.577     0.765     0.657    99\n",
      " disgust     0.577     0.705     0.869    107\n",
      "    fear     0.577     0.661     0.487    80\n",
      "   happy     0.577     0.279     0.662    77\n",
      " neutral     0.577     0.814     0.737    95\n",
      "     sad     0.577     0.000     0.000    91\n",
      "surprise     0.577     0.523     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.577     0.535     0.567    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.487233 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.358138  [    0/ 5482]\n",
      "loss: 1.327020  [  600/ 5482]\n",
      "loss: 1.280300  [ 1200/ 5482]\n",
      "loss: 1.327549  [ 1800/ 5482]\n",
      "loss: 1.547668  [ 2400/ 5482]\n",
      "loss: 1.476200  [ 3000/ 5482]\n",
      "loss: 1.349854  [ 3600/ 5482]\n",
      "loss: 1.416904  [ 4200/ 5482]\n",
      "loss: 1.283021  [ 4800/ 5482]\n",
      "loss: 1.323935  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.620     0.576    99\n",
      " disgust     0.557     0.764     0.757    107\n",
      "    fear     0.557     0.672     0.512    80\n",
      "   happy     0.557     0.302     0.584    77\n",
      " neutral     0.557     0.831     0.726    95\n",
      "     sad     0.557     0.000     0.000    91\n",
      "surprise     0.557     0.395     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.512     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.499723 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.220043  [    0/ 5482]\n",
      "loss: 1.377601  [  600/ 5482]\n",
      "loss: 1.564562  [ 1200/ 5482]\n",
      "loss: 1.423559  [ 1800/ 5482]\n",
      "loss: 1.505612  [ 2400/ 5482]\n",
      "loss: 1.386289  [ 3000/ 5482]\n",
      "loss: 1.276068  [ 3600/ 5482]\n",
      "loss: 1.192472  [ 4200/ 5482]\n",
      "loss: 1.216005  [ 4800/ 5482]\n",
      "loss: 1.250342  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.580     0.603     0.707    99\n",
      " disgust     0.580     0.717     0.757    107\n",
      "    fear     0.580     0.603     0.475    80\n",
      "   happy     0.580     0.307     0.545    77\n",
      " neutral     0.580     0.791     0.758    95\n",
      "     sad     0.580     0.750     0.099    91\n",
      "surprise     0.580     0.538     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.580     0.616     0.576    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.478596 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.380903  [    0/ 5482]\n",
      "loss: 1.509079  [  600/ 5482]\n",
      "loss: 1.235731  [ 1200/ 5482]\n",
      "loss: 1.326119  [ 1800/ 5482]\n",
      "loss: 1.480746  [ 2400/ 5482]\n",
      "loss: 1.432122  [ 3000/ 5482]\n",
      "loss: 1.499162  [ 3600/ 5482]\n",
      "loss: 1.413613  [ 4200/ 5482]\n",
      "loss: 1.372571  [ 4800/ 5482]\n",
      "loss: 1.444176  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.585     0.598     0.646    99\n",
      " disgust     0.585     0.723     0.804    107\n",
      "    fear     0.585     0.735     0.450    80\n",
      "   happy     0.585     0.272     0.286    77\n",
      " neutral     0.585     0.732     0.747    95\n",
      "     sad     0.585     0.583     0.385    91\n",
      "surprise     0.585     0.443     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.585     0.584     0.575    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.483019 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.472347  [    0/ 5482]\n",
      "loss: 1.351518  [  600/ 5482]\n",
      "loss: 1.216670  [ 1200/ 5482]\n",
      "loss: 1.513563  [ 1800/ 5482]\n",
      "loss: 1.191538  [ 2400/ 5482]\n",
      "loss: 1.501322  [ 3000/ 5482]\n",
      "loss: 1.385901  [ 3600/ 5482]\n",
      "loss: 1.314307  [ 4200/ 5482]\n",
      "loss: 1.124925  [ 4800/ 5482]\n",
      "loss: 1.148595  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.613     0.729     0.707    99\n",
      " disgust     0.613     0.734     0.850    107\n",
      "    fear     0.613     0.712     0.463    80\n",
      "   happy     0.613     0.209     0.234    77\n",
      " neutral     0.613     0.763     0.747    95\n",
      "     sad     0.613     0.527     0.527    91\n",
      "surprise     0.613     0.574     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.613     0.607     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.461260 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep54_acc_61.md \n",
      "\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep54_acc_61\"!  new accuracy: 61.3\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.192937  [    0/ 5482]\n",
      "loss: 1.334720  [  600/ 5482]\n",
      "loss: 1.495926  [ 1200/ 5482]\n",
      "loss: 1.319507  [ 1800/ 5482]\n",
      "loss: 1.249970  [ 2400/ 5482]\n",
      "loss: 1.599794  [ 3000/ 5482]\n",
      "loss: 1.167031  [ 3600/ 5482]\n",
      "loss: 1.291516  [ 4200/ 5482]\n",
      "loss: 1.230551  [ 4800/ 5482]\n",
      "loss: 1.404726  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.605     0.622     0.697    99\n",
      " disgust     0.605     0.726     0.794    107\n",
      "    fear     0.605     0.672     0.512    80\n",
      "   happy     0.605     0.233     0.182    77\n",
      " neutral     0.605     0.781     0.789    95\n",
      "     sad     0.605     0.530     0.484    91\n",
      "surprise     0.605     0.500     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.605     0.581     0.590    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.462876 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.219870  [    0/ 5482]\n",
      "loss: 1.375483  [  600/ 5482]\n",
      "loss: 1.234329  [ 1200/ 5482]\n",
      "loss: 1.383741  [ 1800/ 5482]\n",
      "loss: 1.174857  [ 2400/ 5482]\n",
      "loss: 1.355285  [ 3000/ 5482]\n",
      "loss: 1.284154  [ 3600/ 5482]\n",
      "loss: 1.473222  [ 4200/ 5482]\n",
      "loss: 1.280192  [ 4800/ 5482]\n",
      "loss: 1.203313  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.608     0.697     0.697    99\n",
      " disgust     0.608     0.676     0.879    107\n",
      "    fear     0.608     0.717     0.412    80\n",
      "   happy     0.608     0.206     0.169    77\n",
      " neutral     0.608     0.745     0.768    95\n",
      "     sad     0.608     0.539     0.527    91\n",
      "surprise     0.608     0.539     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.608     0.589     0.589    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 1.453191 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.226376  [    0/ 5482]\n",
      "loss: 1.267224  [  600/ 5482]\n",
      "loss: 1.338623  [ 1200/ 5482]\n",
      "loss: 1.173244  [ 1800/ 5482]\n",
      "loss: 1.418655  [ 2400/ 5482]\n",
      "loss: 1.382002  [ 3000/ 5482]\n",
      "loss: 1.512983  [ 3600/ 5482]\n",
      "loss: 1.299493  [ 4200/ 5482]\n",
      "loss: 1.319470  [ 4800/ 5482]\n",
      "loss: 1.373830  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.620     0.745     0.707    99\n",
      " disgust     0.620     0.758     0.850    107\n",
      "    fear     0.620     0.700     0.438    80\n",
      "   happy     0.620     0.213     0.169    77\n",
      " neutral     0.620     0.745     0.800    95\n",
      "     sad     0.620     0.481     0.571    91\n",
      "surprise     0.620     0.547     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.620     0.598     0.601    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.437968 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep57_acc_62.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep54_acc_61\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep57_acc_62\"! Old accuracy: 61.3, new accuracy: 62.0\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.336961  [    0/ 5482]\n",
      "loss: 1.344628  [  600/ 5482]\n",
      "loss: 1.255557  [ 1200/ 5482]\n",
      "loss: 1.294580  [ 1800/ 5482]\n",
      "loss: 1.394000  [ 2400/ 5482]\n",
      "loss: 1.340955  [ 3000/ 5482]\n",
      "loss: 1.622334  [ 3600/ 5482]\n",
      "loss: 1.434966  [ 4200/ 5482]\n",
      "loss: 1.370603  [ 4800/ 5482]\n",
      "loss: 1.431490  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.623     0.775     0.697    99\n",
      " disgust     0.623     0.756     0.897    107\n",
      "    fear     0.623     0.667     0.425    80\n",
      "   happy     0.623     0.151     0.104    77\n",
      " neutral     0.623     0.724     0.800    95\n",
      "     sad     0.623     0.496     0.615    91\n",
      "surprise     0.623     0.569     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.623     0.591     0.602    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.442389 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep58_acc_62.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep57_acc_62\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep58_acc_62\"! Old accuracy: 62.0, new accuracy: 62.3\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.519972  [    0/ 5482]\n",
      "loss: 1.394071  [  600/ 5482]\n",
      "loss: 1.261610  [ 1200/ 5482]\n",
      "loss: 1.396255  [ 1800/ 5482]\n",
      "loss: 1.473128  [ 2400/ 5482]\n",
      "loss: 1.132422  [ 3000/ 5482]\n",
      "loss: 1.420466  [ 3600/ 5482]\n",
      "loss: 1.231115  [ 4200/ 5482]\n",
      "loss: 1.252746  [ 4800/ 5482]\n",
      "loss: 1.221208  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.603     0.676     0.717    99\n",
      " disgust     0.603     0.752     0.822    107\n",
      "    fear     0.603     0.694     0.425    80\n",
      "   happy     0.603     0.189     0.091    77\n",
      " neutral     0.603     0.701     0.789    95\n",
      "     sad     0.603     0.472     0.560    91\n",
      "surprise     0.603     0.483     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.603     0.567     0.585    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.443677 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.283769  [    0/ 5482]\n",
      "loss: 1.223452  [  600/ 5482]\n",
      "loss: 1.166811  [ 1200/ 5482]\n",
      "loss: 1.292867  [ 1800/ 5482]\n",
      "loss: 1.244635  [ 2400/ 5482]\n",
      "loss: 1.232178  [ 3000/ 5482]\n",
      "loss: 1.137786  [ 3600/ 5482]\n",
      "loss: 1.226425  [ 4200/ 5482]\n",
      "loss: 1.504532  [ 4800/ 5482]\n",
      "loss: 1.343861  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.623     0.679     0.727    99\n",
      " disgust     0.623     0.754     0.888    107\n",
      "    fear     0.623     0.677     0.525    80\n",
      "   happy     0.623     0.132     0.091    77\n",
      " neutral     0.623     0.828     0.758    95\n",
      "     sad     0.623     0.472     0.648    91\n",
      "surprise     0.623     0.647     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.623     0.598     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.427456 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.197088  [    0/ 5482]\n",
      "loss: 1.137765  [  600/ 5482]\n",
      "loss: 1.298428  [ 1200/ 5482]\n",
      "loss: 1.269382  [ 1800/ 5482]\n",
      "loss: 1.589747  [ 2400/ 5482]\n",
      "loss: 1.269255  [ 3000/ 5482]\n",
      "loss: 1.237347  [ 3600/ 5482]\n",
      "loss: 1.363718  [ 4200/ 5482]\n",
      "loss: 1.229943  [ 4800/ 5482]\n",
      "loss: 1.242779  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.618     0.750     0.697    99\n",
      " disgust     0.618     0.835     0.804    107\n",
      "    fear     0.618     0.537     0.550    80\n",
      "   happy     0.618     0.176     0.117    77\n",
      " neutral     0.618     0.885     0.726    95\n",
      "     sad     0.618     0.454     0.758    91\n",
      "surprise     0.618     0.596     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.618     0.605     0.594    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.430031 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.223319  [    0/ 5482]\n",
      "loss: 1.113929  [  600/ 5482]\n",
      "loss: 1.219674  [ 1200/ 5482]\n",
      "loss: 1.366738  [ 1800/ 5482]\n",
      "loss: 1.177523  [ 2400/ 5482]\n",
      "loss: 1.164969  [ 3000/ 5482]\n",
      "loss: 1.311151  [ 3600/ 5482]\n",
      "loss: 1.281664  [ 4200/ 5482]\n",
      "loss: 1.291568  [ 4800/ 5482]\n",
      "loss: 1.301681  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.597     0.568     0.758    99\n",
      " disgust     0.597     0.722     0.776    107\n",
      "    fear     0.597     0.750     0.450    80\n",
      "   happy     0.597     0.222     0.078    77\n",
      " neutral     0.597     0.773     0.716    95\n",
      "     sad     0.597     0.500     0.527    91\n",
      "surprise     0.597     0.462     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.597     0.571     0.584    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.465942 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.212484  [    0/ 5482]\n",
      "loss: 1.339477  [  600/ 5482]\n",
      "loss: 1.326588  [ 1200/ 5482]\n",
      "loss: 1.254388  [ 1800/ 5482]\n",
      "loss: 1.655111  [ 2400/ 5482]\n",
      "loss: 1.275443  [ 3000/ 5482]\n",
      "loss: 1.403036  [ 3600/ 5482]\n",
      "loss: 1.147953  [ 4200/ 5482]\n",
      "loss: 1.091199  [ 4800/ 5482]\n",
      "loss: 1.362393  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.621     0.664     0.737    99\n",
      " disgust     0.621     0.693     0.822    107\n",
      "    fear     0.621     0.698     0.550    80\n",
      "   happy     0.621     0.135     0.065    77\n",
      " neutral     0.621     0.783     0.758    95\n",
      "     sad     0.621     0.492     0.670    91\n",
      "surprise     0.621     0.632     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.621     0.585     0.599    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.404778 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.574678  [    0/ 5482]\n",
      "loss: 1.164018  [  600/ 5482]\n",
      "loss: 1.123531  [ 1200/ 5482]\n",
      "loss: 1.353003  [ 1800/ 5482]\n",
      "loss: 1.247074  [ 2400/ 5482]\n",
      "loss: 1.300658  [ 3000/ 5482]\n",
      "loss: 1.216196  [ 3600/ 5482]\n",
      "loss: 1.081973  [ 4200/ 5482]\n",
      "loss: 1.181668  [ 4800/ 5482]\n",
      "loss: 1.310958  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.618     0.692     0.747    99\n",
      " disgust     0.618     0.705     0.850    107\n",
      "    fear     0.618     0.750     0.487    80\n",
      "   happy     0.618     0.122     0.065    77\n",
      " neutral     0.618     0.745     0.768    95\n",
      "     sad     0.618     0.487     0.615    91\n",
      "surprise     0.618     0.574     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.618     0.582     0.596    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.406768 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.068467  [    0/ 5482]\n",
      "loss: 1.175641  [  600/ 5482]\n",
      "loss: 1.192652  [ 1200/ 5482]\n",
      "loss: 1.318299  [ 1800/ 5482]\n",
      "loss: 1.340741  [ 2400/ 5482]\n",
      "loss: 1.215509  [ 3000/ 5482]\n",
      "loss: 1.114741  [ 3600/ 5482]\n",
      "loss: 1.291891  [ 4200/ 5482]\n",
      "loss: 1.180781  [ 4800/ 5482]\n",
      "loss: 1.393306  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.621     0.655     0.768    99\n",
      " disgust     0.621     0.754     0.860    107\n",
      "    fear     0.621     0.707     0.512    80\n",
      "   happy     0.621     0.158     0.078    77\n",
      " neutral     0.621     0.753     0.737    95\n",
      "     sad     0.621     0.468     0.637    91\n",
      "surprise     0.621     0.610     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.621     0.586     0.597    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.403513 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.134526  [    0/ 5482]\n",
      "loss: 1.636389  [  600/ 5482]\n",
      "loss: 1.189818  [ 1200/ 5482]\n",
      "loss: 1.253339  [ 1800/ 5482]\n",
      "loss: 1.167377  [ 2400/ 5482]\n",
      "loss: 1.703808  [ 3000/ 5482]\n",
      "loss: 1.452919  [ 3600/ 5482]\n",
      "loss: 1.118026  [ 4200/ 5482]\n",
      "loss: 1.280029  [ 4800/ 5482]\n",
      "loss: 1.499228  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.618     0.608     0.768    99\n",
      " disgust     0.618     0.731     0.813    107\n",
      "    fear     0.618     0.672     0.512    80\n",
      "   happy     0.618     0.176     0.078    77\n",
      " neutral     0.618     0.774     0.758    95\n",
      "     sad     0.618     0.492     0.659    91\n",
      "surprise     0.618     0.625     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.618     0.583     0.595    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 1.393325 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.276827  [    0/ 5482]\n",
      "loss: 1.080826  [  600/ 5482]\n",
      "loss: 1.151809  [ 1200/ 5482]\n",
      "loss: 1.083136  [ 1800/ 5482]\n",
      "loss: 1.097562  [ 2400/ 5482]\n",
      "loss: 1.065590  [ 3000/ 5482]\n",
      "loss: 1.464215  [ 3600/ 5482]\n",
      "loss: 1.178462  [ 4200/ 5482]\n",
      "loss: 1.311602  [ 4800/ 5482]\n",
      "loss: 1.156477  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.628     0.632     0.747    99\n",
      " disgust     0.628     0.788     0.766    107\n",
      "    fear     0.628     0.652     0.537    80\n",
      "   happy     0.628     0.200     0.065    77\n",
      " neutral     0.628     0.789     0.789    95\n",
      "     sad     0.628     0.496     0.626    91\n",
      "surprise     0.628     0.534     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.628     0.585     0.615    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.404158 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep67_acc_63.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep58_acc_62\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep67_acc_63\"! Old accuracy: 62.3, new accuracy: 62.8\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.107099  [    0/ 5482]\n",
      "loss: 1.194221  [  600/ 5482]\n",
      "loss: 1.094477  [ 1200/ 5482]\n",
      "loss: 1.120246  [ 1800/ 5482]\n",
      "loss: 1.228204  [ 2400/ 5482]\n",
      "loss: 1.060725  [ 3000/ 5482]\n",
      "loss: 1.075027  [ 3600/ 5482]\n",
      "loss: 1.041090  [ 4200/ 5482]\n",
      "loss: 1.155455  [ 4800/ 5482]\n",
      "loss: 1.399417  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.626     0.676     0.737    99\n",
      " disgust     0.626     0.707     0.879    107\n",
      "    fear     0.626     0.679     0.475    80\n",
      "   happy     0.626     0.143     0.065    77\n",
      " neutral     0.626     0.763     0.779    95\n",
      "     sad     0.626     0.492     0.637    91\n",
      "surprise     0.626     0.635     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.626     0.585     0.604    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.383494 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.334332  [    0/ 5482]\n",
      "loss: 1.144314  [  600/ 5482]\n",
      "loss: 1.242334  [ 1200/ 5482]\n",
      "loss: 1.120763  [ 1800/ 5482]\n",
      "loss: 1.157735  [ 2400/ 5482]\n",
      "loss: 1.171551  [ 3000/ 5482]\n",
      "loss: 1.225437  [ 3600/ 5482]\n",
      "loss: 1.311629  [ 4200/ 5482]\n",
      "loss: 1.082885  [ 4800/ 5482]\n",
      "loss: 1.246513  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.602     0.615     0.758    99\n",
      " disgust     0.602     0.730     0.785    107\n",
      "    fear     0.602     0.717     0.475    80\n",
      "   happy     0.602     0.286     0.078    77\n",
      " neutral     0.602     0.701     0.716    95\n",
      "     sad     0.602     0.490     0.549    91\n",
      "surprise     0.602     0.460     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.602     0.571     0.588    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.421625 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.099447  [    0/ 5482]\n",
      "loss: 1.297833  [  600/ 5482]\n",
      "loss: 1.068208  [ 1200/ 5482]\n",
      "loss: 1.180873  [ 1800/ 5482]\n",
      "loss: 1.179166  [ 2400/ 5482]\n",
      "loss: 1.414760  [ 3000/ 5482]\n",
      "loss: 1.164188  [ 3600/ 5482]\n",
      "loss: 1.151698  [ 4200/ 5482]\n",
      "loss: 1.166160  [ 4800/ 5482]\n",
      "loss: 1.116830  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.631     0.643     0.747    99\n",
      " disgust     0.631     0.787     0.794    107\n",
      "    fear     0.631     0.656     0.525    80\n",
      "   happy     0.631     0.269     0.091    77\n",
      " neutral     0.631     0.713     0.811    95\n",
      "     sad     0.631     0.492     0.659    91\n",
      "surprise     0.631     0.597     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.631     0.594     0.612    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.365896 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep70_acc_63.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep67_acc_63\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep70_acc_63\"! Old accuracy: 62.8, new accuracy: 63.1\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.068427  [    0/ 5482]\n",
      "loss: 1.179354  [  600/ 5482]\n",
      "loss: 1.236695  [ 1200/ 5482]\n",
      "loss: 1.248262  [ 1800/ 5482]\n",
      "loss: 1.277036  [ 2400/ 5482]\n",
      "loss: 1.031592  [ 3000/ 5482]\n",
      "loss: 1.148324  [ 3600/ 5482]\n",
      "loss: 1.106581  [ 4200/ 5482]\n",
      "loss: 1.148870  [ 4800/ 5482]\n",
      "loss: 1.102044  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.644     0.768    99\n",
      " disgust     0.636     0.804     0.804    107\n",
      "    fear     0.636     0.640     0.600    80\n",
      "   happy     0.636     0.192     0.065    77\n",
      " neutral     0.636     0.800     0.758    95\n",
      "     sad     0.636     0.477     0.681    91\n",
      "surprise     0.636     0.609     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.595     0.616    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.365245 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep71_acc_64.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep70_acc_63\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep71_acc_64\"! Old accuracy: 63.1, new accuracy: 63.6\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.272410  [    0/ 5482]\n",
      "loss: 1.031671  [  600/ 5482]\n",
      "loss: 1.092199  [ 1200/ 5482]\n",
      "loss: 1.201014  [ 1800/ 5482]\n",
      "loss: 1.168999  [ 2400/ 5482]\n",
      "loss: 1.645335  [ 3000/ 5482]\n",
      "loss: 1.070655  [ 3600/ 5482]\n",
      "loss: 1.062469  [ 4200/ 5482]\n",
      "loss: 1.119813  [ 4800/ 5482]\n",
      "loss: 1.118529  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.636     0.758    99\n",
      " disgust     0.639     0.713     0.860    107\n",
      "    fear     0.639     0.733     0.550    80\n",
      "   happy     0.639     0.294     0.130    77\n",
      " neutral     0.639     0.814     0.737    95\n",
      "     sad     0.639     0.489     0.714    91\n",
      "surprise     0.639     0.680     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.623     0.615    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.358424 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep72_acc_64.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep71_acc_64\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep72_acc_64\"! Old accuracy: 63.6, new accuracy: 63.9\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.189258  [    0/ 5482]\n",
      "loss: 1.088826  [  600/ 5482]\n",
      "loss: 1.248524  [ 1200/ 5482]\n",
      "loss: 1.292799  [ 1800/ 5482]\n",
      "loss: 1.106944  [ 2400/ 5482]\n",
      "loss: 1.026967  [ 3000/ 5482]\n",
      "loss: 1.103240  [ 3600/ 5482]\n",
      "loss: 1.059846  [ 4200/ 5482]\n",
      "loss: 1.213454  [ 4800/ 5482]\n",
      "loss: 1.121214  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.679     0.747    99\n",
      " disgust     0.643     0.732     0.841    107\n",
      "    fear     0.643     0.667     0.600    80\n",
      "   happy     0.643     0.333     0.091    77\n",
      " neutral     0.643     0.779     0.779    95\n",
      "     sad     0.643     0.471     0.703    91\n",
      "surprise     0.643     0.648     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.615     0.619    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.350762 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep73_acc_64.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep72_acc_64\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep73_acc_64\"! Old accuracy: 63.9, new accuracy: 64.3\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.073935  [    0/ 5482]\n",
      "loss: 1.090584  [  600/ 5482]\n",
      "loss: 1.176271  [ 1200/ 5482]\n",
      "loss: 1.238991  [ 1800/ 5482]\n",
      "loss: 1.135419  [ 2400/ 5482]\n",
      "loss: 1.217526  [ 3000/ 5482]\n",
      "loss: 1.421960  [ 3600/ 5482]\n",
      "loss: 1.322944  [ 4200/ 5482]\n",
      "loss: 1.123016  [ 4800/ 5482]\n",
      "loss: 1.117634  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.603     0.566     0.737    99\n",
      " disgust     0.603     0.775     0.738    107\n",
      "    fear     0.603     0.741     0.537    80\n",
      "   happy     0.603     0.143     0.026    77\n",
      " neutral     0.603     0.742     0.758    95\n",
      "     sad     0.603     0.495     0.560    91\n",
      "surprise     0.603     0.449     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.603     0.559     0.592    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 1.398570 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.145962  [    0/ 5482]\n",
      "loss: 1.280256  [  600/ 5482]\n",
      "loss: 1.072786  [ 1200/ 5482]\n",
      "loss: 1.226598  [ 1800/ 5482]\n",
      "loss: 1.618328  [ 2400/ 5482]\n",
      "loss: 1.341214  [ 3000/ 5482]\n",
      "loss: 1.247357  [ 3600/ 5482]\n",
      "loss: 1.104481  [ 4200/ 5482]\n",
      "loss: 1.087160  [ 4800/ 5482]\n",
      "loss: 1.043367  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.719     0.697    99\n",
      " disgust     0.646     0.786     0.860    107\n",
      "    fear     0.646     0.623     0.537    80\n",
      "   happy     0.646     0.318     0.091    77\n",
      " neutral     0.646     0.726     0.811    95\n",
      "     sad     0.646     0.481     0.703    91\n",
      "surprise     0.646     0.627     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.612     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.329678 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep75_acc_65.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep73_acc_64\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep75_acc_65\"! Old accuracy: 64.3, new accuracy: 64.6\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.004296  [    0/ 5482]\n",
      "loss: 1.142069  [  600/ 5482]\n",
      "loss: 0.935083  [ 1200/ 5482]\n",
      "loss: 1.190283  [ 1800/ 5482]\n",
      "loss: 1.074207  [ 2400/ 5482]\n",
      "loss: 1.141308  [ 3000/ 5482]\n",
      "loss: 1.150478  [ 3600/ 5482]\n",
      "loss: 1.019907  [ 4200/ 5482]\n",
      "loss: 1.171849  [ 4800/ 5482]\n",
      "loss: 1.195978  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.658     0.758    99\n",
      " disgust     0.648     0.746     0.822    107\n",
      "    fear     0.648     0.653     0.588    80\n",
      "   happy     0.648     0.318     0.091    77\n",
      " neutral     0.648     0.793     0.768    95\n",
      "     sad     0.648     0.486     0.747    91\n",
      "surprise     0.648     0.712     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.624     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.326314 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep76_acc_65.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep75_acc_65\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep76_acc_65\"! Old accuracy: 64.6, new accuracy: 64.8\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.292764  [    0/ 5482]\n",
      "loss: 1.164476  [  600/ 5482]\n",
      "loss: 0.989877  [ 1200/ 5482]\n",
      "loss: 1.376494  [ 1800/ 5482]\n",
      "loss: 1.002152  [ 2400/ 5482]\n",
      "loss: 1.007756  [ 3000/ 5482]\n",
      "loss: 1.063133  [ 3600/ 5482]\n",
      "loss: 1.044527  [ 4200/ 5482]\n",
      "loss: 1.186188  [ 4800/ 5482]\n",
      "loss: 1.023828  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.692     0.747    99\n",
      " disgust     0.648     0.746     0.907    107\n",
      "    fear     0.648     0.723     0.588    80\n",
      "   happy     0.648     0.185     0.065    77\n",
      " neutral     0.648     0.760     0.800    95\n",
      "     sad     0.648     0.474     0.703    91\n",
      "surprise     0.648     0.696     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.611     0.619    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.327756 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.054948  [    0/ 5482]\n",
      "loss: 0.971205  [  600/ 5482]\n",
      "loss: 1.091922  [ 1200/ 5482]\n",
      "loss: 1.163252  [ 1800/ 5482]\n",
      "loss: 1.294427  [ 2400/ 5482]\n",
      "loss: 1.308986  [ 3000/ 5482]\n",
      "loss: 1.154903  [ 3600/ 5482]\n",
      "loss: 1.046964  [ 4200/ 5482]\n",
      "loss: 1.075684  [ 4800/ 5482]\n",
      "loss: 1.249503  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.673     0.747    99\n",
      " disgust     0.633     0.748     0.888    107\n",
      "    fear     0.633     0.780     0.487    80\n",
      "   happy     0.633     0.125     0.026    77\n",
      " neutral     0.633     0.691     0.800    95\n",
      "     sad     0.633     0.520     0.582    91\n",
      "surprise     0.633     0.495     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.576     0.615    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.376512 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.198564  [    0/ 5482]\n",
      "loss: 0.991602  [  600/ 5482]\n",
      "loss: 0.937467  [ 1200/ 5482]\n",
      "loss: 0.947536  [ 1800/ 5482]\n",
      "loss: 1.076728  [ 2400/ 5482]\n",
      "loss: 1.061853  [ 3000/ 5482]\n",
      "loss: 1.167217  [ 3600/ 5482]\n",
      "loss: 0.944736  [ 4200/ 5482]\n",
      "loss: 1.364237  [ 4800/ 5482]\n",
      "loss: 1.131982  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.718     0.747    99\n",
      " disgust     0.649     0.790     0.879    107\n",
      "    fear     0.649     0.644     0.588    80\n",
      "   happy     0.649     0.188     0.078    77\n",
      " neutral     0.649     0.750     0.789    95\n",
      "     sad     0.649     0.488     0.692    91\n",
      "surprise     0.649     0.685     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.609     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.315321 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep79_acc_65.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep76_acc_65\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep79_acc_65\"! Old accuracy: 64.8, new accuracy: 64.9\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.061659  [    0/ 5482]\n",
      "loss: 1.152506  [  600/ 5482]\n",
      "loss: 1.340842  [ 1200/ 5482]\n",
      "loss: 1.183802  [ 1800/ 5482]\n",
      "loss: 1.245383  [ 2400/ 5482]\n",
      "loss: 1.038274  [ 3000/ 5482]\n",
      "loss: 1.183525  [ 3600/ 5482]\n",
      "loss: 0.950816  [ 4200/ 5482]\n",
      "loss: 0.971706  [ 4800/ 5482]\n",
      "loss: 0.961129  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.701     0.758    99\n",
      " disgust     0.652     0.706     0.897    107\n",
      "    fear     0.652     0.710     0.550    80\n",
      "   happy     0.652     0.321     0.117    77\n",
      " neutral     0.652     0.867     0.758    95\n",
      "     sad     0.652     0.462     0.736    91\n",
      "surprise     0.652     0.714     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.640     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.311092 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep80_acc_65.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep79_acc_65\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep80_acc_65\"! Old accuracy: 64.9, new accuracy: 65.2\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.221045  [    0/ 5482]\n",
      "loss: 1.044547  [  600/ 5482]\n",
      "loss: 0.933502  [ 1200/ 5482]\n",
      "loss: 0.921299  [ 1800/ 5482]\n",
      "loss: 1.130125  [ 2400/ 5482]\n",
      "loss: 1.061258  [ 3000/ 5482]\n",
      "loss: 1.047193  [ 3600/ 5482]\n",
      "loss: 0.983893  [ 4200/ 5482]\n",
      "loss: 0.961858  [ 4800/ 5482]\n",
      "loss: 1.188541  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.639     0.788    99\n",
      " disgust     0.646     0.783     0.841    107\n",
      "    fear     0.646     0.623     0.600    80\n",
      "   happy     0.646     0.241     0.091    77\n",
      " neutral     0.646     0.816     0.747    95\n",
      "     sad     0.646     0.508     0.703    91\n",
      "surprise     0.646     0.667     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.611     0.623    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.302879 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.274626  [    0/ 5482]\n",
      "loss: 0.947167  [  600/ 5482]\n",
      "loss: 0.963098  [ 1200/ 5482]\n",
      "loss: 0.894583  [ 1800/ 5482]\n",
      "loss: 1.115829  [ 2400/ 5482]\n",
      "loss: 0.994126  [ 3000/ 5482]\n",
      "loss: 1.045152  [ 3600/ 5482]\n",
      "loss: 1.086375  [ 4200/ 5482]\n",
      "loss: 1.187207  [ 4800/ 5482]\n",
      "loss: 1.123161  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.679     0.747    99\n",
      " disgust     0.661     0.810     0.879    107\n",
      "    fear     0.661     0.706     0.600    80\n",
      "   happy     0.661     0.273     0.078    77\n",
      " neutral     0.661     0.718     0.832    95\n",
      "     sad     0.661     0.508     0.703    91\n",
      "surprise     0.661     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.620     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.291557 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep82_acc_66.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep80_acc_65\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep82_acc_66\"! Old accuracy: 65.2, new accuracy: 66.1\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.999316  [    0/ 5482]\n",
      "loss: 0.935005  [  600/ 5482]\n",
      "loss: 0.984216  [ 1200/ 5482]\n",
      "loss: 1.044040  [ 1800/ 5482]\n",
      "loss: 0.981440  [ 2400/ 5482]\n",
      "loss: 1.234405  [ 3000/ 5482]\n",
      "loss: 1.090705  [ 3600/ 5482]\n",
      "loss: 1.170212  [ 4200/ 5482]\n",
      "loss: 0.994491  [ 4800/ 5482]\n",
      "loss: 0.988483  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.682     0.758    99\n",
      " disgust     0.644     0.764     0.879    107\n",
      "    fear     0.644     0.662     0.562    80\n",
      "   happy     0.644     0.056     0.013    77\n",
      " neutral     0.644     0.778     0.811    95\n",
      "     sad     0.644     0.504     0.637    91\n",
      "surprise     0.644     0.558     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.572     0.623    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.299797 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.074372  [    0/ 5482]\n",
      "loss: 1.085215  [  600/ 5482]\n",
      "loss: 1.143510  [ 1200/ 5482]\n",
      "loss: 0.991074  [ 1800/ 5482]\n",
      "loss: 1.188937  [ 2400/ 5482]\n",
      "loss: 1.111836  [ 3000/ 5482]\n",
      "loss: 0.947286  [ 3600/ 5482]\n",
      "loss: 1.180595  [ 4200/ 5482]\n",
      "loss: 0.986748  [ 4800/ 5482]\n",
      "loss: 1.074056  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.653     0.778    99\n",
      " disgust     0.648     0.769     0.869    107\n",
      "    fear     0.648     0.712     0.525    80\n",
      "   happy     0.648     0.263     0.065    77\n",
      " neutral     0.648     0.721     0.789    95\n",
      "     sad     0.648     0.512     0.703    91\n",
      "surprise     0.648     0.609     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.606     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.291032 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.189897  [    0/ 5482]\n",
      "loss: 1.003833  [  600/ 5482]\n",
      "loss: 1.023298  [ 1200/ 5482]\n",
      "loss: 1.181847  [ 1800/ 5482]\n",
      "loss: 1.110926  [ 2400/ 5482]\n",
      "loss: 1.045139  [ 3000/ 5482]\n",
      "loss: 1.100180  [ 3600/ 5482]\n",
      "loss: 1.227588  [ 4200/ 5482]\n",
      "loss: 0.947173  [ 4800/ 5482]\n",
      "loss: 0.979972  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.670     0.737    99\n",
      " disgust     0.644     0.773     0.860    107\n",
      "    fear     0.644     0.652     0.562    80\n",
      "   happy     0.644     0.143     0.039    77\n",
      " neutral     0.644     0.712     0.832    95\n",
      "     sad     0.644     0.500     0.681    91\n",
      "surprise     0.644     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.591     0.622    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.283634 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.019910  [    0/ 5482]\n",
      "loss: 1.261869  [  600/ 5482]\n",
      "loss: 1.037541  [ 1200/ 5482]\n",
      "loss: 1.133355  [ 1800/ 5482]\n",
      "loss: 1.127342  [ 2400/ 5482]\n",
      "loss: 1.267068  [ 3000/ 5482]\n",
      "loss: 0.966340  [ 3600/ 5482]\n",
      "loss: 1.219880  [ 4200/ 5482]\n",
      "loss: 0.977870  [ 4800/ 5482]\n",
      "loss: 1.043155  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.712     0.747    99\n",
      " disgust     0.656     0.824     0.832    107\n",
      "    fear     0.656     0.648     0.575    80\n",
      "   happy     0.656     0.273     0.078    77\n",
      " neutral     0.656     0.778     0.811    95\n",
      "     sad     0.656     0.476     0.769    91\n",
      "surprise     0.656     0.644     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.622     0.634    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.272308 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.140535  [    0/ 5482]\n",
      "loss: 1.267118  [  600/ 5482]\n",
      "loss: 1.110464  [ 1200/ 5482]\n",
      "loss: 0.996691  [ 1800/ 5482]\n",
      "loss: 1.026765  [ 2400/ 5482]\n",
      "loss: 0.981264  [ 3000/ 5482]\n",
      "loss: 0.962045  [ 3600/ 5482]\n",
      "loss: 1.016483  [ 4200/ 5482]\n",
      "loss: 1.155297  [ 4800/ 5482]\n",
      "loss: 0.987030  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.623     0.768    99\n",
      " disgust     0.639     0.791     0.850    107\n",
      "    fear     0.639     0.608     0.562    80\n",
      "   happy     0.639     0.111     0.026    77\n",
      " neutral     0.639     0.770     0.811    95\n",
      "     sad     0.639     0.518     0.626    91\n",
      "surprise     0.639     0.592     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.573     0.619    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.277782 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.143353  [    0/ 5482]\n",
      "loss: 0.943234  [  600/ 5482]\n",
      "loss: 1.057202  [ 1200/ 5482]\n",
      "loss: 0.858260  [ 1800/ 5482]\n",
      "loss: 1.140756  [ 2400/ 5482]\n",
      "loss: 1.122975  [ 3000/ 5482]\n",
      "loss: 0.990933  [ 3600/ 5482]\n",
      "loss: 0.909108  [ 4200/ 5482]\n",
      "loss: 1.282355  [ 4800/ 5482]\n",
      "loss: 0.816704  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.655     0.788    99\n",
      " disgust     0.648     0.767     0.832    107\n",
      "    fear     0.648     0.649     0.600    80\n",
      "   happy     0.648     0.240     0.078    77\n",
      " neutral     0.648     0.779     0.779    95\n",
      "     sad     0.648     0.500     0.681    91\n",
      "surprise     0.648     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.608     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.272044 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.019351  [    0/ 5482]\n",
      "loss: 1.110917  [  600/ 5482]\n",
      "loss: 1.261758  [ 1200/ 5482]\n",
      "loss: 1.477494  [ 1800/ 5482]\n",
      "loss: 1.222633  [ 2400/ 5482]\n",
      "loss: 0.976734  [ 3000/ 5482]\n",
      "loss: 1.016790  [ 3600/ 5482]\n",
      "loss: 0.919968  [ 4200/ 5482]\n",
      "loss: 1.092931  [ 4800/ 5482]\n",
      "loss: 0.960882  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.673     0.768    99\n",
      " disgust     0.648     0.760     0.860    107\n",
      "    fear     0.648     0.614     0.637    80\n",
      "   happy     0.648     0.125     0.039    77\n",
      " neutral     0.648     0.787     0.779    95\n",
      "     sad     0.648     0.508     0.692    91\n",
      "surprise     0.648     0.706     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.596     0.624    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.256386 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.923589  [    0/ 5482]\n",
      "loss: 0.937426  [  600/ 5482]\n",
      "loss: 0.857929  [ 1200/ 5482]\n",
      "loss: 0.996786  [ 1800/ 5482]\n",
      "loss: 1.000501  [ 2400/ 5482]\n",
      "loss: 0.836263  [ 3000/ 5482]\n",
      "loss: 1.057166  [ 3600/ 5482]\n",
      "loss: 0.961311  [ 4200/ 5482]\n",
      "loss: 1.168409  [ 4800/ 5482]\n",
      "loss: 1.001236  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.737     0.707    99\n",
      " disgust     0.648     0.780     0.860    107\n",
      "    fear     0.648     0.661     0.512    80\n",
      "   happy     0.648     0.222     0.052    77\n",
      " neutral     0.648     0.689     0.863    95\n",
      "     sad     0.648     0.492     0.703    91\n",
      "surprise     0.648     0.618     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.600     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.268187 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.981996  [    0/ 5482]\n",
      "loss: 0.895698  [  600/ 5482]\n",
      "loss: 0.945793  [ 1200/ 5482]\n",
      "loss: 0.870462  [ 1800/ 5482]\n",
      "loss: 0.840891  [ 2400/ 5482]\n",
      "loss: 0.872198  [ 3000/ 5482]\n",
      "loss: 0.925566  [ 3600/ 5482]\n",
      "loss: 1.219050  [ 4200/ 5482]\n",
      "loss: 0.889337  [ 4800/ 5482]\n",
      "loss: 1.037178  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.667     0.768    99\n",
      " disgust     0.646     0.812     0.850    107\n",
      "    fear     0.646     0.607     0.637    80\n",
      "   happy     0.646     0.158     0.039    77\n",
      " neutral     0.646     0.753     0.768    95\n",
      "     sad     0.646     0.508     0.670    91\n",
      "surprise     0.646     0.609     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.588     0.625    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.248880 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.965250  [    0/ 5482]\n",
      "loss: 0.998046  [  600/ 5482]\n",
      "loss: 0.851090  [ 1200/ 5482]\n",
      "loss: 0.976154  [ 1800/ 5482]\n",
      "loss: 1.213433  [ 2400/ 5482]\n",
      "loss: 1.095880  [ 3000/ 5482]\n",
      "loss: 0.981459  [ 3600/ 5482]\n",
      "loss: 1.045111  [ 4200/ 5482]\n",
      "loss: 1.293591  [ 4800/ 5482]\n",
      "loss: 0.870427  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.647     0.778    99\n",
      " disgust     0.652     0.778     0.850    107\n",
      "    fear     0.652     0.654     0.637    80\n",
      "   happy     0.652     0.190     0.052    77\n",
      " neutral     0.652     0.779     0.779    95\n",
      "     sad     0.652     0.525     0.681    91\n",
      "surprise     0.652     0.629     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.600     0.631    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.241597 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.963898  [    0/ 5482]\n",
      "loss: 0.988783  [  600/ 5482]\n",
      "loss: 0.906447  [ 1200/ 5482]\n",
      "loss: 0.999273  [ 1800/ 5482]\n",
      "loss: 0.806693  [ 2400/ 5482]\n",
      "loss: 0.951723  [ 3000/ 5482]\n",
      "loss: 0.955220  [ 3600/ 5482]\n",
      "loss: 0.967453  [ 4200/ 5482]\n",
      "loss: 1.157352  [ 4800/ 5482]\n",
      "loss: 1.029026  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.702     0.808    99\n",
      " disgust     0.664     0.798     0.850    107\n",
      "    fear     0.664     0.647     0.688    80\n",
      "   happy     0.664     0.250     0.052    77\n",
      " neutral     0.664     0.785     0.768    95\n",
      "     sad     0.664     0.500     0.681    91\n",
      "surprise     0.664     0.625     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.615     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.244630 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep93_acc_66.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep82_acc_66\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep93_acc_66\"! Old accuracy: 66.1, new accuracy: 66.4\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.845202  [    0/ 5482]\n",
      "loss: 1.045973  [  600/ 5482]\n",
      "loss: 0.908556  [ 1200/ 5482]\n",
      "loss: 1.070967  [ 1800/ 5482]\n",
      "loss: 1.175795  [ 2400/ 5482]\n",
      "loss: 0.931533  [ 3000/ 5482]\n",
      "loss: 0.926744  [ 3600/ 5482]\n",
      "loss: 1.053471  [ 4200/ 5482]\n",
      "loss: 0.907412  [ 4800/ 5482]\n",
      "loss: 0.902917  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.727     0.727    99\n",
      " disgust     0.662     0.783     0.841    107\n",
      "    fear     0.662     0.637     0.637    80\n",
      "   happy     0.662     0.190     0.052    77\n",
      " neutral     0.662     0.786     0.853    95\n",
      "     sad     0.662     0.489     0.725    91\n",
      "surprise     0.662     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.616     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.223507 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.837819  [    0/ 5482]\n",
      "loss: 1.135385  [  600/ 5482]\n",
      "loss: 1.074936  [ 1200/ 5482]\n",
      "loss: 0.968262  [ 1800/ 5482]\n",
      "loss: 0.972429  [ 2400/ 5482]\n",
      "loss: 1.174860  [ 3000/ 5482]\n",
      "loss: 0.970929  [ 3600/ 5482]\n",
      "loss: 0.922928  [ 4200/ 5482]\n",
      "loss: 0.931418  [ 4800/ 5482]\n",
      "loss: 0.928657  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.725     0.747    99\n",
      " disgust     0.651     0.788     0.869    107\n",
      "    fear     0.651     0.613     0.575    80\n",
      "   happy     0.651     0.222     0.052    77\n",
      " neutral     0.651     0.705     0.832    95\n",
      "     sad     0.651     0.496     0.670    91\n",
      "surprise     0.651     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.599     0.629    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.220415 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.891557  [    0/ 5482]\n",
      "loss: 0.944823  [  600/ 5482]\n",
      "loss: 0.909769  [ 1200/ 5482]\n",
      "loss: 0.899784  [ 1800/ 5482]\n",
      "loss: 0.920977  [ 2400/ 5482]\n",
      "loss: 1.002896  [ 3000/ 5482]\n",
      "loss: 0.750885  [ 3600/ 5482]\n",
      "loss: 1.039282  [ 4200/ 5482]\n",
      "loss: 0.986757  [ 4800/ 5482]\n",
      "loss: 0.992455  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.743     0.758    99\n",
      " disgust     0.669     0.835     0.804    107\n",
      "    fear     0.669     0.569     0.725    80\n",
      "   happy     0.669     0.250     0.052    77\n",
      " neutral     0.669     0.835     0.800    95\n",
      "     sad     0.669     0.514     0.791    91\n",
      "surprise     0.669     0.649     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.628     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.224182 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep96_acc_67.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep93_acc_66\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep96_acc_67\"! Old accuracy: 66.4, new accuracy: 66.9\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.857528  [    0/ 5482]\n",
      "loss: 0.987413  [  600/ 5482]\n",
      "loss: 0.813560  [ 1200/ 5482]\n",
      "loss: 1.067908  [ 1800/ 5482]\n",
      "loss: 0.925303  [ 2400/ 5482]\n",
      "loss: 1.103029  [ 3000/ 5482]\n",
      "loss: 1.244205  [ 3600/ 5482]\n",
      "loss: 0.745763  [ 4200/ 5482]\n",
      "loss: 0.913787  [ 4800/ 5482]\n",
      "loss: 1.004232  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.755     0.778    99\n",
      " disgust     0.664     0.797     0.879    107\n",
      "    fear     0.664     0.662     0.537    80\n",
      "   happy     0.664     0.316     0.078    77\n",
      " neutral     0.664     0.707     0.863    95\n",
      "     sad     0.664     0.492     0.692    91\n",
      "surprise     0.664     0.645     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.625     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.224240 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.149314  [    0/ 5482]\n",
      "loss: 0.940295  [  600/ 5482]\n",
      "loss: 0.872052  [ 1200/ 5482]\n",
      "loss: 0.866703  [ 1800/ 5482]\n",
      "loss: 0.763103  [ 2400/ 5482]\n",
      "loss: 0.847228  [ 3000/ 5482]\n",
      "loss: 0.916286  [ 3600/ 5482]\n",
      "loss: 0.889084  [ 4200/ 5482]\n",
      "loss: 0.984081  [ 4800/ 5482]\n",
      "loss: 1.041395  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.770     0.778    99\n",
      " disgust     0.680     0.875     0.850    107\n",
      "    fear     0.680     0.606     0.713    80\n",
      "   happy     0.680     0.235     0.052    77\n",
      " neutral     0.680     0.764     0.853    95\n",
      "     sad     0.680     0.504     0.725    91\n",
      "surprise     0.680     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.632     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.199672 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep98_acc_68.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep96_acc_67\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep98_acc_68\"! Old accuracy: 66.9, new accuracy: 68.0\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.909607  [    0/ 5482]\n",
      "loss: 1.154026  [  600/ 5482]\n",
      "loss: 0.991097  [ 1200/ 5482]\n",
      "loss: 0.869874  [ 1800/ 5482]\n",
      "loss: 0.909769  [ 2400/ 5482]\n",
      "loss: 1.041105  [ 3000/ 5482]\n",
      "loss: 0.791516  [ 3600/ 5482]\n",
      "loss: 0.910213  [ 4200/ 5482]\n",
      "loss: 1.112300  [ 4800/ 5482]\n",
      "loss: 1.099422  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.717     0.768    99\n",
      " disgust     0.669     0.800     0.860    107\n",
      "    fear     0.669     0.653     0.613    80\n",
      "   happy     0.669     0.294     0.065    77\n",
      " neutral     0.669     0.748     0.842    95\n",
      "     sad     0.669     0.504     0.703    91\n",
      "surprise     0.669     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.626     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.206110 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.999276  [    0/ 5482]\n",
      "loss: 0.770810  [  600/ 5482]\n",
      "loss: 0.784092  [ 1200/ 5482]\n",
      "loss: 1.093567  [ 1800/ 5482]\n",
      "loss: 0.891196  [ 2400/ 5482]\n",
      "loss: 0.931931  [ 3000/ 5482]\n",
      "loss: 0.992652  [ 3600/ 5482]\n",
      "loss: 0.919331  [ 4200/ 5482]\n",
      "loss: 0.820213  [ 4800/ 5482]\n",
      "loss: 1.059424  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.684     0.788    99\n",
      " disgust     0.666     0.832     0.832    107\n",
      "    fear     0.666     0.600     0.675    80\n",
      "   happy     0.666     0.188     0.039    77\n",
      " neutral     0.666     0.782     0.832    95\n",
      "     sad     0.666     0.516     0.692    91\n",
      "surprise     0.666     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.610     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.194435 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.927191  [    0/ 5482]\n",
      "loss: 0.887812  [  600/ 5482]\n",
      "loss: 0.870802  [ 1200/ 5482]\n",
      "loss: 0.730179  [ 1800/ 5482]\n",
      "loss: 0.783750  [ 2400/ 5482]\n",
      "loss: 0.901748  [ 3000/ 5482]\n",
      "loss: 0.906102  [ 3600/ 5482]\n",
      "loss: 0.783358  [ 4200/ 5482]\n",
      "loss: 0.919817  [ 4800/ 5482]\n",
      "loss: 0.950764  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.661     0.788    99\n",
      " disgust     0.652     0.813     0.813    107\n",
      "    fear     0.652     0.589     0.662    80\n",
      "   happy     0.652     0.111     0.013    77\n",
      " neutral     0.652     0.755     0.842    95\n",
      "     sad     0.652     0.536     0.659    91\n",
      "surprise     0.652     0.574     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.577     0.631    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.209503 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.902240  [    0/ 5482]\n",
      "loss: 0.850480  [  600/ 5482]\n",
      "loss: 0.839533  [ 1200/ 5482]\n",
      "loss: 1.021577  [ 1800/ 5482]\n",
      "loss: 1.040730  [ 2400/ 5482]\n",
      "loss: 0.951648  [ 3000/ 5482]\n",
      "loss: 0.819555  [ 3600/ 5482]\n",
      "loss: 1.069158  [ 4200/ 5482]\n",
      "loss: 0.816856  [ 4800/ 5482]\n",
      "loss: 0.844306  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.782     0.687    99\n",
      " disgust     0.661     0.738     0.869    107\n",
      "    fear     0.661     0.622     0.700    80\n",
      "   happy     0.661     0.227     0.065    77\n",
      " neutral     0.661     0.826     0.800    95\n",
      "     sad     0.661     0.474     0.802    91\n",
      "surprise     0.661     0.821     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.641     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.200239 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.919360  [    0/ 5482]\n",
      "loss: 1.271108  [  600/ 5482]\n",
      "loss: 0.859034  [ 1200/ 5482]\n",
      "loss: 0.957446  [ 1800/ 5482]\n",
      "loss: 0.865575  [ 2400/ 5482]\n",
      "loss: 1.108662  [ 3000/ 5482]\n",
      "loss: 0.833637  [ 3600/ 5482]\n",
      "loss: 0.796860  [ 4200/ 5482]\n",
      "loss: 0.855876  [ 4800/ 5482]\n",
      "loss: 0.910714  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.675     0.778    99\n",
      " disgust     0.659     0.734     0.879    107\n",
      "    fear     0.659     0.598     0.650    80\n",
      "   happy     0.659     0.263     0.065    77\n",
      " neutral     0.659     0.804     0.821    95\n",
      "     sad     0.659     0.530     0.670    91\n",
      "surprise     0.659     0.700     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.615     0.634    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.188866 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.852560  [    0/ 5482]\n",
      "loss: 1.054167  [  600/ 5482]\n",
      "loss: 1.015231  [ 1200/ 5482]\n",
      "loss: 1.033556  [ 1800/ 5482]\n",
      "loss: 1.021595  [ 2400/ 5482]\n",
      "loss: 0.851488  [ 3000/ 5482]\n",
      "loss: 0.858651  [ 3600/ 5482]\n",
      "loss: 0.992992  [ 4200/ 5482]\n",
      "loss: 0.767433  [ 4800/ 5482]\n",
      "loss: 0.850185  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.830     0.737    99\n",
      " disgust     0.684     0.828     0.897    107\n",
      "    fear     0.684     0.663     0.713    80\n",
      "   happy     0.684     0.389     0.091    77\n",
      " neutral     0.684     0.703     0.874    95\n",
      "     sad     0.684     0.492     0.692    91\n",
      "surprise     0.684     0.679     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.655     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.180412 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep104_acc_68.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep98_acc_68\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep104_acc_68\"! Old accuracy: 68.0, new accuracy: 68.4\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.812235  [    0/ 5482]\n",
      "loss: 0.828122  [  600/ 5482]\n",
      "loss: 0.986917  [ 1200/ 5482]\n",
      "loss: 0.993787  [ 1800/ 5482]\n",
      "loss: 0.857043  [ 2400/ 5482]\n",
      "loss: 1.186071  [ 3000/ 5482]\n",
      "loss: 0.670188  [ 3600/ 5482]\n",
      "loss: 0.944274  [ 4200/ 5482]\n",
      "loss: 0.858943  [ 4800/ 5482]\n",
      "loss: 0.812195  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.771     0.747    99\n",
      " disgust     0.664     0.790     0.879    107\n",
      "    fear     0.664     0.671     0.613    80\n",
      "   happy     0.664     0.357     0.065    77\n",
      " neutral     0.664     0.681     0.853    95\n",
      "     sad     0.664     0.496     0.681    91\n",
      "surprise     0.664     0.625     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.627     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.183735 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.771892  [    0/ 5482]\n",
      "loss: 0.802786  [  600/ 5482]\n",
      "loss: 0.945437  [ 1200/ 5482]\n",
      "loss: 0.853540  [ 1800/ 5482]\n",
      "loss: 0.955426  [ 2400/ 5482]\n",
      "loss: 0.723552  [ 3000/ 5482]\n",
      "loss: 0.953925  [ 3600/ 5482]\n",
      "loss: 0.885060  [ 4200/ 5482]\n",
      "loss: 0.885303  [ 4800/ 5482]\n",
      "loss: 0.945737  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.664     0.778    99\n",
      " disgust     0.644     0.704     0.888    107\n",
      "    fear     0.644     0.682     0.562    80\n",
      "   happy     0.644     0.062     0.013    77\n",
      " neutral     0.644     0.750     0.821    95\n",
      "     sad     0.644     0.509     0.615    91\n",
      "surprise     0.644     0.651     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.575     0.621    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.206925 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.926054  [    0/ 5482]\n",
      "loss: 0.900460  [  600/ 5482]\n",
      "loss: 0.958185  [ 1200/ 5482]\n",
      "loss: 0.705761  [ 1800/ 5482]\n",
      "loss: 0.927978  [ 2400/ 5482]\n",
      "loss: 1.036368  [ 3000/ 5482]\n",
      "loss: 0.863738  [ 3600/ 5482]\n",
      "loss: 0.903590  [ 4200/ 5482]\n",
      "loss: 0.950109  [ 4800/ 5482]\n",
      "loss: 0.774006  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.699     0.798    99\n",
      " disgust     0.679     0.790     0.879    107\n",
      "    fear     0.679     0.622     0.700    80\n",
      "   happy     0.679     0.143     0.026    77\n",
      " neutral     0.679     0.806     0.832    95\n",
      "     sad     0.679     0.547     0.703    91\n",
      "surprise     0.679     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.612     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.172129 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.914604  [    0/ 5482]\n",
      "loss: 0.862937  [  600/ 5482]\n",
      "loss: 1.026622  [ 1200/ 5482]\n",
      "loss: 0.907289  [ 1800/ 5482]\n",
      "loss: 0.934954  [ 2400/ 5482]\n",
      "loss: 0.767087  [ 3000/ 5482]\n",
      "loss: 0.929557  [ 3600/ 5482]\n",
      "loss: 0.751316  [ 4200/ 5482]\n",
      "loss: 0.918421  [ 4800/ 5482]\n",
      "loss: 0.819337  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.710     0.768    99\n",
      " disgust     0.670     0.718     0.879    107\n",
      "    fear     0.670     0.696     0.688    80\n",
      "   happy     0.670     0.182     0.052    77\n",
      " neutral     0.670     0.804     0.821    95\n",
      "     sad     0.670     0.534     0.681    91\n",
      "surprise     0.670     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.619     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.180053 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.855888  [    0/ 5482]\n",
      "loss: 0.838618  [  600/ 5482]\n",
      "loss: 0.891209  [ 1200/ 5482]\n",
      "loss: 0.954193  [ 1800/ 5482]\n",
      "loss: 0.987220  [ 2400/ 5482]\n",
      "loss: 0.811654  [ 3000/ 5482]\n",
      "loss: 0.916917  [ 3600/ 5482]\n",
      "loss: 0.859732  [ 4200/ 5482]\n",
      "loss: 0.919561  [ 4800/ 5482]\n",
      "loss: 0.934046  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.717     0.768    99\n",
      " disgust     0.669     0.800     0.860    107\n",
      "    fear     0.669     0.604     0.688    80\n",
      "   happy     0.669     0.231     0.039    77\n",
      " neutral     0.669     0.773     0.789    95\n",
      "     sad     0.669     0.531     0.747    91\n",
      "surprise     0.669     0.650     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.615     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.172605 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.778035  [    0/ 5482]\n",
      "loss: 0.818236  [  600/ 5482]\n",
      "loss: 0.851020  [ 1200/ 5482]\n",
      "loss: 0.783829  [ 1800/ 5482]\n",
      "loss: 1.010350  [ 2400/ 5482]\n",
      "loss: 0.872932  [ 3000/ 5482]\n",
      "loss: 0.557272  [ 3600/ 5482]\n",
      "loss: 0.702079  [ 4200/ 5482]\n",
      "loss: 0.795214  [ 4800/ 5482]\n",
      "loss: 0.835116  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.640     0.808    99\n",
      " disgust     0.674     0.784     0.850    107\n",
      "    fear     0.674     0.622     0.700    80\n",
      "   happy     0.674     0.190     0.052    77\n",
      " neutral     0.674     0.848     0.821    95\n",
      "     sad     0.674     0.559     0.725    91\n",
      "surprise     0.674     0.750     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.628     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.160277 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.796044  [    0/ 5482]\n",
      "loss: 0.694883  [  600/ 5482]\n",
      "loss: 0.770706  [ 1200/ 5482]\n",
      "loss: 0.811228  [ 1800/ 5482]\n",
      "loss: 0.972386  [ 2400/ 5482]\n",
      "loss: 0.733387  [ 3000/ 5482]\n",
      "loss: 0.827096  [ 3600/ 5482]\n",
      "loss: 0.758660  [ 4200/ 5482]\n",
      "loss: 0.907250  [ 4800/ 5482]\n",
      "loss: 1.354935  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.743     0.788    99\n",
      " disgust     0.672     0.864     0.832    107\n",
      "    fear     0.672     0.600     0.787    80\n",
      "   happy     0.672     0.150     0.039    77\n",
      " neutral     0.672     0.776     0.800    95\n",
      "     sad     0.672     0.516     0.703    91\n",
      "surprise     0.672     0.673     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.617     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.155886 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 1.027853  [    0/ 5482]\n",
      "loss: 0.909334  [  600/ 5482]\n",
      "loss: 0.638828  [ 1200/ 5482]\n",
      "loss: 1.040002  [ 1800/ 5482]\n",
      "loss: 0.744292  [ 2400/ 5482]\n",
      "loss: 0.791889  [ 3000/ 5482]\n",
      "loss: 1.046523  [ 3600/ 5482]\n",
      "loss: 0.942511  [ 4200/ 5482]\n",
      "loss: 0.886478  [ 4800/ 5482]\n",
      "loss: 0.886796  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.727     0.808    99\n",
      " disgust     0.680     0.827     0.850    107\n",
      "    fear     0.680     0.615     0.700    80\n",
      "   happy     0.680     0.211     0.052    77\n",
      " neutral     0.680     0.790     0.832    95\n",
      "     sad     0.680     0.539     0.758    91\n",
      "surprise     0.680     0.692     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.629     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.156978 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.881375  [    0/ 5482]\n",
      "loss: 0.931077  [  600/ 5482]\n",
      "loss: 0.834545  [ 1200/ 5482]\n",
      "loss: 0.844262  [ 1800/ 5482]\n",
      "loss: 0.776395  [ 2400/ 5482]\n",
      "loss: 0.744621  [ 3000/ 5482]\n",
      "loss: 0.960030  [ 3600/ 5482]\n",
      "loss: 0.819282  [ 4200/ 5482]\n",
      "loss: 0.699032  [ 4800/ 5482]\n",
      "loss: 0.800274  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.684     0.731     0.798    99\n",
      " disgust     0.684     0.867     0.850    107\n",
      "    fear     0.684     0.573     0.688    80\n",
      "   happy     0.684     0.235     0.052    77\n",
      " neutral     0.684     0.833     0.842    95\n",
      "     sad     0.684     0.528     0.714    91\n",
      "surprise     0.684     0.662     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.684     0.633     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 1.147093 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.672568  [    0/ 5482]\n",
      "loss: 0.728057  [  600/ 5482]\n",
      "loss: 0.858120  [ 1200/ 5482]\n",
      "loss: 0.789890  [ 1800/ 5482]\n",
      "loss: 0.880023  [ 2400/ 5482]\n",
      "loss: 0.859591  [ 3000/ 5482]\n",
      "loss: 0.675403  [ 3600/ 5482]\n",
      "loss: 0.939902  [ 4200/ 5482]\n",
      "loss: 1.260393  [ 4800/ 5482]\n",
      "loss: 0.971420  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.722     0.788    99\n",
      " disgust     0.674     0.809     0.869    107\n",
      "    fear     0.674     0.628     0.675    80\n",
      "   happy     0.674     0.235     0.052    77\n",
      " neutral     0.674     0.722     0.874    95\n",
      "     sad     0.674     0.532     0.648    91\n",
      "surprise     0.674     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.620     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.135108 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.770890  [    0/ 5482]\n",
      "loss: 0.731693  [  600/ 5482]\n",
      "loss: 0.811962  [ 1200/ 5482]\n",
      "loss: 0.818431  [ 1800/ 5482]\n",
      "loss: 0.888739  [ 2400/ 5482]\n",
      "loss: 0.806775  [ 3000/ 5482]\n",
      "loss: 0.781530  [ 3600/ 5482]\n",
      "loss: 0.691820  [ 4200/ 5482]\n",
      "loss: 0.784533  [ 4800/ 5482]\n",
      "loss: 0.763902  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.750     0.758    99\n",
      " disgust     0.667     0.827     0.850    107\n",
      "    fear     0.667     0.596     0.662    80\n",
      "   happy     0.667     0.118     0.026    77\n",
      " neutral     0.667     0.748     0.874    95\n",
      "     sad     0.667     0.516     0.714    91\n",
      "surprise     0.667     0.667     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.603     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.138908 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.751608  [    0/ 5482]\n",
      "loss: 0.856118  [  600/ 5482]\n",
      "loss: 0.815091  [ 1200/ 5482]\n",
      "loss: 0.850847  [ 1800/ 5482]\n",
      "loss: 0.731111  [ 2400/ 5482]\n",
      "loss: 0.920103  [ 3000/ 5482]\n",
      "loss: 0.732577  [ 3600/ 5482]\n",
      "loss: 0.941098  [ 4200/ 5482]\n",
      "loss: 0.708654  [ 4800/ 5482]\n",
      "loss: 0.792371  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.653     0.798    99\n",
      " disgust     0.656     0.795     0.869    107\n",
      "    fear     0.656     0.609     0.662    80\n",
      "   happy     0.656     0.000     0.000    77\n",
      " neutral     0.656     0.762     0.811    95\n",
      "     sad     0.656     0.533     0.615    91\n",
      "surprise     0.656     0.646     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.571     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.157190 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.717897  [    0/ 5482]\n",
      "loss: 0.835636  [  600/ 5482]\n",
      "loss: 0.871969  [ 1200/ 5482]\n",
      "loss: 0.840116  [ 1800/ 5482]\n",
      "loss: 0.979124  [ 2400/ 5482]\n",
      "loss: 0.933061  [ 3000/ 5482]\n",
      "loss: 0.881474  [ 3600/ 5482]\n",
      "loss: 0.736560  [ 4200/ 5482]\n",
      "loss: 0.672969  [ 4800/ 5482]\n",
      "loss: 0.691800  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.770     0.778    99\n",
      " disgust     0.666     0.738     0.897    107\n",
      "    fear     0.666     0.667     0.600    80\n",
      "   happy     0.666     0.111     0.026    77\n",
      " neutral     0.666     0.750     0.884    95\n",
      "     sad     0.666     0.480     0.648    91\n",
      "surprise     0.666     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.606     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.144164 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.841815  [    0/ 5482]\n",
      "loss: 0.652502  [  600/ 5482]\n",
      "loss: 0.591447  [ 1200/ 5482]\n",
      "loss: 1.182695  [ 1800/ 5482]\n",
      "loss: 1.047529  [ 2400/ 5482]\n",
      "loss: 0.798129  [ 3000/ 5482]\n",
      "loss: 0.917099  [ 3600/ 5482]\n",
      "loss: 0.693424  [ 4200/ 5482]\n",
      "loss: 0.883531  [ 4800/ 5482]\n",
      "loss: 0.891707  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.675     0.778    99\n",
      " disgust     0.667     0.783     0.879    107\n",
      "    fear     0.667     0.649     0.625    80\n",
      "   happy     0.667     0.188     0.039    77\n",
      " neutral     0.667     0.733     0.895    95\n",
      "     sad     0.667     0.527     0.637    91\n",
      "surprise     0.667     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.608     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.134207 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.958049  [    0/ 5482]\n",
      "loss: 0.773416  [  600/ 5482]\n",
      "loss: 0.677483  [ 1200/ 5482]\n",
      "loss: 0.755075  [ 1800/ 5482]\n",
      "loss: 0.804847  [ 2400/ 5482]\n",
      "loss: 0.798652  [ 3000/ 5482]\n",
      "loss: 0.994156  [ 3600/ 5482]\n",
      "loss: 0.841163  [ 4200/ 5482]\n",
      "loss: 0.806870  [ 4800/ 5482]\n",
      "loss: 0.801623  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.820     0.737    99\n",
      " disgust     0.682     0.785     0.888    107\n",
      "    fear     0.682     0.644     0.700    80\n",
      "   happy     0.682     0.241     0.091    77\n",
      " neutral     0.682     0.827     0.853    95\n",
      "     sad     0.682     0.496     0.758    91\n",
      "surprise     0.682     0.745     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.651     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.126713 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.741874  [    0/ 5482]\n",
      "loss: 0.611094  [  600/ 5482]\n",
      "loss: 0.738722  [ 1200/ 5482]\n",
      "loss: 0.725217  [ 1800/ 5482]\n",
      "loss: 1.043109  [ 2400/ 5482]\n",
      "loss: 0.659947  [ 3000/ 5482]\n",
      "loss: 0.802215  [ 3600/ 5482]\n",
      "loss: 0.803745  [ 4200/ 5482]\n",
      "loss: 0.843172  [ 4800/ 5482]\n",
      "loss: 0.877427  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.717     0.768    99\n",
      " disgust     0.689     0.803     0.879    107\n",
      "    fear     0.689     0.684     0.675    80\n",
      "   happy     0.689     0.400     0.052    77\n",
      " neutral     0.689     0.759     0.895    95\n",
      "     sad     0.689     0.535     0.747    91\n",
      "surprise     0.689     0.661     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.651     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.131131 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep120_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep104_acc_68\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep120_acc_69\"! Old accuracy: 68.4, new accuracy: 68.9\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.827759  [    0/ 5482]\n",
      "loss: 0.844257  [  600/ 5482]\n",
      "loss: 0.807239  [ 1200/ 5482]\n",
      "loss: 0.727185  [ 1800/ 5482]\n",
      "loss: 0.828022  [ 2400/ 5482]\n",
      "loss: 1.112837  [ 3000/ 5482]\n",
      "loss: 0.852466  [ 3600/ 5482]\n",
      "loss: 0.745034  [ 4200/ 5482]\n",
      "loss: 0.712759  [ 4800/ 5482]\n",
      "loss: 0.793097  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.675     0.798    99\n",
      " disgust     0.654     0.843     0.850    107\n",
      "    fear     0.654     0.671     0.613    80\n",
      "   happy     0.654     0.077     0.013    77\n",
      " neutral     0.654     0.696     0.821    95\n",
      "     sad     0.654     0.524     0.593    91\n",
      "surprise     0.654     0.560     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.578     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.170743 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.869858  [    0/ 5482]\n",
      "loss: 0.697679  [  600/ 5482]\n",
      "loss: 0.745523  [ 1200/ 5482]\n",
      "loss: 0.863168  [ 1800/ 5482]\n",
      "loss: 0.863771  [ 2400/ 5482]\n",
      "loss: 0.709081  [ 3000/ 5482]\n",
      "loss: 0.817181  [ 3600/ 5482]\n",
      "loss: 0.892816  [ 4200/ 5482]\n",
      "loss: 0.819179  [ 4800/ 5482]\n",
      "loss: 0.614044  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.786     0.778    99\n",
      " disgust     0.687     0.814     0.860    107\n",
      "    fear     0.687     0.610     0.762    80\n",
      "   happy     0.687     0.176     0.039    77\n",
      " neutral     0.687     0.818     0.853    95\n",
      "     sad     0.687     0.507     0.758    91\n",
      "surprise     0.687     0.766     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.640     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.131221 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.699392  [    0/ 5482]\n",
      "loss: 0.837818  [  600/ 5482]\n",
      "loss: 0.850212  [ 1200/ 5482]\n",
      "loss: 0.789771  [ 1800/ 5482]\n",
      "loss: 0.850908  [ 2400/ 5482]\n",
      "loss: 0.781683  [ 3000/ 5482]\n",
      "loss: 0.689806  [ 3600/ 5482]\n",
      "loss: 0.845358  [ 4200/ 5482]\n",
      "loss: 0.806335  [ 4800/ 5482]\n",
      "loss: 0.744295  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.755     0.747    99\n",
      " disgust     0.670     0.792     0.925    107\n",
      "    fear     0.670     0.671     0.613    80\n",
      "   happy     0.670     0.273     0.078    77\n",
      " neutral     0.670     0.726     0.863    95\n",
      "     sad     0.670     0.492     0.703    91\n",
      "surprise     0.670     0.714     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.632     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.125188 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.968350  [    0/ 5482]\n",
      "loss: 0.862038  [  600/ 5482]\n",
      "loss: 0.911496  [ 1200/ 5482]\n",
      "loss: 0.720904  [ 1800/ 5482]\n",
      "loss: 0.636403  [ 2400/ 5482]\n",
      "loss: 0.701415  [ 3000/ 5482]\n",
      "loss: 0.775189  [ 3600/ 5482]\n",
      "loss: 0.862937  [ 4200/ 5482]\n",
      "loss: 0.953955  [ 4800/ 5482]\n",
      "loss: 0.795394  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.706     0.727    99\n",
      " disgust     0.680     0.777     0.879    107\n",
      "    fear     0.680     0.629     0.700    80\n",
      "   happy     0.680     0.267     0.052    77\n",
      " neutral     0.680     0.750     0.916    95\n",
      "     sad     0.680     0.560     0.714    91\n",
      "surprise     0.680     0.725     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.631     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.103374 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.954465  [    0/ 5482]\n",
      "loss: 0.635864  [  600/ 5482]\n",
      "loss: 0.986603  [ 1200/ 5482]\n",
      "loss: 0.703674  [ 1800/ 5482]\n",
      "loss: 0.734937  [ 2400/ 5482]\n",
      "loss: 0.583016  [ 3000/ 5482]\n",
      "loss: 0.661789  [ 3600/ 5482]\n",
      "loss: 0.715674  [ 4200/ 5482]\n",
      "loss: 0.705033  [ 4800/ 5482]\n",
      "loss: 0.736662  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.785     0.737    99\n",
      " disgust     0.682     0.825     0.879    107\n",
      "    fear     0.682     0.583     0.787    80\n",
      "   happy     0.682     0.231     0.078    77\n",
      " neutral     0.682     0.806     0.832    95\n",
      "     sad     0.682     0.516     0.725    91\n",
      "surprise     0.682     0.814     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.651     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.112330 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.712776  [    0/ 5482]\n",
      "loss: 0.809171  [  600/ 5482]\n",
      "loss: 0.720565  [ 1200/ 5482]\n",
      "loss: 0.738258  [ 1800/ 5482]\n",
      "loss: 0.892091  [ 2400/ 5482]\n",
      "loss: 0.617694  [ 3000/ 5482]\n",
      "loss: 0.801422  [ 3600/ 5482]\n",
      "loss: 0.798843  [ 4200/ 5482]\n",
      "loss: 0.669182  [ 4800/ 5482]\n",
      "loss: 0.655784  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.750     0.788    99\n",
      " disgust     0.690     0.808     0.907    107\n",
      "    fear     0.690     0.610     0.762    80\n",
      "   happy     0.690     0.192     0.065    77\n",
      " neutral     0.690     0.865     0.811    95\n",
      "     sad     0.690     0.541     0.725    91\n",
      "surprise     0.690     0.755     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.646     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.091508 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep126_acc_69.md \n",
      "\n",
      "Old best model deleted from \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep120_acc_69\"\n",
      "New best model saved to \"/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv/emo_reco_best_ep126_acc_69\"! Old accuracy: 68.9, new accuracy: 69.0\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.696970  [    0/ 5482]\n",
      "loss: 0.618325  [  600/ 5482]\n",
      "loss: 0.731196  [ 1200/ 5482]\n",
      "loss: 1.009189  [ 1800/ 5482]\n",
      "loss: 0.780324  [ 2400/ 5482]\n",
      "loss: 0.795017  [ 3000/ 5482]\n",
      "loss: 0.682109  [ 3600/ 5482]\n",
      "loss: 0.897226  [ 4200/ 5482]\n",
      "loss: 0.644014  [ 4800/ 5482]\n",
      "loss: 0.726010  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.706     0.778    99\n",
      " disgust     0.666     0.770     0.879    107\n",
      "    fear     0.666     0.696     0.600    80\n",
      "   happy     0.666     0.167     0.039    77\n",
      " neutral     0.666     0.735     0.874    95\n",
      "     sad     0.666     0.526     0.659    91\n",
      "surprise     0.666     0.631     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.604     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.131982 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.920361  [    0/ 5482]\n",
      "loss: 0.665828  [  600/ 5482]\n",
      "loss: 0.746824  [ 1200/ 5482]\n",
      "loss: 0.807458  [ 1800/ 5482]\n",
      "loss: 0.614531  [ 2400/ 5482]\n",
      "loss: 0.696380  [ 3000/ 5482]\n",
      "loss: 0.770801  [ 3600/ 5482]\n",
      "loss: 0.653087  [ 4200/ 5482]\n",
      "loss: 0.875889  [ 4800/ 5482]\n",
      "loss: 0.841942  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.743     0.788    99\n",
      " disgust     0.690     0.802     0.869    107\n",
      "    fear     0.690     0.632     0.750    80\n",
      "   happy     0.690     0.211     0.052    77\n",
      " neutral     0.690     0.839     0.821    95\n",
      "     sad     0.690     0.532     0.725    91\n",
      "surprise     0.690     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.640     0.671    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.101885 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.755987  [    0/ 5482]\n",
      "loss: 0.788118  [  600/ 5482]\n",
      "loss: 0.861240  [ 1200/ 5482]\n",
      "loss: 0.714122  [ 1800/ 5482]\n",
      "loss: 0.820619  [ 2400/ 5482]\n",
      "loss: 0.706441  [ 3000/ 5482]\n",
      "loss: 0.822454  [ 3600/ 5482]\n",
      "loss: 0.721879  [ 4200/ 5482]\n",
      "loss: 0.885596  [ 4800/ 5482]\n",
      "loss: 0.640205  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.710     0.768    99\n",
      " disgust     0.666     0.810     0.879    107\n",
      "    fear     0.666     0.646     0.637    80\n",
      "   happy     0.666     0.062     0.013    77\n",
      " neutral     0.666     0.726     0.863    95\n",
      "     sad     0.666     0.521     0.681    91\n",
      "surprise     0.666     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.592     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.091166 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.779349  [    0/ 5482]\n",
      "loss: 0.609020  [  600/ 5482]\n",
      "loss: 0.687824  [ 1200/ 5482]\n",
      "loss: 0.742517  [ 1800/ 5482]\n",
      "loss: 0.859682  [ 2400/ 5482]\n",
      "loss: 0.811746  [ 3000/ 5482]\n",
      "loss: 0.847260  [ 3600/ 5482]\n",
      "loss: 0.554525  [ 4200/ 5482]\n",
      "loss: 0.653603  [ 4800/ 5482]\n",
      "loss: 0.554265  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.706     0.778    99\n",
      " disgust     0.674     0.783     0.879    107\n",
      "    fear     0.674     0.646     0.637    80\n",
      "   happy     0.674     0.357     0.065    77\n",
      " neutral     0.674     0.714     0.895    95\n",
      "     sad     0.674     0.546     0.648    91\n",
      "surprise     0.674     0.656     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.630     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.102126 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.863824  [    0/ 5482]\n",
      "loss: 0.678203  [  600/ 5482]\n",
      "loss: 0.727792  [ 1200/ 5482]\n",
      "loss: 0.835752  [ 1800/ 5482]\n",
      "loss: 0.789727  [ 2400/ 5482]\n",
      "loss: 0.694281  [ 3000/ 5482]\n",
      "loss: 0.660234  [ 3600/ 5482]\n",
      "loss: 0.801800  [ 4200/ 5482]\n",
      "loss: 0.750599  [ 4800/ 5482]\n",
      "loss: 0.832716  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.736     0.788    99\n",
      " disgust     0.674     0.850     0.850    107\n",
      "    fear     0.674     0.605     0.650    80\n",
      "   happy     0.674     0.158     0.039    77\n",
      " neutral     0.674     0.762     0.842    95\n",
      "     sad     0.674     0.525     0.703    91\n",
      "surprise     0.674     0.662     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.614     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.095610 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.827513  [    0/ 5482]\n",
      "loss: 0.662019  [  600/ 5482]\n",
      "loss: 0.851173  [ 1200/ 5482]\n",
      "loss: 0.703142  [ 1800/ 5482]\n",
      "loss: 0.831771  [ 2400/ 5482]\n",
      "loss: 0.837696  [ 3000/ 5482]\n",
      "loss: 0.749921  [ 3600/ 5482]\n",
      "loss: 0.738053  [ 4200/ 5482]\n",
      "loss: 0.667687  [ 4800/ 5482]\n",
      "loss: 0.556405  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.682     0.758    99\n",
      " disgust     0.675     0.803     0.879    107\n",
      "    fear     0.675     0.598     0.688    80\n",
      "   happy     0.675     0.188     0.039    77\n",
      " neutral     0.675     0.808     0.842    95\n",
      "     sad     0.675     0.537     0.714    91\n",
      "surprise     0.675     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.620     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.096972 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.722522  [    0/ 5482]\n",
      "loss: 0.616791  [  600/ 5482]\n",
      "loss: 0.684469  [ 1200/ 5482]\n",
      "loss: 0.727103  [ 1800/ 5482]\n",
      "loss: 0.733039  [ 2400/ 5482]\n",
      "loss: 0.496641  [ 3000/ 5482]\n",
      "loss: 0.750618  [ 3600/ 5482]\n",
      "loss: 0.848247  [ 4200/ 5482]\n",
      "loss: 0.746909  [ 4800/ 5482]\n",
      "loss: 0.769210  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.758     0.758    99\n",
      " disgust     0.680     0.805     0.888    107\n",
      "    fear     0.680     0.627     0.650    80\n",
      "   happy     0.680     0.250     0.039    77\n",
      " neutral     0.680     0.761     0.874    95\n",
      "     sad     0.680     0.496     0.714    91\n",
      "surprise     0.680     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.632     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.099013 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.838113  [    0/ 5482]\n",
      "loss: 0.686265  [  600/ 5482]\n",
      "loss: 0.720199  [ 1200/ 5482]\n",
      "loss: 0.603118  [ 1800/ 5482]\n",
      "loss: 0.719265  [ 2400/ 5482]\n",
      "loss: 0.862214  [ 3000/ 5482]\n",
      "loss: 0.746919  [ 3600/ 5482]\n",
      "loss: 0.816343  [ 4200/ 5482]\n",
      "loss: 0.632842  [ 4800/ 5482]\n",
      "loss: 0.684563  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.704     0.768    99\n",
      " disgust     0.687     0.766     0.888    107\n",
      "    fear     0.687     0.679     0.713    80\n",
      "   happy     0.687     0.235     0.052    77\n",
      " neutral     0.687     0.820     0.863    95\n",
      "     sad     0.687     0.533     0.703    91\n",
      "surprise     0.687     0.719     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.637     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.099821 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.637579  [    0/ 5482]\n",
      "loss: 0.774347  [  600/ 5482]\n",
      "loss: 0.704294  [ 1200/ 5482]\n",
      "loss: 0.687778  [ 1800/ 5482]\n",
      "loss: 0.996247  [ 2400/ 5482]\n",
      "loss: 0.622711  [ 3000/ 5482]\n",
      "loss: 0.808182  [ 3600/ 5482]\n",
      "loss: 0.638005  [ 4200/ 5482]\n",
      "loss: 0.686432  [ 4800/ 5482]\n",
      "loss: 0.902398  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.793     0.697    99\n",
      " disgust     0.666     0.758     0.879    107\n",
      "    fear     0.666     0.619     0.750    80\n",
      "   happy     0.666     0.192     0.065    77\n",
      " neutral     0.666     0.804     0.821    95\n",
      "     sad     0.666     0.482     0.736    91\n",
      "surprise     0.666     0.825     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.639     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.115420 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.697013  [    0/ 5482]\n",
      "loss: 0.678655  [  600/ 5482]\n",
      "loss: 0.723056  [ 1200/ 5482]\n",
      "loss: 0.756585  [ 1800/ 5482]\n",
      "loss: 0.642068  [ 2400/ 5482]\n",
      "loss: 0.798848  [ 3000/ 5482]\n",
      "loss: 0.676583  [ 3600/ 5482]\n",
      "loss: 0.815070  [ 4200/ 5482]\n",
      "loss: 0.717679  [ 4800/ 5482]\n",
      "loss: 1.038986  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.687     0.798    99\n",
      " disgust     0.662     0.756     0.869    107\n",
      "    fear     0.662     0.619     0.650    80\n",
      "   happy     0.662     0.143     0.039    77\n",
      " neutral     0.662     0.819     0.811    95\n",
      "     sad     0.662     0.538     0.626    91\n",
      "surprise     0.662     0.642     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.601     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.119156 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.757394  [    0/ 5482]\n",
      "loss: 0.654653  [  600/ 5482]\n",
      "loss: 0.619481  [ 1200/ 5482]\n",
      "loss: 0.721131  [ 1800/ 5482]\n",
      "loss: 0.617761  [ 2400/ 5482]\n",
      "loss: 0.823749  [ 3000/ 5482]\n",
      "loss: 0.615184  [ 3600/ 5482]\n",
      "loss: 0.583133  [ 4200/ 5482]\n",
      "loss: 0.769712  [ 4800/ 5482]\n",
      "loss: 0.656143  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.685     0.716     0.788    99\n",
      " disgust     0.685     0.797     0.879    107\n",
      "    fear     0.685     0.634     0.738    80\n",
      "   happy     0.685     0.176     0.039    77\n",
      " neutral     0.685     0.798     0.874    95\n",
      "     sad     0.685     0.545     0.670    91\n",
      "surprise     0.685     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.685     0.624     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 1.086749 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.579712  [    0/ 5482]\n",
      "loss: 0.605243  [  600/ 5482]\n",
      "loss: 0.734033  [ 1200/ 5482]\n",
      "loss: 0.776818  [ 1800/ 5482]\n",
      "loss: 0.755492  [ 2400/ 5482]\n",
      "loss: 0.678775  [ 3000/ 5482]\n",
      "loss: 0.778703  [ 3600/ 5482]\n",
      "loss: 0.756151  [ 4200/ 5482]\n",
      "loss: 0.722577  [ 4800/ 5482]\n",
      "loss: 0.728388  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.710     0.768    99\n",
      " disgust     0.682     0.783     0.879    107\n",
      "    fear     0.682     0.631     0.662    80\n",
      "   happy     0.682     0.154     0.026    77\n",
      " neutral     0.682     0.766     0.863    95\n",
      "     sad     0.682     0.549     0.736    91\n",
      "surprise     0.682     0.737     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.619     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.087638 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.712409  [    0/ 5482]\n",
      "loss: 0.663503  [  600/ 5482]\n",
      "loss: 0.748632  [ 1200/ 5482]\n",
      "loss: 0.845817  [ 1800/ 5482]\n",
      "loss: 0.711860  [ 2400/ 5482]\n",
      "loss: 0.766974  [ 3000/ 5482]\n",
      "loss: 0.677177  [ 3600/ 5482]\n",
      "loss: 0.783228  [ 4200/ 5482]\n",
      "loss: 0.638561  [ 4800/ 5482]\n",
      "loss: 0.681921  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.738     0.768    99\n",
      " disgust     0.680     0.823     0.869    107\n",
      "    fear     0.680     0.622     0.700    80\n",
      "   happy     0.680     0.190     0.052    77\n",
      " neutral     0.680     0.769     0.874    95\n",
      "     sad     0.680     0.529     0.692    91\n",
      "surprise     0.680     0.714     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.627     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.076455 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.718762  [    0/ 5482]\n",
      "loss: 0.757295  [  600/ 5482]\n",
      "loss: 0.806980  [ 1200/ 5482]\n",
      "loss: 0.607581  [ 1800/ 5482]\n",
      "loss: 0.632370  [ 2400/ 5482]\n",
      "loss: 0.700409  [ 3000/ 5482]\n",
      "loss: 0.508925  [ 3600/ 5482]\n",
      "loss: 0.651797  [ 4200/ 5482]\n",
      "loss: 0.689125  [ 4800/ 5482]\n",
      "loss: 0.612488  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.679     0.770     0.778    99\n",
      " disgust     0.679     0.805     0.888    107\n",
      "    fear     0.679     0.637     0.637    80\n",
      "   happy     0.679     0.190     0.052    77\n",
      " neutral     0.679     0.745     0.863    95\n",
      "     sad     0.679     0.520     0.703    91\n",
      "surprise     0.679     0.707     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.679     0.625     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.095925 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.557019  [    0/ 5482]\n",
      "loss: 0.635479  [  600/ 5482]\n",
      "loss: 0.608491  [ 1200/ 5482]\n",
      "loss: 0.607913  [ 1800/ 5482]\n",
      "loss: 0.582132  [ 2400/ 5482]\n",
      "loss: 0.549257  [ 3000/ 5482]\n",
      "loss: 0.662881  [ 3600/ 5482]\n",
      "loss: 0.722854  [ 4200/ 5482]\n",
      "loss: 0.629025  [ 4800/ 5482]\n",
      "loss: 0.746751  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.793     0.737    99\n",
      " disgust     0.689     0.832     0.879    107\n",
      "    fear     0.689     0.615     0.738    80\n",
      "   happy     0.689     0.200     0.078    77\n",
      " neutral     0.689     0.818     0.853    95\n",
      "     sad     0.689     0.536     0.736    91\n",
      "surprise     0.689     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.646     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.077675 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.548559  [    0/ 5482]\n",
      "loss: 0.534217  [  600/ 5482]\n",
      "loss: 0.805315  [ 1200/ 5482]\n",
      "loss: 0.583807  [ 1800/ 5482]\n",
      "loss: 0.695905  [ 2400/ 5482]\n",
      "loss: 0.748964  [ 3000/ 5482]\n",
      "loss: 0.706356  [ 3600/ 5482]\n",
      "loss: 0.718973  [ 4200/ 5482]\n",
      "loss: 0.821296  [ 4800/ 5482]\n",
      "loss: 0.755975  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.729     0.788    99\n",
      " disgust     0.669     0.817     0.879    107\n",
      "    fear     0.669     0.639     0.662    80\n",
      "   happy     0.669     0.133     0.026    77\n",
      " neutral     0.669     0.708     0.842    95\n",
      "     sad     0.669     0.532     0.637    91\n",
      "surprise     0.669     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.599     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.093147 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.602653  [    0/ 5482]\n",
      "loss: 0.668959  [  600/ 5482]\n",
      "loss: 0.750658  [ 1200/ 5482]\n",
      "loss: 0.867946  [ 1800/ 5482]\n",
      "loss: 0.529191  [ 2400/ 5482]\n",
      "loss: 0.648530  [ 3000/ 5482]\n",
      "loss: 0.798265  [ 3600/ 5482]\n",
      "loss: 0.688336  [ 4200/ 5482]\n",
      "loss: 0.741320  [ 4800/ 5482]\n",
      "loss: 0.630410  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.721     0.758    99\n",
      " disgust     0.669     0.764     0.879    107\n",
      "    fear     0.669     0.645     0.613    80\n",
      "   happy     0.669     0.087     0.026    77\n",
      " neutral     0.669     0.783     0.874    95\n",
      "     sad     0.669     0.538     0.692    91\n",
      "surprise     0.669     0.689     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.604     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.086144 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.531929  [    0/ 5482]\n",
      "loss: 0.797942  [  600/ 5482]\n",
      "loss: 0.714625  [ 1200/ 5482]\n",
      "loss: 0.536268  [ 1800/ 5482]\n",
      "loss: 0.724939  [ 2400/ 5482]\n",
      "loss: 0.590191  [ 3000/ 5482]\n",
      "loss: 0.680919  [ 3600/ 5482]\n",
      "loss: 0.556057  [ 4200/ 5482]\n",
      "loss: 0.678347  [ 4800/ 5482]\n",
      "loss: 0.734048  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.661     0.788    99\n",
      " disgust     0.666     0.822     0.822    107\n",
      "    fear     0.666     0.635     0.675    80\n",
      "   happy     0.666     0.214     0.039    77\n",
      " neutral     0.666     0.771     0.853    95\n",
      "     sad     0.666     0.527     0.648    91\n",
      "surprise     0.666     0.623     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.608     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.104393 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.659066  [    0/ 5482]\n",
      "loss: 0.858102  [  600/ 5482]\n",
      "loss: 0.729919  [ 1200/ 5482]\n",
      "loss: 0.710082  [ 1800/ 5482]\n",
      "loss: 0.690838  [ 2400/ 5482]\n",
      "loss: 0.750247  [ 3000/ 5482]\n",
      "loss: 0.628052  [ 3600/ 5482]\n",
      "loss: 0.530485  [ 4200/ 5482]\n",
      "loss: 0.602894  [ 4800/ 5482]\n",
      "loss: 0.751906  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.784     0.768    99\n",
      " disgust     0.674     0.750     0.897    107\n",
      "    fear     0.674     0.667     0.625    80\n",
      "   happy     0.674     0.125     0.052    77\n",
      " neutral     0.674     0.806     0.832    95\n",
      "     sad     0.674     0.523     0.758    91\n",
      "surprise     0.674     0.771     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.632     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.100474 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.752466  [    0/ 5482]\n",
      "loss: 0.639963  [  600/ 5482]\n",
      "loss: 0.641733  [ 1200/ 5482]\n",
      "loss: 0.529912  [ 1800/ 5482]\n",
      "loss: 0.756033  [ 2400/ 5482]\n",
      "loss: 0.558099  [ 3000/ 5482]\n",
      "loss: 0.816554  [ 3600/ 5482]\n",
      "loss: 0.599457  [ 4200/ 5482]\n",
      "loss: 0.594627  [ 4800/ 5482]\n",
      "loss: 0.716754  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.731     0.768    99\n",
      " disgust     0.669     0.790     0.879    107\n",
      "    fear     0.669     0.667     0.625    80\n",
      "   happy     0.669     0.130     0.039    77\n",
      " neutral     0.669     0.752     0.863    95\n",
      "     sad     0.669     0.521     0.670    91\n",
      "surprise     0.669     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.608     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.085288 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.644684  [    0/ 5482]\n",
      "loss: 0.551464  [  600/ 5482]\n",
      "loss: 0.665793  [ 1200/ 5482]\n",
      "loss: 0.778524  [ 1800/ 5482]\n",
      "loss: 0.647895  [ 2400/ 5482]\n",
      "loss: 0.696522  [ 3000/ 5482]\n",
      "loss: 0.661935  [ 3600/ 5482]\n",
      "loss: 0.594515  [ 4200/ 5482]\n",
      "loss: 0.588703  [ 4800/ 5482]\n",
      "loss: 0.499045  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.690     0.828     0.778    99\n",
      " disgust     0.690     0.874     0.841    107\n",
      "    fear     0.690     0.577     0.800    80\n",
      "   happy     0.690     0.188     0.078    77\n",
      " neutral     0.690     0.876     0.821    95\n",
      "     sad     0.690     0.518     0.780    91\n",
      "surprise     0.690     0.778     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.690     0.663     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.086606 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.686167  [    0/ 5482]\n",
      "loss: 0.633921  [  600/ 5482]\n",
      "loss: 0.562936  [ 1200/ 5482]\n",
      "loss: 0.584890  [ 1800/ 5482]\n",
      "loss: 0.627194  [ 2400/ 5482]\n",
      "loss: 1.051127  [ 3000/ 5482]\n",
      "loss: 0.550505  [ 3600/ 5482]\n",
      "loss: 0.640818  [ 4200/ 5482]\n",
      "loss: 0.625360  [ 4800/ 5482]\n",
      "loss: 0.713041  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.675     0.705     0.798    99\n",
      " disgust     0.675     0.792     0.888    107\n",
      "    fear     0.675     0.640     0.688    80\n",
      "   happy     0.675     0.125     0.039    77\n",
      " neutral     0.675     0.792     0.800    95\n",
      "     sad     0.675     0.555     0.725    91\n",
      "surprise     0.675     0.717     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.675     0.618     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 1.071593 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.637587  [    0/ 5482]\n",
      "loss: 0.531621  [  600/ 5482]\n",
      "loss: 0.590039  [ 1200/ 5482]\n",
      "loss: 0.790110  [ 1800/ 5482]\n",
      "loss: 0.646772  [ 2400/ 5482]\n",
      "loss: 0.787047  [ 3000/ 5482]\n",
      "loss: 0.798553  [ 3600/ 5482]\n",
      "loss: 0.508014  [ 4200/ 5482]\n",
      "loss: 0.564376  [ 4800/ 5482]\n",
      "loss: 0.611353  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.717     0.768    99\n",
      " disgust     0.674     0.710     0.916    107\n",
      "    fear     0.674     0.667     0.625    80\n",
      "   happy     0.674     0.143     0.039    77\n",
      " neutral     0.674     0.786     0.853    95\n",
      "     sad     0.674     0.543     0.692    91\n",
      "surprise     0.674     0.784     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.621     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.076825 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.667975  [    0/ 5482]\n",
      "loss: 0.575335  [  600/ 5482]\n",
      "loss: 0.653866  [ 1200/ 5482]\n",
      "loss: 0.782334  [ 1800/ 5482]\n",
      "loss: 0.580099  [ 2400/ 5482]\n",
      "loss: 0.733862  [ 3000/ 5482]\n",
      "loss: 0.557463  [ 3600/ 5482]\n",
      "loss: 0.431109  [ 4200/ 5482]\n",
      "loss: 0.510857  [ 4800/ 5482]\n",
      "loss: 0.736079  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.796     0.747    99\n",
      " disgust     0.662     0.724     0.860    107\n",
      "    fear     0.662     0.609     0.662    80\n",
      "   happy     0.662     0.167     0.091    77\n",
      " neutral     0.662     0.840     0.832    95\n",
      "     sad     0.662     0.500     0.681    91\n",
      "surprise     0.662     0.860     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.642     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.110141 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.847392  [    0/ 5482]\n",
      "loss: 0.455748  [  600/ 5482]\n",
      "loss: 0.558263  [ 1200/ 5482]\n",
      "loss: 0.419882  [ 1800/ 5482]\n",
      "loss: 0.623893  [ 2400/ 5482]\n",
      "loss: 0.665108  [ 3000/ 5482]\n",
      "loss: 0.912015  [ 3600/ 5482]\n",
      "loss: 0.552610  [ 4200/ 5482]\n",
      "loss: 0.871799  [ 4800/ 5482]\n",
      "loss: 0.656244  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.709     0.788    99\n",
      " disgust     0.662     0.774     0.832    107\n",
      "    fear     0.662     0.632     0.688    80\n",
      "   happy     0.662     0.208     0.065    77\n",
      " neutral     0.662     0.745     0.800    95\n",
      "     sad     0.662     0.532     0.637    91\n",
      "surprise     0.662     0.683     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.612     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.091860 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.708473  [    0/ 5482]\n",
      "loss: 0.633499  [  600/ 5482]\n",
      "loss: 0.564081  [ 1200/ 5482]\n",
      "loss: 0.565599  [ 1800/ 5482]\n",
      "loss: 0.613673  [ 2400/ 5482]\n",
      "loss: 0.580511  [ 3000/ 5482]\n",
      "loss: 0.466327  [ 3600/ 5482]\n",
      "loss: 0.726334  [ 4200/ 5482]\n",
      "loss: 0.671296  [ 4800/ 5482]\n",
      "loss: 0.609155  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.661     0.828    99\n",
      " disgust     0.674     0.784     0.850    107\n",
      "    fear     0.674     0.667     0.725    80\n",
      "   happy     0.674     0.176     0.039    77\n",
      " neutral     0.674     0.755     0.779    95\n",
      "     sad     0.674     0.556     0.659    91\n",
      "surprise     0.674     0.717     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.617     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.079677 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.658987  [    0/ 5482]\n",
      "loss: 0.636829  [  600/ 5482]\n",
      "loss: 0.692149  [ 1200/ 5482]\n",
      "loss: 0.440478  [ 1800/ 5482]\n",
      "loss: 0.576980  [ 2400/ 5482]\n",
      "loss: 0.805713  [ 3000/ 5482]\n",
      "loss: 0.614554  [ 3600/ 5482]\n",
      "loss: 0.682035  [ 4200/ 5482]\n",
      "loss: 0.617702  [ 4800/ 5482]\n",
      "loss: 0.683459  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.678     0.788    99\n",
      " disgust     0.659     0.744     0.869    107\n",
      "    fear     0.659     0.634     0.650    80\n",
      "   happy     0.659     0.167     0.052    77\n",
      " neutral     0.659     0.798     0.789    95\n",
      "     sad     0.659     0.546     0.648    91\n",
      "surprise     0.659     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.604     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.102714 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.621197  [    0/ 5482]\n",
      "loss: 0.593192  [  600/ 5482]\n",
      "loss: 0.578302  [ 1200/ 5482]\n",
      "loss: 0.722079  [ 1800/ 5482]\n",
      "loss: 0.498993  [ 2400/ 5482]\n",
      "loss: 0.599220  [ 3000/ 5482]\n",
      "loss: 0.359561  [ 3600/ 5482]\n",
      "loss: 0.825357  [ 4200/ 5482]\n",
      "loss: 0.650000  [ 4800/ 5482]\n",
      "loss: 0.516880  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.745     0.798    99\n",
      " disgust     0.680     0.771     0.850    107\n",
      "    fear     0.680     0.630     0.725    80\n",
      "   happy     0.680     0.150     0.039    77\n",
      " neutral     0.680     0.782     0.832    95\n",
      "     sad     0.680     0.539     0.681    91\n",
      "surprise     0.680     0.741     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.623     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.073561 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.689779  [    0/ 5482]\n",
      "loss: 0.586597  [  600/ 5482]\n",
      "loss: 0.616310  [ 1200/ 5482]\n",
      "loss: 0.829467  [ 1800/ 5482]\n",
      "loss: 0.651765  [ 2400/ 5482]\n",
      "loss: 0.774090  [ 3000/ 5482]\n",
      "loss: 0.628862  [ 3600/ 5482]\n",
      "loss: 0.627661  [ 4200/ 5482]\n",
      "loss: 0.591817  [ 4800/ 5482]\n",
      "loss: 0.545026  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.680     0.706     0.778    99\n",
      " disgust     0.680     0.812     0.850    107\n",
      "    fear     0.680     0.663     0.713    80\n",
      "   happy     0.680     0.120     0.039    77\n",
      " neutral     0.680     0.781     0.863    95\n",
      "     sad     0.680     0.544     0.681    91\n",
      "surprise     0.680     0.729     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.680     0.622     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 1.057279 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.670827  [    0/ 5482]\n",
      "loss: 0.628669  [  600/ 5482]\n",
      "loss: 0.590806  [ 1200/ 5482]\n",
      "loss: 0.503188  [ 1800/ 5482]\n",
      "loss: 0.699299  [ 2400/ 5482]\n",
      "loss: 0.637106  [ 3000/ 5482]\n",
      "loss: 0.511004  [ 3600/ 5482]\n",
      "loss: 0.734705  [ 4200/ 5482]\n",
      "loss: 0.496041  [ 4800/ 5482]\n",
      "loss: 0.667043  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.738     0.768    99\n",
      " disgust     0.672     0.768     0.897    107\n",
      "    fear     0.672     0.651     0.675    80\n",
      "   happy     0.672     0.100     0.039    77\n",
      " neutral     0.672     0.796     0.863    95\n",
      "     sad     0.672     0.522     0.659    91\n",
      "surprise     0.672     0.765     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.620     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.068856 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.642318  [    0/ 5482]\n",
      "loss: 0.618329  [  600/ 5482]\n",
      "loss: 0.665100  [ 1200/ 5482]\n",
      "loss: 0.626252  [ 1800/ 5482]\n",
      "loss: 0.693500  [ 2400/ 5482]\n",
      "loss: 0.695291  [ 3000/ 5482]\n",
      "loss: 0.663245  [ 3600/ 5482]\n",
      "loss: 0.555864  [ 4200/ 5482]\n",
      "loss: 0.745472  [ 4800/ 5482]\n",
      "loss: 0.672866  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.729     0.788    99\n",
      " disgust     0.674     0.812     0.850    107\n",
      "    fear     0.674     0.682     0.725    80\n",
      "   happy     0.674     0.150     0.039    77\n",
      " neutral     0.674     0.717     0.853    95\n",
      "     sad     0.674     0.528     0.615    91\n",
      "surprise     0.674     0.657     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.611     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.086022 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.812618  [    0/ 5482]\n",
      "loss: 0.775948  [  600/ 5482]\n",
      "loss: 0.717875  [ 1200/ 5482]\n",
      "loss: 0.539260  [ 1800/ 5482]\n",
      "loss: 0.448634  [ 2400/ 5482]\n",
      "loss: 0.544721  [ 3000/ 5482]\n",
      "loss: 0.514811  [ 3600/ 5482]\n",
      "loss: 0.553364  [ 4200/ 5482]\n",
      "loss: 0.649966  [ 4800/ 5482]\n",
      "loss: 0.526107  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.696     0.808    99\n",
      " disgust     0.672     0.817     0.832    107\n",
      "    fear     0.672     0.617     0.725    80\n",
      "   happy     0.672     0.125     0.026    77\n",
      " neutral     0.672     0.812     0.821    95\n",
      "     sad     0.672     0.528     0.615    91\n",
      "surprise     0.672     0.635     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.604     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.067596 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.685427  [    0/ 5482]\n",
      "loss: 0.597878  [  600/ 5482]\n",
      "loss: 0.525542  [ 1200/ 5482]\n",
      "loss: 0.577245  [ 1800/ 5482]\n",
      "loss: 0.486491  [ 2400/ 5482]\n",
      "loss: 0.814188  [ 3000/ 5482]\n",
      "loss: 0.384524  [ 3600/ 5482]\n",
      "loss: 0.580907  [ 4200/ 5482]\n",
      "loss: 0.655627  [ 4800/ 5482]\n",
      "loss: 0.567392  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.684     0.788    99\n",
      " disgust     0.664     0.748     0.888    107\n",
      "    fear     0.664     0.639     0.662    80\n",
      "   happy     0.664     0.133     0.052    77\n",
      " neutral     0.664     0.828     0.811    95\n",
      "     sad     0.664     0.536     0.648    91\n",
      "surprise     0.664     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.615     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.070616 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.654697  [    0/ 5482]\n",
      "loss: 0.917297  [  600/ 5482]\n",
      "loss: 0.758233  [ 1200/ 5482]\n",
      "loss: 0.737418  [ 1800/ 5482]\n",
      "loss: 0.753166  [ 2400/ 5482]\n",
      "loss: 0.761562  [ 3000/ 5482]\n",
      "loss: 0.524101  [ 3600/ 5482]\n",
      "loss: 0.600817  [ 4200/ 5482]\n",
      "loss: 0.623279  [ 4800/ 5482]\n",
      "loss: 0.574190  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.694     0.778    99\n",
      " disgust     0.667     0.802     0.869    107\n",
      "    fear     0.667     0.640     0.713    80\n",
      "   happy     0.667     0.143     0.052    77\n",
      " neutral     0.667     0.784     0.842    95\n",
      "     sad     0.667     0.538     0.615    91\n",
      "surprise     0.667     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.610     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.063805 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.586193  [    0/ 5482]\n",
      "loss: 0.592594  [  600/ 5482]\n",
      "loss: 0.834618  [ 1200/ 5482]\n",
      "loss: 0.631205  [ 1800/ 5482]\n",
      "loss: 0.497420  [ 2400/ 5482]\n",
      "loss: 0.499942  [ 3000/ 5482]\n",
      "loss: 0.857115  [ 3600/ 5482]\n",
      "loss: 0.572285  [ 4200/ 5482]\n",
      "loss: 0.480419  [ 4800/ 5482]\n",
      "loss: 0.594279  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.771     0.747    99\n",
      " disgust     0.687     0.843     0.850    107\n",
      "    fear     0.687     0.648     0.738    80\n",
      "   happy     0.687     0.156     0.065    77\n",
      " neutral     0.687     0.776     0.874    95\n",
      "     sad     0.687     0.553     0.747    91\n",
      "surprise     0.687     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.640     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.057830 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.526427  [    0/ 5482]\n",
      "loss: 0.565107  [  600/ 5482]\n",
      "loss: 0.581513  [ 1200/ 5482]\n",
      "loss: 0.707409  [ 1800/ 5482]\n",
      "loss: 0.698189  [ 2400/ 5482]\n",
      "loss: 0.443416  [ 3000/ 5482]\n",
      "loss: 0.467989  [ 3600/ 5482]\n",
      "loss: 0.444393  [ 4200/ 5482]\n",
      "loss: 0.492360  [ 4800/ 5482]\n",
      "loss: 0.637427  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.724     0.768    99\n",
      " disgust     0.664     0.756     0.869    107\n",
      "    fear     0.664     0.650     0.650    80\n",
      "   happy     0.664     0.048     0.013    77\n",
      " neutral     0.664     0.728     0.874    95\n",
      "     sad     0.664     0.536     0.659    91\n",
      "surprise     0.664     0.727     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.596     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.060620 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.495867  [    0/ 5482]\n",
      "loss: 0.573826  [  600/ 5482]\n",
      "loss: 0.698832  [ 1200/ 5482]\n",
      "loss: 0.435884  [ 1800/ 5482]\n",
      "loss: 0.587500  [ 2400/ 5482]\n",
      "loss: 0.522847  [ 3000/ 5482]\n",
      "loss: 0.993418  [ 3600/ 5482]\n",
      "loss: 0.755211  [ 4200/ 5482]\n",
      "loss: 0.533431  [ 4800/ 5482]\n",
      "loss: 0.583200  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.717     0.768    99\n",
      " disgust     0.672     0.809     0.869    107\n",
      "    fear     0.672     0.656     0.738    80\n",
      "   happy     0.672     0.107     0.039    77\n",
      " neutral     0.672     0.792     0.842    95\n",
      "     sad     0.672     0.527     0.648    91\n",
      "surprise     0.672     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.614     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.061150 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.850088  [    0/ 5482]\n",
      "loss: 0.531349  [  600/ 5482]\n",
      "loss: 0.603588  [ 1200/ 5482]\n",
      "loss: 0.663147  [ 1800/ 5482]\n",
      "loss: 0.506112  [ 2400/ 5482]\n",
      "loss: 0.628547  [ 3000/ 5482]\n",
      "loss: 0.572299  [ 3600/ 5482]\n",
      "loss: 0.521644  [ 4200/ 5482]\n",
      "loss: 0.558195  [ 4800/ 5482]\n",
      "loss: 0.521615  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.690     0.808    99\n",
      " disgust     0.677     0.852     0.860    107\n",
      "    fear     0.677     0.633     0.713    80\n",
      "   happy     0.677     0.167     0.052    77\n",
      " neutral     0.677     0.800     0.842    95\n",
      "     sad     0.677     0.538     0.615    91\n",
      "surprise     0.677     0.647     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.618     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.055632 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.810574  [    0/ 5482]\n",
      "loss: 0.660265  [  600/ 5482]\n",
      "loss: 0.675444  [ 1200/ 5482]\n",
      "loss: 0.595340  [ 1800/ 5482]\n",
      "loss: 0.626759  [ 2400/ 5482]\n",
      "loss: 0.698671  [ 3000/ 5482]\n",
      "loss: 0.466198  [ 3600/ 5482]\n",
      "loss: 0.755707  [ 4200/ 5482]\n",
      "loss: 0.964263  [ 4800/ 5482]\n",
      "loss: 0.585580  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.780     0.788    99\n",
      " disgust     0.689     0.812     0.850    107\n",
      "    fear     0.689     0.618     0.787    80\n",
      "   happy     0.689     0.208     0.065    77\n",
      " neutral     0.689     0.794     0.853    95\n",
      "     sad     0.689     0.554     0.681    91\n",
      "surprise     0.689     0.690     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.637     0.669    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.053623 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.405154  [    0/ 5482]\n",
      "loss: 0.584828  [  600/ 5482]\n",
      "loss: 0.645579  [ 1200/ 5482]\n",
      "loss: 0.563627  [ 1800/ 5482]\n",
      "loss: 0.783214  [ 2400/ 5482]\n",
      "loss: 0.577724  [ 3000/ 5482]\n",
      "loss: 0.501392  [ 3600/ 5482]\n",
      "loss: 0.555482  [ 4200/ 5482]\n",
      "loss: 0.522578  [ 4800/ 5482]\n",
      "loss: 0.424799  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.725     0.747    99\n",
      " disgust     0.666     0.733     0.897    107\n",
      "    fear     0.666     0.650     0.650    80\n",
      "   happy     0.666     0.139     0.065    77\n",
      " neutral     0.666     0.833     0.842    95\n",
      "     sad     0.666     0.528     0.615    91\n",
      "surprise     0.666     0.729     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.620     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.075024 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.465952  [    0/ 5482]\n",
      "loss: 0.363125  [  600/ 5482]\n",
      "loss: 0.589723  [ 1200/ 5482]\n",
      "loss: 0.532887  [ 1800/ 5482]\n",
      "loss: 0.573270  [ 2400/ 5482]\n",
      "loss: 0.641533  [ 3000/ 5482]\n",
      "loss: 0.417563  [ 3600/ 5482]\n",
      "loss: 0.431880  [ 4200/ 5482]\n",
      "loss: 0.676226  [ 4800/ 5482]\n",
      "loss: 0.761873  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.689     0.752     0.798    99\n",
      " disgust     0.689     0.865     0.841    107\n",
      "    fear     0.689     0.622     0.762    80\n",
      "   happy     0.689     0.261     0.078    77\n",
      " neutral     0.689     0.723     0.853    95\n",
      "     sad     0.689     0.555     0.670    91\n",
      "surprise     0.689     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.689     0.643     0.670    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.036414 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.537788  [    0/ 5482]\n",
      "loss: 0.622547  [  600/ 5482]\n",
      "loss: 0.443843  [ 1200/ 5482]\n",
      "loss: 0.667391  [ 1800/ 5482]\n",
      "loss: 0.521470  [ 2400/ 5482]\n",
      "loss: 0.656501  [ 3000/ 5482]\n",
      "loss: 0.784326  [ 3600/ 5482]\n",
      "loss: 0.436054  [ 4200/ 5482]\n",
      "loss: 0.656961  [ 4800/ 5482]\n",
      "loss: 0.639178  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.691     0.768    99\n",
      " disgust     0.666     0.769     0.869    107\n",
      "    fear     0.666     0.651     0.700    80\n",
      "   happy     0.666     0.167     0.052    77\n",
      " neutral     0.666     0.748     0.811    95\n",
      "     sad     0.666     0.548     0.626    91\n",
      "surprise     0.666     0.694     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.610     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.076053 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.413997  [    0/ 5482]\n",
      "loss: 0.701179  [  600/ 5482]\n",
      "loss: 0.692642  [ 1200/ 5482]\n",
      "loss: 0.513630  [ 1800/ 5482]\n",
      "loss: 0.446660  [ 2400/ 5482]\n",
      "loss: 0.700628  [ 3000/ 5482]\n",
      "loss: 0.800241  [ 3600/ 5482]\n",
      "loss: 0.693513  [ 4200/ 5482]\n",
      "loss: 0.681649  [ 4800/ 5482]\n",
      "loss: 0.482929  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.705     0.798    99\n",
      " disgust     0.669     0.793     0.860    107\n",
      "    fear     0.669     0.655     0.713    80\n",
      "   happy     0.669     0.120     0.039    77\n",
      " neutral     0.669     0.786     0.811    95\n",
      "     sad     0.669     0.529     0.593    91\n",
      "surprise     0.669     0.657     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.607     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.066028 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.577628  [    0/ 5482]\n",
      "loss: 0.630495  [  600/ 5482]\n",
      "loss: 0.712518  [ 1200/ 5482]\n",
      "loss: 0.874352  [ 1800/ 5482]\n",
      "loss: 0.608071  [ 2400/ 5482]\n",
      "loss: 0.527018  [ 3000/ 5482]\n",
      "loss: 0.555803  [ 3600/ 5482]\n",
      "loss: 0.602528  [ 4200/ 5482]\n",
      "loss: 0.441833  [ 4800/ 5482]\n",
      "loss: 0.557581  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.687     0.772     0.788    99\n",
      " disgust     0.687     0.790     0.879    107\n",
      "    fear     0.687     0.663     0.713    80\n",
      "   happy     0.687     0.120     0.039    77\n",
      " neutral     0.687     0.778     0.884    95\n",
      "     sad     0.687     0.551     0.648    91\n",
      "surprise     0.687     0.688     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.687     0.623     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 1.040405 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.395779  [    0/ 5482]\n",
      "loss: 0.638196  [  600/ 5482]\n",
      "loss: 0.616759  [ 1200/ 5482]\n",
      "loss: 0.618487  [ 1800/ 5482]\n",
      "loss: 0.561826  [ 2400/ 5482]\n",
      "loss: 0.621811  [ 3000/ 5482]\n",
      "loss: 0.609784  [ 3600/ 5482]\n",
      "loss: 0.651942  [ 4200/ 5482]\n",
      "loss: 0.743167  [ 4800/ 5482]\n",
      "loss: 0.614681  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.792     0.768    99\n",
      " disgust     0.682     0.805     0.850    107\n",
      "    fear     0.682     0.659     0.725    80\n",
      "   happy     0.682     0.111     0.052    77\n",
      " neutral     0.682     0.788     0.863    95\n",
      "     sad     0.682     0.559     0.725    91\n",
      "surprise     0.682     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.632     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.050961 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.508421  [    0/ 5482]\n",
      "loss: 0.652709  [  600/ 5482]\n",
      "loss: 0.597286  [ 1200/ 5482]\n",
      "loss: 0.653689  [ 1800/ 5482]\n",
      "loss: 0.439162  [ 2400/ 5482]\n",
      "loss: 0.664551  [ 3000/ 5482]\n",
      "loss: 0.559959  [ 3600/ 5482]\n",
      "loss: 0.622258  [ 4200/ 5482]\n",
      "loss: 0.490572  [ 4800/ 5482]\n",
      "loss: 0.458714  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.702     0.808    99\n",
      " disgust     0.670     0.805     0.850    107\n",
      "    fear     0.670     0.644     0.725    80\n",
      "   happy     0.670     0.217     0.065    77\n",
      " neutral     0.670     0.743     0.821    95\n",
      "     sad     0.670     0.524     0.593    91\n",
      "surprise     0.670     0.694     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.619     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.074049 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.660858  [    0/ 5482]\n",
      "loss: 0.471996  [  600/ 5482]\n",
      "loss: 0.556388  [ 1200/ 5482]\n",
      "loss: 0.590883  [ 1800/ 5482]\n",
      "loss: 0.515742  [ 2400/ 5482]\n",
      "loss: 0.428303  [ 3000/ 5482]\n",
      "loss: 0.695352  [ 3600/ 5482]\n",
      "loss: 0.465430  [ 4200/ 5482]\n",
      "loss: 0.618401  [ 4800/ 5482]\n",
      "loss: 0.471191  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.702     0.808    99\n",
      " disgust     0.669     0.892     0.776    107\n",
      "    fear     0.669     0.570     0.863    80\n",
      "   happy     0.669     0.171     0.078    77\n",
      " neutral     0.669     0.851     0.779    95\n",
      "     sad     0.669     0.551     0.593    91\n",
      "surprise     0.669     0.677     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.631     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.050185 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.601261  [    0/ 5482]\n",
      "loss: 0.661355  [  600/ 5482]\n",
      "loss: 0.811020  [ 1200/ 5482]\n",
      "loss: 0.510623  [ 1800/ 5482]\n",
      "loss: 0.770858  [ 2400/ 5482]\n",
      "loss: 0.665420  [ 3000/ 5482]\n",
      "loss: 0.621064  [ 3600/ 5482]\n",
      "loss: 0.612030  [ 4200/ 5482]\n",
      "loss: 0.519593  [ 4800/ 5482]\n",
      "loss: 0.586545  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.763     0.747    99\n",
      " disgust     0.677     0.758     0.879    107\n",
      "    fear     0.677     0.674     0.725    80\n",
      "   happy     0.677     0.162     0.078    77\n",
      " neutral     0.677     0.804     0.863    95\n",
      "     sad     0.677     0.541     0.659    91\n",
      "surprise     0.677     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.634     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.033537 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.745620  [    0/ 5482]\n",
      "loss: 0.634928  [  600/ 5482]\n",
      "loss: 0.591023  [ 1200/ 5482]\n",
      "loss: 0.691256  [ 1800/ 5482]\n",
      "loss: 0.559202  [ 2400/ 5482]\n",
      "loss: 0.348959  [ 3000/ 5482]\n",
      "loss: 0.593885  [ 3600/ 5482]\n",
      "loss: 0.537757  [ 4200/ 5482]\n",
      "loss: 0.569055  [ 4800/ 5482]\n",
      "loss: 0.649262  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.729     0.788    99\n",
      " disgust     0.670     0.752     0.879    107\n",
      "    fear     0.670     0.659     0.725    80\n",
      "   happy     0.670     0.077     0.026    77\n",
      " neutral     0.670     0.780     0.821    95\n",
      "     sad     0.670     0.532     0.637    91\n",
      "surprise     0.670     0.745     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.611     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.044002 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.714986  [    0/ 5482]\n",
      "loss: 0.491264  [  600/ 5482]\n",
      "loss: 0.575049  [ 1200/ 5482]\n",
      "loss: 0.698673  [ 1800/ 5482]\n",
      "loss: 0.463267  [ 2400/ 5482]\n",
      "loss: 0.544724  [ 3000/ 5482]\n",
      "loss: 0.506362  [ 3600/ 5482]\n",
      "loss: 0.635247  [ 4200/ 5482]\n",
      "loss: 0.689657  [ 4800/ 5482]\n",
      "loss: 0.483232  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.720     0.778    99\n",
      " disgust     0.672     0.748     0.860    107\n",
      "    fear     0.672     0.655     0.688    80\n",
      "   happy     0.672     0.150     0.039    77\n",
      " neutral     0.672     0.771     0.884    95\n",
      "     sad     0.672     0.552     0.637    91\n",
      "surprise     0.672     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.608     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.062092 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.647478  [    0/ 5482]\n",
      "loss: 0.681163  [  600/ 5482]\n",
      "loss: 0.398022  [ 1200/ 5482]\n",
      "loss: 0.408411  [ 1800/ 5482]\n",
      "loss: 0.710890  [ 2400/ 5482]\n",
      "loss: 0.522766  [ 3000/ 5482]\n",
      "loss: 0.341719  [ 3600/ 5482]\n",
      "loss: 0.580067  [ 4200/ 5482]\n",
      "loss: 0.572173  [ 4800/ 5482]\n",
      "loss: 0.492287  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.698     0.818    99\n",
      " disgust     0.667     0.754     0.860    107\n",
      "    fear     0.667     0.618     0.688    80\n",
      "   happy     0.667     0.158     0.039    77\n",
      " neutral     0.667     0.782     0.832    95\n",
      "     sad     0.667     0.535     0.593    91\n",
      "surprise     0.667     0.694     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.606     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.052123 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.550487  [    0/ 5482]\n",
      "loss: 0.607035  [  600/ 5482]\n",
      "loss: 0.608979  [ 1200/ 5482]\n",
      "loss: 0.647459  [ 1800/ 5482]\n",
      "loss: 0.509436  [ 2400/ 5482]\n",
      "loss: 0.555037  [ 3000/ 5482]\n",
      "loss: 0.634849  [ 3600/ 5482]\n",
      "loss: 0.505811  [ 4200/ 5482]\n",
      "loss: 0.601629  [ 4800/ 5482]\n",
      "loss: 0.516233  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.706     0.778    99\n",
      " disgust     0.674     0.782     0.869    107\n",
      "    fear     0.674     0.630     0.725    80\n",
      "   happy     0.674     0.138     0.052    77\n",
      " neutral     0.674     0.800     0.842    95\n",
      "     sad     0.674     0.558     0.637    91\n",
      "surprise     0.674     0.719     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.619     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.038031 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.664240  [    0/ 5482]\n",
      "loss: 0.714370  [  600/ 5482]\n",
      "loss: 0.651586  [ 1200/ 5482]\n",
      "loss: 0.520492  [ 1800/ 5482]\n",
      "loss: 0.551143  [ 2400/ 5482]\n",
      "loss: 0.401794  [ 3000/ 5482]\n",
      "loss: 0.557273  [ 3600/ 5482]\n",
      "loss: 0.506482  [ 4200/ 5482]\n",
      "loss: 0.442645  [ 4800/ 5482]\n",
      "loss: 0.540586  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.758     0.758    99\n",
      " disgust     0.674     0.790     0.879    107\n",
      "    fear     0.674     0.598     0.762    80\n",
      "   happy     0.674     0.167     0.091    77\n",
      " neutral     0.674     0.878     0.832    95\n",
      "     sad     0.674     0.529     0.593    91\n",
      "surprise     0.674     0.732     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.636     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.021534 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.652675  [    0/ 5482]\n",
      "loss: 0.615571  [  600/ 5482]\n",
      "loss: 0.602671  [ 1200/ 5482]\n",
      "loss: 0.748630  [ 1800/ 5482]\n",
      "loss: 0.509200  [ 2400/ 5482]\n",
      "loss: 0.606011  [ 3000/ 5482]\n",
      "loss: 0.690071  [ 3600/ 5482]\n",
      "loss: 0.640128  [ 4200/ 5482]\n",
      "loss: 0.399216  [ 4800/ 5482]\n",
      "loss: 0.554627  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.745     0.737    99\n",
      " disgust     0.670     0.782     0.869    107\n",
      "    fear     0.670     0.651     0.700    80\n",
      "   happy     0.670     0.109     0.065    77\n",
      " neutral     0.670     0.825     0.842    95\n",
      "     sad     0.670     0.562     0.692    91\n",
      "surprise     0.670     0.750     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.632     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.057541 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.425230  [    0/ 5482]\n",
      "loss: 0.870651  [  600/ 5482]\n",
      "loss: 0.738760  [ 1200/ 5482]\n",
      "loss: 0.614580  [ 1800/ 5482]\n",
      "loss: 0.575794  [ 2400/ 5482]\n",
      "loss: 0.566520  [ 3000/ 5482]\n",
      "loss: 0.412987  [ 3600/ 5482]\n",
      "loss: 0.509468  [ 4200/ 5482]\n",
      "loss: 0.483242  [ 4800/ 5482]\n",
      "loss: 0.704395  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.675     0.798    99\n",
      " disgust     0.662     0.775     0.869    107\n",
      "    fear     0.662     0.654     0.637    80\n",
      "   happy     0.662     0.133     0.052    77\n",
      " neutral     0.662     0.782     0.832    95\n",
      "     sad     0.662     0.540     0.593    91\n",
      "surprise     0.662     0.688     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.607     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.066254 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.616253  [    0/ 5482]\n",
      "loss: 0.532721  [  600/ 5482]\n",
      "loss: 0.704369  [ 1200/ 5482]\n",
      "loss: 0.641493  [ 1800/ 5482]\n",
      "loss: 0.461455  [ 2400/ 5482]\n",
      "loss: 0.730179  [ 3000/ 5482]\n",
      "loss: 0.380158  [ 3600/ 5482]\n",
      "loss: 0.397070  [ 4200/ 5482]\n",
      "loss: 0.686896  [ 4800/ 5482]\n",
      "loss: 0.320426  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.758     0.758    99\n",
      " disgust     0.666     0.805     0.850    107\n",
      "    fear     0.666     0.648     0.713    80\n",
      "   happy     0.666     0.135     0.091    77\n",
      " neutral     0.666     0.776     0.800    95\n",
      "     sad     0.666     0.560     0.670    91\n",
      "surprise     0.666     0.765     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.635     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.069562 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.440407  [    0/ 5482]\n",
      "loss: 0.749361  [  600/ 5482]\n",
      "loss: 0.537058  [ 1200/ 5482]\n",
      "loss: 0.490768  [ 1800/ 5482]\n",
      "loss: 0.448329  [ 2400/ 5482]\n",
      "loss: 0.663782  [ 3000/ 5482]\n",
      "loss: 0.605569  [ 3600/ 5482]\n",
      "loss: 0.380890  [ 4200/ 5482]\n",
      "loss: 0.621280  [ 4800/ 5482]\n",
      "loss: 0.507320  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.714     0.758    99\n",
      " disgust     0.661     0.711     0.897    107\n",
      "    fear     0.661     0.647     0.688    80\n",
      "   happy     0.661     0.074     0.026    77\n",
      " neutral     0.661     0.826     0.800    95\n",
      "     sad     0.661     0.556     0.659    91\n",
      "surprise     0.661     0.672     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.600     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.062537 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.384182  [    0/ 5482]\n",
      "loss: 0.684071  [  600/ 5482]\n",
      "loss: 0.771322  [ 1200/ 5482]\n",
      "loss: 0.586939  [ 1800/ 5482]\n",
      "loss: 0.538076  [ 2400/ 5482]\n",
      "loss: 0.638595  [ 3000/ 5482]\n",
      "loss: 0.548360  [ 3600/ 5482]\n",
      "loss: 0.510301  [ 4200/ 5482]\n",
      "loss: 0.507870  [ 4800/ 5482]\n",
      "loss: 0.394954  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.697     0.768    99\n",
      " disgust     0.656     0.730     0.860    107\n",
      "    fear     0.656     0.611     0.725    80\n",
      "   happy     0.656     0.152     0.065    77\n",
      " neutral     0.656     0.844     0.800    95\n",
      "     sad     0.656     0.541     0.582    91\n",
      "surprise     0.656     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.608     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.054989 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.498485  [    0/ 5482]\n",
      "loss: 0.601530  [  600/ 5482]\n",
      "loss: 0.316305  [ 1200/ 5482]\n",
      "loss: 0.639620  [ 1800/ 5482]\n",
      "loss: 0.516957  [ 2400/ 5482]\n",
      "loss: 0.358973  [ 3000/ 5482]\n",
      "loss: 0.521346  [ 3600/ 5482]\n",
      "loss: 0.577477  [ 4200/ 5482]\n",
      "loss: 0.569132  [ 4800/ 5482]\n",
      "loss: 0.612900  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.828     0.727    99\n",
      " disgust     0.670     0.727     0.897    107\n",
      "    fear     0.670     0.675     0.675    80\n",
      "   happy     0.670     0.081     0.039    77\n",
      " neutral     0.670     0.792     0.884    95\n",
      "     sad     0.670     0.537     0.637    91\n",
      "surprise     0.670     0.700     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.620     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.045902 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.466071  [    0/ 5482]\n",
      "loss: 0.555924  [  600/ 5482]\n",
      "loss: 0.582017  [ 1200/ 5482]\n",
      "loss: 0.484352  [ 1800/ 5482]\n",
      "loss: 0.497172  [ 2400/ 5482]\n",
      "loss: 0.671240  [ 3000/ 5482]\n",
      "loss: 0.449633  [ 3600/ 5482]\n",
      "loss: 0.553960  [ 4200/ 5482]\n",
      "loss: 0.595273  [ 4800/ 5482]\n",
      "loss: 0.534313  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.785     0.737    99\n",
      " disgust     0.667     0.787     0.897    107\n",
      "    fear     0.667     0.675     0.650    80\n",
      "   happy     0.667     0.105     0.052    77\n",
      " neutral     0.667     0.769     0.874    95\n",
      "     sad     0.667     0.528     0.615    91\n",
      "surprise     0.667     0.652     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.614     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.037545 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.616332  [    0/ 5482]\n",
      "loss: 0.402918  [  600/ 5482]\n",
      "loss: 0.554140  [ 1200/ 5482]\n",
      "loss: 0.425566  [ 1800/ 5482]\n",
      "loss: 0.504177  [ 2400/ 5482]\n",
      "loss: 0.498086  [ 3000/ 5482]\n",
      "loss: 0.652227  [ 3600/ 5482]\n",
      "loss: 0.579922  [ 4200/ 5482]\n",
      "loss: 0.516459  [ 4800/ 5482]\n",
      "loss: 0.568282  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.780     0.788    99\n",
      " disgust     0.669     0.805     0.888    107\n",
      "    fear     0.669     0.645     0.613    80\n",
      "   happy     0.669     0.154     0.052    77\n",
      " neutral     0.669     0.680     0.895    95\n",
      "     sad     0.669     0.541     0.582    91\n",
      "surprise     0.669     0.657     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.609     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.054074 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.543179  [    0/ 5482]\n",
      "loss: 0.686077  [  600/ 5482]\n",
      "loss: 0.515729  [ 1200/ 5482]\n",
      "loss: 0.385183  [ 1800/ 5482]\n",
      "loss: 0.594914  [ 2400/ 5482]\n",
      "loss: 0.608032  [ 3000/ 5482]\n",
      "loss: 0.533918  [ 3600/ 5482]\n",
      "loss: 0.428590  [ 4200/ 5482]\n",
      "loss: 0.480585  [ 4800/ 5482]\n",
      "loss: 0.567625  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.672     0.740     0.778    99\n",
      " disgust     0.672     0.782     0.869    107\n",
      "    fear     0.672     0.640     0.688    80\n",
      "   happy     0.672     0.194     0.091    77\n",
      " neutral     0.672     0.788     0.863    95\n",
      "     sad     0.672     0.551     0.593    91\n",
      "surprise     0.672     0.667     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.672     0.623     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.060351 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.558050  [    0/ 5482]\n",
      "loss: 0.410431  [  600/ 5482]\n",
      "loss: 0.554742  [ 1200/ 5482]\n",
      "loss: 0.472684  [ 1800/ 5482]\n",
      "loss: 0.524757  [ 2400/ 5482]\n",
      "loss: 0.507890  [ 3000/ 5482]\n",
      "loss: 0.624444  [ 3600/ 5482]\n",
      "loss: 0.419134  [ 4200/ 5482]\n",
      "loss: 0.585244  [ 4800/ 5482]\n",
      "loss: 0.356005  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.722     0.788    99\n",
      " disgust     0.669     0.775     0.869    107\n",
      "    fear     0.669     0.624     0.662    80\n",
      "   happy     0.669     0.152     0.065    77\n",
      " neutral     0.669     0.783     0.874    95\n",
      "     sad     0.669     0.539     0.604    91\n",
      "surprise     0.669     0.732     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.618     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.047082 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.410155  [    0/ 5482]\n",
      "loss: 0.450697  [  600/ 5482]\n",
      "loss: 0.513193  [ 1200/ 5482]\n",
      "loss: 0.490812  [ 1800/ 5482]\n",
      "loss: 0.299021  [ 2400/ 5482]\n",
      "loss: 0.625370  [ 3000/ 5482]\n",
      "loss: 0.564150  [ 3600/ 5482]\n",
      "loss: 0.437417  [ 4200/ 5482]\n",
      "loss: 0.507506  [ 4800/ 5482]\n",
      "loss: 0.656693  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.777     0.737    99\n",
      " disgust     0.662     0.817     0.832    107\n",
      "    fear     0.662     0.629     0.700    80\n",
      "   happy     0.662     0.163     0.091    77\n",
      " neutral     0.662     0.755     0.842    95\n",
      "     sad     0.662     0.554     0.615    91\n",
      "surprise     0.662     0.632     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.618     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.058465 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.369327  [    0/ 5482]\n",
      "loss: 0.636550  [  600/ 5482]\n",
      "loss: 0.526484  [ 1200/ 5482]\n",
      "loss: 0.426354  [ 1800/ 5482]\n",
      "loss: 0.502681  [ 2400/ 5482]\n",
      "loss: 0.764193  [ 3000/ 5482]\n",
      "loss: 0.541157  [ 3600/ 5482]\n",
      "loss: 0.429641  [ 4200/ 5482]\n",
      "loss: 0.511283  [ 4800/ 5482]\n",
      "loss: 0.405385  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.674     0.700     0.778    99\n",
      " disgust     0.674     0.814     0.860    107\n",
      "    fear     0.674     0.631     0.662    80\n",
      "   happy     0.674     0.289     0.143    77\n",
      " neutral     0.674     0.767     0.832    95\n",
      "     sad     0.674     0.554     0.560    91\n",
      "surprise     0.674     0.686     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.674     0.635     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.064265 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.568801  [    0/ 5482]\n",
      "loss: 0.568276  [  600/ 5482]\n",
      "loss: 0.477481  [ 1200/ 5482]\n",
      "loss: 0.544254  [ 1800/ 5482]\n",
      "loss: 0.670739  [ 2400/ 5482]\n",
      "loss: 0.668460  [ 3000/ 5482]\n",
      "loss: 0.387251  [ 3600/ 5482]\n",
      "loss: 0.481267  [ 4200/ 5482]\n",
      "loss: 0.307610  [ 4800/ 5482]\n",
      "loss: 0.443271  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.720     0.778    99\n",
      " disgust     0.670     0.770     0.879    107\n",
      "    fear     0.670     0.616     0.662    80\n",
      "   happy     0.670     0.256     0.130    77\n",
      " neutral     0.670     0.786     0.853    95\n",
      "     sad     0.670     0.548     0.560    91\n",
      "surprise     0.670     0.717     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.631     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.052388 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.487673  [    0/ 5482]\n",
      "loss: 0.404847  [  600/ 5482]\n",
      "loss: 0.493570  [ 1200/ 5482]\n",
      "loss: 0.469507  [ 1800/ 5482]\n",
      "loss: 0.325259  [ 2400/ 5482]\n",
      "loss: 0.593285  [ 3000/ 5482]\n",
      "loss: 0.434380  [ 3600/ 5482]\n",
      "loss: 0.494044  [ 4200/ 5482]\n",
      "loss: 0.344380  [ 4800/ 5482]\n",
      "loss: 0.408412  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.743     0.788    99\n",
      " disgust     0.667     0.819     0.888    107\n",
      "    fear     0.667     0.635     0.675    80\n",
      "   happy     0.667     0.152     0.091    77\n",
      " neutral     0.667     0.757     0.853    95\n",
      "     sad     0.667     0.536     0.571    91\n",
      "surprise     0.667     0.741     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.626     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.027638 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.580357  [    0/ 5482]\n",
      "loss: 0.737221  [  600/ 5482]\n",
      "loss: 0.742503  [ 1200/ 5482]\n",
      "loss: 0.566620  [ 1800/ 5482]\n",
      "loss: 0.464563  [ 2400/ 5482]\n",
      "loss: 0.408537  [ 3000/ 5482]\n",
      "loss: 0.473181  [ 3600/ 5482]\n",
      "loss: 0.572489  [ 4200/ 5482]\n",
      "loss: 0.506272  [ 4800/ 5482]\n",
      "loss: 0.443678  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.730     0.818    99\n",
      " disgust     0.682     0.805     0.888    107\n",
      "    fear     0.682     0.633     0.713    80\n",
      "   happy     0.682     0.227     0.130    77\n",
      " neutral     0.682     0.811     0.811    95\n",
      "     sad     0.682     0.578     0.571    91\n",
      "surprise     0.682     0.710     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.642     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.034200 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.690750  [    0/ 5482]\n",
      "loss: 0.575999  [  600/ 5482]\n",
      "loss: 0.479236  [ 1200/ 5482]\n",
      "loss: 0.586495  [ 1800/ 5482]\n",
      "loss: 0.452162  [ 2400/ 5482]\n",
      "loss: 0.514432  [ 3000/ 5482]\n",
      "loss: 0.618802  [ 3600/ 5482]\n",
      "loss: 0.628997  [ 4200/ 5482]\n",
      "loss: 0.451520  [ 4800/ 5482]\n",
      "loss: 0.508264  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.682     0.718     0.798    99\n",
      " disgust     0.682     0.790     0.879    107\n",
      "    fear     0.682     0.655     0.713    80\n",
      "   happy     0.682     0.209     0.117    77\n",
      " neutral     0.682     0.828     0.863    95\n",
      "     sad     0.682     0.556     0.549    91\n",
      "surprise     0.682     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.682     0.640     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.039014 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.404045  [    0/ 5482]\n",
      "loss: 0.473027  [  600/ 5482]\n",
      "loss: 0.626356  [ 1200/ 5482]\n",
      "loss: 0.544842  [ 1800/ 5482]\n",
      "loss: 0.637200  [ 2400/ 5482]\n",
      "loss: 0.470447  [ 3000/ 5482]\n",
      "loss: 0.654503  [ 3600/ 5482]\n",
      "loss: 0.407593  [ 4200/ 5482]\n",
      "loss: 0.508112  [ 4800/ 5482]\n",
      "loss: 0.431952  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.669     0.818    99\n",
      " disgust     0.670     0.812     0.850    107\n",
      "    fear     0.670     0.671     0.688    80\n",
      "   happy     0.670     0.265     0.169    77\n",
      " neutral     0.670     0.804     0.779    95\n",
      "     sad     0.670     0.585     0.527    91\n",
      "surprise     0.670     0.653     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.637     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.070337 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.494224  [    0/ 5482]\n",
      "loss: 0.731738  [  600/ 5482]\n",
      "loss: 0.464158  [ 1200/ 5482]\n",
      "loss: 0.407597  [ 1800/ 5482]\n",
      "loss: 0.600903  [ 2400/ 5482]\n",
      "loss: 0.585083  [ 3000/ 5482]\n",
      "loss: 0.418879  [ 3600/ 5482]\n",
      "loss: 0.665249  [ 4200/ 5482]\n",
      "loss: 0.510586  [ 4800/ 5482]\n",
      "loss: 0.807181  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.709     0.788    99\n",
      " disgust     0.661     0.791     0.850    107\n",
      "    fear     0.661     0.655     0.688    80\n",
      "   happy     0.661     0.246     0.182    77\n",
      " neutral     0.661     0.750     0.853    95\n",
      "     sad     0.661     0.570     0.495    91\n",
      "surprise     0.661     0.684     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.629     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.039963 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.527417  [    0/ 5482]\n",
      "loss: 0.296275  [  600/ 5482]\n",
      "loss: 0.770922  [ 1200/ 5482]\n",
      "loss: 0.406923  [ 1800/ 5482]\n",
      "loss: 0.550721  [ 2400/ 5482]\n",
      "loss: 0.473224  [ 3000/ 5482]\n",
      "loss: 0.683843  [ 3600/ 5482]\n",
      "loss: 0.538889  [ 4200/ 5482]\n",
      "loss: 0.425294  [ 4800/ 5482]\n",
      "loss: 0.473505  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.675     0.798    99\n",
      " disgust     0.659     0.752     0.850    107\n",
      "    fear     0.659     0.651     0.675    80\n",
      "   happy     0.659     0.250     0.182    77\n",
      " neutral     0.659     0.843     0.789    95\n",
      "     sad     0.659     0.579     0.484    91\n",
      "surprise     0.659     0.662     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.630     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.061799 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.430979  [    0/ 5482]\n",
      "loss: 0.653577  [  600/ 5482]\n",
      "loss: 0.545924  [ 1200/ 5482]\n",
      "loss: 0.587228  [ 1800/ 5482]\n",
      "loss: 0.520186  [ 2400/ 5482]\n",
      "loss: 0.407159  [ 3000/ 5482]\n",
      "loss: 0.633448  [ 3600/ 5482]\n",
      "loss: 0.644933  [ 4200/ 5482]\n",
      "loss: 0.510383  [ 4800/ 5482]\n",
      "loss: 0.420196  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.740     0.747    99\n",
      " disgust     0.677     0.841     0.841    107\n",
      "    fear     0.677     0.653     0.775    80\n",
      "   happy     0.677     0.242     0.208    77\n",
      " neutral     0.677     0.837     0.811    95\n",
      "     sad     0.677     0.578     0.527    91\n",
      "surprise     0.677     0.687     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.654     0.666    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.030235 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.652709  [    0/ 5482]\n",
      "loss: 0.797138  [  600/ 5482]\n",
      "loss: 0.582595  [ 1200/ 5482]\n",
      "loss: 0.796185  [ 1800/ 5482]\n",
      "loss: 0.383572  [ 2400/ 5482]\n",
      "loss: 0.471815  [ 3000/ 5482]\n",
      "loss: 0.530910  [ 3600/ 5482]\n",
      "loss: 0.552619  [ 4200/ 5482]\n",
      "loss: 0.571511  [ 4800/ 5482]\n",
      "loss: 0.397767  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.677     0.793     0.737    99\n",
      " disgust     0.677     0.748     0.888    107\n",
      "    fear     0.677     0.675     0.650    80\n",
      "   happy     0.677     0.330     0.416    77\n",
      " neutral     0.677     0.764     0.853    95\n",
      "     sad     0.677     0.731     0.418    91\n",
      "surprise     0.677     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.677     0.679     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.047481 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.427006  [    0/ 5482]\n",
      "loss: 1.047225  [  600/ 5482]\n",
      "loss: 0.435239  [ 1200/ 5482]\n",
      "loss: 0.440542  [ 1800/ 5482]\n",
      "loss: 0.663376  [ 2400/ 5482]\n",
      "loss: 0.445980  [ 3000/ 5482]\n",
      "loss: 0.446324  [ 3600/ 5482]\n",
      "loss: 0.409248  [ 4200/ 5482]\n",
      "loss: 0.511533  [ 4800/ 5482]\n",
      "loss: 0.362791  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.661     0.788    99\n",
      " disgust     0.644     0.879     0.813    107\n",
      "    fear     0.644     0.619     0.750    80\n",
      "   happy     0.644     0.333     0.532    77\n",
      " neutral     0.644     0.794     0.811    95\n",
      "     sad     0.644     0.500     0.033    91\n",
      "surprise     0.644     0.671     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.637     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.072782 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.512883  [    0/ 5482]\n",
      "loss: 0.428344  [  600/ 5482]\n",
      "loss: 0.389112  [ 1200/ 5482]\n",
      "loss: 0.686176  [ 1800/ 5482]\n",
      "loss: 0.583056  [ 2400/ 5482]\n",
      "loss: 0.927913  [ 3000/ 5482]\n",
      "loss: 0.423802  [ 3600/ 5482]\n",
      "loss: 0.549966  [ 4200/ 5482]\n",
      "loss: 0.382561  [ 4800/ 5482]\n",
      "loss: 0.476762  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.672     0.788    99\n",
      " disgust     0.657     0.814     0.860    107\n",
      "    fear     0.657     0.674     0.725    80\n",
      "   happy     0.657     0.352     0.571    77\n",
      " neutral     0.657     0.802     0.853    95\n",
      "     sad     0.657     0.500     0.033    91\n",
      "surprise     0.657     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.647     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.036724 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.557653  [    0/ 5482]\n",
      "loss: 0.437710  [  600/ 5482]\n",
      "loss: 0.649628  [ 1200/ 5482]\n",
      "loss: 0.451093  [ 1800/ 5482]\n",
      "loss: 0.518123  [ 2400/ 5482]\n",
      "loss: 0.322185  [ 3000/ 5482]\n",
      "loss: 0.686730  [ 3600/ 5482]\n",
      "loss: 0.648253  [ 4200/ 5482]\n",
      "loss: 0.536921  [ 4800/ 5482]\n",
      "loss: 0.590403  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.747     0.747    99\n",
      " disgust     0.654     0.766     0.888    107\n",
      "    fear     0.654     0.670     0.762    80\n",
      "   happy     0.654     0.338     0.610    77\n",
      " neutral     0.654     0.790     0.832    95\n",
      "     sad     0.654     0.667     0.022    91\n",
      "surprise     0.654     0.759     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.677     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.035728 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.263959  [    0/ 5482]\n",
      "loss: 0.547595  [  600/ 5482]\n",
      "loss: 0.312934  [ 1200/ 5482]\n",
      "loss: 0.509513  [ 1800/ 5482]\n",
      "loss: 0.523048  [ 2400/ 5482]\n",
      "loss: 0.464776  [ 3000/ 5482]\n",
      "loss: 0.359544  [ 3600/ 5482]\n",
      "loss: 0.496554  [ 4200/ 5482]\n",
      "loss: 0.550650  [ 4800/ 5482]\n",
      "loss: 0.394306  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.781     0.758    99\n",
      " disgust     0.644     0.788     0.869    107\n",
      "    fear     0.644     0.644     0.700    80\n",
      "   happy     0.644     0.325     0.649    77\n",
      " neutral     0.644     0.780     0.821    95\n",
      "     sad     0.644     0.667     0.022    91\n",
      "surprise     0.644     0.750     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.676     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.040827 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.607032  [    0/ 5482]\n",
      "loss: 0.507529  [  600/ 5482]\n",
      "loss: 0.649582  [ 1200/ 5482]\n",
      "loss: 0.644915  [ 1800/ 5482]\n",
      "loss: 0.422161  [ 2400/ 5482]\n",
      "loss: 0.510122  [ 3000/ 5482]\n",
      "loss: 0.372774  [ 3600/ 5482]\n",
      "loss: 0.431169  [ 4200/ 5482]\n",
      "loss: 0.421451  [ 4800/ 5482]\n",
      "loss: 0.683411  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.764     0.818    99\n",
      " disgust     0.659     0.811     0.841    107\n",
      "    fear     0.659     0.624     0.787    80\n",
      "   happy     0.659     0.348     0.597    77\n",
      " neutral     0.659     0.815     0.789    95\n",
      "     sad     0.659     0.667     0.022    91\n",
      "surprise     0.659     0.692     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.674     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.014911 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.564678  [    0/ 5482]\n",
      "loss: 0.375562  [  600/ 5482]\n",
      "loss: 0.457085  [ 1200/ 5482]\n",
      "loss: 0.549367  [ 1800/ 5482]\n",
      "loss: 0.364707  [ 2400/ 5482]\n",
      "loss: 0.616770  [ 3000/ 5482]\n",
      "loss: 0.572041  [ 3600/ 5482]\n",
      "loss: 0.609202  [ 4200/ 5482]\n",
      "loss: 0.646001  [ 4800/ 5482]\n",
      "loss: 0.722063  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.710     0.768    99\n",
      " disgust     0.648     0.746     0.879    107\n",
      "    fear     0.648     0.679     0.713    80\n",
      "   happy     0.648     0.329     0.597    77\n",
      " neutral     0.648     0.800     0.842    95\n",
      "     sad     0.648     0.500     0.011    91\n",
      "surprise     0.648     0.804     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.652     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.042112 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.400086  [    0/ 5482]\n",
      "loss: 0.521605  [  600/ 5482]\n",
      "loss: 0.691808  [ 1200/ 5482]\n",
      "loss: 0.683977  [ 1800/ 5482]\n",
      "loss: 0.593585  [ 2400/ 5482]\n",
      "loss: 0.494182  [ 3000/ 5482]\n",
      "loss: 0.643042  [ 3600/ 5482]\n",
      "loss: 0.617077  [ 4200/ 5482]\n",
      "loss: 0.538081  [ 4800/ 5482]\n",
      "loss: 0.458568  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.750     0.758    99\n",
      " disgust     0.652     0.762     0.869    107\n",
      "    fear     0.652     0.688     0.662    80\n",
      "   happy     0.652     0.345     0.623    77\n",
      " neutral     0.652     0.771     0.884    95\n",
      "     sad     0.652     0.500     0.011    91\n",
      "surprise     0.652     0.721     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.648     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.031977 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.530175  [    0/ 5482]\n",
      "loss: 0.482597  [  600/ 5482]\n",
      "loss: 0.326476  [ 1200/ 5482]\n",
      "loss: 0.458746  [ 1800/ 5482]\n",
      "loss: 0.648625  [ 2400/ 5482]\n",
      "loss: 0.494124  [ 3000/ 5482]\n",
      "loss: 0.389533  [ 3600/ 5482]\n",
      "loss: 0.629702  [ 4200/ 5482]\n",
      "loss: 0.285313  [ 4800/ 5482]\n",
      "loss: 0.427093  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.813     0.747    99\n",
      " disgust     0.648     0.820     0.850    107\n",
      "    fear     0.648     0.642     0.762    80\n",
      "   happy     0.648     0.311     0.662    77\n",
      " neutral     0.648     0.775     0.832    95\n",
      "     sad     0.648     1.000     0.022    91\n",
      "surprise     0.648     0.822     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.740     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.026511 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.772809  [    0/ 5482]\n",
      "loss: 0.520704  [  600/ 5482]\n",
      "loss: 0.582530  [ 1200/ 5482]\n",
      "loss: 0.535430  [ 1800/ 5482]\n",
      "loss: 0.419225  [ 2400/ 5482]\n",
      "loss: 0.377534  [ 3000/ 5482]\n",
      "loss: 0.481976  [ 3600/ 5482]\n",
      "loss: 0.406159  [ 4200/ 5482]\n",
      "loss: 0.294552  [ 4800/ 5482]\n",
      "loss: 0.733309  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.787     0.747    99\n",
      " disgust     0.651     0.784     0.850    107\n",
      "    fear     0.651     0.622     0.762    80\n",
      "   happy     0.651     0.336     0.662    77\n",
      " neutral     0.651     0.800     0.842    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.800     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.590     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.030032 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.677887  [    0/ 5482]\n",
      "loss: 0.546627  [  600/ 5482]\n",
      "loss: 0.534975  [ 1200/ 5482]\n",
      "loss: 0.440897  [ 1800/ 5482]\n",
      "loss: 0.444217  [ 2400/ 5482]\n",
      "loss: 0.629291  [ 3000/ 5482]\n",
      "loss: 0.507813  [ 3600/ 5482]\n",
      "loss: 0.598966  [ 4200/ 5482]\n",
      "loss: 0.515428  [ 4800/ 5482]\n",
      "loss: 0.625114  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.702     0.808    99\n",
      " disgust     0.656     0.796     0.841    107\n",
      "    fear     0.656     0.648     0.738    80\n",
      "   happy     0.656     0.353     0.623    77\n",
      " neutral     0.656     0.823     0.832    95\n",
      "     sad     0.656     1.000     0.011    91\n",
      "surprise     0.656     0.729     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.722     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.024377 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.525890  [    0/ 5482]\n",
      "loss: 0.641700  [  600/ 5482]\n",
      "loss: 0.568207  [ 1200/ 5482]\n",
      "loss: 0.881490  [ 1800/ 5482]\n",
      "loss: 0.444613  [ 2400/ 5482]\n",
      "loss: 0.369470  [ 3000/ 5482]\n",
      "loss: 0.350301  [ 3600/ 5482]\n",
      "loss: 0.614795  [ 4200/ 5482]\n",
      "loss: 0.583864  [ 4800/ 5482]\n",
      "loss: 0.517326  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.717     0.768    99\n",
      " disgust     0.664     0.805     0.850    107\n",
      "    fear     0.664     0.659     0.725    80\n",
      "   happy     0.664     0.369     0.623    77\n",
      " neutral     0.664     0.814     0.874    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.690     0.803    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.579     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.022578 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.586505  [    0/ 5482]\n",
      "loss: 0.747525  [  600/ 5482]\n",
      "loss: 0.307222  [ 1200/ 5482]\n",
      "loss: 0.552251  [ 1800/ 5482]\n",
      "loss: 0.420820  [ 2400/ 5482]\n",
      "loss: 0.709443  [ 3000/ 5482]\n",
      "loss: 0.478768  [ 3600/ 5482]\n",
      "loss: 0.419670  [ 4200/ 5482]\n",
      "loss: 0.411857  [ 4800/ 5482]\n",
      "loss: 0.262207  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.755     0.808    99\n",
      " disgust     0.666     0.811     0.841    107\n",
      "    fear     0.666     0.663     0.738    80\n",
      "   happy     0.666     0.345     0.623    77\n",
      " neutral     0.666     0.822     0.874    95\n",
      "     sad     0.666     0.000     0.000    91\n",
      "surprise     0.666     0.719     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.588     0.663    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.004084 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.522266  [    0/ 5482]\n",
      "loss: 0.448801  [  600/ 5482]\n",
      "loss: 0.263794  [ 1200/ 5482]\n",
      "loss: 0.477340  [ 1800/ 5482]\n",
      "loss: 0.476129  [ 2400/ 5482]\n",
      "loss: 0.635504  [ 3000/ 5482]\n",
      "loss: 0.464038  [ 3600/ 5482]\n",
      "loss: 0.716283  [ 4200/ 5482]\n",
      "loss: 0.583961  [ 4800/ 5482]\n",
      "loss: 0.695697  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.760     0.798    99\n",
      " disgust     0.666     0.821     0.897    107\n",
      "    fear     0.666     0.649     0.762    80\n",
      "   happy     0.666     0.346     0.610    77\n",
      " neutral     0.666     0.800     0.842    95\n",
      "     sad     0.666     1.000     0.011    91\n",
      "surprise     0.666     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.728     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.989908 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.447691  [    0/ 5482]\n",
      "loss: 0.393086  [  600/ 5482]\n",
      "loss: 0.366484  [ 1200/ 5482]\n",
      "loss: 0.548433  [ 1800/ 5482]\n",
      "loss: 0.596016  [ 2400/ 5482]\n",
      "loss: 0.528905  [ 3000/ 5482]\n",
      "loss: 0.447367  [ 3600/ 5482]\n",
      "loss: 0.585122  [ 4200/ 5482]\n",
      "loss: 0.388239  [ 4800/ 5482]\n",
      "loss: 0.370113  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.755     0.808    99\n",
      " disgust     0.661     0.812     0.888    107\n",
      "    fear     0.661     0.667     0.750    80\n",
      "   happy     0.661     0.340     0.636    77\n",
      " neutral     0.661     0.800     0.842    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.736     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.587     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.997564 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.535143  [    0/ 5482]\n",
      "loss: 0.669307  [  600/ 5482]\n",
      "loss: 0.420233  [ 1200/ 5482]\n",
      "loss: 0.516458  [ 1800/ 5482]\n",
      "loss: 0.508111  [ 2400/ 5482]\n",
      "loss: 0.346788  [ 3000/ 5482]\n",
      "loss: 0.234150  [ 3600/ 5482]\n",
      "loss: 0.303839  [ 4200/ 5482]\n",
      "loss: 0.507021  [ 4800/ 5482]\n",
      "loss: 0.474342  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.712     0.798    99\n",
      " disgust     0.649     0.688     0.907    107\n",
      "    fear     0.649     0.753     0.688    80\n",
      "   happy     0.649     0.364     0.558    77\n",
      " neutral     0.649     0.760     0.832    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.683     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.566     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.065014 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.352010  [    0/ 5482]\n",
      "loss: 0.443363  [  600/ 5482]\n",
      "loss: 0.486175  [ 1200/ 5482]\n",
      "loss: 0.566591  [ 1800/ 5482]\n",
      "loss: 0.559154  [ 2400/ 5482]\n",
      "loss: 0.562330  [ 3000/ 5482]\n",
      "loss: 0.493582  [ 3600/ 5482]\n",
      "loss: 0.509033  [ 4200/ 5482]\n",
      "loss: 0.440432  [ 4800/ 5482]\n",
      "loss: 0.327727  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.723     0.818    99\n",
      " disgust     0.649     0.788     0.869    107\n",
      "    fear     0.649     0.654     0.662    80\n",
      "   happy     0.649     0.338     0.597    77\n",
      " neutral     0.649     0.786     0.853    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.700     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.570     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.016659 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.362107  [    0/ 5482]\n",
      "loss: 0.400243  [  600/ 5482]\n",
      "loss: 0.675333  [ 1200/ 5482]\n",
      "loss: 0.613710  [ 1800/ 5482]\n",
      "loss: 0.506789  [ 2400/ 5482]\n",
      "loss: 0.482889  [ 3000/ 5482]\n",
      "loss: 0.592558  [ 3600/ 5482]\n",
      "loss: 0.558653  [ 4200/ 5482]\n",
      "loss: 0.421261  [ 4800/ 5482]\n",
      "loss: 0.577824  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.767     0.798    99\n",
      " disgust     0.662     0.844     0.860    107\n",
      "    fear     0.662     0.659     0.725    80\n",
      "   happy     0.662     0.366     0.636    77\n",
      " neutral     0.662     0.757     0.853    95\n",
      "     sad     0.662     0.000     0.000    91\n",
      "surprise     0.662     0.652     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.578     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.998372 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.413078  [    0/ 5482]\n",
      "loss: 0.454117  [  600/ 5482]\n",
      "loss: 0.490497  [ 1200/ 5482]\n",
      "loss: 0.602640  [ 1800/ 5482]\n",
      "loss: 0.410080  [ 2400/ 5482]\n",
      "loss: 0.576533  [ 3000/ 5482]\n",
      "loss: 0.390395  [ 3600/ 5482]\n",
      "loss: 0.366726  [ 4200/ 5482]\n",
      "loss: 0.286607  [ 4800/ 5482]\n",
      "loss: 0.448125  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.712     0.798    99\n",
      " disgust     0.656     0.791     0.850    107\n",
      "    fear     0.656     0.695     0.713    80\n",
      "   happy     0.656     0.370     0.571    77\n",
      " neutral     0.656     0.755     0.874    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.630     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.565     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.038619 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.427735  [    0/ 5482]\n",
      "loss: 0.524610  [  600/ 5482]\n",
      "loss: 0.345604  [ 1200/ 5482]\n",
      "loss: 0.434753  [ 1800/ 5482]\n",
      "loss: 0.367968  [ 2400/ 5482]\n",
      "loss: 0.514454  [ 3000/ 5482]\n",
      "loss: 0.484741  [ 3600/ 5482]\n",
      "loss: 0.465236  [ 4200/ 5482]\n",
      "loss: 0.476124  [ 4800/ 5482]\n",
      "loss: 0.409473  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.694     0.758    99\n",
      " disgust     0.659     0.830     0.869    107\n",
      "    fear     0.659     0.656     0.738    80\n",
      "   happy     0.659     0.355     0.636    77\n",
      " neutral     0.659     0.818     0.853    95\n",
      "     sad     0.659     0.000     0.000    91\n",
      "surprise     0.659     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.581     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.025289 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.497233  [    0/ 5482]\n",
      "loss: 0.686374  [  600/ 5482]\n",
      "loss: 0.500775  [ 1200/ 5482]\n",
      "loss: 0.399880  [ 1800/ 5482]\n",
      "loss: 0.620195  [ 2400/ 5482]\n",
      "loss: 0.324624  [ 3000/ 5482]\n",
      "loss: 0.608975  [ 3600/ 5482]\n",
      "loss: 0.362926  [ 4200/ 5482]\n",
      "loss: 0.463536  [ 4800/ 5482]\n",
      "loss: 0.367200  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.720     0.778    99\n",
      " disgust     0.639     0.811     0.841    107\n",
      "    fear     0.639     0.652     0.725    80\n",
      "   happy     0.639     0.315     0.610    77\n",
      " neutral     0.639     0.792     0.800    95\n",
      "     sad     0.639     0.000     0.000    91\n",
      "surprise     0.639     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.573     0.635    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.027826 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.579030  [    0/ 5482]\n",
      "loss: 0.477352  [  600/ 5482]\n",
      "loss: 0.435048  [ 1200/ 5482]\n",
      "loss: 0.390529  [ 1800/ 5482]\n",
      "loss: 0.431207  [ 2400/ 5482]\n",
      "loss: 0.349947  [ 3000/ 5482]\n",
      "loss: 0.279429  [ 3600/ 5482]\n",
      "loss: 0.279860  [ 4200/ 5482]\n",
      "loss: 0.375651  [ 4800/ 5482]\n",
      "loss: 0.346744  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.718     0.798    99\n",
      " disgust     0.659     0.782     0.869    107\n",
      "    fear     0.659     0.700     0.700    80\n",
      "   happy     0.659     0.343     0.597    77\n",
      " neutral     0.659     0.816     0.842    95\n",
      "     sad     0.659     0.000     0.000    91\n",
      "surprise     0.659     0.696     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.579     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.016710 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.588676  [    0/ 5482]\n",
      "loss: 0.384508  [  600/ 5482]\n",
      "loss: 0.421419  [ 1200/ 5482]\n",
      "loss: 0.515310  [ 1800/ 5482]\n",
      "loss: 0.516276  [ 2400/ 5482]\n",
      "loss: 0.352674  [ 3000/ 5482]\n",
      "loss: 0.463678  [ 3600/ 5482]\n",
      "loss: 0.407639  [ 4200/ 5482]\n",
      "loss: 0.546447  [ 4800/ 5482]\n",
      "loss: 0.581214  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.755     0.747    99\n",
      " disgust     0.652     0.760     0.860    107\n",
      "    fear     0.652     0.662     0.637    80\n",
      "   happy     0.652     0.350     0.636    77\n",
      " neutral     0.652     0.810     0.895    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.681     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.574     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.043458 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.297296  [    0/ 5482]\n",
      "loss: 0.428939  [  600/ 5482]\n",
      "loss: 0.446720  [ 1200/ 5482]\n",
      "loss: 0.622308  [ 1800/ 5482]\n",
      "loss: 0.365550  [ 2400/ 5482]\n",
      "loss: 0.636764  [ 3000/ 5482]\n",
      "loss: 0.396611  [ 3600/ 5482]\n",
      "loss: 0.551533  [ 4200/ 5482]\n",
      "loss: 0.426850  [ 4800/ 5482]\n",
      "loss: 0.618146  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.705     0.798    99\n",
      " disgust     0.652     0.791     0.850    107\n",
      "    fear     0.652     0.651     0.700    80\n",
      "   happy     0.652     0.346     0.597    77\n",
      " neutral     0.652     0.794     0.853    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.573     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.000545 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.339742  [    0/ 5482]\n",
      "loss: 0.552256  [  600/ 5482]\n",
      "loss: 0.365854  [ 1200/ 5482]\n",
      "loss: 0.262343  [ 1800/ 5482]\n",
      "loss: 0.352213  [ 2400/ 5482]\n",
      "loss: 0.391538  [ 3000/ 5482]\n",
      "loss: 0.604634  [ 3600/ 5482]\n",
      "loss: 0.426592  [ 4200/ 5482]\n",
      "loss: 0.563541  [ 4800/ 5482]\n",
      "loss: 0.302735  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.698     0.818    99\n",
      " disgust     0.657     0.846     0.822    107\n",
      "    fear     0.657     0.652     0.750    80\n",
      "   happy     0.657     0.343     0.623    77\n",
      " neutral     0.657     0.808     0.842    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.746     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.585     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.988302 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.645246  [    0/ 5482]\n",
      "loss: 0.350933  [  600/ 5482]\n",
      "loss: 0.393279  [ 1200/ 5482]\n",
      "loss: 0.524846  [ 1800/ 5482]\n",
      "loss: 0.357837  [ 2400/ 5482]\n",
      "loss: 0.381600  [ 3000/ 5482]\n",
      "loss: 0.380992  [ 3600/ 5482]\n",
      "loss: 0.474797  [ 4200/ 5482]\n",
      "loss: 0.336279  [ 4800/ 5482]\n",
      "loss: 0.317078  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.661     0.747    99\n",
      " disgust     0.639     0.833     0.794    107\n",
      "    fear     0.639     0.651     0.700    80\n",
      "   happy     0.639     0.377     0.597    77\n",
      " neutral     0.639     0.762     0.842    95\n",
      "     sad     0.639     0.000     0.000    91\n",
      "surprise     0.639     0.590     0.803    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.554     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.080128 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.359718  [    0/ 5482]\n",
      "loss: 0.240389  [  600/ 5482]\n",
      "loss: 0.736605  [ 1200/ 5482]\n",
      "loss: 0.496281  [ 1800/ 5482]\n",
      "loss: 0.297122  [ 2400/ 5482]\n",
      "loss: 0.355119  [ 3000/ 5482]\n",
      "loss: 0.531336  [ 3600/ 5482]\n",
      "loss: 0.488272  [ 4200/ 5482]\n",
      "loss: 0.564485  [ 4800/ 5482]\n",
      "loss: 0.360331  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.762     0.778    99\n",
      " disgust     0.652     0.790     0.879    107\n",
      "    fear     0.652     0.644     0.700    80\n",
      "   happy     0.652     0.331     0.636    77\n",
      " neutral     0.652     0.812     0.821    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.746     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.584     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.007800 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.580328  [    0/ 5482]\n",
      "loss: 0.272401  [  600/ 5482]\n",
      "loss: 0.826336  [ 1200/ 5482]\n",
      "loss: 0.395820  [ 1800/ 5482]\n",
      "loss: 0.369598  [ 2400/ 5482]\n",
      "loss: 0.604987  [ 3000/ 5482]\n",
      "loss: 0.352597  [ 3600/ 5482]\n",
      "loss: 0.389142  [ 4200/ 5482]\n",
      "loss: 0.394057  [ 4800/ 5482]\n",
      "loss: 0.669155  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.670     0.739     0.828    99\n",
      " disgust     0.670     0.865     0.841    107\n",
      "    fear     0.670     0.636     0.787    80\n",
      "   happy     0.670     0.369     0.623    77\n",
      " neutral     0.670     0.769     0.842    95\n",
      "     sad     0.670     0.000     0.000    91\n",
      "surprise     0.670     0.742     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.670     0.589     0.668    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.981193 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.355083  [    0/ 5482]\n",
      "loss: 0.436847  [  600/ 5482]\n",
      "loss: 0.378251  [ 1200/ 5482]\n",
      "loss: 0.516412  [ 1800/ 5482]\n",
      "loss: 0.425828  [ 2400/ 5482]\n",
      "loss: 0.465348  [ 3000/ 5482]\n",
      "loss: 0.422175  [ 3600/ 5482]\n",
      "loss: 0.498712  [ 4200/ 5482]\n",
      "loss: 0.363144  [ 4800/ 5482]\n",
      "loss: 0.361422  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.762     0.808    99\n",
      " disgust     0.666     0.817     0.879    107\n",
      "    fear     0.666     0.693     0.650    80\n",
      "   happy     0.666     0.348     0.636    77\n",
      " neutral     0.666     0.759     0.895    95\n",
      "     sad     0.666     0.000     0.000    91\n",
      "surprise     0.666     0.742     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.589     0.660    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.001637 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.439147  [    0/ 5482]\n",
      "loss: 0.451195  [  600/ 5482]\n",
      "loss: 0.384965  [ 1200/ 5482]\n",
      "loss: 0.392712  [ 1800/ 5482]\n",
      "loss: 0.485118  [ 2400/ 5482]\n",
      "loss: 0.421457  [ 3000/ 5482]\n",
      "loss: 0.473819  [ 3600/ 5482]\n",
      "loss: 0.426553  [ 4200/ 5482]\n",
      "loss: 0.360010  [ 4800/ 5482]\n",
      "loss: 0.560847  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.734     0.808    99\n",
      " disgust     0.662     0.795     0.869    107\n",
      "    fear     0.662     0.671     0.688    80\n",
      "   happy     0.662     0.372     0.623    77\n",
      " neutral     0.662     0.724     0.884    95\n",
      "     sad     0.662     0.000     0.000    91\n",
      "surprise     0.662     0.772     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.581     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.986163 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.372485  [    0/ 5482]\n",
      "loss: 0.245015  [  600/ 5482]\n",
      "loss: 0.303571  [ 1200/ 5482]\n",
      "loss: 0.445796  [ 1800/ 5482]\n",
      "loss: 0.623629  [ 2400/ 5482]\n",
      "loss: 0.528295  [ 3000/ 5482]\n",
      "loss: 0.343667  [ 3600/ 5482]\n",
      "loss: 0.549433  [ 4200/ 5482]\n",
      "loss: 0.458572  [ 4800/ 5482]\n",
      "loss: 0.308011  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.755     0.808    99\n",
      " disgust     0.654     0.780     0.860    107\n",
      "    fear     0.654     0.634     0.738    80\n",
      "   happy     0.654     0.338     0.597    77\n",
      " neutral     0.654     0.831     0.779    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.706     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.578     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.002426 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.484450  [    0/ 5482]\n",
      "loss: 0.785843  [  600/ 5482]\n",
      "loss: 0.282241  [ 1200/ 5482]\n",
      "loss: 0.760449  [ 1800/ 5482]\n",
      "loss: 0.437371  [ 2400/ 5482]\n",
      "loss: 0.289188  [ 3000/ 5482]\n",
      "loss: 0.436489  [ 3600/ 5482]\n",
      "loss: 0.358308  [ 4200/ 5482]\n",
      "loss: 0.585582  [ 4800/ 5482]\n",
      "loss: 0.419091  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.734     0.808    99\n",
      " disgust     0.651     0.754     0.888    107\n",
      "    fear     0.651     0.671     0.713    80\n",
      "   happy     0.651     0.341     0.597    77\n",
      " neutral     0.651     0.796     0.821    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.719     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.573     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.007812 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.527800  [    0/ 5482]\n",
      "loss: 0.556230  [  600/ 5482]\n",
      "loss: 0.376093  [ 1200/ 5482]\n",
      "loss: 0.470464  [ 1800/ 5482]\n",
      "loss: 0.730486  [ 2400/ 5482]\n",
      "loss: 0.590574  [ 3000/ 5482]\n",
      "loss: 0.337119  [ 3600/ 5482]\n",
      "loss: 0.683232  [ 4200/ 5482]\n",
      "loss: 0.599733  [ 4800/ 5482]\n",
      "loss: 0.420727  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.806     0.758    99\n",
      " disgust     0.652     0.772     0.888    107\n",
      "    fear     0.652     0.631     0.662    80\n",
      "   happy     0.652     0.343     0.636    77\n",
      " neutral     0.652     0.755     0.874    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.754     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.580     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.998428 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.530180  [    0/ 5482]\n",
      "loss: 0.378716  [  600/ 5482]\n",
      "loss: 0.490949  [ 1200/ 5482]\n",
      "loss: 0.434485  [ 1800/ 5482]\n",
      "loss: 0.569790  [ 2400/ 5482]\n",
      "loss: 0.340651  [ 3000/ 5482]\n",
      "loss: 0.402213  [ 3600/ 5482]\n",
      "loss: 0.473267  [ 4200/ 5482]\n",
      "loss: 0.385794  [ 4800/ 5482]\n",
      "loss: 0.315598  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.725     0.798    99\n",
      " disgust     0.664     0.863     0.822    107\n",
      "    fear     0.664     0.632     0.750    80\n",
      "   happy     0.664     0.378     0.623    77\n",
      " neutral     0.664     0.786     0.853    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.662     0.803    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.578     0.664    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.002419 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.712275  [    0/ 5482]\n",
      "loss: 0.380353  [  600/ 5482]\n",
      "loss: 0.366181  [ 1200/ 5482]\n",
      "loss: 0.527007  [ 1800/ 5482]\n",
      "loss: 0.447267  [ 2400/ 5482]\n",
      "loss: 0.304792  [ 3000/ 5482]\n",
      "loss: 0.464633  [ 3600/ 5482]\n",
      "loss: 0.394682  [ 4200/ 5482]\n",
      "loss: 0.284470  [ 4800/ 5482]\n",
      "loss: 0.394409  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.764     0.818    99\n",
      " disgust     0.664     0.812     0.888    107\n",
      "    fear     0.664     0.687     0.713    80\n",
      "   happy     0.664     0.329     0.610    77\n",
      " neutral     0.664     0.779     0.853    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.772     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.592     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.984833 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.538738  [    0/ 5482]\n",
      "loss: 0.556630  [  600/ 5482]\n",
      "loss: 0.573285  [ 1200/ 5482]\n",
      "loss: 0.618714  [ 1800/ 5482]\n",
      "loss: 0.434036  [ 2400/ 5482]\n",
      "loss: 0.524230  [ 3000/ 5482]\n",
      "loss: 0.401062  [ 3600/ 5482]\n",
      "loss: 0.382351  [ 4200/ 5482]\n",
      "loss: 0.426137  [ 4800/ 5482]\n",
      "loss: 0.381977  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.766     0.828    99\n",
      " disgust     0.662     0.807     0.860    107\n",
      "    fear     0.662     0.653     0.775    80\n",
      "   happy     0.662     0.336     0.623    77\n",
      " neutral     0.662     0.792     0.842    95\n",
      "     sad     0.662     0.000     0.000    91\n",
      "surprise     0.662     0.800     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.593     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.983482 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.468729  [    0/ 5482]\n",
      "loss: 0.458606  [  600/ 5482]\n",
      "loss: 0.377193  [ 1200/ 5482]\n",
      "loss: 0.475191  [ 1800/ 5482]\n",
      "loss: 0.567570  [ 2400/ 5482]\n",
      "loss: 0.662577  [ 3000/ 5482]\n",
      "loss: 0.353772  [ 3600/ 5482]\n",
      "loss: 0.375044  [ 4200/ 5482]\n",
      "loss: 0.542677  [ 4800/ 5482]\n",
      "loss: 0.463109  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.738     0.798    99\n",
      " disgust     0.661     0.798     0.850    107\n",
      "    fear     0.661     0.655     0.713    80\n",
      "   happy     0.661     0.348     0.623    77\n",
      " neutral     0.661     0.796     0.863    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.754     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.584     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.989222 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.413064  [    0/ 5482]\n",
      "loss: 0.578562  [  600/ 5482]\n",
      "loss: 0.265712  [ 1200/ 5482]\n",
      "loss: 0.381367  [ 1800/ 5482]\n",
      "loss: 0.361354  [ 2400/ 5482]\n",
      "loss: 0.344583  [ 3000/ 5482]\n",
      "loss: 0.485678  [ 3600/ 5482]\n",
      "loss: 0.260445  [ 4200/ 5482]\n",
      "loss: 0.444154  [ 4800/ 5482]\n",
      "loss: 0.522601  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.755     0.808    99\n",
      " disgust     0.656     0.802     0.869    107\n",
      "    fear     0.656     0.688     0.688    80\n",
      "   happy     0.656     0.341     0.597    77\n",
      " neutral     0.656     0.735     0.874    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.717     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.577     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.006984 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.313103  [    0/ 5482]\n",
      "loss: 0.470101  [  600/ 5482]\n",
      "loss: 0.285987  [ 1200/ 5482]\n",
      "loss: 0.454457  [ 1800/ 5482]\n",
      "loss: 0.479964  [ 2400/ 5482]\n",
      "loss: 0.442924  [ 3000/ 5482]\n",
      "loss: 0.729027  [ 3600/ 5482]\n",
      "loss: 0.470692  [ 4200/ 5482]\n",
      "loss: 0.534077  [ 4800/ 5482]\n",
      "loss: 0.561781  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.692     0.838    99\n",
      " disgust     0.652     0.829     0.860    107\n",
      "    fear     0.652     0.637     0.725    80\n",
      "   happy     0.652     0.346     0.597    77\n",
      " neutral     0.652     0.765     0.821    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.774     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.577     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.990099 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.311101  [    0/ 5482]\n",
      "loss: 0.352575  [  600/ 5482]\n",
      "loss: 0.342629  [ 1200/ 5482]\n",
      "loss: 0.568779  [ 1800/ 5482]\n",
      "loss: 0.535430  [ 2400/ 5482]\n",
      "loss: 0.393642  [ 3000/ 5482]\n",
      "loss: 0.576807  [ 3600/ 5482]\n",
      "loss: 0.389574  [ 4200/ 5482]\n",
      "loss: 0.371588  [ 4800/ 5482]\n",
      "loss: 0.639331  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.840     0.798    99\n",
      " disgust     0.664     0.810     0.879    107\n",
      "    fear     0.664     0.702     0.738    80\n",
      "   happy     0.664     0.312     0.649    77\n",
      " neutral     0.664     0.771     0.853    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.824     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.609     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.973988 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.727576  [    0/ 5482]\n",
      "loss: 0.346786  [  600/ 5482]\n",
      "loss: 0.550783  [ 1200/ 5482]\n",
      "loss: 0.453358  [ 1800/ 5482]\n",
      "loss: 0.395206  [ 2400/ 5482]\n",
      "loss: 0.402455  [ 3000/ 5482]\n",
      "loss: 0.285966  [ 3600/ 5482]\n",
      "loss: 0.669528  [ 4200/ 5482]\n",
      "loss: 0.452115  [ 4800/ 5482]\n",
      "loss: 0.642893  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.745     0.798    99\n",
      " disgust     0.648     0.784     0.850    107\n",
      "    fear     0.648     0.634     0.650    80\n",
      "   happy     0.648     0.362     0.597    77\n",
      " neutral     0.648     0.701     0.863    95\n",
      "     sad     0.648     0.000     0.000    91\n",
      "surprise     0.648     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.565     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.016985 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.532931  [    0/ 5482]\n",
      "loss: 0.559898  [  600/ 5482]\n",
      "loss: 0.345031  [ 1200/ 5482]\n",
      "loss: 0.317073  [ 1800/ 5482]\n",
      "loss: 0.503222  [ 2400/ 5482]\n",
      "loss: 0.424146  [ 3000/ 5482]\n",
      "loss: 0.617341  [ 3600/ 5482]\n",
      "loss: 0.475304  [ 4200/ 5482]\n",
      "loss: 0.447119  [ 4800/ 5482]\n",
      "loss: 0.300946  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.775     0.798    99\n",
      " disgust     0.656     0.780     0.897    107\n",
      "    fear     0.656     0.646     0.662    80\n",
      "   happy     0.656     0.341     0.597    77\n",
      " neutral     0.656     0.767     0.832    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.723     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.576     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.003276 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.273575  [    0/ 5482]\n",
      "loss: 0.504626  [  600/ 5482]\n",
      "loss: 0.471403  [ 1200/ 5482]\n",
      "loss: 0.284886  [ 1800/ 5482]\n",
      "loss: 0.307443  [ 2400/ 5482]\n",
      "loss: 0.304888  [ 3000/ 5482]\n",
      "loss: 0.365924  [ 3600/ 5482]\n",
      "loss: 0.297008  [ 4200/ 5482]\n",
      "loss: 0.435969  [ 4800/ 5482]\n",
      "loss: 0.315240  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.781     0.758    99\n",
      " disgust     0.659     0.785     0.888    107\n",
      "    fear     0.659     0.640     0.713    80\n",
      "   happy     0.659     0.340     0.636    77\n",
      " neutral     0.659     0.792     0.842    95\n",
      "     sad     0.659     0.000     0.000    91\n",
      "surprise     0.659     0.780     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.588     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.993426 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.346852  [    0/ 5482]\n",
      "loss: 0.423858  [  600/ 5482]\n",
      "loss: 0.721424  [ 1200/ 5482]\n",
      "loss: 1.013582  [ 1800/ 5482]\n",
      "loss: 0.461228  [ 2400/ 5482]\n",
      "loss: 0.553987  [ 3000/ 5482]\n",
      "loss: 0.388840  [ 3600/ 5482]\n",
      "loss: 0.418153  [ 4200/ 5482]\n",
      "loss: 0.446998  [ 4800/ 5482]\n",
      "loss: 0.274974  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.790     0.798    99\n",
      " disgust     0.651     0.811     0.841    107\n",
      "    fear     0.651     0.659     0.675    80\n",
      "   happy     0.651     0.342     0.662    77\n",
      " neutral     0.651     0.755     0.842    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.694     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.579     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.996591 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.530070  [    0/ 5482]\n",
      "loss: 0.245584  [  600/ 5482]\n",
      "loss: 0.333365  [ 1200/ 5482]\n",
      "loss: 0.396511  [ 1800/ 5482]\n",
      "loss: 0.391643  [ 2400/ 5482]\n",
      "loss: 0.365231  [ 3000/ 5482]\n",
      "loss: 0.581641  [ 3600/ 5482]\n",
      "loss: 0.433249  [ 4200/ 5482]\n",
      "loss: 0.189820  [ 4800/ 5482]\n",
      "loss: 0.733035  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.745     0.768    99\n",
      " disgust     0.644     0.780     0.897    107\n",
      "    fear     0.644     0.647     0.688    80\n",
      "   happy     0.644     0.333     0.649    77\n",
      " neutral     0.644     0.784     0.800    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.755     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.578     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.020329 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.406312  [    0/ 5482]\n",
      "loss: 0.422615  [  600/ 5482]\n",
      "loss: 0.352413  [ 1200/ 5482]\n",
      "loss: 0.366089  [ 1800/ 5482]\n",
      "loss: 0.634309  [ 2400/ 5482]\n",
      "loss: 0.548929  [ 3000/ 5482]\n",
      "loss: 0.483694  [ 3600/ 5482]\n",
      "loss: 0.314294  [ 4200/ 5482]\n",
      "loss: 0.293921  [ 4800/ 5482]\n",
      "loss: 0.531341  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.765     0.758    99\n",
      " disgust     0.651     0.766     0.916    107\n",
      "    fear     0.651     0.688     0.662    80\n",
      "   happy     0.651     0.318     0.636    77\n",
      " neutral     0.651     0.811     0.811    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.776     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.589     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.021010 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.943258  [    0/ 5482]\n",
      "loss: 0.521026  [  600/ 5482]\n",
      "loss: 0.493112  [ 1200/ 5482]\n",
      "loss: 0.440371  [ 1800/ 5482]\n",
      "loss: 0.353012  [ 2400/ 5482]\n",
      "loss: 0.516065  [ 3000/ 5482]\n",
      "loss: 0.388960  [ 3600/ 5482]\n",
      "loss: 0.311020  [ 4200/ 5482]\n",
      "loss: 0.474625  [ 4800/ 5482]\n",
      "loss: 0.321442  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.703     0.788    99\n",
      " disgust     0.643     0.686     0.879    107\n",
      "    fear     0.643     0.693     0.650    80\n",
      "   happy     0.643     0.361     0.558    77\n",
      " neutral     0.643     0.778     0.811    95\n",
      "     sad     0.643     0.000     0.000    91\n",
      "surprise     0.643     0.696     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.560     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.085395 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.359066  [    0/ 5482]\n",
      "loss: 0.467936  [  600/ 5482]\n",
      "loss: 0.440599  [ 1200/ 5482]\n",
      "loss: 0.434716  [ 1800/ 5482]\n",
      "loss: 0.839954  [ 2400/ 5482]\n",
      "loss: 0.293902  [ 3000/ 5482]\n",
      "loss: 0.663606  [ 3600/ 5482]\n",
      "loss: 0.556893  [ 4200/ 5482]\n",
      "loss: 0.248285  [ 4800/ 5482]\n",
      "loss: 0.532511  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.706     0.778    99\n",
      " disgust     0.646     0.767     0.860    107\n",
      "    fear     0.646     0.662     0.637    80\n",
      "   happy     0.646     0.324     0.610    77\n",
      " neutral     0.646     0.820     0.863    95\n",
      "     sad     0.646     0.000     0.000    91\n",
      "surprise     0.646     0.763     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.577     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.036423 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.661554  [    0/ 5482]\n",
      "loss: 0.226182  [  600/ 5482]\n",
      "loss: 0.624449  [ 1200/ 5482]\n",
      "loss: 0.413889  [ 1800/ 5482]\n",
      "loss: 0.438135  [ 2400/ 5482]\n",
      "loss: 0.277107  [ 3000/ 5482]\n",
      "loss: 0.525836  [ 3600/ 5482]\n",
      "loss: 0.538668  [ 4200/ 5482]\n",
      "loss: 0.460327  [ 4800/ 5482]\n",
      "loss: 0.471794  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.738     0.768    99\n",
      " disgust     0.649     0.785     0.888    107\n",
      "    fear     0.649     0.707     0.662    80\n",
      "   happy     0.649     0.326     0.610    77\n",
      " neutral     0.649     0.755     0.842    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.738     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.578     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.028230 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.457017  [    0/ 5482]\n",
      "loss: 0.343674  [  600/ 5482]\n",
      "loss: 0.398692  [ 1200/ 5482]\n",
      "loss: 0.446369  [ 1800/ 5482]\n",
      "loss: 0.446109  [ 2400/ 5482]\n",
      "loss: 0.433260  [ 3000/ 5482]\n",
      "loss: 0.430372  [ 3600/ 5482]\n",
      "loss: 0.741475  [ 4200/ 5482]\n",
      "loss: 0.416575  [ 4800/ 5482]\n",
      "loss: 0.839791  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.616     0.706     0.778    99\n",
      " disgust     0.616     0.667     0.897    107\n",
      "    fear     0.616     0.745     0.512    80\n",
      "   happy     0.616     0.356     0.545    77\n",
      " neutral     0.616     0.750     0.789    95\n",
      "     sad     0.616     0.000     0.000    91\n",
      "surprise     0.616     0.536     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.616     0.537     0.609    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.158590 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.573561  [    0/ 5482]\n",
      "loss: 0.463852  [  600/ 5482]\n",
      "loss: 0.333221  [ 1200/ 5482]\n",
      "loss: 0.433199  [ 1800/ 5482]\n",
      "loss: 0.404238  [ 2400/ 5482]\n",
      "loss: 0.426278  [ 3000/ 5482]\n",
      "loss: 0.217342  [ 3600/ 5482]\n",
      "loss: 0.332183  [ 4200/ 5482]\n",
      "loss: 0.606480  [ 4800/ 5482]\n",
      "loss: 0.480621  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.638     0.681     0.778    99\n",
      " disgust     0.638     0.767     0.860    107\n",
      "    fear     0.638     0.636     0.700    80\n",
      "   happy     0.638     0.329     0.597    77\n",
      " neutral     0.638     0.817     0.800    95\n",
      "     sad     0.638     0.000     0.000    91\n",
      "surprise     0.638     0.750     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.638     0.569     0.632    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.031737 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.224428  [    0/ 5482]\n",
      "loss: 0.442508  [  600/ 5482]\n",
      "loss: 0.464186  [ 1200/ 5482]\n",
      "loss: 0.254219  [ 1800/ 5482]\n",
      "loss: 0.247960  [ 2400/ 5482]\n",
      "loss: 0.300361  [ 3000/ 5482]\n",
      "loss: 0.561231  [ 3600/ 5482]\n",
      "loss: 0.212833  [ 4200/ 5482]\n",
      "loss: 0.258143  [ 4800/ 5482]\n",
      "loss: 0.373313  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.708     0.808    99\n",
      " disgust     0.649     0.818     0.841    107\n",
      "    fear     0.649     0.635     0.675    80\n",
      "   happy     0.649     0.333     0.623    77\n",
      " neutral     0.649     0.806     0.832    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.750     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.579     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.047217 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.435937  [    0/ 5482]\n",
      "loss: 0.401899  [  600/ 5482]\n",
      "loss: 0.408552  [ 1200/ 5482]\n",
      "loss: 0.464579  [ 1800/ 5482]\n",
      "loss: 0.249819  [ 2400/ 5482]\n",
      "loss: 0.602624  [ 3000/ 5482]\n",
      "loss: 0.460554  [ 3600/ 5482]\n",
      "loss: 0.318868  [ 4200/ 5482]\n",
      "loss: 0.399223  [ 4800/ 5482]\n",
      "loss: 0.426238  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.720     0.778    99\n",
      " disgust     0.654     0.780     0.860    107\n",
      "    fear     0.654     0.616     0.762    80\n",
      "   happy     0.654     0.358     0.623    77\n",
      " neutral     0.654     0.811     0.811    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.772     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.579     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.017503 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.508648  [    0/ 5482]\n",
      "loss: 0.241680  [  600/ 5482]\n",
      "loss: 0.545936  [ 1200/ 5482]\n",
      "loss: 0.423924  [ 1800/ 5482]\n",
      "loss: 0.505340  [ 2400/ 5482]\n",
      "loss: 0.416634  [ 3000/ 5482]\n",
      "loss: 0.366662  [ 3600/ 5482]\n",
      "loss: 0.653206  [ 4200/ 5482]\n",
      "loss: 0.550192  [ 4800/ 5482]\n",
      "loss: 0.525172  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.731     0.768    99\n",
      " disgust     0.643     0.802     0.869    107\n",
      "    fear     0.643     0.684     0.675    80\n",
      "   happy     0.643     0.341     0.584    77\n",
      " neutral     0.643     0.693     0.832    95\n",
      "     sad     0.643     0.000     0.000    91\n",
      "surprise     0.643     0.692     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.563     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.051486 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.433007  [    0/ 5482]\n",
      "loss: 0.450136  [  600/ 5482]\n",
      "loss: 0.340465  [ 1200/ 5482]\n",
      "loss: 0.430336  [ 1800/ 5482]\n",
      "loss: 0.658682  [ 2400/ 5482]\n",
      "loss: 0.343481  [ 3000/ 5482]\n",
      "loss: 0.456825  [ 3600/ 5482]\n",
      "loss: 0.571506  [ 4200/ 5482]\n",
      "loss: 0.327035  [ 4800/ 5482]\n",
      "loss: 0.754276  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.811     0.778    99\n",
      " disgust     0.646     0.835     0.850    107\n",
      "    fear     0.646     0.628     0.738    80\n",
      "   happy     0.646     0.314     0.688    77\n",
      " neutral     0.646     0.778     0.811    95\n",
      "     sad     0.646     0.000     0.000    91\n",
      "surprise     0.646     0.841     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.601     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.029432 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.363036  [    0/ 5482]\n",
      "loss: 0.408373  [  600/ 5482]\n",
      "loss: 0.643770  [ 1200/ 5482]\n",
      "loss: 0.489556  [ 1800/ 5482]\n",
      "loss: 0.420816  [ 2400/ 5482]\n",
      "loss: 0.516117  [ 3000/ 5482]\n",
      "loss: 0.614460  [ 3600/ 5482]\n",
      "loss: 0.547414  [ 4200/ 5482]\n",
      "loss: 0.465392  [ 4800/ 5482]\n",
      "loss: 0.407551  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.822     0.747    99\n",
      " disgust     0.636     0.671     0.897    107\n",
      "    fear     0.636     0.714     0.562    80\n",
      "   happy     0.636     0.309     0.662    77\n",
      " neutral     0.636     0.820     0.863    95\n",
      "     sad     0.636     0.000     0.000    91\n",
      "surprise     0.636     0.816     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.593     0.627    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.070274 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.222864  [    0/ 5482]\n",
      "loss: 0.737095  [  600/ 5482]\n",
      "loss: 0.515338  [ 1200/ 5482]\n",
      "loss: 0.517775  [ 1800/ 5482]\n",
      "loss: 0.614420  [ 2400/ 5482]\n",
      "loss: 0.371962  [ 3000/ 5482]\n",
      "loss: 0.432207  [ 3600/ 5482]\n",
      "loss: 0.335294  [ 4200/ 5482]\n",
      "loss: 0.327463  [ 4800/ 5482]\n",
      "loss: 0.480969  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.702     0.808    99\n",
      " disgust     0.656     0.830     0.869    107\n",
      "    fear     0.656     0.659     0.725    80\n",
      "   happy     0.656     0.348     0.610    77\n",
      " neutral     0.656     0.778     0.811    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.578     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.029953 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.319871  [    0/ 5482]\n",
      "loss: 0.533587  [  600/ 5482]\n",
      "loss: 0.452838  [ 1200/ 5482]\n",
      "loss: 0.649671  [ 1800/ 5482]\n",
      "loss: 0.529841  [ 2400/ 5482]\n",
      "loss: 0.349176  [ 3000/ 5482]\n",
      "loss: 0.468193  [ 3600/ 5482]\n",
      "loss: 0.298141  [ 4200/ 5482]\n",
      "loss: 0.264611  [ 4800/ 5482]\n",
      "loss: 0.321998  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.725     0.798    99\n",
      " disgust     0.654     0.823     0.869    107\n",
      "    fear     0.654     0.638     0.750    80\n",
      "   happy     0.654     0.338     0.623    77\n",
      " neutral     0.654     0.806     0.789    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.746     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.582     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.012423 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.569538  [    0/ 5482]\n",
      "loss: 0.445086  [  600/ 5482]\n",
      "loss: 0.430516  [ 1200/ 5482]\n",
      "loss: 0.493421  [ 1800/ 5482]\n",
      "loss: 0.313555  [ 2400/ 5482]\n",
      "loss: 0.319850  [ 3000/ 5482]\n",
      "loss: 0.418987  [ 3600/ 5482]\n",
      "loss: 0.544748  [ 4200/ 5482]\n",
      "loss: 0.345653  [ 4800/ 5482]\n",
      "loss: 0.323071  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.738     0.768    99\n",
      " disgust     0.661     0.792     0.888    107\n",
      "    fear     0.661     0.700     0.700    80\n",
      "   happy     0.661     0.360     0.636    77\n",
      " neutral     0.661     0.750     0.853    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.730     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.581     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.003540 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.537608  [    0/ 5482]\n",
      "loss: 0.432407  [  600/ 5482]\n",
      "loss: 0.409182  [ 1200/ 5482]\n",
      "loss: 0.299166  [ 1800/ 5482]\n",
      "loss: 0.285453  [ 2400/ 5482]\n",
      "loss: 0.858806  [ 3000/ 5482]\n",
      "loss: 0.417773  [ 3600/ 5482]\n",
      "loss: 0.444201  [ 4200/ 5482]\n",
      "loss: 0.544947  [ 4800/ 5482]\n",
      "loss: 0.357958  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.718     0.798    99\n",
      " disgust     0.648     0.817     0.879    107\n",
      "    fear     0.648     0.689     0.637    80\n",
      "   happy     0.648     0.318     0.636    77\n",
      " neutral     0.648     0.775     0.832    95\n",
      "     sad     0.648     0.000     0.000    91\n",
      "surprise     0.648     0.782     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.586     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.034107 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.408083  [    0/ 5482]\n",
      "loss: 0.626442  [  600/ 5482]\n",
      "loss: 0.465506  [ 1200/ 5482]\n",
      "loss: 1.004613  [ 1800/ 5482]\n",
      "loss: 0.330142  [ 2400/ 5482]\n",
      "loss: 0.274660  [ 3000/ 5482]\n",
      "loss: 0.432134  [ 3600/ 5482]\n",
      "loss: 0.538014  [ 4200/ 5482]\n",
      "loss: 0.733726  [ 4800/ 5482]\n",
      "loss: 0.636257  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.633     0.669     0.798    99\n",
      " disgust     0.633     0.784     0.850    107\n",
      "    fear     0.633     0.630     0.725    80\n",
      "   happy     0.633     0.336     0.623    77\n",
      " neutral     0.633     0.826     0.747    95\n",
      "     sad     0.633     0.000     0.000    91\n",
      "surprise     0.633     0.709     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.633     0.565     0.626    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 1.034091 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.330585  [    0/ 5482]\n",
      "loss: 0.305977  [  600/ 5482]\n",
      "loss: 0.442140  [ 1200/ 5482]\n",
      "loss: 0.320995  [ 1800/ 5482]\n",
      "loss: 0.267778  [ 2400/ 5482]\n",
      "loss: 0.423742  [ 3000/ 5482]\n",
      "loss: 0.442690  [ 3600/ 5482]\n",
      "loss: 0.344259  [ 4200/ 5482]\n",
      "loss: 0.375009  [ 4800/ 5482]\n",
      "loss: 0.612282  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.667     0.750     0.818    99\n",
      " disgust     0.667     0.862     0.879    107\n",
      "    fear     0.667     0.663     0.812    80\n",
      "   happy     0.667     0.353     0.636    77\n",
      " neutral     0.667     0.811     0.811    95\n",
      "     sad     0.667     0.000     0.000    91\n",
      "surprise     0.667     0.672     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.667     0.587     0.661    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.992555 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.399775  [    0/ 5482]\n",
      "loss: 0.405338  [  600/ 5482]\n",
      "loss: 0.460695  [ 1200/ 5482]\n",
      "loss: 0.447363  [ 1800/ 5482]\n",
      "loss: 0.621968  [ 2400/ 5482]\n",
      "loss: 0.502610  [ 3000/ 5482]\n",
      "loss: 0.343857  [ 3600/ 5482]\n",
      "loss: 0.393068  [ 4200/ 5482]\n",
      "loss: 0.525071  [ 4800/ 5482]\n",
      "loss: 0.588412  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.760     0.768    99\n",
      " disgust     0.657     0.777     0.879    107\n",
      "    fear     0.657     0.704     0.713    80\n",
      "   happy     0.657     0.358     0.623    77\n",
      " neutral     0.657     0.730     0.853    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.578     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.023198 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.333295  [    0/ 5482]\n",
      "loss: 0.365690  [  600/ 5482]\n",
      "loss: 0.426273  [ 1200/ 5482]\n",
      "loss: 0.615462  [ 1800/ 5482]\n",
      "loss: 0.349015  [ 2400/ 5482]\n",
      "loss: 0.694265  [ 3000/ 5482]\n",
      "loss: 0.216541  [ 3600/ 5482]\n",
      "loss: 0.341610  [ 4200/ 5482]\n",
      "loss: 0.348531  [ 4800/ 5482]\n",
      "loss: 0.524529  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.738     0.798    99\n",
      " disgust     0.654     0.839     0.879    107\n",
      "    fear     0.654     0.692     0.675    80\n",
      "   happy     0.654     0.345     0.623    77\n",
      " neutral     0.654     0.730     0.853    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.683     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.575     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.018702 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.437858  [    0/ 5482]\n",
      "loss: 0.474082  [  600/ 5482]\n",
      "loss: 0.341773  [ 1200/ 5482]\n",
      "loss: 0.414291  [ 1800/ 5482]\n",
      "loss: 0.304211  [ 2400/ 5482]\n",
      "loss: 0.832086  [ 3000/ 5482]\n",
      "loss: 0.488543  [ 3600/ 5482]\n",
      "loss: 0.471658  [ 4200/ 5482]\n",
      "loss: 0.266272  [ 4800/ 5482]\n",
      "loss: 0.384588  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.710     0.768    99\n",
      " disgust     0.646     0.807     0.860    107\n",
      "    fear     0.646     0.671     0.662    80\n",
      "   happy     0.646     0.349     0.584    77\n",
      " neutral     0.646     0.722     0.874    95\n",
      "     sad     0.646     0.000     0.000    91\n",
      "surprise     0.646     0.682     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.563     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.061175 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.618229  [    0/ 5482]\n",
      "loss: 0.436101  [  600/ 5482]\n",
      "loss: 0.413484  [ 1200/ 5482]\n",
      "loss: 0.550997  [ 1800/ 5482]\n",
      "loss: 0.263443  [ 2400/ 5482]\n",
      "loss: 0.337712  [ 3000/ 5482]\n",
      "loss: 0.278604  [ 3600/ 5482]\n",
      "loss: 0.615933  [ 4200/ 5482]\n",
      "loss: 0.382058  [ 4800/ 5482]\n",
      "loss: 0.430062  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.752     0.768    99\n",
      " disgust     0.649     0.798     0.888    107\n",
      "    fear     0.649     0.648     0.738    80\n",
      "   happy     0.649     0.321     0.649    77\n",
      " neutral     0.649     0.817     0.800    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.800     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.591     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.022779 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.329018  [    0/ 5482]\n",
      "loss: 0.500080  [  600/ 5482]\n",
      "loss: 0.427136  [ 1200/ 5482]\n",
      "loss: 0.840551  [ 1800/ 5482]\n",
      "loss: 0.191547  [ 2400/ 5482]\n",
      "loss: 0.428108  [ 3000/ 5482]\n",
      "loss: 0.417880  [ 3600/ 5482]\n",
      "loss: 0.681027  [ 4200/ 5482]\n",
      "loss: 0.448998  [ 4800/ 5482]\n",
      "loss: 0.643946  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.760     0.768    99\n",
      " disgust     0.659     0.883     0.850    107\n",
      "    fear     0.659     0.610     0.762    80\n",
      "   happy     0.659     0.338     0.649    77\n",
      " neutral     0.659     0.786     0.853    95\n",
      "     sad     0.659     0.000     0.000    91\n",
      "surprise     0.659     0.768     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.592     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.002566 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.602232  [    0/ 5482]\n",
      "loss: 0.436634  [  600/ 5482]\n",
      "loss: 0.345208  [ 1200/ 5482]\n",
      "loss: 0.448694  [ 1800/ 5482]\n",
      "loss: 0.419969  [ 2400/ 5482]\n",
      "loss: 0.594638  [ 3000/ 5482]\n",
      "loss: 0.458353  [ 3600/ 5482]\n",
      "loss: 0.350502  [ 4200/ 5482]\n",
      "loss: 0.413797  [ 4800/ 5482]\n",
      "loss: 0.261442  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.716     0.838    99\n",
      " disgust     0.657     0.838     0.869    107\n",
      "    fear     0.657     0.659     0.700    80\n",
      "   happy     0.657     0.342     0.649    77\n",
      " neutral     0.657     0.812     0.821    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.732     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.586     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.000335 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.432769  [    0/ 5482]\n",
      "loss: 0.419836  [  600/ 5482]\n",
      "loss: 0.518411  [ 1200/ 5482]\n",
      "loss: 0.453311  [ 1800/ 5482]\n",
      "loss: 0.372652  [ 2400/ 5482]\n",
      "loss: 0.300105  [ 3000/ 5482]\n",
      "loss: 0.418016  [ 3600/ 5482]\n",
      "loss: 0.212144  [ 4200/ 5482]\n",
      "loss: 0.325571  [ 4800/ 5482]\n",
      "loss: 0.433657  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.764     0.818    99\n",
      " disgust     0.669     0.893     0.860    107\n",
      "    fear     0.669     0.670     0.738    80\n",
      "   happy     0.669     0.338     0.649    77\n",
      " neutral     0.669     0.798     0.832    95\n",
      "     sad     0.669     0.000     0.000    91\n",
      "surprise     0.669     0.712     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.597     0.667    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.006079 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.196737  [    0/ 5482]\n",
      "loss: 0.310023  [  600/ 5482]\n",
      "loss: 0.409503  [ 1200/ 5482]\n",
      "loss: 0.388990  [ 1800/ 5482]\n",
      "loss: 0.421474  [ 2400/ 5482]\n",
      "loss: 0.531366  [ 3000/ 5482]\n",
      "loss: 0.517289  [ 3600/ 5482]\n",
      "loss: 0.422510  [ 4200/ 5482]\n",
      "loss: 0.606065  [ 4800/ 5482]\n",
      "loss: 0.364951  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.750     0.788    99\n",
      " disgust     0.661     0.825     0.879    107\n",
      "    fear     0.661     0.679     0.662    80\n",
      "   happy     0.661     0.340     0.649    77\n",
      " neutral     0.661     0.769     0.874    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.763     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.589     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.006814 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.498637  [    0/ 5482]\n",
      "loss: 0.441503  [  600/ 5482]\n",
      "loss: 0.518974  [ 1200/ 5482]\n",
      "loss: 0.396063  [ 1800/ 5482]\n",
      "loss: 0.524716  [ 2400/ 5482]\n",
      "loss: 0.485431  [ 3000/ 5482]\n",
      "loss: 0.322050  [ 3600/ 5482]\n",
      "loss: 0.615182  [ 4200/ 5482]\n",
      "loss: 0.509001  [ 4800/ 5482]\n",
      "loss: 0.504582  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.782     0.798    99\n",
      " disgust     0.656     0.802     0.907    107\n",
      "    fear     0.656     0.667     0.650    80\n",
      "   happy     0.656     0.333     0.636    77\n",
      " neutral     0.656     0.771     0.853    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.724     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.583     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.010813 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.417410  [    0/ 5482]\n",
      "loss: 0.340248  [  600/ 5482]\n",
      "loss: 0.496766  [ 1200/ 5482]\n",
      "loss: 0.318133  [ 1800/ 5482]\n",
      "loss: 0.528532  [ 2400/ 5482]\n",
      "loss: 0.255299  [ 3000/ 5482]\n",
      "loss: 0.314707  [ 3600/ 5482]\n",
      "loss: 0.344863  [ 4200/ 5482]\n",
      "loss: 0.413403  [ 4800/ 5482]\n",
      "loss: 0.528212  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.698     0.818    99\n",
      " disgust     0.649     0.841     0.841    107\n",
      "    fear     0.649     0.647     0.688    80\n",
      "   happy     0.649     0.343     0.636    77\n",
      " neutral     0.649     0.790     0.832    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.712     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.576     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.018599 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.224583  [    0/ 5482]\n",
      "loss: 0.354290  [  600/ 5482]\n",
      "loss: 0.336197  [ 1200/ 5482]\n",
      "loss: 0.481906  [ 1800/ 5482]\n",
      "loss: 0.329393  [ 2400/ 5482]\n",
      "loss: 0.543786  [ 3000/ 5482]\n",
      "loss: 0.508701  [ 3600/ 5482]\n",
      "loss: 0.209192  [ 4200/ 5482]\n",
      "loss: 0.354716  [ 4800/ 5482]\n",
      "loss: 0.221798  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.757     0.788    99\n",
      " disgust     0.651     0.782     0.869    107\n",
      "    fear     0.651     0.656     0.738    80\n",
      "   happy     0.651     0.340     0.636    77\n",
      " neutral     0.651     0.770     0.811    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.759     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.581     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.011604 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.481928  [    0/ 5482]\n",
      "loss: 0.304692  [  600/ 5482]\n",
      "loss: 0.642722  [ 1200/ 5482]\n",
      "loss: 0.337306  [ 1800/ 5482]\n",
      "loss: 0.509448  [ 2400/ 5482]\n",
      "loss: 0.280945  [ 3000/ 5482]\n",
      "loss: 0.419685  [ 3600/ 5482]\n",
      "loss: 0.615261  [ 4200/ 5482]\n",
      "loss: 0.533788  [ 4800/ 5482]\n",
      "loss: 0.382504  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.808     0.808    99\n",
      " disgust     0.661     0.795     0.869    107\n",
      "    fear     0.661     0.682     0.750    80\n",
      "   happy     0.661     0.331     0.662    77\n",
      " neutral     0.661     0.760     0.832    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.833     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.601     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.992215 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.516440  [    0/ 5482]\n",
      "loss: 0.438657  [  600/ 5482]\n",
      "loss: 0.445035  [ 1200/ 5482]\n",
      "loss: 0.417293  [ 1800/ 5482]\n",
      "loss: 0.341168  [ 2400/ 5482]\n",
      "loss: 0.439686  [ 3000/ 5482]\n",
      "loss: 0.426037  [ 3600/ 5482]\n",
      "loss: 0.328364  [ 4200/ 5482]\n",
      "loss: 0.203874  [ 4800/ 5482]\n",
      "loss: 0.417292  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.702     0.808    99\n",
      " disgust     0.651     0.807     0.860    107\n",
      "    fear     0.651     0.640     0.688    80\n",
      "   happy     0.651     0.340     0.623    77\n",
      " neutral     0.651     0.798     0.832    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.768     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.579     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.009810 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.438802  [    0/ 5482]\n",
      "loss: 0.422160  [  600/ 5482]\n",
      "loss: 0.391158  [ 1200/ 5482]\n",
      "loss: 0.426643  [ 1800/ 5482]\n",
      "loss: 0.432434  [ 2400/ 5482]\n",
      "loss: 0.515404  [ 3000/ 5482]\n",
      "loss: 0.304925  [ 3600/ 5482]\n",
      "loss: 0.183078  [ 4200/ 5482]\n",
      "loss: 0.489748  [ 4800/ 5482]\n",
      "loss: 0.416072  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.711     0.818    99\n",
      " disgust     0.657     0.786     0.860    107\n",
      "    fear     0.657     0.684     0.675    80\n",
      "   happy     0.657     0.346     0.584    77\n",
      " neutral     0.657     0.788     0.863    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.712     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.575     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.021478 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.435722  [    0/ 5482]\n",
      "loss: 0.501774  [  600/ 5482]\n",
      "loss: 0.429371  [ 1200/ 5482]\n",
      "loss: 0.270161  [ 1800/ 5482]\n",
      "loss: 0.400874  [ 2400/ 5482]\n",
      "loss: 0.423973  [ 3000/ 5482]\n",
      "loss: 0.390361  [ 3600/ 5482]\n",
      "loss: 0.412007  [ 4200/ 5482]\n",
      "loss: 0.625249  [ 4800/ 5482]\n",
      "loss: 0.322301  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.748     0.808    99\n",
      " disgust     0.654     0.804     0.841    107\n",
      "    fear     0.654     0.637     0.725    80\n",
      "   happy     0.654     0.336     0.610    77\n",
      " neutral     0.654     0.794     0.811    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.746     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.581     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.021153 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.521956  [    0/ 5482]\n",
      "loss: 0.498403  [  600/ 5482]\n",
      "loss: 0.313121  [ 1200/ 5482]\n",
      "loss: 0.343805  [ 1800/ 5482]\n",
      "loss: 0.276486  [ 2400/ 5482]\n",
      "loss: 0.436291  [ 3000/ 5482]\n",
      "loss: 0.401664  [ 3600/ 5482]\n",
      "loss: 0.253716  [ 4200/ 5482]\n",
      "loss: 0.430476  [ 4800/ 5482]\n",
      "loss: 0.177756  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.745     0.768    99\n",
      " disgust     0.652     0.769     0.869    107\n",
      "    fear     0.652     0.679     0.713    80\n",
      "   happy     0.652     0.322     0.597    77\n",
      " neutral     0.652     0.779     0.853    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.804     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.585     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.021188 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.391315  [    0/ 5482]\n",
      "loss: 0.340475  [  600/ 5482]\n",
      "loss: 0.320620  [ 1200/ 5482]\n",
      "loss: 0.494490  [ 1800/ 5482]\n",
      "loss: 0.495230  [ 2400/ 5482]\n",
      "loss: 0.303830  [ 3000/ 5482]\n",
      "loss: 0.338392  [ 3600/ 5482]\n",
      "loss: 0.255501  [ 4200/ 5482]\n",
      "loss: 0.301908  [ 4800/ 5482]\n",
      "loss: 0.520617  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.758     0.758    99\n",
      " disgust     0.656     0.825     0.879    107\n",
      "    fear     0.656     0.671     0.662    80\n",
      "   happy     0.656     0.327     0.636    77\n",
      " neutral     0.656     0.775     0.905    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.754     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.587     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.022208 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.398335  [    0/ 5482]\n",
      "loss: 0.381946  [  600/ 5482]\n",
      "loss: 0.542003  [ 1200/ 5482]\n",
      "loss: 0.414951  [ 1800/ 5482]\n",
      "loss: 0.541202  [ 2400/ 5482]\n",
      "loss: 0.410521  [ 3000/ 5482]\n",
      "loss: 0.310800  [ 3600/ 5482]\n",
      "loss: 0.346999  [ 4200/ 5482]\n",
      "loss: 0.515418  [ 4800/ 5482]\n",
      "loss: 0.610875  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.731     0.768    99\n",
      " disgust     0.661     0.814     0.860    107\n",
      "    fear     0.661     0.650     0.812    80\n",
      "   happy     0.661     0.345     0.623    77\n",
      " neutral     0.661     0.794     0.853    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.788     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.589     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.989173 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.193366  [    0/ 5482]\n",
      "loss: 0.496976  [  600/ 5482]\n",
      "loss: 0.404031  [ 1200/ 5482]\n",
      "loss: 0.212608  [ 1800/ 5482]\n",
      "loss: 0.499505  [ 2400/ 5482]\n",
      "loss: 0.600838  [ 3000/ 5482]\n",
      "loss: 0.405254  [ 3600/ 5482]\n",
      "loss: 0.446313  [ 4200/ 5482]\n",
      "loss: 0.536464  [ 4800/ 5482]\n",
      "loss: 0.288628  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.743     0.788    99\n",
      " disgust     0.652     0.844     0.860    107\n",
      "    fear     0.652     0.663     0.762    80\n",
      "   happy     0.652     0.316     0.636    77\n",
      " neutral     0.652     0.806     0.832    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.765     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.591     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.026156 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.312231  [    0/ 5482]\n",
      "loss: 0.509060  [  600/ 5482]\n",
      "loss: 0.406024  [ 1200/ 5482]\n",
      "loss: 0.325785  [ 1800/ 5482]\n",
      "loss: 0.445253  [ 2400/ 5482]\n",
      "loss: 0.499934  [ 3000/ 5482]\n",
      "loss: 0.232795  [ 3600/ 5482]\n",
      "loss: 0.439425  [ 4200/ 5482]\n",
      "loss: 0.491505  [ 4800/ 5482]\n",
      "loss: 0.395418  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.628     0.704     0.768    99\n",
      " disgust     0.628     0.775     0.869    107\n",
      "    fear     0.628     0.641     0.625    80\n",
      "   happy     0.628     0.310     0.636    77\n",
      " neutral     0.628     0.789     0.789    95\n",
      "     sad     0.628     0.000     0.000    91\n",
      "surprise     0.628     0.784     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.628     0.572     0.620    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.035317 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.510152  [    0/ 5482]\n",
      "loss: 0.512556  [  600/ 5482]\n",
      "loss: 0.501493  [ 1200/ 5482]\n",
      "loss: 0.287528  [ 1800/ 5482]\n",
      "loss: 0.288705  [ 2400/ 5482]\n",
      "loss: 0.585665  [ 3000/ 5482]\n",
      "loss: 0.626176  [ 3600/ 5482]\n",
      "loss: 0.316458  [ 4200/ 5482]\n",
      "loss: 0.414998  [ 4800/ 5482]\n",
      "loss: 0.323905  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.734     0.808    99\n",
      " disgust     0.654     0.807     0.860    107\n",
      "    fear     0.654     0.636     0.700    80\n",
      "   happy     0.654     0.342     0.649    77\n",
      " neutral     0.654     0.835     0.800    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.583     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.005787 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.544755  [    0/ 5482]\n",
      "loss: 0.299153  [  600/ 5482]\n",
      "loss: 0.513424  [ 1200/ 5482]\n",
      "loss: 0.406144  [ 1800/ 5482]\n",
      "loss: 0.399997  [ 2400/ 5482]\n",
      "loss: 0.405169  [ 3000/ 5482]\n",
      "loss: 0.406585  [ 3600/ 5482]\n",
      "loss: 0.508529  [ 4200/ 5482]\n",
      "loss: 0.396251  [ 4800/ 5482]\n",
      "loss: 0.216910  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.752     0.768    99\n",
      " disgust     0.652     0.782     0.869    107\n",
      "    fear     0.652     0.640     0.713    80\n",
      "   happy     0.652     0.340     0.636    77\n",
      " neutral     0.652     0.792     0.842    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.768     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.582     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.008343 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.327468  [    0/ 5482]\n",
      "loss: 0.455416  [  600/ 5482]\n",
      "loss: 0.430046  [ 1200/ 5482]\n",
      "loss: 0.670947  [ 1800/ 5482]\n",
      "loss: 0.616429  [ 2400/ 5482]\n",
      "loss: 0.508489  [ 3000/ 5482]\n",
      "loss: 0.505744  [ 3600/ 5482]\n",
      "loss: 0.504887  [ 4200/ 5482]\n",
      "loss: 0.159779  [ 4800/ 5482]\n",
      "loss: 0.396862  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.716     0.788    99\n",
      " disgust     0.657     0.814     0.860    107\n",
      "    fear     0.657     0.644     0.725    80\n",
      "   happy     0.657     0.348     0.636    77\n",
      " neutral     0.657     0.816     0.842    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.746     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.583     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.011138 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.606197  [    0/ 5482]\n",
      "loss: 0.494642  [  600/ 5482]\n",
      "loss: 0.348143  [ 1200/ 5482]\n",
      "loss: 0.328697  [ 1800/ 5482]\n",
      "loss: 0.422114  [ 2400/ 5482]\n",
      "loss: 0.383721  [ 3000/ 5482]\n",
      "loss: 0.350566  [ 3600/ 5482]\n",
      "loss: 0.403479  [ 4200/ 5482]\n",
      "loss: 0.613524  [ 4800/ 5482]\n",
      "loss: 0.273433  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.743     0.788    99\n",
      " disgust     0.651     0.804     0.841    107\n",
      "    fear     0.651     0.633     0.713    80\n",
      "   happy     0.651     0.336     0.636    77\n",
      " neutral     0.651     0.794     0.853    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.764     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.582     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.025202 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.494517  [    0/ 5482]\n",
      "loss: 0.381346  [  600/ 5482]\n",
      "loss: 0.537782  [ 1200/ 5482]\n",
      "loss: 0.259294  [ 1800/ 5482]\n",
      "loss: 0.395671  [ 2400/ 5482]\n",
      "loss: 0.175345  [ 3000/ 5482]\n",
      "loss: 0.424708  [ 3600/ 5482]\n",
      "loss: 0.416751  [ 4200/ 5482]\n",
      "loss: 0.503087  [ 4800/ 5482]\n",
      "loss: 0.272529  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.683     0.828    99\n",
      " disgust     0.656     0.769     0.869    107\n",
      "    fear     0.656     0.667     0.650    80\n",
      "   happy     0.656     0.348     0.623    77\n",
      " neutral     0.656     0.827     0.853    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.800     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.585     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.020922 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.514311  [    0/ 5482]\n",
      "loss: 0.294157  [  600/ 5482]\n",
      "loss: 0.830352  [ 1200/ 5482]\n",
      "loss: 0.403326  [ 1800/ 5482]\n",
      "loss: 0.417753  [ 2400/ 5482]\n",
      "loss: 0.618053  [ 3000/ 5482]\n",
      "loss: 0.407031  [ 3600/ 5482]\n",
      "loss: 0.397523  [ 4200/ 5482]\n",
      "loss: 0.610799  [ 4800/ 5482]\n",
      "loss: 0.308452  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.757     0.818    99\n",
      " disgust     0.659     0.880     0.822    107\n",
      "    fear     0.659     0.652     0.750    80\n",
      "   happy     0.659     0.341     0.610    77\n",
      " neutral     0.659     0.780     0.821    95\n",
      "     sad     0.659     0.000     0.000    91\n",
      "surprise     0.659     0.658     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.581     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.046230 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.191316  [    0/ 5482]\n",
      "loss: 0.616694  [  600/ 5482]\n",
      "loss: 0.195257  [ 1200/ 5482]\n",
      "loss: 0.411083  [ 1800/ 5482]\n",
      "loss: 0.514840  [ 2400/ 5482]\n",
      "loss: 0.494009  [ 3000/ 5482]\n",
      "loss: 0.319632  [ 3600/ 5482]\n",
      "loss: 0.416428  [ 4200/ 5482]\n",
      "loss: 0.303998  [ 4800/ 5482]\n",
      "loss: 0.494530  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.776     0.768    99\n",
      " disgust     0.649     0.841     0.841    107\n",
      "    fear     0.649     0.615     0.700    80\n",
      "   happy     0.649     0.323     0.649    77\n",
      " neutral     0.649     0.782     0.832    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.776     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.588     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.028654 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.329724  [    0/ 5482]\n",
      "loss: 0.292976  [  600/ 5482]\n",
      "loss: 0.309591  [ 1200/ 5482]\n",
      "loss: 0.282959  [ 1800/ 5482]\n",
      "loss: 0.622478  [ 2400/ 5482]\n",
      "loss: 0.523728  [ 3000/ 5482]\n",
      "loss: 0.283315  [ 3600/ 5482]\n",
      "loss: 0.384701  [ 4200/ 5482]\n",
      "loss: 0.601336  [ 4800/ 5482]\n",
      "loss: 0.505301  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.721     0.808    99\n",
      " disgust     0.652     0.809     0.832    107\n",
      "    fear     0.652     0.634     0.738    80\n",
      "   happy     0.652     0.350     0.623    77\n",
      " neutral     0.652     0.811     0.768    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.710     0.803    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.577     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.051337 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.294051  [    0/ 5482]\n",
      "loss: 0.325156  [  600/ 5482]\n",
      "loss: 0.599667  [ 1200/ 5482]\n",
      "loss: 0.309341  [ 1800/ 5482]\n",
      "loss: 0.360582  [ 2400/ 5482]\n",
      "loss: 0.398662  [ 3000/ 5482]\n",
      "loss: 0.511405  [ 3600/ 5482]\n",
      "loss: 0.160335  [ 4200/ 5482]\n",
      "loss: 0.297356  [ 4800/ 5482]\n",
      "loss: 0.180098  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.656     0.828    99\n",
      " disgust     0.649     0.789     0.841    107\n",
      "    fear     0.649     0.699     0.637    80\n",
      "   happy     0.649     0.375     0.623    77\n",
      " neutral     0.649     0.782     0.832    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.667     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.567     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.060987 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.256732  [    0/ 5482]\n",
      "loss: 0.619432  [  600/ 5482]\n",
      "loss: 0.603863  [ 1200/ 5482]\n",
      "loss: 0.381648  [ 1800/ 5482]\n",
      "loss: 0.494416  [ 2400/ 5482]\n",
      "loss: 0.286960  [ 3000/ 5482]\n",
      "loss: 0.316691  [ 3600/ 5482]\n",
      "loss: 0.180532  [ 4200/ 5482]\n",
      "loss: 0.425842  [ 4800/ 5482]\n",
      "loss: 0.330356  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.743     0.788    99\n",
      " disgust     0.652     0.824     0.832    107\n",
      "    fear     0.652     0.626     0.713    80\n",
      "   happy     0.652     0.338     0.636    77\n",
      " neutral     0.652     0.786     0.853    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.759     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.582     0.649    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.006910 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.513258  [    0/ 5482]\n",
      "loss: 0.585666  [  600/ 5482]\n",
      "loss: 0.490687  [ 1200/ 5482]\n",
      "loss: 0.505829  [ 1800/ 5482]\n",
      "loss: 0.209907  [ 2400/ 5482]\n",
      "loss: 0.492668  [ 3000/ 5482]\n",
      "loss: 0.531805  [ 3600/ 5482]\n",
      "loss: 0.340074  [ 4200/ 5482]\n",
      "loss: 0.482280  [ 4800/ 5482]\n",
      "loss: 0.312021  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.738     0.798    99\n",
      " disgust     0.656     0.864     0.832    107\n",
      "    fear     0.656     0.667     0.700    80\n",
      "   happy     0.656     0.345     0.649    77\n",
      " neutral     0.656     0.759     0.863    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.698     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.582     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.023778 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.368420  [    0/ 5482]\n",
      "loss: 0.416229  [  600/ 5482]\n",
      "loss: 0.498880  [ 1200/ 5482]\n",
      "loss: 0.283997  [ 1800/ 5482]\n",
      "loss: 0.313801  [ 2400/ 5482]\n",
      "loss: 0.404224  [ 3000/ 5482]\n",
      "loss: 0.296309  [ 3600/ 5482]\n",
      "loss: 0.400518  [ 4200/ 5482]\n",
      "loss: 0.598719  [ 4800/ 5482]\n",
      "loss: 0.514174  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.733     0.778    99\n",
      " disgust     0.656     0.800     0.860    107\n",
      "    fear     0.656     0.663     0.713    80\n",
      "   happy     0.656     0.350     0.649    77\n",
      " neutral     0.656     0.769     0.842    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.772     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.584     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.025767 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.208554  [    0/ 5482]\n",
      "loss: 0.414163  [  600/ 5482]\n",
      "loss: 0.582775  [ 1200/ 5482]\n",
      "loss: 0.415049  [ 1800/ 5482]\n",
      "loss: 0.505697  [ 2400/ 5482]\n",
      "loss: 0.275546  [ 3000/ 5482]\n",
      "loss: 0.409757  [ 3600/ 5482]\n",
      "loss: 0.286916  [ 4200/ 5482]\n",
      "loss: 0.398440  [ 4800/ 5482]\n",
      "loss: 0.409327  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.780     0.788    99\n",
      " disgust     0.656     0.861     0.813    107\n",
      "    fear     0.656     0.645     0.750    80\n",
      "   happy     0.656     0.327     0.649    77\n",
      " neutral     0.656     0.792     0.842    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.726     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.590     0.654    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.041589 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.372208  [    0/ 5482]\n",
      "loss: 0.615268  [  600/ 5482]\n",
      "loss: 0.215455  [ 1200/ 5482]\n",
      "loss: 0.412771  [ 1800/ 5482]\n",
      "loss: 0.507049  [ 2400/ 5482]\n",
      "loss: 0.354416  [ 3000/ 5482]\n",
      "loss: 0.315145  [ 3600/ 5482]\n",
      "loss: 0.189021  [ 4200/ 5482]\n",
      "loss: 0.419724  [ 4800/ 5482]\n",
      "loss: 0.395721  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.730     0.818    99\n",
      " disgust     0.656     0.859     0.794    107\n",
      "    fear     0.656     0.626     0.775    80\n",
      "   happy     0.656     0.353     0.636    77\n",
      " neutral     0.656     0.784     0.800    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.723     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.582     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.034609 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.301243  [    0/ 5482]\n",
      "loss: 0.427878  [  600/ 5482]\n",
      "loss: 0.296118  [ 1200/ 5482]\n",
      "loss: 0.502447  [ 1800/ 5482]\n",
      "loss: 0.370581  [ 2400/ 5482]\n",
      "loss: 0.523187  [ 3000/ 5482]\n",
      "loss: 0.316247  [ 3600/ 5482]\n",
      "loss: 0.500801  [ 4200/ 5482]\n",
      "loss: 0.424962  [ 4800/ 5482]\n",
      "loss: 0.469471  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.639     0.752     0.768    99\n",
      " disgust     0.639     0.777     0.879    107\n",
      "    fear     0.639     0.659     0.675    80\n",
      "   happy     0.639     0.312     0.636    77\n",
      " neutral     0.639     0.776     0.800    95\n",
      "     sad     0.639     0.000     0.000    91\n",
      "surprise     0.639     0.804     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.639     0.583     0.633    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 1.043847 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.289698  [    0/ 5482]\n",
      "loss: 0.387847  [  600/ 5482]\n",
      "loss: 0.281893  [ 1200/ 5482]\n",
      "loss: 0.271840  [ 1800/ 5482]\n",
      "loss: 0.290607  [ 2400/ 5482]\n",
      "loss: 0.688138  [ 3000/ 5482]\n",
      "loss: 0.307670  [ 3600/ 5482]\n",
      "loss: 1.213371  [ 4200/ 5482]\n",
      "loss: 0.427742  [ 4800/ 5482]\n",
      "loss: 0.505454  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.750     0.758    99\n",
      " disgust     0.654     0.824     0.832    107\n",
      "    fear     0.654     0.625     0.750    80\n",
      "   happy     0.654     0.336     0.662    77\n",
      " neutral     0.654     0.823     0.832    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.776     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.590     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.027513 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.308567  [    0/ 5482]\n",
      "loss: 0.292090  [  600/ 5482]\n",
      "loss: 0.192818  [ 1200/ 5482]\n",
      "loss: 0.288296  [ 1800/ 5482]\n",
      "loss: 0.603917  [ 2400/ 5482]\n",
      "loss: 0.482468  [ 3000/ 5482]\n",
      "loss: 0.395341  [ 3600/ 5482]\n",
      "loss: 0.277948  [ 4200/ 5482]\n",
      "loss: 0.390660  [ 4800/ 5482]\n",
      "loss: 0.392992  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.713     0.778    99\n",
      " disgust     0.649     0.805     0.850    107\n",
      "    fear     0.649     0.651     0.675    80\n",
      "   happy     0.649     0.343     0.636    77\n",
      " neutral     0.649     0.800     0.842    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.575     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.043637 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.317786  [    0/ 5482]\n",
      "loss: 0.393769  [  600/ 5482]\n",
      "loss: 0.371236  [ 1200/ 5482]\n",
      "loss: 0.401604  [ 1800/ 5482]\n",
      "loss: 0.625317  [ 2400/ 5482]\n",
      "loss: 0.392746  [ 3000/ 5482]\n",
      "loss: 0.492259  [ 3600/ 5482]\n",
      "loss: 0.386662  [ 4200/ 5482]\n",
      "loss: 0.197905  [ 4800/ 5482]\n",
      "loss: 0.294599  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.738     0.768    99\n",
      " disgust     0.656     0.803     0.879    107\n",
      "    fear     0.656     0.663     0.688    80\n",
      "   happy     0.656     0.350     0.649    77\n",
      " neutral     0.656     0.769     0.842    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.750     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.582     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.003045 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.263094  [    0/ 5482]\n",
      "loss: 0.409071  [  600/ 5482]\n",
      "loss: 0.368114  [ 1200/ 5482]\n",
      "loss: 0.374305  [ 1800/ 5482]\n",
      "loss: 0.303963  [ 2400/ 5482]\n",
      "loss: 0.671971  [ 3000/ 5482]\n",
      "loss: 0.502803  [ 3600/ 5482]\n",
      "loss: 0.512200  [ 4200/ 5482]\n",
      "loss: 0.703931  [ 4800/ 5482]\n",
      "loss: 0.381775  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.731     0.798    99\n",
      " disgust     0.661     0.785     0.888    107\n",
      "    fear     0.661     0.691     0.700    80\n",
      "   happy     0.661     0.348     0.597    77\n",
      " neutral     0.661     0.781     0.863    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.579     0.655    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.033621 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.283347  [    0/ 5482]\n",
      "loss: 0.395052  [  600/ 5482]\n",
      "loss: 0.204493  [ 1200/ 5482]\n",
      "loss: 0.307748  [ 1800/ 5482]\n",
      "loss: 0.403860  [ 2400/ 5482]\n",
      "loss: 0.491959  [ 3000/ 5482]\n",
      "loss: 0.410513  [ 3600/ 5482]\n",
      "loss: 0.310225  [ 4200/ 5482]\n",
      "loss: 0.182345  [ 4800/ 5482]\n",
      "loss: 0.496093  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.752     0.798    99\n",
      " disgust     0.664     0.816     0.869    107\n",
      "    fear     0.664     0.671     0.688    80\n",
      "   happy     0.664     0.338     0.623    77\n",
      " neutral     0.664     0.792     0.884    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.754     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.589     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.997938 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.276758  [    0/ 5482]\n",
      "loss: 0.513382  [  600/ 5482]\n",
      "loss: 0.295462  [ 1200/ 5482]\n",
      "loss: 0.315362  [ 1800/ 5482]\n",
      "loss: 0.504659  [ 2400/ 5482]\n",
      "loss: 0.474284  [ 3000/ 5482]\n",
      "loss: 0.706338  [ 3600/ 5482]\n",
      "loss: 0.310221  [ 4200/ 5482]\n",
      "loss: 0.593242  [ 4800/ 5482]\n",
      "loss: 0.212873  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.734     0.808    99\n",
      " disgust     0.664     0.797     0.879    107\n",
      "    fear     0.664     0.670     0.738    80\n",
      "   happy     0.664     0.343     0.610    77\n",
      " neutral     0.664     0.796     0.863    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.782     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.589     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.001182 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.299474  [    0/ 5482]\n",
      "loss: 0.381900  [  600/ 5482]\n",
      "loss: 0.705781  [ 1200/ 5482]\n",
      "loss: 0.392188  [ 1800/ 5482]\n",
      "loss: 0.277803  [ 2400/ 5482]\n",
      "loss: 0.501663  [ 3000/ 5482]\n",
      "loss: 0.415635  [ 3600/ 5482]\n",
      "loss: 0.284672  [ 4200/ 5482]\n",
      "loss: 0.179745  [ 4800/ 5482]\n",
      "loss: 0.424746  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.680     0.838    99\n",
      " disgust     0.649     0.789     0.804    107\n",
      "    fear     0.649     0.716     0.662    80\n",
      "   happy     0.649     0.358     0.623    77\n",
      " neutral     0.649     0.767     0.832    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.691     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.572     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.059689 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.301341  [    0/ 5482]\n",
      "loss: 0.578306  [  600/ 5482]\n",
      "loss: 0.607474  [ 1200/ 5482]\n",
      "loss: 0.405522  [ 1800/ 5482]\n",
      "loss: 0.894978  [ 2400/ 5482]\n",
      "loss: 0.393721  [ 3000/ 5482]\n",
      "loss: 0.481988  [ 3600/ 5482]\n",
      "loss: 0.365997  [ 4200/ 5482]\n",
      "loss: 0.510927  [ 4800/ 5482]\n",
      "loss: 0.476121  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.656     0.828    99\n",
      " disgust     0.648     0.752     0.850    107\n",
      "    fear     0.648     0.697     0.662    80\n",
      "   happy     0.648     0.336     0.610    77\n",
      " neutral     0.648     0.844     0.800    95\n",
      "     sad     0.648     0.000     0.000    91\n",
      "surprise     0.648     0.793     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.583     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.037084 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.461906  [    0/ 5482]\n",
      "loss: 0.400303  [  600/ 5482]\n",
      "loss: 0.296550  [ 1200/ 5482]\n",
      "loss: 0.584439  [ 1800/ 5482]\n",
      "loss: 0.518510  [ 2400/ 5482]\n",
      "loss: 0.388480  [ 3000/ 5482]\n",
      "loss: 0.610423  [ 3600/ 5482]\n",
      "loss: 0.272405  [ 4200/ 5482]\n",
      "loss: 0.390028  [ 4800/ 5482]\n",
      "loss: 0.389467  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.755     0.778    99\n",
      " disgust     0.651     0.829     0.860    107\n",
      "    fear     0.651     0.730     0.675    80\n",
      "   happy     0.651     0.356     0.623    77\n",
      " neutral     0.651     0.727     0.842    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.590     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.569     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.064467 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.478215  [    0/ 5482]\n",
      "loss: 0.289982  [  600/ 5482]\n",
      "loss: 0.488335  [ 1200/ 5482]\n",
      "loss: 0.204897  [ 1800/ 5482]\n",
      "loss: 0.292933  [ 2400/ 5482]\n",
      "loss: 0.592441  [ 3000/ 5482]\n",
      "loss: 0.401000  [ 3600/ 5482]\n",
      "loss: 0.377047  [ 4200/ 5482]\n",
      "loss: 0.472753  [ 4800/ 5482]\n",
      "loss: 0.304099  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.641     0.745     0.768    99\n",
      " disgust     0.641     0.814     0.860    107\n",
      "    fear     0.641     0.654     0.662    80\n",
      "   happy     0.641     0.323     0.649    77\n",
      " neutral     0.641     0.760     0.800    95\n",
      "     sad     0.641     0.000     0.000    91\n",
      "surprise     0.641     0.746     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.641     0.577     0.637    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.034190 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.265828  [    0/ 5482]\n",
      "loss: 0.363145  [  600/ 5482]\n",
      "loss: 0.377726  [ 1200/ 5482]\n",
      "loss: 0.303643  [ 1800/ 5482]\n",
      "loss: 0.461811  [ 2400/ 5482]\n",
      "loss: 0.398612  [ 3000/ 5482]\n",
      "loss: 0.599710  [ 3600/ 5482]\n",
      "loss: 0.407715  [ 4200/ 5482]\n",
      "loss: 0.490729  [ 4800/ 5482]\n",
      "loss: 0.411150  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.772     0.788    99\n",
      " disgust     0.662     0.826     0.841    107\n",
      "    fear     0.662     0.702     0.738    80\n",
      "   happy     0.662     0.336     0.649    77\n",
      " neutral     0.662     0.757     0.884    95\n",
      "     sad     0.662     0.000     0.000    91\n",
      "surprise     0.662     0.768     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.594     0.658    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.020738 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.309575  [    0/ 5482]\n",
      "loss: 0.504461  [  600/ 5482]\n",
      "loss: 0.516799  [ 1200/ 5482]\n",
      "loss: 0.488230  [ 1800/ 5482]\n",
      "loss: 0.393023  [ 2400/ 5482]\n",
      "loss: 0.274261  [ 3000/ 5482]\n",
      "loss: 0.196855  [ 3600/ 5482]\n",
      "loss: 0.496204  [ 4200/ 5482]\n",
      "loss: 0.369508  [ 4800/ 5482]\n",
      "loss: 0.494712  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.740     0.778    99\n",
      " disgust     0.652     0.786     0.860    107\n",
      "    fear     0.652     0.699     0.725    80\n",
      "   happy     0.652     0.322     0.636    77\n",
      " neutral     0.652     0.802     0.811    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.776     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.589     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.029830 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.311297  [    0/ 5482]\n",
      "loss: 0.318158  [  600/ 5482]\n",
      "loss: 0.270922  [ 1200/ 5482]\n",
      "loss: 0.609222  [ 1800/ 5482]\n",
      "loss: 0.475168  [ 2400/ 5482]\n",
      "loss: 0.294030  [ 3000/ 5482]\n",
      "loss: 0.307122  [ 3600/ 5482]\n",
      "loss: 0.493009  [ 4200/ 5482]\n",
      "loss: 0.301566  [ 4800/ 5482]\n",
      "loss: 0.280381  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.695     0.828    99\n",
      " disgust     0.648     0.807     0.860    107\n",
      "    fear     0.648     0.699     0.725    80\n",
      "   happy     0.648     0.326     0.610    77\n",
      " neutral     0.648     0.798     0.789    95\n",
      "     sad     0.648     0.000     0.000    91\n",
      "surprise     0.648     0.719     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.578     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.054474 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.299330  [    0/ 5482]\n",
      "loss: 0.475577  [  600/ 5482]\n",
      "loss: 0.271654  [ 1200/ 5482]\n",
      "loss: 0.166999  [ 1800/ 5482]\n",
      "loss: 0.400644  [ 2400/ 5482]\n",
      "loss: 0.490343  [ 3000/ 5482]\n",
      "loss: 0.398982  [ 3600/ 5482]\n",
      "loss: 0.168920  [ 4200/ 5482]\n",
      "loss: 0.683294  [ 4800/ 5482]\n",
      "loss: 0.491960  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.654     0.859    99\n",
      " disgust     0.652     0.859     0.794    107\n",
      "    fear     0.652     0.624     0.787    80\n",
      "   happy     0.652     0.343     0.597    77\n",
      " neutral     0.652     0.851     0.779    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.763     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.585     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.039056 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.393817  [    0/ 5482]\n",
      "loss: 0.166718  [  600/ 5482]\n",
      "loss: 0.627837  [ 1200/ 5482]\n",
      "loss: 0.291495  [ 1800/ 5482]\n",
      "loss: 0.393466  [ 2400/ 5482]\n",
      "loss: 0.374791  [ 3000/ 5482]\n",
      "loss: 0.187653  [ 3600/ 5482]\n",
      "loss: 0.390186  [ 4200/ 5482]\n",
      "loss: 0.304251  [ 4800/ 5482]\n",
      "loss: 0.295287  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.755     0.778    99\n",
      " disgust     0.651     0.795     0.832    107\n",
      "    fear     0.651     0.682     0.725    80\n",
      "   happy     0.651     0.338     0.636    77\n",
      " neutral     0.651     0.771     0.884    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.702     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.577     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.035323 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.234058  [    0/ 5482]\n",
      "loss: 0.458877  [  600/ 5482]\n",
      "loss: 0.292483  [ 1200/ 5482]\n",
      "loss: 0.386313  [ 1800/ 5482]\n",
      "loss: 0.219310  [ 2400/ 5482]\n",
      "loss: 0.466618  [ 3000/ 5482]\n",
      "loss: 0.388616  [ 3600/ 5482]\n",
      "loss: 0.204902  [ 4200/ 5482]\n",
      "loss: 0.363888  [ 4800/ 5482]\n",
      "loss: 0.381137  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.708     0.808    99\n",
      " disgust     0.652     0.869     0.804    107\n",
      "    fear     0.652     0.673     0.825    80\n",
      "   happy     0.652     0.320     0.636    77\n",
      " neutral     0.652     0.806     0.832    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.776     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.593     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.028905 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.487794  [    0/ 5482]\n",
      "loss: 0.392256  [  600/ 5482]\n",
      "loss: 0.508083  [ 1200/ 5482]\n",
      "loss: 0.376997  [ 1800/ 5482]\n",
      "loss: 0.597846  [ 2400/ 5482]\n",
      "loss: 0.575840  [ 3000/ 5482]\n",
      "loss: 0.148992  [ 3600/ 5482]\n",
      "loss: 0.460684  [ 4200/ 5482]\n",
      "loss: 0.373434  [ 4800/ 5482]\n",
      "loss: 0.408338  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.739     0.828    99\n",
      " disgust     0.652     0.796     0.841    107\n",
      "    fear     0.652     0.659     0.675    80\n",
      "   happy     0.652     0.340     0.636    77\n",
      " neutral     0.652     0.774     0.863    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.759     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.581     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.014650 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.604624  [    0/ 5482]\n",
      "loss: 0.476112  [  600/ 5482]\n",
      "loss: 0.383428  [ 1200/ 5482]\n",
      "loss: 0.259753  [ 1800/ 5482]\n",
      "loss: 0.290965  [ 2400/ 5482]\n",
      "loss: 0.156815  [ 3000/ 5482]\n",
      "loss: 0.394940  [ 3600/ 5482]\n",
      "loss: 0.363462  [ 4200/ 5482]\n",
      "loss: 0.523195  [ 4800/ 5482]\n",
      "loss: 0.395348  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.698     0.818    99\n",
      " disgust     0.652     0.786     0.860    107\n",
      "    fear     0.652     0.722     0.650    80\n",
      "   happy     0.652     0.343     0.636    77\n",
      " neutral     0.652     0.775     0.832    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.750     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.582     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.017756 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.305494  [    0/ 5482]\n",
      "loss: 0.482256  [  600/ 5482]\n",
      "loss: 0.192179  [ 1200/ 5482]\n",
      "loss: 0.294302  [ 1800/ 5482]\n",
      "loss: 0.396958  [ 2400/ 5482]\n",
      "loss: 0.978424  [ 3000/ 5482]\n",
      "loss: 0.380439  [ 3600/ 5482]\n",
      "loss: 0.481746  [ 4200/ 5482]\n",
      "loss: 0.184975  [ 4800/ 5482]\n",
      "loss: 0.167479  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.778     0.778    99\n",
      " disgust     0.657     0.823     0.869    107\n",
      "    fear     0.657     0.654     0.662    80\n",
      "   happy     0.657     0.363     0.636    77\n",
      " neutral     0.657     0.743     0.884    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.652     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.573     0.653    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.024442 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.380939  [    0/ 5482]\n",
      "loss: 0.313974  [  600/ 5482]\n",
      "loss: 0.484693  [ 1200/ 5482]\n",
      "loss: 0.399652  [ 1800/ 5482]\n",
      "loss: 0.262471  [ 2400/ 5482]\n",
      "loss: 0.284863  [ 3000/ 5482]\n",
      "loss: 0.188191  [ 3600/ 5482]\n",
      "loss: 0.196418  [ 4200/ 5482]\n",
      "loss: 0.489735  [ 4800/ 5482]\n",
      "loss: 0.373524  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.736     0.788    99\n",
      " disgust     0.652     0.783     0.879    107\n",
      "    fear     0.652     0.675     0.650    80\n",
      "   happy     0.652     0.340     0.636    77\n",
      " neutral     0.652     0.766     0.863    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.768     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.581     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.012313 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.183568  [    0/ 5482]\n",
      "loss: 0.400022  [  600/ 5482]\n",
      "loss: 0.178719  [ 1200/ 5482]\n",
      "loss: 0.590651  [ 1800/ 5482]\n",
      "loss: 0.285784  [ 2400/ 5482]\n",
      "loss: 0.364286  [ 3000/ 5482]\n",
      "loss: 0.485158  [ 3600/ 5482]\n",
      "loss: 0.406228  [ 4200/ 5482]\n",
      "loss: 0.569438  [ 4800/ 5482]\n",
      "loss: 0.379580  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.760     0.768    99\n",
      " disgust     0.644     0.887     0.804    107\n",
      "    fear     0.644     0.615     0.800    80\n",
      "   happy     0.644     0.325     0.649    77\n",
      " neutral     0.644     0.802     0.811    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.678     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.581     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.042419 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.381488  [    0/ 5482]\n",
      "loss: 0.474980  [  600/ 5482]\n",
      "loss: 0.275230  [ 1200/ 5482]\n",
      "loss: 0.367686  [ 1800/ 5482]\n",
      "loss: 0.291947  [ 2400/ 5482]\n",
      "loss: 0.393403  [ 3000/ 5482]\n",
      "loss: 0.164861  [ 3600/ 5482]\n",
      "loss: 0.383034  [ 4200/ 5482]\n",
      "loss: 0.499329  [ 4800/ 5482]\n",
      "loss: 0.180294  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.748     0.778    99\n",
      " disgust     0.651     0.882     0.841    107\n",
      "    fear     0.651     0.620     0.775    80\n",
      "   happy     0.651     0.315     0.675    77\n",
      " neutral     0.651     0.833     0.842    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.818     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.602     0.643    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.024134 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.351261  [    0/ 5482]\n",
      "loss: 0.277268  [  600/ 5482]\n",
      "loss: 0.396088  [ 1200/ 5482]\n",
      "loss: 0.260316  [ 1800/ 5482]\n",
      "loss: 0.392161  [ 2400/ 5482]\n",
      "loss: 0.373423  [ 3000/ 5482]\n",
      "loss: 0.585731  [ 3600/ 5482]\n",
      "loss: 0.402039  [ 4200/ 5482]\n",
      "loss: 0.684818  [ 4800/ 5482]\n",
      "loss: 0.469817  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.664     0.748     0.808    99\n",
      " disgust     0.664     0.867     0.850    107\n",
      "    fear     0.664     0.638     0.750    80\n",
      "   happy     0.664     0.350     0.649    77\n",
      " neutral     0.664     0.764     0.853    95\n",
      "     sad     0.664     0.000     0.000    91\n",
      "surprise     0.664     0.782     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.664     0.593     0.659    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.000795 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.370196  [    0/ 5482]\n",
      "loss: 0.612376  [  600/ 5482]\n",
      "loss: 0.289421  [ 1200/ 5482]\n",
      "loss: 0.363878  [ 1800/ 5482]\n",
      "loss: 0.295426  [ 2400/ 5482]\n",
      "loss: 0.386144  [ 3000/ 5482]\n",
      "loss: 0.468834  [ 3600/ 5482]\n",
      "loss: 0.148877  [ 4200/ 5482]\n",
      "loss: 0.475777  [ 4800/ 5482]\n",
      "loss: 0.287896  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.716     0.788    99\n",
      " disgust     0.644     0.791     0.850    107\n",
      "    fear     0.644     0.630     0.637    80\n",
      "   happy     0.644     0.333     0.649    77\n",
      " neutral     0.644     0.800     0.884    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.780     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.579     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.021868 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.270564  [    0/ 5482]\n",
      "loss: 0.370907  [  600/ 5482]\n",
      "loss: 0.274785  [ 1200/ 5482]\n",
      "loss: 0.482701  [ 1800/ 5482]\n",
      "loss: 0.270717  [ 2400/ 5482]\n",
      "loss: 0.387966  [ 3000/ 5482]\n",
      "loss: 0.465385  [ 3600/ 5482]\n",
      "loss: 0.364880  [ 4200/ 5482]\n",
      "loss: 0.156984  [ 4800/ 5482]\n",
      "loss: 0.389156  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.666     0.743     0.818    99\n",
      " disgust     0.666     0.858     0.850    107\n",
      "    fear     0.666     0.640     0.713    80\n",
      "   happy     0.666     0.336     0.649    77\n",
      " neutral     0.666     0.820     0.863    95\n",
      "     sad     0.666     0.000     0.000    91\n",
      "surprise     0.666     0.789     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.666     0.598     0.662    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.998620 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.481815  [    0/ 5482]\n",
      "loss: 0.369564  [  600/ 5482]\n",
      "loss: 0.276564  [ 1200/ 5482]\n",
      "loss: 0.186681  [ 1800/ 5482]\n",
      "loss: 0.585176  [ 2400/ 5482]\n",
      "loss: 0.275816  [ 3000/ 5482]\n",
      "loss: 0.569186  [ 3600/ 5482]\n",
      "loss: 0.204030  [ 4200/ 5482]\n",
      "loss: 0.413092  [ 4800/ 5482]\n",
      "loss: 0.478946  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.699     0.798    99\n",
      " disgust     0.644     0.832     0.832    107\n",
      "    fear     0.644     0.651     0.675    80\n",
      "   happy     0.644     0.345     0.649    77\n",
      " neutral     0.644     0.769     0.842    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.707     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.572     0.638    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.047752 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.381005  [    0/ 5482]\n",
      "loss: 0.261186  [  600/ 5482]\n",
      "loss: 0.493576  [ 1200/ 5482]\n",
      "loss: 0.584767  [ 1800/ 5482]\n",
      "loss: 0.185026  [ 2400/ 5482]\n",
      "loss: 0.523069  [ 3000/ 5482]\n",
      "loss: 0.168515  [ 3600/ 5482]\n",
      "loss: 1.013981  [ 4200/ 5482]\n",
      "loss: 0.566957  [ 4800/ 5482]\n",
      "loss: 0.272106  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.748     0.778    99\n",
      " disgust     0.651     0.811     0.841    107\n",
      "    fear     0.651     0.643     0.675    80\n",
      "   happy     0.651     0.352     0.649    77\n",
      " neutral     0.651     0.743     0.853    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.738     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.576     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.028944 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.424214  [    0/ 5482]\n",
      "loss: 0.163628  [  600/ 5482]\n",
      "loss: 0.392177  [ 1200/ 5482]\n",
      "loss: 0.373554  [ 1800/ 5482]\n",
      "loss: 0.358398  [ 2400/ 5482]\n",
      "loss: 0.314292  [ 3000/ 5482]\n",
      "loss: 0.253629  [ 3600/ 5482]\n",
      "loss: 0.347223  [ 4200/ 5482]\n",
      "loss: 0.499197  [ 4800/ 5482]\n",
      "loss: 0.686507  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.726     0.828    99\n",
      " disgust     0.651     0.796     0.841    107\n",
      "    fear     0.651     0.671     0.613    80\n",
      "   happy     0.651     0.354     0.597    77\n",
      " neutral     0.651     0.776     0.874    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.635     0.770    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.565     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.083663 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.277293  [    0/ 5482]\n",
      "loss: 0.572528  [  600/ 5482]\n",
      "loss: 0.376658  [ 1200/ 5482]\n",
      "loss: 0.280611  [ 1800/ 5482]\n",
      "loss: 0.147721  [ 2400/ 5482]\n",
      "loss: 0.378791  [ 3000/ 5482]\n",
      "loss: 0.267664  [ 3600/ 5482]\n",
      "loss: 0.159673  [ 4200/ 5482]\n",
      "loss: 0.287284  [ 4800/ 5482]\n",
      "loss: 0.472958  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.656     0.714     0.808    99\n",
      " disgust     0.656     0.776     0.841    107\n",
      "    fear     0.656     0.656     0.738    80\n",
      "   happy     0.656     0.358     0.623    77\n",
      " neutral     0.656     0.800     0.842    95\n",
      "     sad     0.656     0.000     0.000    91\n",
      "surprise     0.656     0.741     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.656     0.578     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.007091 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.278472  [    0/ 5482]\n",
      "loss: 0.271553  [  600/ 5482]\n",
      "loss: 0.388843  [ 1200/ 5482]\n",
      "loss: 0.492715  [ 1800/ 5482]\n",
      "loss: 0.249788  [ 2400/ 5482]\n",
      "loss: 0.261698  [ 3000/ 5482]\n",
      "loss: 0.248789  [ 3600/ 5482]\n",
      "loss: 0.470124  [ 4200/ 5482]\n",
      "loss: 0.255798  [ 4800/ 5482]\n",
      "loss: 0.399996  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.643     0.748     0.778    99\n",
      " disgust     0.643     0.807     0.860    107\n",
      "    fear     0.643     0.720     0.675    80\n",
      "   happy     0.643     0.318     0.636    77\n",
      " neutral     0.643     0.712     0.832    95\n",
      "     sad     0.643     0.000     0.000    91\n",
      "surprise     0.643     0.774     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.643     0.583     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.045357 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.367496  [    0/ 5482]\n",
      "loss: 0.556687  [  600/ 5482]\n",
      "loss: 0.494657  [ 1200/ 5482]\n",
      "loss: 0.392899  [ 1800/ 5482]\n",
      "loss: 0.187503  [ 2400/ 5482]\n",
      "loss: 0.262673  [ 3000/ 5482]\n",
      "loss: 0.505091  [ 3600/ 5482]\n",
      "loss: 0.286976  [ 4200/ 5482]\n",
      "loss: 0.375576  [ 4800/ 5482]\n",
      "loss: 0.374770  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.636     0.704     0.818    99\n",
      " disgust     0.636     0.765     0.850    107\n",
      "    fear     0.636     0.641     0.625    80\n",
      "   happy     0.636     0.329     0.610    77\n",
      " neutral     0.636     0.776     0.800    95\n",
      "     sad     0.636     0.000     0.000    91\n",
      "surprise     0.636     0.754     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.636     0.567     0.630    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.060439 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.280755  [    0/ 5482]\n",
      "loss: 0.261863  [  600/ 5482]\n",
      "loss: 0.486053  [ 1200/ 5482]\n",
      "loss: 0.485259  [ 1800/ 5482]\n",
      "loss: 0.389820  [ 2400/ 5482]\n",
      "loss: 0.255397  [ 3000/ 5482]\n",
      "loss: 0.581033  [ 3600/ 5482]\n",
      "loss: 0.464328  [ 4200/ 5482]\n",
      "loss: 0.383789  [ 4800/ 5482]\n",
      "loss: 0.369124  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.743     0.788    99\n",
      " disgust     0.654     0.857     0.841    107\n",
      "    fear     0.654     0.653     0.800    80\n",
      "   happy     0.654     0.343     0.636    77\n",
      " neutral     0.654     0.794     0.811    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.661     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.579     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.010232 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.462303  [    0/ 5482]\n",
      "loss: 0.764905  [  600/ 5482]\n",
      "loss: 0.456914  [ 1200/ 5482]\n",
      "loss: 0.487865  [ 1800/ 5482]\n",
      "loss: 0.263784  [ 2400/ 5482]\n",
      "loss: 0.362710  [ 3000/ 5482]\n",
      "loss: 0.351790  [ 3600/ 5482]\n",
      "loss: 0.281014  [ 4200/ 5482]\n",
      "loss: 0.477521  [ 4800/ 5482]\n",
      "loss: 0.253600  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.654     0.708     0.808    99\n",
      " disgust     0.654     0.849     0.841    107\n",
      "    fear     0.654     0.693     0.762    80\n",
      "   happy     0.654     0.329     0.649    77\n",
      " neutral     0.654     0.800     0.800    95\n",
      "     sad     0.654     0.000     0.000    91\n",
      "surprise     0.654     0.750     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.654     0.590     0.650    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 1.035336 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.279750  [    0/ 5482]\n",
      "loss: 0.285621  [  600/ 5482]\n",
      "loss: 0.383523  [ 1200/ 5482]\n",
      "loss: 0.392505  [ 1800/ 5482]\n",
      "loss: 0.568619  [ 2400/ 5482]\n",
      "loss: 0.471926  [ 3000/ 5482]\n",
      "loss: 0.247643  [ 3600/ 5482]\n",
      "loss: 0.278496  [ 4200/ 5482]\n",
      "loss: 0.270027  [ 4800/ 5482]\n",
      "loss: 0.482693  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.669     0.838    99\n",
      " disgust     0.644     0.758     0.850    107\n",
      "    fear     0.644     0.731     0.613    80\n",
      "   happy     0.644     0.348     0.623    77\n",
      " neutral     0.644     0.786     0.811    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.714     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.572     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.095921 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.415118  [    0/ 5482]\n",
      "loss: 0.588436  [  600/ 5482]\n",
      "loss: 0.369311  [ 1200/ 5482]\n",
      "loss: 0.239248  [ 1800/ 5482]\n",
      "loss: 0.470324  [ 2400/ 5482]\n",
      "loss: 0.455960  [ 3000/ 5482]\n",
      "loss: 0.809849  [ 3600/ 5482]\n",
      "loss: 0.365196  [ 4200/ 5482]\n",
      "loss: 0.392071  [ 4800/ 5482]\n",
      "loss: 0.174962  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.748     0.778    99\n",
      " disgust     0.644     0.840     0.832    107\n",
      "    fear     0.644     0.678     0.762    80\n",
      "   happy     0.644     0.302     0.662    77\n",
      " neutral     0.644     0.835     0.800    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.765     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.595     0.639    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.045359 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.367673  [    0/ 5482]\n",
      "loss: 0.355131  [  600/ 5482]\n",
      "loss: 0.578134  [ 1200/ 5482]\n",
      "loss: 0.469365  [ 1800/ 5482]\n",
      "loss: 0.474189  [ 2400/ 5482]\n",
      "loss: 0.278647  [ 3000/ 5482]\n",
      "loss: 0.272836  [ 3600/ 5482]\n",
      "loss: 0.181222  [ 4200/ 5482]\n",
      "loss: 0.281174  [ 4800/ 5482]\n",
      "loss: 0.479357  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.644     0.728     0.758    99\n",
      " disgust     0.644     0.795     0.869    107\n",
      "    fear     0.644     0.691     0.700    80\n",
      "   happy     0.644     0.310     0.636    77\n",
      " neutral     0.644     0.802     0.853    95\n",
      "     sad     0.644     0.000     0.000    91\n",
      "surprise     0.644     0.780     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.644     0.587     0.636    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.045205 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.392506  [    0/ 5482]\n",
      "loss: 0.265982  [  600/ 5482]\n",
      "loss: 0.295889  [ 1200/ 5482]\n",
      "loss: 0.300591  [ 1800/ 5482]\n",
      "loss: 0.266052  [ 2400/ 5482]\n",
      "loss: 0.279753  [ 3000/ 5482]\n",
      "loss: 0.392833  [ 3600/ 5482]\n",
      "loss: 0.303526  [ 4200/ 5482]\n",
      "loss: 0.189223  [ 4800/ 5482]\n",
      "loss: 0.288078  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.736     0.788    99\n",
      " disgust     0.646     0.758     0.850    107\n",
      "    fear     0.646     0.713     0.713    80\n",
      "   happy     0.646     0.318     0.649    77\n",
      " neutral     0.646     0.819     0.811    95\n",
      "     sad     0.646     0.000     0.000    91\n",
      "surprise     0.646     0.774     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.588     0.640    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.047376 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.392314  [    0/ 5482]\n",
      "loss: 0.257013  [  600/ 5482]\n",
      "loss: 0.287031  [ 1200/ 5482]\n",
      "loss: 0.481398  [ 1800/ 5482]\n",
      "loss: 0.363208  [ 2400/ 5482]\n",
      "loss: 0.295884  [ 3000/ 5482]\n",
      "loss: 0.492428  [ 3600/ 5482]\n",
      "loss: 0.355197  [ 4200/ 5482]\n",
      "loss: 0.224703  [ 4800/ 5482]\n",
      "loss: 0.691304  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.669     0.728     0.838    99\n",
      " disgust     0.669     0.818     0.841    107\n",
      "    fear     0.669     0.670     0.738    80\n",
      "   happy     0.669     0.364     0.623    77\n",
      " neutral     0.669     0.783     0.874    95\n",
      "     sad     0.669     0.000     0.000    91\n",
      "surprise     0.669     0.750     0.738    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.669     0.588     0.665    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.012578 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.275623  [    0/ 5482]\n",
      "loss: 0.343595  [  600/ 5482]\n",
      "loss: 0.477380  [ 1200/ 5482]\n",
      "loss: 0.593014  [ 1800/ 5482]\n",
      "loss: 0.155155  [ 2400/ 5482]\n",
      "loss: 0.278164  [ 3000/ 5482]\n",
      "loss: 0.377335  [ 3600/ 5482]\n",
      "loss: 0.396937  [ 4200/ 5482]\n",
      "loss: 0.347201  [ 4800/ 5482]\n",
      "loss: 0.186634  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.750     0.758    99\n",
      " disgust     0.646     0.807     0.860    107\n",
      "    fear     0.646     0.655     0.688    80\n",
      "   happy     0.646     0.320     0.636    77\n",
      " neutral     0.646     0.782     0.832    95\n",
      "     sad     0.646     0.000     0.000    91\n",
      "surprise     0.646     0.759     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.582     0.642    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.041191 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.194311  [    0/ 5482]\n",
      "loss: 0.163412  [  600/ 5482]\n",
      "loss: 0.472436  [ 1200/ 5482]\n",
      "loss: 0.382375  [ 1800/ 5482]\n",
      "loss: 0.155684  [ 2400/ 5482]\n",
      "loss: 0.474947  [ 3000/ 5482]\n",
      "loss: 0.376641  [ 3600/ 5482]\n",
      "loss: 0.462428  [ 4200/ 5482]\n",
      "loss: 0.394192  [ 4800/ 5482]\n",
      "loss: 0.579677  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.657     0.721     0.808    99\n",
      " disgust     0.657     0.852     0.860    107\n",
      "    fear     0.657     0.652     0.725    80\n",
      "   happy     0.657     0.329     0.636    77\n",
      " neutral     0.657     0.806     0.832    95\n",
      "     sad     0.657     0.000     0.000    91\n",
      "surprise     0.657     0.782     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.657     0.592     0.652    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 1.016712 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.488591  [    0/ 5482]\n",
      "loss: 0.383200  [  600/ 5482]\n",
      "loss: 0.370672  [ 1200/ 5482]\n",
      "loss: 0.486408  [ 1800/ 5482]\n",
      "loss: 0.374404  [ 2400/ 5482]\n",
      "loss: 0.368580  [ 3000/ 5482]\n",
      "loss: 0.359852  [ 3600/ 5482]\n",
      "loss: 0.377924  [ 4200/ 5482]\n",
      "loss: 0.375019  [ 4800/ 5482]\n",
      "loss: 0.451408  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.659     0.745     0.737    99\n",
      " disgust     0.659     0.820     0.850    107\n",
      "    fear     0.659     0.674     0.775    80\n",
      "   happy     0.659     0.342     0.662    77\n",
      " neutral     0.659     0.810     0.853    95\n",
      "     sad     0.659     0.000     0.000    91\n",
      "surprise     0.659     0.733     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.659     0.589     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 1.023551 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.567028  [    0/ 5482]\n",
      "loss: 0.174523  [  600/ 5482]\n",
      "loss: 0.247532  [ 1200/ 5482]\n",
      "loss: 0.239912  [ 1800/ 5482]\n",
      "loss: 0.345612  [ 2400/ 5482]\n",
      "loss: 0.372246  [ 3000/ 5482]\n",
      "loss: 0.491618  [ 3600/ 5482]\n",
      "loss: 0.345868  [ 4200/ 5482]\n",
      "loss: 0.372163  [ 4800/ 5482]\n",
      "loss: 0.372616  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.661     0.848    99\n",
      " disgust     0.651     0.822     0.822    107\n",
      "    fear     0.651     0.675     0.700    80\n",
      "   happy     0.651     0.336     0.623    77\n",
      " neutral     0.651     0.862     0.789    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.730     0.754    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.584     0.648    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.046904 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.483363  [    0/ 5482]\n",
      "loss: 0.568997  [  600/ 5482]\n",
      "loss: 0.361577  [ 1200/ 5482]\n",
      "loss: 0.251752  [ 1800/ 5482]\n",
      "loss: 0.351705  [ 2400/ 5482]\n",
      "loss: 0.161556  [ 3000/ 5482]\n",
      "loss: 0.384528  [ 3600/ 5482]\n",
      "loss: 0.259377  [ 4200/ 5482]\n",
      "loss: 0.469502  [ 4800/ 5482]\n",
      "loss: 0.174432  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.646     0.696     0.808    99\n",
      " disgust     0.646     0.776     0.841    107\n",
      "    fear     0.646     0.727     0.700    80\n",
      "   happy     0.646     0.303     0.610    77\n",
      " neutral     0.646     0.839     0.821    95\n",
      "     sad     0.646     0.000     0.000    91\n",
      "surprise     0.646     0.796     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.646     0.591     0.641    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.048783 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.266979  [    0/ 5482]\n",
      "loss: 0.461119  [  600/ 5482]\n",
      "loss: 0.378216  [ 1200/ 5482]\n",
      "loss: 0.362659  [ 1800/ 5482]\n",
      "loss: 0.462206  [ 2400/ 5482]\n",
      "loss: 0.362656  [ 3000/ 5482]\n",
      "loss: 0.262407  [ 3600/ 5482]\n",
      "loss: 0.368478  [ 4200/ 5482]\n",
      "loss: 0.465792  [ 4800/ 5482]\n",
      "loss: 0.457227  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.731     0.768    99\n",
      " disgust     0.651     0.784     0.850    107\n",
      "    fear     0.651     0.682     0.750    80\n",
      "   happy     0.651     0.320     0.636    77\n",
      " neutral     0.651     0.833     0.842    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.774     0.672    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.589     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.029080 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.374950  [    0/ 5482]\n",
      "loss: 0.261880  [  600/ 5482]\n",
      "loss: 0.388272  [ 1200/ 5482]\n",
      "loss: 0.383838  [ 1800/ 5482]\n",
      "loss: 0.262999  [ 2400/ 5482]\n",
      "loss: 0.267086  [ 3000/ 5482]\n",
      "loss: 0.144538  [ 3600/ 5482]\n",
      "loss: 0.272544  [ 4200/ 5482]\n",
      "loss: 0.574018  [ 4800/ 5482]\n",
      "loss: 0.240297  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.700     0.778    99\n",
      " disgust     0.652     0.769     0.841    107\n",
      "    fear     0.652     0.667     0.650    80\n",
      "   happy     0.652     0.340     0.623    77\n",
      " neutral     0.652     0.837     0.863    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.742     0.803    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.579     0.651    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.041292 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.365667  [    0/ 5482]\n",
      "loss: 0.372153  [  600/ 5482]\n",
      "loss: 0.360221  [ 1200/ 5482]\n",
      "loss: 0.479056  [ 1800/ 5482]\n",
      "loss: 0.475444  [ 2400/ 5482]\n",
      "loss: 0.250699  [ 3000/ 5482]\n",
      "loss: 0.471128  [ 3600/ 5482]\n",
      "loss: 0.288184  [ 4200/ 5482]\n",
      "loss: 0.489554  [ 4800/ 5482]\n",
      "loss: 0.578663  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.778     0.778    99\n",
      " disgust     0.649     0.829     0.860    107\n",
      "    fear     0.649     0.646     0.662    80\n",
      "   happy     0.649     0.316     0.649    77\n",
      " neutral     0.649     0.764     0.853    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.796     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.590     0.644    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.039700 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.352500  [    0/ 5482]\n",
      "loss: 0.162970  [  600/ 5482]\n",
      "loss: 0.269565  [ 1200/ 5482]\n",
      "loss: 0.373296  [ 1800/ 5482]\n",
      "loss: 0.269962  [ 2400/ 5482]\n",
      "loss: 0.360229  [ 3000/ 5482]\n",
      "loss: 0.368783  [ 3600/ 5482]\n",
      "loss: 0.152216  [ 4200/ 5482]\n",
      "loss: 0.265504  [ 4800/ 5482]\n",
      "loss: 0.182132  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.649     0.722     0.788    99\n",
      " disgust     0.649     0.838     0.822    107\n",
      "    fear     0.649     0.635     0.675    80\n",
      "   happy     0.649     0.376     0.571    77\n",
      " neutral     0.649     0.718     0.884    95\n",
      "     sad     0.649     0.000     0.000    91\n",
      "surprise     0.649     0.615     0.787    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.649     0.558     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.108247 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.260242  [    0/ 5482]\n",
      "loss: 0.271090  [  600/ 5482]\n",
      "loss: 0.265127  [ 1200/ 5482]\n",
      "loss: 0.486074  [ 1800/ 5482]\n",
      "loss: 0.366536  [ 2400/ 5482]\n",
      "loss: 0.580803  [ 3000/ 5482]\n",
      "loss: 0.372280  [ 3600/ 5482]\n",
      "loss: 0.467252  [ 4200/ 5482]\n",
      "loss: 0.247901  [ 4800/ 5482]\n",
      "loss: 0.569364  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.648     0.683     0.828    99\n",
      " disgust     0.648     0.847     0.776    107\n",
      "    fear     0.648     0.628     0.738    80\n",
      "   happy     0.648     0.343     0.636    77\n",
      " neutral     0.648     0.814     0.832    95\n",
      "     sad     0.648     0.000     0.000    91\n",
      "surprise     0.648     0.741     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.648     0.579     0.645    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.006015 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.283482  [    0/ 5482]\n",
      "loss: 0.270437  [  600/ 5482]\n",
      "loss: 0.358088  [ 1200/ 5482]\n",
      "loss: 0.588537  [ 1800/ 5482]\n",
      "loss: 0.139410  [ 2400/ 5482]\n",
      "loss: 0.264565  [ 3000/ 5482]\n",
      "loss: 0.371085  [ 3600/ 5482]\n",
      "loss: 0.469575  [ 4200/ 5482]\n",
      "loss: 0.467377  [ 4800/ 5482]\n",
      "loss: 0.365186  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.719     0.828    99\n",
      " disgust     0.661     0.769     0.841    107\n",
      "    fear     0.661     0.652     0.725    80\n",
      "   happy     0.661     0.353     0.610    77\n",
      " neutral     0.661     0.837     0.863    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.746     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.582     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.017331 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.362659  [    0/ 5482]\n",
      "loss: 0.249793  [  600/ 5482]\n",
      "loss: 0.253101  [ 1200/ 5482]\n",
      "loss: 0.267242  [ 1800/ 5482]\n",
      "loss: 0.235196  [ 2400/ 5482]\n",
      "loss: 0.166738  [ 3000/ 5482]\n",
      "loss: 0.259038  [ 3600/ 5482]\n",
      "loss: 0.466855  [ 4200/ 5482]\n",
      "loss: 0.354317  [ 4800/ 5482]\n",
      "loss: 0.143842  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.661     0.741     0.808    99\n",
      " disgust     0.661     0.832     0.832    107\n",
      "    fear     0.661     0.642     0.762    80\n",
      "   happy     0.661     0.356     0.623    77\n",
      " neutral     0.661     0.788     0.863    95\n",
      "     sad     0.661     0.000     0.000    91\n",
      "surprise     0.661     0.705     0.705    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.661     0.581     0.656    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.990012 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.156648  [    0/ 5482]\n",
      "loss: 0.270064  [  600/ 5482]\n",
      "loss: 0.384372  [ 1200/ 5482]\n",
      "loss: 0.286321  [ 1800/ 5482]\n",
      "loss: 0.573137  [ 2400/ 5482]\n",
      "loss: 0.457566  [ 3000/ 5482]\n",
      "loss: 0.486411  [ 3600/ 5482]\n",
      "loss: 0.343149  [ 4200/ 5482]\n",
      "loss: 0.154158  [ 4800/ 5482]\n",
      "loss: 0.363173  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.652     0.716     0.788    99\n",
      " disgust     0.652     0.824     0.832    107\n",
      "    fear     0.652     0.655     0.713    80\n",
      "   happy     0.652     0.331     0.623    77\n",
      " neutral     0.652     0.778     0.884    95\n",
      "     sad     0.652     0.000     0.000    91\n",
      "surprise     0.652     0.792     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.652     0.585     0.647    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.007849 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.158119  [    0/ 5482]\n",
      "loss: 0.355321  [  600/ 5482]\n",
      "loss: 0.569810  [ 1200/ 5482]\n",
      "loss: 0.247464  [ 1800/ 5482]\n",
      "loss: 0.270839  [ 2400/ 5482]\n",
      "loss: 0.472617  [ 3000/ 5482]\n",
      "loss: 0.456667  [ 3600/ 5482]\n",
      "loss: 0.354832  [ 4200/ 5482]\n",
      "loss: 0.367567  [ 4800/ 5482]\n",
      "loss: 0.383265  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.662     0.750     0.788    99\n",
      " disgust     0.662     0.829     0.860    107\n",
      "    fear     0.662     0.663     0.713    80\n",
      "   happy     0.662     0.336     0.636    77\n",
      " neutral     0.662     0.792     0.884    95\n",
      "     sad     0.662     0.000     0.000    91\n",
      "surprise     0.662     0.772     0.721    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.662     0.592     0.657    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.003912 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.155642  [    0/ 5482]\n",
      "loss: 0.264591  [  600/ 5482]\n",
      "loss: 0.569417  [ 1200/ 5482]\n",
      "loss: 0.462907  [ 1800/ 5482]\n",
      "loss: 0.481559  [ 2400/ 5482]\n",
      "loss: 0.362233  [ 3000/ 5482]\n",
      "loss: 0.373474  [ 3600/ 5482]\n",
      "loss: 0.161624  [ 4200/ 5482]\n",
      "loss: 0.475540  [ 4800/ 5482]\n",
      "loss: 0.250726  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.651     0.729     0.788    99\n",
      " disgust     0.651     0.835     0.850    107\n",
      "    fear     0.651     0.648     0.713    80\n",
      "   happy     0.651     0.336     0.649    77\n",
      " neutral     0.651     0.775     0.832    95\n",
      "     sad     0.651     0.000     0.000    91\n",
      "surprise     0.651     0.764     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.651     0.584     0.646    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.004542 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/conv.md \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.760172  [    0/ 5482]\n",
      "loss: 1.980093  [ 1200/ 5482]\n",
      "loss: 2.012942  [ 2400/ 5482]\n",
      "loss: 1.926213  [ 3600/ 5482]\n",
      "loss: 1.985711  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.189     0.000     0.000    99\n",
      " disgust     0.189     0.191     0.813    107\n",
      "    fear     0.189     0.181     0.350    80\n",
      "   happy     0.189     0.000     0.000    77\n",
      " neutral     0.189     0.000     0.000    95\n",
      "     sad     0.189     0.000     0.000    91\n",
      "surprise     0.189     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.189     0.053     0.166    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 18.9%, Avg loss: 1.929257 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.920068  [    0/ 5482]\n",
      "loss: 1.870876  [ 1200/ 5482]\n",
      "loss: 1.918571  [ 2400/ 5482]\n",
      "loss: 1.964879  [ 3600/ 5482]\n",
      "loss: 1.956040  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.207     0.000     0.000    99\n",
      " disgust     0.207     0.204     0.822    107\n",
      "    fear     0.207     0.215     0.475    80\n",
      "   happy     0.207     0.000     0.000    77\n",
      " neutral     0.207     0.000     0.000    95\n",
      "     sad     0.207     0.000     0.000    91\n",
      "surprise     0.207     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.207     0.060     0.185    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 20.7%, Avg loss: 1.906948 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.830187  [    0/ 5482]\n",
      "loss: 2.000047  [ 1200/ 5482]\n",
      "loss: 2.006704  [ 2400/ 5482]\n",
      "loss: 1.972755  [ 3600/ 5482]\n",
      "loss: 1.858698  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.218     0.000     0.000    99\n",
      " disgust     0.218     0.217     0.832    107\n",
      "    fear     0.218     0.228     0.550    80\n",
      "   happy     0.218     0.000     0.000    77\n",
      " neutral     0.218     0.000     0.000    95\n",
      "     sad     0.218     0.000     0.000    91\n",
      "surprise     0.218     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.218     0.064     0.197    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 21.8%, Avg loss: 1.890581 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.863686  [    0/ 5482]\n",
      "loss: 1.938603  [ 1200/ 5482]\n",
      "loss: 1.991540  [ 2400/ 5482]\n",
      "loss: 1.929923  [ 3600/ 5482]\n",
      "loss: 1.944454  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.228     0.000     0.000    99\n",
      " disgust     0.228     0.226     0.813    107\n",
      "    fear     0.228     0.234     0.650    80\n",
      "   happy     0.228     0.000     0.000    77\n",
      " neutral     0.228     0.000     0.000    95\n",
      "     sad     0.228     0.000     0.000    91\n",
      "surprise     0.228     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.228     0.066     0.209    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 22.8%, Avg loss: 1.879063 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.987429  [    0/ 5482]\n",
      "loss: 1.929815  [ 1200/ 5482]\n",
      "loss: 1.925671  [ 2400/ 5482]\n",
      "loss: 1.875366  [ 3600/ 5482]\n",
      "loss: 1.885176  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.239     0.000     0.000    99\n",
      " disgust     0.239     0.236     0.813    107\n",
      "    fear     0.239     0.246     0.738    80\n",
      "   happy     0.239     0.000     0.000    77\n",
      " neutral     0.239     0.000     0.000    95\n",
      "     sad     0.239     0.000     0.000    91\n",
      "surprise     0.239     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.239     0.069     0.222    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 23.9%, Avg loss: 1.870024 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.830985  [    0/ 5482]\n",
      "loss: 2.013487  [ 1200/ 5482]\n",
      "loss: 1.905607  [ 2400/ 5482]\n",
      "loss: 1.928760  [ 3600/ 5482]\n",
      "loss: 1.828423  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.244     0.000     0.000    99\n",
      " disgust     0.244     0.238     0.822    107\n",
      "    fear     0.244     0.255     0.762    80\n",
      "   happy     0.244     0.000     0.000    77\n",
      " neutral     0.244     0.000     0.000    95\n",
      "     sad     0.244     0.000     0.000    91\n",
      "surprise     0.244     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.244     0.070     0.226    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.4%, Avg loss: 1.860806 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.892110  [    0/ 5482]\n",
      "loss: 1.832537  [ 1200/ 5482]\n",
      "loss: 1.840800  [ 2400/ 5482]\n",
      "loss: 1.827902  [ 3600/ 5482]\n",
      "loss: 1.870429  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.244     0.000     0.000    99\n",
      " disgust     0.244     0.243     0.822    107\n",
      "    fear     0.244     0.246     0.762    80\n",
      "   happy     0.244     0.000     0.000    77\n",
      " neutral     0.244     0.000     0.000    95\n",
      "     sad     0.244     0.000     0.000    91\n",
      "surprise     0.244     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.244     0.070     0.226    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.4%, Avg loss: 1.853277 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.780711  [    0/ 5482]\n",
      "loss: 1.892400  [ 1200/ 5482]\n",
      "loss: 1.788556  [ 2400/ 5482]\n",
      "loss: 1.840192  [ 3600/ 5482]\n",
      "loss: 1.869359  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.249     0.000     0.000    99\n",
      " disgust     0.249     0.247     0.822    107\n",
      "    fear     0.249     0.252     0.800    80\n",
      "   happy     0.249     0.000     0.000    77\n",
      " neutral     0.249     0.000     0.000    95\n",
      "     sad     0.249     0.000     0.000    91\n",
      "surprise     0.249     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.249     0.071     0.232    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 1.846864 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.965741  [    0/ 5482]\n",
      "loss: 1.840234  [ 1200/ 5482]\n",
      "loss: 1.861780  [ 2400/ 5482]\n",
      "loss: 1.927232  [ 3600/ 5482]\n",
      "loss: 1.912881  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.246     0.000     0.000    99\n",
      " disgust     0.246     0.246     0.804    107\n",
      "    fear     0.246     0.245     0.800    80\n",
      "   happy     0.246     0.000     0.000    77\n",
      " neutral     0.246     0.000     0.000    95\n",
      "     sad     0.246     0.000     0.000    91\n",
      "surprise     0.246     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.246     0.070     0.229    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 1.842187 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.848283  [    0/ 5482]\n",
      "loss: 1.863259  [ 1200/ 5482]\n",
      "loss: 1.826650  [ 2400/ 5482]\n",
      "loss: 1.841680  [ 3600/ 5482]\n",
      "loss: 1.948899  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.246     0.000     0.000    99\n",
      " disgust     0.246     0.246     0.804    107\n",
      "    fear     0.246     0.247     0.800    80\n",
      "   happy     0.246     0.000     0.000    77\n",
      " neutral     0.246     0.000     0.000    95\n",
      "     sad     0.246     0.000     0.000    91\n",
      "surprise     0.246     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.246     0.070     0.229    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 1.836367 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.825903  [    0/ 5482]\n",
      "loss: 1.795583  [ 1200/ 5482]\n",
      "loss: 1.867026  [ 2400/ 5482]\n",
      "loss: 1.869265  [ 3600/ 5482]\n",
      "loss: 1.895911  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.251     0.000     0.000    99\n",
      " disgust     0.251     0.251     0.841    107\n",
      "    fear     0.251     0.251     0.787    80\n",
      "   happy     0.251     0.000     0.000    77\n",
      " neutral     0.251     0.000     0.000    95\n",
      "     sad     0.251     0.000     0.000    91\n",
      "surprise     0.251     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.251     0.072     0.233    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.1%, Avg loss: 1.829598 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.982492  [    0/ 5482]\n",
      "loss: 1.819694  [ 1200/ 5482]\n",
      "loss: 1.793140  [ 2400/ 5482]\n",
      "loss: 1.864533  [ 3600/ 5482]\n",
      "loss: 1.791918  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.248     0.000     0.000    99\n",
      " disgust     0.248     0.249     0.804    107\n",
      "    fear     0.248     0.244     0.800    80\n",
      "   happy     0.248     0.000     0.000    77\n",
      " neutral     0.248     0.000     0.000    95\n",
      "     sad     0.248     0.333     0.011    91\n",
      "surprise     0.248     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.248     0.118     0.231    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.8%, Avg loss: 1.827362 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.878392  [    0/ 5482]\n",
      "loss: 1.860546  [ 1200/ 5482]\n",
      "loss: 1.964907  [ 2400/ 5482]\n",
      "loss: 1.868726  [ 3600/ 5482]\n",
      "loss: 1.856689  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.246     0.000     0.000    99\n",
      " disgust     0.246     0.249     0.804    107\n",
      "    fear     0.246     0.244     0.800    80\n",
      "   happy     0.246     0.000     0.000    77\n",
      " neutral     0.246     0.000     0.000    95\n",
      "     sad     0.246     0.000     0.000    91\n",
      "surprise     0.246     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.246     0.070     0.229    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 1.821302 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.818015  [    0/ 5482]\n",
      "loss: 1.797675  [ 1200/ 5482]\n",
      "loss: 1.826728  [ 2400/ 5482]\n",
      "loss: 1.886362  [ 3600/ 5482]\n",
      "loss: 1.816607  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.251     0.000     0.000    99\n",
      " disgust     0.251     0.254     0.832    107\n",
      "    fear     0.251     0.246     0.787    80\n",
      "   happy     0.251     0.000     0.000    77\n",
      " neutral     0.251     0.000     0.000    95\n",
      "     sad     0.251     0.333     0.011    91\n",
      "surprise     0.251     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.251     0.119     0.233    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.1%, Avg loss: 1.815736 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.884663  [    0/ 5482]\n",
      "loss: 1.882300  [ 1200/ 5482]\n",
      "loss: 1.856514  [ 2400/ 5482]\n",
      "loss: 1.796554  [ 3600/ 5482]\n",
      "loss: 1.880777  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.249     0.000     0.000    99\n",
      " disgust     0.249     0.254     0.813    107\n",
      "    fear     0.249     0.242     0.800    80\n",
      "   happy     0.249     0.000     0.000    77\n",
      " neutral     0.249     0.000     0.000    95\n",
      "     sad     0.249     0.333     0.011    91\n",
      "surprise     0.249     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.249     0.118     0.232    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 1.812482 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.730961  [    0/ 5482]\n",
      "loss: 1.729946  [ 1200/ 5482]\n",
      "loss: 1.807437  [ 2400/ 5482]\n",
      "loss: 1.949215  [ 3600/ 5482]\n",
      "loss: 1.844806  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.249     0.000     0.000    99\n",
      " disgust     0.249     0.254     0.832    107\n",
      "    fear     0.249     0.242     0.775    80\n",
      "   happy     0.249     0.000     0.000    77\n",
      " neutral     0.249     0.000     0.000    95\n",
      "     sad     0.249     0.333     0.011    91\n",
      "surprise     0.249     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.249     0.118     0.231    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 1.807223 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.806167  [    0/ 5482]\n",
      "loss: 1.837212  [ 1200/ 5482]\n",
      "loss: 1.813255  [ 2400/ 5482]\n",
      "loss: 1.823801  [ 3600/ 5482]\n",
      "loss: 1.831373  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.248     0.000     0.000    99\n",
      " disgust     0.248     0.254     0.822    107\n",
      "    fear     0.248     0.238     0.775    80\n",
      "   happy     0.248     0.000     0.000    77\n",
      " neutral     0.248     0.000     0.000    95\n",
      "     sad     0.248     0.333     0.011    91\n",
      "surprise     0.248     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.248     0.118     0.230    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.8%, Avg loss: 1.803242 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.816403  [    0/ 5482]\n",
      "loss: 1.793247  [ 1200/ 5482]\n",
      "loss: 1.849910  [ 2400/ 5482]\n",
      "loss: 1.763444  [ 3600/ 5482]\n",
      "loss: 1.805908  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.248     0.000     0.000    99\n",
      " disgust     0.248     0.255     0.813    107\n",
      "    fear     0.248     0.238     0.787    80\n",
      "   happy     0.248     0.000     0.000    77\n",
      " neutral     0.248     0.000     0.000    95\n",
      "     sad     0.248     0.250     0.011    91\n",
      "surprise     0.248     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.248     0.106     0.230    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.8%, Avg loss: 1.800116 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.871283  [    0/ 5482]\n",
      "loss: 1.947584  [ 1200/ 5482]\n",
      "loss: 1.836459  [ 2400/ 5482]\n",
      "loss: 1.730116  [ 3600/ 5482]\n",
      "loss: 1.816511  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.249     0.000     0.000    99\n",
      " disgust     0.249     0.259     0.822    107\n",
      "    fear     0.249     0.237     0.787    80\n",
      "   happy     0.249     0.000     0.000    77\n",
      " neutral     0.249     0.000     0.000    95\n",
      "     sad     0.249     0.250     0.011    91\n",
      "surprise     0.249     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.249     0.107     0.232    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.9%, Avg loss: 1.795943 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.894055  [    0/ 5482]\n",
      "loss: 1.899123  [ 1200/ 5482]\n",
      "loss: 1.794746  [ 2400/ 5482]\n",
      "loss: 1.847109  [ 3600/ 5482]\n",
      "loss: 1.876433  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.248     0.000     0.000    99\n",
      " disgust     0.248     0.256     0.822    107\n",
      "    fear     0.248     0.237     0.775    80\n",
      "   happy     0.248     0.000     0.000    77\n",
      " neutral     0.248     0.000     0.000    95\n",
      "     sad     0.248     0.250     0.011    91\n",
      "surprise     0.248     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.248     0.106     0.230    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 24.8%, Avg loss: 1.791185 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.775040  [    0/ 5482]\n",
      "loss: 1.814564  [ 1200/ 5482]\n",
      "loss: 1.766138  [ 2400/ 5482]\n",
      "loss: 1.805238  [ 3600/ 5482]\n",
      "loss: 1.775763  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.252     0.000     0.000    99\n",
      " disgust     0.252     0.262     0.841    107\n",
      "    fear     0.252     0.239     0.775    80\n",
      "   happy     0.252     0.000     0.000    77\n",
      " neutral     0.252     0.000     0.000    95\n",
      "     sad     0.252     0.286     0.022    91\n",
      "surprise     0.252     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.252     0.112     0.234    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.2%, Avg loss: 1.787077 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.759588  [    0/ 5482]\n",
      "loss: 1.794550  [ 1200/ 5482]\n",
      "loss: 1.807520  [ 2400/ 5482]\n",
      "loss: 1.928712  [ 3600/ 5482]\n",
      "loss: 1.842427  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.252     0.000     0.000    99\n",
      " disgust     0.252     0.262     0.822    107\n",
      "    fear     0.252     0.240     0.800    80\n",
      "   happy     0.252     0.000     0.000    77\n",
      " neutral     0.252     0.000     0.000    95\n",
      "     sad     0.252     0.286     0.022    91\n",
      "surprise     0.252     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.252     0.112     0.235    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.2%, Avg loss: 1.784157 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.832120  [    0/ 5482]\n",
      "loss: 1.880225  [ 1200/ 5482]\n",
      "loss: 1.735350  [ 2400/ 5482]\n",
      "loss: 1.897816  [ 3600/ 5482]\n",
      "loss: 1.832023  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.256     0.000     0.000    99\n",
      " disgust     0.256     0.272     0.813    107\n",
      "    fear     0.256     0.231     0.812    80\n",
      "   happy     0.256     0.000     0.000    77\n",
      " neutral     0.256     0.000     0.000    95\n",
      "     sad     0.256     0.444     0.044    91\n",
      "surprise     0.256     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.256     0.135     0.239    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 1.783333 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.815792  [    0/ 5482]\n",
      "loss: 1.794826  [ 1200/ 5482]\n",
      "loss: 1.845540  [ 2400/ 5482]\n",
      "loss: 1.840764  [ 3600/ 5482]\n",
      "loss: 1.717732  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.252     0.000     0.000    99\n",
      " disgust     0.252     0.263     0.804    107\n",
      "    fear     0.252     0.235     0.800    80\n",
      "   happy     0.252     0.000     0.000    77\n",
      " neutral     0.252     0.000     0.000    95\n",
      "     sad     0.252     0.364     0.044    91\n",
      "surprise     0.252     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.252     0.123     0.235    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.2%, Avg loss: 1.777151 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.622142  [    0/ 5482]\n",
      "loss: 1.844517  [ 1200/ 5482]\n",
      "loss: 2.006774  [ 2400/ 5482]\n",
      "loss: 1.818534  [ 3600/ 5482]\n",
      "loss: 1.762950  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.257     0.000     0.000    99\n",
      " disgust     0.257     0.268     0.813    107\n",
      "    fear     0.257     0.233     0.787    80\n",
      "   happy     0.257     0.000     0.000    77\n",
      " neutral     0.257     0.000     0.000    95\n",
      "     sad     0.257     0.467     0.077    91\n",
      "surprise     0.257     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.257     0.138     0.240    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.7%, Avg loss: 1.773386 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.757148  [    0/ 5482]\n",
      "loss: 1.663008  [ 1200/ 5482]\n",
      "loss: 1.762793  [ 2400/ 5482]\n",
      "loss: 1.741983  [ 3600/ 5482]\n",
      "loss: 1.867803  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.269     0.000     0.000    99\n",
      " disgust     0.269     0.279     0.804    107\n",
      "    fear     0.269     0.232     0.812    80\n",
      "   happy     0.269     0.000     0.000    77\n",
      " neutral     0.269     0.000     0.000    95\n",
      "     sad     0.269     0.591     0.143    91\n",
      "surprise     0.269     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.269     0.157     0.251    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 1.771300 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.801593  [    0/ 5482]\n",
      "loss: 1.815525  [ 1200/ 5482]\n",
      "loss: 1.893833  [ 2400/ 5482]\n",
      "loss: 1.789359  [ 3600/ 5482]\n",
      "loss: 1.749553  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.269     0.000     0.000    99\n",
      " disgust     0.269     0.281     0.794    107\n",
      "    fear     0.269     0.229     0.812    80\n",
      "   happy     0.269     0.000     0.000    77\n",
      " neutral     0.269     0.000     0.000    95\n",
      "     sad     0.269     0.609     0.154    91\n",
      "surprise     0.269     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.269     0.160     0.252    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 1.768581 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.713846  [    0/ 5482]\n",
      "loss: 1.699242  [ 1200/ 5482]\n",
      "loss: 1.682878  [ 2400/ 5482]\n",
      "loss: 1.817354  [ 3600/ 5482]\n",
      "loss: 1.759717  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.279     0.000     0.000    99\n",
      " disgust     0.279     0.292     0.785    107\n",
      "    fear     0.279     0.225     0.825    80\n",
      "   happy     0.279     0.000     0.000    77\n",
      " neutral     0.279     0.000     0.000    95\n",
      "     sad     0.279     0.690     0.220    91\n",
      "surprise     0.279     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.279     0.172     0.261    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 27.9%, Avg loss: 1.766776 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.858890  [    0/ 5482]\n",
      "loss: 1.808460  [ 1200/ 5482]\n",
      "loss: 1.770046  [ 2400/ 5482]\n",
      "loss: 1.686485  [ 3600/ 5482]\n",
      "loss: 1.652781  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.277     0.000     0.000    99\n",
      " disgust     0.277     0.292     0.785    107\n",
      "    fear     0.277     0.224     0.825    80\n",
      "   happy     0.277     0.000     0.000    77\n",
      " neutral     0.277     0.000     0.000    95\n",
      "     sad     0.277     0.679     0.209    91\n",
      "surprise     0.277     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.277     0.171     0.260    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 27.7%, Avg loss: 1.763397 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.676243  [    0/ 5482]\n",
      "loss: 1.797171  [ 1200/ 5482]\n",
      "loss: 1.821169  [ 2400/ 5482]\n",
      "loss: 1.879003  [ 3600/ 5482]\n",
      "loss: 1.767748  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.287     1.000     0.010    99\n",
      " disgust     0.287     0.290     0.794    107\n",
      "    fear     0.287     0.230     0.812    80\n",
      "   happy     0.287     0.000     0.000    77\n",
      " neutral     0.287     0.000     0.000    95\n",
      "     sad     0.287     0.706     0.264    91\n",
      "surprise     0.287     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.287     0.318     0.269    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 1.759213 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.631107  [    0/ 5482]\n",
      "loss: 1.833488  [ 1200/ 5482]\n",
      "loss: 1.799662  [ 2400/ 5482]\n",
      "loss: 1.854584  [ 3600/ 5482]\n",
      "loss: 1.781086  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.287     1.000     0.010    99\n",
      " disgust     0.287     0.289     0.804    107\n",
      "    fear     0.287     0.231     0.800    80\n",
      "   happy     0.287     0.000     0.000    77\n",
      " neutral     0.287     0.000     0.000    95\n",
      "     sad     0.287     0.706     0.264    91\n",
      "surprise     0.287     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.287     0.318     0.268    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 1.754077 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.673856  [    0/ 5482]\n",
      "loss: 1.840818  [ 1200/ 5482]\n",
      "loss: 1.835054  [ 2400/ 5482]\n",
      "loss: 1.875617  [ 3600/ 5482]\n",
      "loss: 1.800935  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.287     1.000     0.010    99\n",
      " disgust     0.287     0.294     0.776    107\n",
      "    fear     0.287     0.223     0.812    80\n",
      "   happy     0.287     0.000     0.000    77\n",
      " neutral     0.287     0.000     0.000    95\n",
      "     sad     0.287     0.722     0.286    91\n",
      "surprise     0.287     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.287     0.320     0.269    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 1.753235 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.698433  [    0/ 5482]\n",
      "loss: 1.791339  [ 1200/ 5482]\n",
      "loss: 1.728557  [ 2400/ 5482]\n",
      "loss: 1.789172  [ 3600/ 5482]\n",
      "loss: 1.723738  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.292     1.000     0.010    99\n",
      " disgust     0.292     0.291     0.776    107\n",
      "    fear     0.292     0.229     0.812    80\n",
      "   happy     0.292     0.000     0.000    77\n",
      " neutral     0.292     0.000     0.000    95\n",
      "     sad     0.292     0.725     0.319    91\n",
      "surprise     0.292     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.292     0.321     0.274    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 29.2%, Avg loss: 1.749070 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.786052  [    0/ 5482]\n",
      "loss: 1.697543  [ 1200/ 5482]\n",
      "loss: 1.954381  [ 2400/ 5482]\n",
      "loss: 1.801623  [ 3600/ 5482]\n",
      "loss: 1.819735  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.297     1.000     0.010    99\n",
      " disgust     0.297     0.300     0.776    107\n",
      "    fear     0.297     0.225     0.812    80\n",
      "   happy     0.297     0.000     0.000    77\n",
      " neutral     0.297     0.000     0.000    95\n",
      "     sad     0.297     0.744     0.352    91\n",
      "surprise     0.297     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.297     0.324     0.279    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 29.7%, Avg loss: 1.746922 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.782287  [    0/ 5482]\n",
      "loss: 1.836422  [ 1200/ 5482]\n",
      "loss: 1.761101  [ 2400/ 5482]\n",
      "loss: 1.831347  [ 3600/ 5482]\n",
      "loss: 1.726484  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.303     1.000     0.010    99\n",
      " disgust     0.303     0.303     0.813    107\n",
      "    fear     0.303     0.233     0.812    80\n",
      "   happy     0.303     0.000     0.000    77\n",
      " neutral     0.303     0.000     0.000    95\n",
      "     sad     0.303     0.744     0.352    91\n",
      "surprise     0.303     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.303     0.326     0.284    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 1.741779 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.734468  [    0/ 5482]\n",
      "loss: 1.721619  [ 1200/ 5482]\n",
      "loss: 1.850081  [ 2400/ 5482]\n",
      "loss: 1.673847  [ 3600/ 5482]\n",
      "loss: 1.885998  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.320     1.000     0.010    99\n",
      " disgust     0.320     0.313     0.813    107\n",
      "    fear     0.320     0.236     0.812    80\n",
      "   happy     0.320     0.000     0.000    77\n",
      " neutral     0.320     0.000     0.000    95\n",
      "     sad     0.320     0.764     0.462    91\n",
      "surprise     0.320     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.320     0.330     0.300    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 1.738633 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.756243  [    0/ 5482]\n",
      "loss: 1.833438  [ 1200/ 5482]\n",
      "loss: 1.693396  [ 2400/ 5482]\n",
      "loss: 1.635006  [ 3600/ 5482]\n",
      "loss: 1.881450  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     1.000     0.010    99\n",
      " disgust     0.321     0.314     0.804    107\n",
      "    fear     0.321     0.235     0.812    80\n",
      "   happy     0.321     0.000     0.000    77\n",
      " neutral     0.321     0.000     0.000    95\n",
      "     sad     0.321     0.759     0.484    91\n",
      "surprise     0.321     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.330     0.301    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.736033 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.898909  [    0/ 5482]\n",
      "loss: 1.932449  [ 1200/ 5482]\n",
      "loss: 1.706873  [ 2400/ 5482]\n",
      "loss: 1.862456  [ 3600/ 5482]\n",
      "loss: 1.745170  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.313     1.000     0.010    99\n",
      " disgust     0.313     0.309     0.766    107\n",
      "    fear     0.313     0.226     0.812    80\n",
      "   happy     0.313     0.000     0.000    77\n",
      " neutral     0.313     0.000     0.000    95\n",
      "     sad     0.313     0.754     0.473    91\n",
      "surprise     0.313     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.313     0.327     0.294    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 1.734467 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.893759  [    0/ 5482]\n",
      "loss: 1.865579  [ 1200/ 5482]\n",
      "loss: 1.631504  [ 2400/ 5482]\n",
      "loss: 1.736372  [ 3600/ 5482]\n",
      "loss: 1.772019  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.311     1.000     0.010    99\n",
      " disgust     0.311     0.314     0.748    107\n",
      "    fear     0.311     0.220     0.812    80\n",
      "   happy     0.311     0.000     0.000    77\n",
      " neutral     0.311     0.000     0.000    95\n",
      "     sad     0.311     0.746     0.484    91\n",
      "surprise     0.311     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.311     0.326     0.293    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.1%, Avg loss: 1.733563 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.696142  [    0/ 5482]\n",
      "loss: 1.837582  [ 1200/ 5482]\n",
      "loss: 1.894845  [ 2400/ 5482]\n",
      "loss: 1.706256  [ 3600/ 5482]\n",
      "loss: 1.713608  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.315     1.000     0.020    99\n",
      " disgust     0.315     0.317     0.729    107\n",
      "    fear     0.315     0.222     0.812    80\n",
      "   happy     0.315     0.000     0.000    77\n",
      " neutral     0.315     0.000     0.000    95\n",
      "     sad     0.315     0.681     0.516    91\n",
      "surprise     0.315     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.315     0.317     0.297    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.5%, Avg loss: 1.730279 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.696757  [    0/ 5482]\n",
      "loss: 1.778992  [ 1200/ 5482]\n",
      "loss: 1.724746  [ 2400/ 5482]\n",
      "loss: 1.743583  [ 3600/ 5482]\n",
      "loss: 1.679860  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.316     1.000     0.020    99\n",
      " disgust     0.316     0.328     0.710    107\n",
      "    fear     0.316     0.219     0.825    80\n",
      "   happy     0.316     0.000     0.000    77\n",
      " neutral     0.316     0.000     0.000    95\n",
      "     sad     0.316     0.653     0.538    91\n",
      "surprise     0.316     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.316     0.314     0.299    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 1.730260 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.648904  [    0/ 5482]\n",
      "loss: 1.751947  [ 1200/ 5482]\n",
      "loss: 1.703556  [ 2400/ 5482]\n",
      "loss: 1.720795  [ 3600/ 5482]\n",
      "loss: 1.669821  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.323     1.000     0.030    99\n",
      " disgust     0.323     0.321     0.757    107\n",
      "    fear     0.323     0.234     0.812    80\n",
      "   happy     0.323     0.000     0.000    77\n",
      " neutral     0.323     0.000     0.000    95\n",
      "     sad     0.323     0.623     0.527    91\n",
      "surprise     0.323     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.323     0.311     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 1.722972 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.802306  [    0/ 5482]\n",
      "loss: 1.746355  [ 1200/ 5482]\n",
      "loss: 1.766685  [ 2400/ 5482]\n",
      "loss: 1.830261  [ 3600/ 5482]\n",
      "loss: 1.731239  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.323     1.000     0.030    99\n",
      " disgust     0.323     0.331     0.757    107\n",
      "    fear     0.323     0.230     0.812    80\n",
      "   happy     0.323     0.000     0.000    77\n",
      " neutral     0.323     0.000     0.000    95\n",
      "     sad     0.323     0.608     0.527    91\n",
      "surprise     0.323     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.323     0.310     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.3%, Avg loss: 1.721093 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.730662  [    0/ 5482]\n",
      "loss: 1.690910  [ 1200/ 5482]\n",
      "loss: 1.780610  [ 2400/ 5482]\n",
      "loss: 1.648683  [ 3600/ 5482]\n",
      "loss: 1.673574  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.326     1.000     0.030    99\n",
      " disgust     0.326     0.328     0.766    107\n",
      "    fear     0.326     0.236     0.812    80\n",
      "   happy     0.326     0.000     0.000    77\n",
      " neutral     0.326     0.000     0.000    95\n",
      "     sad     0.326     0.605     0.538    91\n",
      "surprise     0.326     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.326     0.310     0.307    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.717742 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.601356  [    0/ 5482]\n",
      "loss: 1.653191  [ 1200/ 5482]\n",
      "loss: 1.825491  [ 2400/ 5482]\n",
      "loss: 1.732220  [ 3600/ 5482]\n",
      "loss: 1.799167  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.326     1.000     0.040    99\n",
      " disgust     0.326     0.349     0.710    107\n",
      "    fear     0.326     0.226     0.812    80\n",
      "   happy     0.326     0.000     0.000    77\n",
      " neutral     0.326     0.000     0.000    95\n",
      "     sad     0.326     0.540     0.593    91\n",
      "surprise     0.326     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.326     0.302     0.308    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.717546 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.692040  [    0/ 5482]\n",
      "loss: 1.673440  [ 1200/ 5482]\n",
      "loss: 1.630215  [ 2400/ 5482]\n",
      "loss: 1.697699  [ 3600/ 5482]\n",
      "loss: 1.715968  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.333     1.000     0.040    99\n",
      " disgust     0.333     0.348     0.748    107\n",
      "    fear     0.333     0.233     0.812    80\n",
      "   happy     0.333     0.000     0.000    77\n",
      " neutral     0.333     0.000     0.000    95\n",
      "     sad     0.333     0.557     0.593    91\n",
      "surprise     0.333     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.333     0.305     0.313    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.713533 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.661058  [    0/ 5482]\n",
      "loss: 1.736519  [ 1200/ 5482]\n",
      "loss: 1.831289  [ 2400/ 5482]\n",
      "loss: 1.648554  [ 3600/ 5482]\n",
      "loss: 1.574440  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.331     1.000     0.040    99\n",
      " disgust     0.331     0.342     0.776    107\n",
      "    fear     0.331     0.228     0.812    80\n",
      "   happy     0.331     0.000     0.000    77\n",
      " neutral     0.331     0.000     0.000    95\n",
      "     sad     0.331     0.641     0.549    91\n",
      "surprise     0.331     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.331     0.316     0.311    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.711330 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.727736  [    0/ 5482]\n",
      "loss: 1.724858  [ 1200/ 5482]\n",
      "loss: 1.600907  [ 2400/ 5482]\n",
      "loss: 1.651146  [ 3600/ 5482]\n",
      "loss: 1.635665  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.331     1.000     0.040    99\n",
      " disgust     0.331     0.343     0.776    107\n",
      "    fear     0.331     0.228     0.812    80\n",
      "   happy     0.331     0.000     0.000    77\n",
      " neutral     0.331     0.000     0.000    95\n",
      "     sad     0.331     0.633     0.549    91\n",
      "surprise     0.331     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.331     0.315     0.311    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.709471 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.644352  [    0/ 5482]\n",
      "loss: 1.647350  [ 1200/ 5482]\n",
      "loss: 1.825834  [ 2400/ 5482]\n",
      "loss: 1.608812  [ 3600/ 5482]\n",
      "loss: 1.730689  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.326     0.857     0.061    99\n",
      " disgust     0.326     0.347     0.710    107\n",
      "    fear     0.326     0.223     0.787    80\n",
      "   happy     0.326     0.000     0.000    77\n",
      " neutral     0.326     0.000     0.000    95\n",
      "     sad     0.326     0.535     0.593    91\n",
      "surprise     0.326     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.326     0.280     0.307    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.707494 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.819690  [    0/ 5482]\n",
      "loss: 1.770203  [ 1200/ 5482]\n",
      "loss: 1.790562  [ 2400/ 5482]\n",
      "loss: 1.808143  [ 3600/ 5482]\n",
      "loss: 1.808525  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.328     0.833     0.051    99\n",
      " disgust     0.328     0.345     0.738    107\n",
      "    fear     0.328     0.220     0.787    80\n",
      "   happy     0.328     0.000     0.000    77\n",
      " neutral     0.328     0.000     0.000    95\n",
      "     sad     0.328     0.602     0.582    91\n",
      "surprise     0.328     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.328     0.286     0.308    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.705511 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.727006  [    0/ 5482]\n",
      "loss: 1.748716  [ 1200/ 5482]\n",
      "loss: 1.694882  [ 2400/ 5482]\n",
      "loss: 1.700399  [ 3600/ 5482]\n",
      "loss: 1.618626  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.326     0.667     0.061    99\n",
      " disgust     0.326     0.357     0.710    107\n",
      "    fear     0.326     0.220     0.787    80\n",
      "   happy     0.326     0.000     0.000    77\n",
      " neutral     0.326     0.000     0.000    95\n",
      "     sad     0.326     0.529     0.593    91\n",
      "surprise     0.326     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.326     0.253     0.307    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.703725 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.803309  [    0/ 5482]\n",
      "loss: 1.691476  [ 1200/ 5482]\n",
      "loss: 1.723776  [ 2400/ 5482]\n",
      "loss: 1.913316  [ 3600/ 5482]\n",
      "loss: 1.795534  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.325     0.667     0.061    99\n",
      " disgust     0.325     0.357     0.710    107\n",
      "    fear     0.325     0.219     0.775    80\n",
      "   happy     0.325     0.000     0.000    77\n",
      " neutral     0.325     0.000     0.000    95\n",
      "     sad     0.325     0.514     0.593    91\n",
      "surprise     0.325     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.325     0.251     0.306    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 1.701060 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.638999  [    0/ 5482]\n",
      "loss: 1.707643  [ 1200/ 5482]\n",
      "loss: 1.573433  [ 2400/ 5482]\n",
      "loss: 1.804049  [ 3600/ 5482]\n",
      "loss: 1.982053  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.330     0.600     0.061    99\n",
      " disgust     0.330     0.359     0.748    107\n",
      "    fear     0.330     0.226     0.762    80\n",
      "   happy     0.330     0.000     0.000    77\n",
      " neutral     0.330     0.000     0.000    95\n",
      "     sad     0.330     0.505     0.593    91\n",
      "surprise     0.330     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.330     0.241     0.309    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.697341 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.652094  [    0/ 5482]\n",
      "loss: 1.805957  [ 1200/ 5482]\n",
      "loss: 1.715705  [ 2400/ 5482]\n",
      "loss: 1.650494  [ 3600/ 5482]\n",
      "loss: 1.578113  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.326     0.545     0.061    99\n",
      " disgust     0.326     0.361     0.701    107\n",
      "    fear     0.326     0.222     0.800    80\n",
      "   happy     0.326     0.000     0.000    77\n",
      " neutral     0.326     0.000     0.000    95\n",
      "     sad     0.326     0.524     0.593    91\n",
      "surprise     0.326     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.326     0.236     0.308    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.697542 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.591761  [    0/ 5482]\n",
      "loss: 1.792954  [ 1200/ 5482]\n",
      "loss: 1.693559  [ 2400/ 5482]\n",
      "loss: 1.713055  [ 3600/ 5482]\n",
      "loss: 1.786905  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.330     0.571     0.081    99\n",
      " disgust     0.330     0.364     0.701    107\n",
      "    fear     0.330     0.222     0.787    80\n",
      "   happy     0.330     0.000     0.000    77\n",
      " neutral     0.330     0.000     0.000    95\n",
      "     sad     0.330     0.519     0.604    91\n",
      "surprise     0.330     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.330     0.239     0.311    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.696922 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.692261  [    0/ 5482]\n",
      "loss: 1.849579  [ 1200/ 5482]\n",
      "loss: 1.853169  [ 2400/ 5482]\n",
      "loss: 1.714657  [ 3600/ 5482]\n",
      "loss: 1.684822  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.667     0.101    99\n",
      " disgust     0.338     0.362     0.748    107\n",
      "    fear     0.338     0.229     0.762    80\n",
      "   happy     0.338     0.000     0.000    77\n",
      " neutral     0.338     0.000     0.000    95\n",
      "     sad     0.338     0.509     0.604    91\n",
      "surprise     0.338     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.252     0.317    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.691317 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.715776  [    0/ 5482]\n",
      "loss: 1.528129  [ 1200/ 5482]\n",
      "loss: 1.547847  [ 2400/ 5482]\n",
      "loss: 1.697507  [ 3600/ 5482]\n",
      "loss: 1.679080  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.331     0.588     0.101    99\n",
      " disgust     0.331     0.367     0.710    107\n",
      "    fear     0.331     0.217     0.750    80\n",
      "   happy     0.331     0.000     0.000    77\n",
      " neutral     0.331     0.000     0.000    95\n",
      "     sad     0.331     0.514     0.615    91\n",
      "surprise     0.331     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.331     0.241     0.311    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.690389 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.725122  [    0/ 5482]\n",
      "loss: 1.707836  [ 1200/ 5482]\n",
      "loss: 1.629765  [ 2400/ 5482]\n",
      "loss: 1.589137  [ 3600/ 5482]\n",
      "loss: 1.590333  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.533     0.081    99\n",
      " disgust     0.321     0.361     0.654    107\n",
      "    fear     0.321     0.214     0.775    80\n",
      "   happy     0.321     0.000     0.000    77\n",
      " neutral     0.321     0.000     0.000    95\n",
      "     sad     0.321     0.505     0.615    91\n",
      "surprise     0.321     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.230     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.692739 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.664358  [    0/ 5482]\n",
      "loss: 1.688738  [ 1200/ 5482]\n",
      "loss: 1.674109  [ 2400/ 5482]\n",
      "loss: 1.775913  [ 3600/ 5482]\n",
      "loss: 1.777156  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.331     0.562     0.091    99\n",
      " disgust     0.331     0.364     0.710    107\n",
      "    fear     0.331     0.219     0.762    80\n",
      "   happy     0.331     0.000     0.000    77\n",
      " neutral     0.331     0.000     0.000    95\n",
      "     sad     0.331     0.528     0.615    91\n",
      "surprise     0.331     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.331     0.239     0.311    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.686786 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.697441  [    0/ 5482]\n",
      "loss: 1.846583  [ 1200/ 5482]\n",
      "loss: 1.734102  [ 2400/ 5482]\n",
      "loss: 1.676575  [ 3600/ 5482]\n",
      "loss: 1.712638  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.632     0.121    99\n",
      " disgust     0.336     0.376     0.720    107\n",
      "    fear     0.336     0.220     0.750    80\n",
      "   happy     0.336     0.000     0.000    77\n",
      " neutral     0.336     0.000     0.000    95\n",
      "     sad     0.336     0.496     0.615    91\n",
      "surprise     0.336     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.246     0.315    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.684280 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.669692  [    0/ 5482]\n",
      "loss: 1.664116  [ 1200/ 5482]\n",
      "loss: 1.687961  [ 2400/ 5482]\n",
      "loss: 1.597903  [ 3600/ 5482]\n",
      "loss: 1.662323  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.333     0.565     0.131    99\n",
      " disgust     0.333     0.381     0.692    107\n",
      "    fear     0.333     0.219     0.738    80\n",
      "   happy     0.333     0.000     0.000    77\n",
      " neutral     0.333     0.000     0.000    95\n",
      "     sad     0.333     0.463     0.626    91\n",
      "surprise     0.333     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.333     0.233     0.312    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.683147 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.590146  [    0/ 5482]\n",
      "loss: 1.729078  [ 1200/ 5482]\n",
      "loss: 1.894608  [ 2400/ 5482]\n",
      "loss: 1.681498  [ 3600/ 5482]\n",
      "loss: 1.756383  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.339     0.591     0.131    99\n",
      " disgust     0.339     0.374     0.720    107\n",
      "    fear     0.339     0.224     0.762    80\n",
      "   happy     0.339     0.000     0.000    77\n",
      " neutral     0.339     0.000     0.000    95\n",
      "     sad     0.339     0.509     0.615    91\n",
      "surprise     0.339     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.339     0.243     0.318    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.681384 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.656115  [    0/ 5482]\n",
      "loss: 1.682508  [ 1200/ 5482]\n",
      "loss: 1.732312  [ 2400/ 5482]\n",
      "loss: 1.845417  [ 3600/ 5482]\n",
      "loss: 1.716554  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.565     0.131    99\n",
      " disgust     0.338     0.371     0.701    107\n",
      "    fear     0.338     0.226     0.775    80\n",
      "   happy     0.338     0.000     0.000    77\n",
      " neutral     0.338     0.000     0.000    95\n",
      "     sad     0.338     0.505     0.615    91\n",
      "surprise     0.338     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.238     0.318    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.680291 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.719831  [    0/ 5482]\n",
      "loss: 1.710643  [ 1200/ 5482]\n",
      "loss: 1.672203  [ 2400/ 5482]\n",
      "loss: 1.720371  [ 3600/ 5482]\n",
      "loss: 1.750217  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.560     0.141    99\n",
      " disgust     0.338     0.373     0.701    107\n",
      "    fear     0.338     0.224     0.762    80\n",
      "   happy     0.338     0.000     0.000    77\n",
      " neutral     0.338     0.000     0.000    95\n",
      "     sad     0.338     0.500     0.615    91\n",
      "surprise     0.338     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.237     0.317    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.679300 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.614011  [    0/ 5482]\n",
      "loss: 1.676692  [ 1200/ 5482]\n",
      "loss: 1.714070  [ 2400/ 5482]\n",
      "loss: 1.710669  [ 3600/ 5482]\n",
      "loss: 1.681152  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.346     0.591     0.131    99\n",
      " disgust     0.346     0.383     0.748    107\n",
      "    fear     0.346     0.226     0.750    80\n",
      "   happy     0.346     0.000     0.000    77\n",
      " neutral     0.346     0.000     0.000    95\n",
      "     sad     0.346     0.509     0.637    91\n",
      "surprise     0.346     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.346     0.244     0.324    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 1.674377 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.791809  [    0/ 5482]\n",
      "loss: 1.926191  [ 1200/ 5482]\n",
      "loss: 1.721189  [ 2400/ 5482]\n",
      "loss: 1.801251  [ 3600/ 5482]\n",
      "loss: 1.844587  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.318     0.591     0.131    99\n",
      " disgust     0.318     0.380     0.738    107\n",
      "    fear     0.318     0.176     0.537    80\n",
      "   happy     0.318     0.050     0.013    77\n",
      " neutral     0.318     0.000     0.000    95\n",
      "     sad     0.318     0.500     0.637    91\n",
      "surprise     0.318     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.318     0.242     0.294    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 1.672321 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.602594  [    0/ 5482]\n",
      "loss: 1.584609  [ 1200/ 5482]\n",
      "loss: 1.736870  [ 2400/ 5482]\n",
      "loss: 1.634594  [ 3600/ 5482]\n",
      "loss: 1.733244  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.328     0.613     0.192    99\n",
      " disgust     0.328     0.394     0.692    107\n",
      "    fear     0.328     0.150     0.412    80\n",
      "   happy     0.328     0.304     0.182    77\n",
      " neutral     0.328     0.000     0.000    95\n",
      "     sad     0.328     0.480     0.659    91\n",
      "surprise     0.328     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.328     0.277     0.305    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.672935 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.558071  [    0/ 5482]\n",
      "loss: 1.739138  [ 1200/ 5482]\n",
      "loss: 1.820488  [ 2400/ 5482]\n",
      "loss: 1.935128  [ 3600/ 5482]\n",
      "loss: 1.685236  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.650     0.263    99\n",
      " disgust     0.344     0.397     0.720    107\n",
      "    fear     0.344     0.145     0.350    80\n",
      "   happy     0.344     0.328     0.247    77\n",
      " neutral     0.344     0.000     0.000    95\n",
      "     sad     0.344     0.480     0.659    91\n",
      "surprise     0.344     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.286     0.320    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.669736 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.653967  [    0/ 5482]\n",
      "loss: 1.657886  [ 1200/ 5482]\n",
      "loss: 1.648544  [ 2400/ 5482]\n",
      "loss: 1.669592  [ 3600/ 5482]\n",
      "loss: 1.710849  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.341     0.600     0.182    99\n",
      " disgust     0.341     0.380     0.710    107\n",
      "    fear     0.341     0.146     0.338    80\n",
      "   happy     0.341     0.359     0.364    77\n",
      " neutral     0.341     0.000     0.000    95\n",
      "     sad     0.341     0.504     0.648    91\n",
      "surprise     0.341     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.341     0.284     0.320    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.668556 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.632617  [    0/ 5482]\n",
      "loss: 1.683758  [ 1200/ 5482]\n",
      "loss: 1.804467  [ 2400/ 5482]\n",
      "loss: 1.758579  [ 3600/ 5482]\n",
      "loss: 1.570962  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.343     0.647     0.222    99\n",
      " disgust     0.343     0.372     0.692    107\n",
      "    fear     0.343     0.140     0.300    80\n",
      "   happy     0.343     0.337     0.416    77\n",
      " neutral     0.343     0.000     0.000    95\n",
      "     sad     0.343     0.518     0.626    91\n",
      "surprise     0.343     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.343     0.288     0.322    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.670104 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.914115  [    0/ 5482]\n",
      "loss: 1.551542  [ 1200/ 5482]\n",
      "loss: 1.726804  [ 2400/ 5482]\n",
      "loss: 1.817242  [ 3600/ 5482]\n",
      "loss: 1.621978  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.362     0.652     0.303    99\n",
      " disgust     0.362     0.392     0.692    107\n",
      "    fear     0.362     0.144     0.275    80\n",
      "   happy     0.362     0.344     0.429    77\n",
      " neutral     0.362     0.000     0.000    95\n",
      "     sad     0.362     0.492     0.681    91\n",
      "surprise     0.362     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.362     0.289     0.340    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 1.666617 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.839878  [    0/ 5482]\n",
      "loss: 1.766683  [ 1200/ 5482]\n",
      "loss: 1.666102  [ 2400/ 5482]\n",
      "loss: 1.925621  [ 3600/ 5482]\n",
      "loss: 1.784409  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.369     0.660     0.313    99\n",
      " disgust     0.369     0.406     0.710    107\n",
      "    fear     0.369     0.144     0.250    80\n",
      "   happy     0.369     0.340     0.455    77\n",
      " neutral     0.369     0.000     0.000    95\n",
      "     sad     0.369     0.470     0.692    91\n",
      "surprise     0.369     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.369     0.289     0.346    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.662774 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.797388  [    0/ 5482]\n",
      "loss: 1.562422  [ 1200/ 5482]\n",
      "loss: 1.799866  [ 2400/ 5482]\n",
      "loss: 1.892592  [ 3600/ 5482]\n",
      "loss: 1.745313  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.369     0.660     0.313    99\n",
      " disgust     0.369     0.398     0.710    107\n",
      "    fear     0.369     0.141     0.225    80\n",
      "   happy     0.369     0.325     0.494    77\n",
      " neutral     0.369     0.000     0.000    95\n",
      "     sad     0.369     0.488     0.681    91\n",
      "surprise     0.369     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.369     0.287     0.346    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.661972 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.919673  [    0/ 5482]\n",
      "loss: 1.605849  [ 1200/ 5482]\n",
      "loss: 1.495914  [ 2400/ 5482]\n",
      "loss: 1.786375  [ 3600/ 5482]\n",
      "loss: 1.670796  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.367     0.660     0.313    99\n",
      " disgust     0.367     0.386     0.729    107\n",
      "    fear     0.367     0.142     0.212    80\n",
      "   happy     0.367     0.310     0.506    77\n",
      " neutral     0.367     0.000     0.000    95\n",
      "     sad     0.367     0.513     0.648    91\n",
      "surprise     0.367     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.367     0.287     0.344    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.660749 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.793347  [    0/ 5482]\n",
      "loss: 1.658888  [ 1200/ 5482]\n",
      "loss: 1.789328  [ 2400/ 5482]\n",
      "loss: 1.655140  [ 3600/ 5482]\n",
      "loss: 1.490862  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.379     0.649     0.374    99\n",
      " disgust     0.379     0.403     0.720    107\n",
      "    fear     0.379     0.140     0.188    80\n",
      "   happy     0.379     0.320     0.506    77\n",
      " neutral     0.379     0.000     0.000    95\n",
      "     sad     0.379     0.474     0.692    91\n",
      "surprise     0.379     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.379     0.284     0.354    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 1.657630 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.630956  [    0/ 5482]\n",
      "loss: 1.656343  [ 1200/ 5482]\n",
      "loss: 1.688066  [ 2400/ 5482]\n",
      "loss: 1.811675  [ 3600/ 5482]\n",
      "loss: 1.664137  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.367     0.660     0.313    99\n",
      " disgust     0.367     0.383     0.701    107\n",
      "    fear     0.367     0.139     0.175    80\n",
      "   happy     0.367     0.295     0.558    77\n",
      " neutral     0.367     0.000     0.000    95\n",
      "     sad     0.367     0.508     0.670    91\n",
      "surprise     0.367     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.367     0.283     0.345    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.657462 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.772039  [    0/ 5482]\n",
      "loss: 1.736538  [ 1200/ 5482]\n",
      "loss: 1.647417  [ 2400/ 5482]\n",
      "loss: 1.745288  [ 3600/ 5482]\n",
      "loss: 1.472589  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.375     0.653     0.323    99\n",
      " disgust     0.375     0.391     0.720    107\n",
      "    fear     0.375     0.152     0.175    80\n",
      "   happy     0.375     0.306     0.571    77\n",
      " neutral     0.375     0.000     0.000    95\n",
      "     sad     0.375     0.484     0.681    91\n",
      "surprise     0.375     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.375     0.284     0.353    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 1.653945 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.686381  [    0/ 5482]\n",
      "loss: 1.761602  [ 1200/ 5482]\n",
      "loss: 1.570071  [ 2400/ 5482]\n",
      "loss: 1.960397  [ 3600/ 5482]\n",
      "loss: 1.722323  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.384     0.672     0.414    99\n",
      " disgust     0.384     0.393     0.720    107\n",
      "    fear     0.384     0.127     0.125    80\n",
      "   happy     0.384     0.301     0.571    77\n",
      " neutral     0.384     0.000     0.000    95\n",
      "     sad     0.384     0.484     0.681    91\n",
      "surprise     0.384     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.384     0.282     0.359    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 1.652188 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.655320  [    0/ 5482]\n",
      "loss: 1.561177  [ 1200/ 5482]\n",
      "loss: 1.644315  [ 2400/ 5482]\n",
      "loss: 1.614794  [ 3600/ 5482]\n",
      "loss: 1.923992  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.384     0.660     0.354    99\n",
      " disgust     0.384     0.396     0.710    107\n",
      "    fear     0.384     0.169     0.150    80\n",
      "   happy     0.384     0.282     0.636    77\n",
      " neutral     0.384     0.000     0.000    95\n",
      "     sad     0.384     0.517     0.681    91\n",
      "surprise     0.384     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.384     0.289     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 1.654636 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.728501  [    0/ 5482]\n",
      "loss: 1.693590  [ 1200/ 5482]\n",
      "loss: 1.775943  [ 2400/ 5482]\n",
      "loss: 1.859910  [ 3600/ 5482]\n",
      "loss: 1.627776  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.389     0.662     0.455    99\n",
      " disgust     0.389     0.405     0.720    107\n",
      "    fear     0.389     0.139     0.125    80\n",
      "   happy     0.389     0.293     0.532    77\n",
      " neutral     0.389     0.000     0.000    95\n",
      "     sad     0.389     0.457     0.703    91\n",
      "surprise     0.389     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.389     0.279     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.9%, Avg loss: 1.649031 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.713677  [    0/ 5482]\n",
      "loss: 1.583485  [ 1200/ 5482]\n",
      "loss: 1.665910  [ 2400/ 5482]\n",
      "loss: 1.757077  [ 3600/ 5482]\n",
      "loss: 1.753827  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.656     0.404    99\n",
      " disgust     0.390     0.396     0.710    107\n",
      "    fear     0.390     0.175     0.138    80\n",
      "   happy     0.390     0.292     0.636    77\n",
      " neutral     0.390     0.000     0.000    95\n",
      "     sad     0.390     0.492     0.681    91\n",
      "surprise     0.390     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.287     0.367    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.649136 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.859969  [    0/ 5482]\n",
      "loss: 1.645318  [ 1200/ 5482]\n",
      "loss: 1.751824  [ 2400/ 5482]\n",
      "loss: 1.571917  [ 3600/ 5482]\n",
      "loss: 1.773430  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.662     0.434    99\n",
      " disgust     0.393     0.417     0.701    107\n",
      "    fear     0.393     0.150     0.113    80\n",
      "   happy     0.393     0.292     0.649    77\n",
      " neutral     0.393     0.000     0.000    95\n",
      "     sad     0.393     0.470     0.692    91\n",
      "surprise     0.393     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.284     0.370    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.648815 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.685632  [    0/ 5482]\n",
      "loss: 1.509086  [ 1200/ 5482]\n",
      "loss: 1.736641  [ 2400/ 5482]\n",
      "loss: 1.492491  [ 3600/ 5482]\n",
      "loss: 1.785435  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.656     0.424    99\n",
      " disgust     0.390     0.405     0.720    107\n",
      "    fear     0.390     0.129     0.100    80\n",
      "   happy     0.390     0.300     0.623    77\n",
      " neutral     0.390     0.000     0.000    95\n",
      "     sad     0.390     0.470     0.692    91\n",
      "surprise     0.390     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.280     0.366    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.644794 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.847420  [    0/ 5482]\n",
      "loss: 1.709630  [ 1200/ 5482]\n",
      "loss: 1.755968  [ 2400/ 5482]\n",
      "loss: 1.561883  [ 3600/ 5482]\n",
      "loss: 1.628650  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.392     0.642     0.434    99\n",
      " disgust     0.392     0.410     0.701    107\n",
      "    fear     0.392     0.135     0.087    80\n",
      "   happy     0.392     0.291     0.662    77\n",
      " neutral     0.392     0.000     0.000    95\n",
      "     sad     0.392     0.474     0.692    91\n",
      "surprise     0.392     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.392     0.279     0.368    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.645895 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.585788  [    0/ 5482]\n",
      "loss: 1.802944  [ 1200/ 5482]\n",
      "loss: 1.661760  [ 2400/ 5482]\n",
      "loss: 1.669622  [ 3600/ 5482]\n",
      "loss: 1.827533  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.402     0.667     0.465    99\n",
      " disgust     0.402     0.409     0.710    107\n",
      "    fear     0.402     0.173     0.113    80\n",
      "   happy     0.402     0.302     0.662    77\n",
      " neutral     0.402     0.000     0.000    95\n",
      "     sad     0.402     0.470     0.692    91\n",
      "surprise     0.402     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.402     0.289     0.377    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.642915 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.639756  [    0/ 5482]\n",
      "loss: 1.543856  [ 1200/ 5482]\n",
      "loss: 1.633040  [ 2400/ 5482]\n",
      "loss: 1.797933  [ 3600/ 5482]\n",
      "loss: 1.640735  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.387     0.642     0.434    99\n",
      " disgust     0.387     0.402     0.692    107\n",
      "    fear     0.387     0.109     0.062    80\n",
      "   happy     0.387     0.281     0.675    77\n",
      " neutral     0.387     0.000     0.000    95\n",
      "     sad     0.387     0.484     0.681    91\n",
      "surprise     0.387     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.387     0.274     0.364    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.643676 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.682307  [    0/ 5482]\n",
      "loss: 1.775086  [ 1200/ 5482]\n",
      "loss: 1.783958  [ 2400/ 5482]\n",
      "loss: 1.612192  [ 3600/ 5482]\n",
      "loss: 1.641879  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.397     0.652     0.434    99\n",
      " disgust     0.397     0.402     0.729    107\n",
      "    fear     0.397     0.152     0.087    80\n",
      "   happy     0.397     0.289     0.675    77\n",
      " neutral     0.397     0.000     0.000    95\n",
      "     sad     0.397     0.500     0.681    91\n",
      "surprise     0.397     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.397     0.285     0.372    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.640179 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.809036  [    0/ 5482]\n",
      "loss: 1.682721  [ 1200/ 5482]\n",
      "loss: 1.716603  [ 2400/ 5482]\n",
      "loss: 1.565300  [ 3600/ 5482]\n",
      "loss: 1.680960  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.385     0.623     0.434    99\n",
      " disgust     0.385     0.404     0.673    107\n",
      "    fear     0.385     0.104     0.062    80\n",
      "   happy     0.385     0.280     0.675    77\n",
      " neutral     0.385     0.000     0.000    95\n",
      "     sad     0.385     0.488     0.692    91\n",
      "surprise     0.385     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.385     0.271     0.362    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: 1.642374 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.385361  [    0/ 5482]\n",
      "loss: 1.610254  [ 1200/ 5482]\n",
      "loss: 1.718581  [ 2400/ 5482]\n",
      "loss: 1.676280  [ 3600/ 5482]\n",
      "loss: 1.518383  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.652     0.455    99\n",
      " disgust     0.393     0.408     0.701    107\n",
      "    fear     0.393     0.111     0.062    80\n",
      "   happy     0.393     0.284     0.675    77\n",
      " neutral     0.393     0.000     0.000    95\n",
      "     sad     0.393     0.488     0.692    91\n",
      "surprise     0.393     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.278     0.369    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.638816 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.439587  [    0/ 5482]\n",
      "loss: 1.623124  [ 1200/ 5482]\n",
      "loss: 1.604312  [ 2400/ 5482]\n",
      "loss: 1.665411  [ 3600/ 5482]\n",
      "loss: 1.709047  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.397     0.672     0.455    99\n",
      " disgust     0.397     0.393     0.720    107\n",
      "    fear     0.397     0.149     0.087    80\n",
      "   happy     0.397     0.301     0.649    77\n",
      " neutral     0.397     0.000     0.000    95\n",
      "     sad     0.397     0.470     0.692    91\n",
      "surprise     0.397     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.397     0.284     0.372    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.633661 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.812914  [    0/ 5482]\n",
      "loss: 1.793843  [ 1200/ 5482]\n",
      "loss: 1.790237  [ 2400/ 5482]\n",
      "loss: 1.531573  [ 3600/ 5482]\n",
      "loss: 1.537474  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.657     0.465    99\n",
      " disgust     0.400     0.409     0.710    107\n",
      "    fear     0.400     0.146     0.075    80\n",
      "   happy     0.400     0.292     0.675    77\n",
      " neutral     0.400     0.000     0.000    95\n",
      "     sad     0.400     0.474     0.703    91\n",
      "surprise     0.400     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.283     0.376    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.633773 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.871546  [    0/ 5482]\n",
      "loss: 1.613310  [ 1200/ 5482]\n",
      "loss: 1.652828  [ 2400/ 5482]\n",
      "loss: 1.582539  [ 3600/ 5482]\n",
      "loss: 1.542770  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.397     0.657     0.465    99\n",
      " disgust     0.397     0.416     0.692    107\n",
      "    fear     0.397     0.116     0.062    80\n",
      "   happy     0.397     0.286     0.675    77\n",
      " neutral     0.397     0.000     0.000    95\n",
      "     sad     0.397     0.474     0.714    91\n",
      "surprise     0.397     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.397     0.278     0.373    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.633243 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.611400  [    0/ 5482]\n",
      "loss: 1.703054  [ 1200/ 5482]\n",
      "loss: 1.852402  [ 2400/ 5482]\n",
      "loss: 1.485559  [ 3600/ 5482]\n",
      "loss: 1.592967  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.395     0.634     0.455    99\n",
      " disgust     0.395     0.412     0.701    107\n",
      "    fear     0.395     0.122     0.062    80\n",
      "   happy     0.395     0.284     0.675    77\n",
      " neutral     0.395     0.000     0.000    95\n",
      "     sad     0.395     0.481     0.703    91\n",
      "surprise     0.395     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.395     0.276     0.371    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 1.631860 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.638628  [    0/ 5482]\n",
      "loss: 1.593556  [ 1200/ 5482]\n",
      "loss: 1.617397  [ 2400/ 5482]\n",
      "loss: 1.616042  [ 3600/ 5482]\n",
      "loss: 1.637433  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.676     0.465    99\n",
      " disgust     0.400     0.388     0.710    107\n",
      "    fear     0.400     0.179     0.087    80\n",
      "   happy     0.400     0.290     0.662    77\n",
      " neutral     0.400     0.000     0.000    95\n",
      "     sad     0.400     0.489     0.703    91\n",
      "surprise     0.400     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.289     0.375    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.627627 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.523021  [    0/ 5482]\n",
      "loss: 1.582179  [ 1200/ 5482]\n",
      "loss: 1.560196  [ 2400/ 5482]\n",
      "loss: 1.649152  [ 3600/ 5482]\n",
      "loss: 1.474639  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.652     0.455    99\n",
      " disgust     0.393     0.401     0.701    107\n",
      "    fear     0.393     0.100     0.050    80\n",
      "   happy     0.393     0.284     0.701    77\n",
      " neutral     0.393     0.000     0.000    95\n",
      "     sad     0.393     0.500     0.681    91\n",
      "surprise     0.393     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.277     0.370    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.629754 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.554035  [    0/ 5482]\n",
      "loss: 1.569633  [ 1200/ 5482]\n",
      "loss: 1.596468  [ 2400/ 5482]\n",
      "loss: 1.707274  [ 3600/ 5482]\n",
      "loss: 1.532928  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.658     0.485    99\n",
      " disgust     0.400     0.401     0.701    107\n",
      "    fear     0.400     0.125     0.062    80\n",
      "   happy     0.400     0.289     0.675    77\n",
      " neutral     0.400     0.000     0.000    95\n",
      "     sad     0.400     0.492     0.703    91\n",
      "surprise     0.400     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.281     0.375    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.626637 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.501092  [    0/ 5482]\n",
      "loss: 1.460011  [ 1200/ 5482]\n",
      "loss: 1.596141  [ 2400/ 5482]\n",
      "loss: 1.639014  [ 3600/ 5482]\n",
      "loss: 1.585194  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.667     0.465    99\n",
      " disgust     0.400     0.398     0.710    107\n",
      "    fear     0.400     0.146     0.075    80\n",
      "   happy     0.400     0.290     0.701    77\n",
      " neutral     0.400     0.000     0.000    95\n",
      "     sad     0.400     0.504     0.681    91\n",
      "surprise     0.400     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.286     0.376    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.625392 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.683886  [    0/ 5482]\n",
      "loss: 1.556897  [ 1200/ 5482]\n",
      "loss: 1.608693  [ 2400/ 5482]\n",
      "loss: 1.473422  [ 3600/ 5482]\n",
      "loss: 1.483664  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.405     0.649     0.485    99\n",
      " disgust     0.405     0.411     0.692    107\n",
      "    fear     0.405     0.118     0.050    80\n",
      "   happy     0.405     0.298     0.727    77\n",
      " neutral     0.405     0.000     0.000    95\n",
      "     sad     0.405     0.485     0.714    91\n",
      "surprise     0.405     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.405     0.280     0.381    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 1.625460 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.787145  [    0/ 5482]\n",
      "loss: 1.550656  [ 1200/ 5482]\n",
      "loss: 1.588171  [ 2400/ 5482]\n",
      "loss: 1.565636  [ 3600/ 5482]\n",
      "loss: 1.548114  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.405     0.676     0.485    99\n",
      " disgust     0.405     0.406     0.729    107\n",
      "    fear     0.405     0.146     0.075    80\n",
      "   happy     0.405     0.287     0.675    77\n",
      " neutral     0.405     0.000     0.000    95\n",
      "     sad     0.405     0.504     0.692    91\n",
      "surprise     0.405     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.405     0.289     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 1.620837 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.552470  [    0/ 5482]\n",
      "loss: 1.626604  [ 1200/ 5482]\n",
      "loss: 1.515475  [ 2400/ 5482]\n",
      "loss: 1.598656  [ 3600/ 5482]\n",
      "loss: 1.469094  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.402     0.676     0.485    99\n",
      " disgust     0.402     0.380     0.710    107\n",
      "    fear     0.402     0.129     0.050    80\n",
      "   happy     0.402     0.294     0.675    77\n",
      " neutral     0.402     0.000     0.000    95\n",
      "     sad     0.402     0.496     0.714    91\n",
      "surprise     0.402     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.402     0.282     0.376    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.618133 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.505066  [    0/ 5482]\n",
      "loss: 1.668111  [ 1200/ 5482]\n",
      "loss: 1.657542  [ 2400/ 5482]\n",
      "loss: 1.611717  [ 3600/ 5482]\n",
      "loss: 1.761737  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.398     0.645     0.495    99\n",
      " disgust     0.398     0.412     0.682    107\n",
      "    fear     0.398     0.122     0.062    80\n",
      "   happy     0.398     0.287     0.662    77\n",
      " neutral     0.398     0.000     0.000    95\n",
      "     sad     0.398     0.471     0.714    91\n",
      "surprise     0.398     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.398     0.277     0.374    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 1.618408 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 1.726974  [    0/ 5482]\n",
      "loss: 1.497841  [ 1200/ 5482]\n",
      "loss: 1.603566  [ 2400/ 5482]\n",
      "loss: 1.655128  [ 3600/ 5482]\n",
      "loss: 1.534591  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.405     0.635     0.475    99\n",
      " disgust     0.405     0.406     0.710    107\n",
      "    fear     0.405     0.125     0.050    80\n",
      "   happy     0.405     0.295     0.727    77\n",
      " neutral     0.405     0.000     0.000    95\n",
      "     sad     0.405     0.504     0.703    91\n",
      "surprise     0.405     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.405     0.281     0.381    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.5%, Avg loss: 1.619397 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 1.825613  [    0/ 5482]\n",
      "loss: 1.593977  [ 1200/ 5482]\n",
      "loss: 1.714225  [ 2400/ 5482]\n",
      "loss: 1.736786  [ 3600/ 5482]\n",
      "loss: 1.657204  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.407     0.653     0.495    99\n",
      " disgust     0.407     0.402     0.710    107\n",
      "    fear     0.407     0.139     0.062    80\n",
      "   happy     0.407     0.291     0.688    77\n",
      " neutral     0.407     1.000     0.011    95\n",
      "     sad     0.407     0.504     0.703    91\n",
      "surprise     0.407     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.407     0.427     0.381    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.615772 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 1.639069  [    0/ 5482]\n",
      "loss: 1.733103  [ 1200/ 5482]\n",
      "loss: 1.590918  [ 2400/ 5482]\n",
      "loss: 1.646735  [ 3600/ 5482]\n",
      "loss: 1.851102  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.413     0.645     0.495    99\n",
      " disgust     0.413     0.414     0.701    107\n",
      "    fear     0.413     0.133     0.050    80\n",
      "   happy     0.413     0.302     0.740    77\n",
      " neutral     0.413     1.000     0.021    95\n",
      "     sad     0.413     0.492     0.714    91\n",
      "surprise     0.413     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.413     0.427     0.389    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.615563 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 1.767780  [    0/ 5482]\n",
      "loss: 1.513827  [ 1200/ 5482]\n",
      "loss: 1.741031  [ 2400/ 5482]\n",
      "loss: 1.562677  [ 3600/ 5482]\n",
      "loss: 1.660293  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.418     0.645     0.495    99\n",
      " disgust     0.418     0.420     0.710    107\n",
      "    fear     0.418     0.154     0.050    80\n",
      "   happy     0.418     0.297     0.753    77\n",
      " neutral     0.418     1.000     0.032    95\n",
      "     sad     0.418     0.504     0.714    91\n",
      "surprise     0.418     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.418     0.431     0.393    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.615679 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 1.694419  [    0/ 5482]\n",
      "loss: 1.646718  [ 1200/ 5482]\n",
      "loss: 1.596238  [ 2400/ 5482]\n",
      "loss: 1.769608  [ 3600/ 5482]\n",
      "loss: 1.584085  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.411     0.645     0.495    99\n",
      " disgust     0.411     0.410     0.701    107\n",
      "    fear     0.411     0.138     0.050    80\n",
      "   happy     0.411     0.296     0.727    77\n",
      " neutral     0.411     1.000     0.032    95\n",
      "     sad     0.411     0.492     0.703    91\n",
      "surprise     0.411     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.411     0.426     0.387    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.612427 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 1.657902  [    0/ 5482]\n",
      "loss: 1.637483  [ 1200/ 5482]\n",
      "loss: 1.692901  [ 2400/ 5482]\n",
      "loss: 1.756219  [ 3600/ 5482]\n",
      "loss: 1.778145  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.413     0.649     0.505    99\n",
      " disgust     0.413     0.419     0.701    107\n",
      "    fear     0.413     0.139     0.062    80\n",
      "   happy     0.413     0.297     0.701    77\n",
      " neutral     0.413     1.000     0.042    95\n",
      "     sad     0.413     0.485     0.703    91\n",
      "surprise     0.413     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.413     0.427     0.388    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.608714 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 1.521930  [    0/ 5482]\n",
      "loss: 1.524443  [ 1200/ 5482]\n",
      "loss: 1.648763  [ 2400/ 5482]\n",
      "loss: 1.779971  [ 3600/ 5482]\n",
      "loss: 1.586193  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.418     0.645     0.495    99\n",
      " disgust     0.418     0.423     0.692    107\n",
      "    fear     0.418     0.133     0.050    80\n",
      "   happy     0.418     0.298     0.727    77\n",
      " neutral     0.418     1.000     0.074    95\n",
      "     sad     0.418     0.485     0.714    91\n",
      "surprise     0.418     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.418     0.426     0.393    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.608492 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 1.617130  [    0/ 5482]\n",
      "loss: 1.728015  [ 1200/ 5482]\n",
      "loss: 1.836143  [ 2400/ 5482]\n",
      "loss: 1.540621  [ 3600/ 5482]\n",
      "loss: 1.705636  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.418     0.681     0.495    99\n",
      " disgust     0.418     0.418     0.710    107\n",
      "    fear     0.418     0.121     0.050    80\n",
      "   happy     0.418     0.296     0.714    77\n",
      " neutral     0.418     1.000     0.074    95\n",
      "     sad     0.418     0.492     0.703    91\n",
      "surprise     0.418     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.418     0.430     0.392    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.605450 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.595646  [    0/ 5482]\n",
      "loss: 1.593505  [ 1200/ 5482]\n",
      "loss: 1.774981  [ 2400/ 5482]\n",
      "loss: 1.803941  [ 3600/ 5482]\n",
      "loss: 1.432411  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.423     0.654     0.515    99\n",
      " disgust     0.423     0.409     0.710    107\n",
      "    fear     0.423     0.148     0.050    80\n",
      "   happy     0.423     0.292     0.649    77\n",
      " neutral     0.423     0.857     0.126    95\n",
      "     sad     0.423     0.485     0.714    91\n",
      "surprise     0.423     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.423     0.406     0.395    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 1.602893 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 1.470676  [    0/ 5482]\n",
      "loss: 1.730271  [ 1200/ 5482]\n",
      "loss: 1.801182  [ 2400/ 5482]\n",
      "loss: 1.549654  [ 3600/ 5482]\n",
      "loss: 1.590197  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.426     0.658     0.505    99\n",
      " disgust     0.426     0.429     0.701    107\n",
      "    fear     0.426     0.143     0.050    80\n",
      "   happy     0.426     0.300     0.740    77\n",
      " neutral     0.426     0.909     0.105    95\n",
      "     sad     0.426     0.492     0.703    91\n",
      "surprise     0.426     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.426     0.419     0.401    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 1.604035 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 1.693200  [    0/ 5482]\n",
      "loss: 1.864378  [ 1200/ 5482]\n",
      "loss: 1.982706  [ 2400/ 5482]\n",
      "loss: 1.584779  [ 3600/ 5482]\n",
      "loss: 1.478892  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.431     0.645     0.495    99\n",
      " disgust     0.431     0.444     0.710    107\n",
      "    fear     0.431     0.160     0.050    80\n",
      "   happy     0.431     0.296     0.753    77\n",
      " neutral     0.431     0.923     0.126    95\n",
      "     sad     0.431     0.496     0.703    91\n",
      "surprise     0.431     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.431     0.423     0.405    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 1.604413 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 1.572065  [    0/ 5482]\n",
      "loss: 1.389791  [ 1200/ 5482]\n",
      "loss: 1.578050  [ 2400/ 5482]\n",
      "loss: 1.648362  [ 3600/ 5482]\n",
      "loss: 1.366735  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.434     0.630     0.515    99\n",
      " disgust     0.434     0.443     0.692    107\n",
      "    fear     0.434     0.174     0.050    80\n",
      "   happy     0.434     0.300     0.740    77\n",
      " neutral     0.434     0.875     0.147    95\n",
      "     sad     0.434     0.489     0.714    91\n",
      "surprise     0.434     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.434     0.416     0.408    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 1.602644 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 1.806633  [    0/ 5482]\n",
      "loss: 1.555252  [ 1200/ 5482]\n",
      "loss: 1.819818  [ 2400/ 5482]\n",
      "loss: 1.730432  [ 3600/ 5482]\n",
      "loss: 1.547880  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.436     0.614     0.515    99\n",
      " disgust     0.436     0.451     0.692    107\n",
      "    fear     0.436     0.138     0.050    80\n",
      "   happy     0.436     0.302     0.714    77\n",
      " neutral     0.436     0.895     0.179    95\n",
      "     sad     0.436     0.489     0.714    91\n",
      "surprise     0.436     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.436     0.413     0.409    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.598881 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 1.549456  [    0/ 5482]\n",
      "loss: 1.604509  [ 1200/ 5482]\n",
      "loss: 1.700155  [ 2400/ 5482]\n",
      "loss: 1.575070  [ 3600/ 5482]\n",
      "loss: 1.739625  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.446     0.662     0.515    99\n",
      " disgust     0.446     0.452     0.710    107\n",
      "    fear     0.446     0.143     0.050    80\n",
      "   happy     0.446     0.302     0.740    77\n",
      " neutral     0.446     0.952     0.211    95\n",
      "     sad     0.446     0.504     0.703    91\n",
      "surprise     0.446     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.446     0.431     0.419    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 1.597690 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 1.492299  [    0/ 5482]\n",
      "loss: 1.668623  [ 1200/ 5482]\n",
      "loss: 1.498700  [ 2400/ 5482]\n",
      "loss: 1.649404  [ 3600/ 5482]\n",
      "loss: 1.573131  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.444     0.630     0.515    99\n",
      " disgust     0.444     0.466     0.701    107\n",
      "    fear     0.444     0.152     0.062    80\n",
      "   happy     0.444     0.296     0.688    77\n",
      " neutral     0.444     0.917     0.232    95\n",
      "     sad     0.444     0.492     0.714    91\n",
      "surprise     0.444     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.444     0.422     0.416    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.595630 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 1.654589  [    0/ 5482]\n",
      "loss: 1.499381  [ 1200/ 5482]\n",
      "loss: 1.533922  [ 2400/ 5482]\n",
      "loss: 1.451667  [ 3600/ 5482]\n",
      "loss: 1.562801  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.449     0.646     0.515    99\n",
      " disgust     0.449     0.455     0.701    107\n",
      "    fear     0.449     0.185     0.062    80\n",
      "   happy     0.449     0.303     0.740    77\n",
      " neutral     0.449     0.917     0.232    95\n",
      "     sad     0.449     0.504     0.703    91\n",
      "surprise     0.449     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.449     0.430     0.422    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 1.595311 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 1.575980  [    0/ 5482]\n",
      "loss: 1.652814  [ 1200/ 5482]\n",
      "loss: 1.633447  [ 2400/ 5482]\n",
      "loss: 1.651307  [ 3600/ 5482]\n",
      "loss: 1.719454  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.454     0.662     0.515    99\n",
      " disgust     0.454     0.447     0.710    107\n",
      "    fear     0.454     0.167     0.062    80\n",
      "   happy     0.454     0.305     0.740    77\n",
      " neutral     0.454     0.923     0.253    95\n",
      "     sad     0.454     0.533     0.703    91\n",
      "surprise     0.454     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.454     0.434     0.426    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.592476 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 1.659591  [    0/ 5482]\n",
      "loss: 1.496219  [ 1200/ 5482]\n",
      "loss: 1.365504  [ 2400/ 5482]\n",
      "loss: 1.547375  [ 3600/ 5482]\n",
      "loss: 1.571473  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.452     0.646     0.515    99\n",
      " disgust     0.452     0.449     0.701    107\n",
      "    fear     0.452     0.192     0.062    80\n",
      "   happy     0.452     0.304     0.753    77\n",
      " neutral     0.452     0.920     0.242    95\n",
      "     sad     0.452     0.525     0.703    91\n",
      "surprise     0.452     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.452     0.434     0.425    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.592542 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 1.451580  [    0/ 5482]\n",
      "loss: 1.489593  [ 1200/ 5482]\n",
      "loss: 1.919258  [ 2400/ 5482]\n",
      "loss: 1.434760  [ 3600/ 5482]\n",
      "loss: 1.706602  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.456     0.628     0.545    99\n",
      " disgust     0.456     0.460     0.692    107\n",
      "    fear     0.456     0.185     0.062    80\n",
      "   happy     0.456     0.302     0.714    77\n",
      " neutral     0.456     0.893     0.263    95\n",
      "     sad     0.456     0.516     0.714    91\n",
      "surprise     0.456     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.456     0.426     0.427    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.590363 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 1.717456  [    0/ 5482]\n",
      "loss: 1.685680  [ 1200/ 5482]\n",
      "loss: 1.649219  [ 2400/ 5482]\n",
      "loss: 1.596152  [ 3600/ 5482]\n",
      "loss: 1.803030  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.449     0.624     0.535    99\n",
      " disgust     0.449     0.471     0.673    107\n",
      "    fear     0.449     0.133     0.050    80\n",
      "   happy     0.449     0.298     0.688    77\n",
      " neutral     0.449     0.900     0.284    95\n",
      "     sad     0.449     0.485     0.714    91\n",
      "surprise     0.449     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.449     0.416     0.421    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 1.588589 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 1.536715  [    0/ 5482]\n",
      "loss: 1.521351  [ 1200/ 5482]\n",
      "loss: 1.672261  [ 2400/ 5482]\n",
      "loss: 1.920173  [ 3600/ 5482]\n",
      "loss: 1.532596  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.454     0.631     0.535    99\n",
      " disgust     0.454     0.471     0.673    107\n",
      "    fear     0.454     0.154     0.050    80\n",
      "   happy     0.454     0.301     0.714    77\n",
      " neutral     0.454     0.903     0.295    95\n",
      "     sad     0.454     0.489     0.714    91\n",
      "surprise     0.454     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.454     0.421     0.426    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.587737 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 1.352061  [    0/ 5482]\n",
      "loss: 1.611349  [ 1200/ 5482]\n",
      "loss: 1.482753  [ 2400/ 5482]\n",
      "loss: 1.399282  [ 3600/ 5482]\n",
      "loss: 1.466195  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.454     0.637     0.515    99\n",
      " disgust     0.454     0.457     0.701    107\n",
      "    fear     0.454     0.160     0.050    80\n",
      "   happy     0.454     0.299     0.753    77\n",
      " neutral     0.454     0.926     0.263    95\n",
      "     sad     0.454     0.533     0.703    91\n",
      "surprise     0.454     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.454     0.430     0.427    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.587825 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 1.742484  [    0/ 5482]\n",
      "loss: 1.463369  [ 1200/ 5482]\n",
      "loss: 1.652969  [ 2400/ 5482]\n",
      "loss: 1.721889  [ 3600/ 5482]\n",
      "loss: 1.664781  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.454     0.637     0.515    99\n",
      " disgust     0.454     0.465     0.692    107\n",
      "    fear     0.454     0.154     0.050    80\n",
      "   happy     0.454     0.301     0.753    77\n",
      " neutral     0.454     0.897     0.274    95\n",
      "     sad     0.454     0.520     0.703    91\n",
      "surprise     0.454     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.454     0.425     0.427    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.585920 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 1.603023  [    0/ 5482]\n",
      "loss: 1.812648  [ 1200/ 5482]\n",
      "loss: 1.607365  [ 2400/ 5482]\n",
      "loss: 1.559274  [ 3600/ 5482]\n",
      "loss: 1.494986  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.459     0.662     0.535    99\n",
      " disgust     0.459     0.459     0.682    107\n",
      "    fear     0.459     0.143     0.050    80\n",
      "   happy     0.459     0.299     0.675    77\n",
      " neutral     0.459     0.868     0.347    95\n",
      "     sad     0.459     0.496     0.714    91\n",
      "surprise     0.459     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.459     0.418     0.429    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 1.580982 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 1.846414  [    0/ 5482]\n",
      "loss: 1.528564  [ 1200/ 5482]\n",
      "loss: 1.543101  [ 2400/ 5482]\n",
      "loss: 1.825988  [ 3600/ 5482]\n",
      "loss: 1.713853  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.464     0.607     0.545    99\n",
      " disgust     0.464     0.497     0.692    107\n",
      "    fear     0.464     0.167     0.050    80\n",
      "   happy     0.464     0.308     0.727    77\n",
      " neutral     0.464     0.909     0.316    95\n",
      "     sad     0.464     0.489     0.714    91\n",
      "surprise     0.464     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.464     0.425     0.435    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.582812 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 1.538795  [    0/ 5482]\n",
      "loss: 1.650094  [ 1200/ 5482]\n",
      "loss: 1.750186  [ 2400/ 5482]\n",
      "loss: 1.514332  [ 3600/ 5482]\n",
      "loss: 1.596669  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.466     0.628     0.545    99\n",
      " disgust     0.466     0.484     0.692    107\n",
      "    fear     0.466     0.167     0.050    80\n",
      "   happy     0.466     0.304     0.714    77\n",
      " neutral     0.466     0.892     0.347    95\n",
      "     sad     0.466     0.496     0.703    91\n",
      "surprise     0.466     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.466     0.424     0.436    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 1.578719 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 1.496753  [    0/ 5482]\n",
      "loss: 1.517786  [ 1200/ 5482]\n",
      "loss: 1.520609  [ 2400/ 5482]\n",
      "loss: 1.886310  [ 3600/ 5482]\n",
      "loss: 1.469430  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.452     0.616     0.535    99\n",
      " disgust     0.452     0.465     0.682    107\n",
      "    fear     0.452     0.087     0.025    80\n",
      "   happy     0.452     0.301     0.766    77\n",
      " neutral     0.452     0.929     0.274    95\n",
      "     sad     0.452     0.525     0.692    91\n",
      "surprise     0.452     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.452     0.418     0.425    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.585492 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 1.687074  [    0/ 5482]\n",
      "loss: 1.823118  [ 1200/ 5482]\n",
      "loss: 1.521283  [ 2400/ 5482]\n",
      "loss: 1.553141  [ 3600/ 5482]\n",
      "loss: 1.587379  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.461     0.639     0.535    99\n",
      " disgust     0.461     0.443     0.654    107\n",
      "    fear     0.461     0.200     0.062    80\n",
      "   happy     0.461     0.303     0.688    77\n",
      " neutral     0.461     0.833     0.368    95\n",
      "     sad     0.461     0.512     0.714    91\n",
      "surprise     0.461     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.461     0.419     0.432    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 1.575956 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 1.596739  [    0/ 5482]\n",
      "loss: 1.480853  [ 1200/ 5482]\n",
      "loss: 1.949671  [ 2400/ 5482]\n",
      "loss: 1.553199  [ 3600/ 5482]\n",
      "loss: 1.342129  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.467     0.624     0.535    99\n",
      " disgust     0.467     0.474     0.692    107\n",
      "    fear     0.467     0.182     0.050    80\n",
      "   happy     0.467     0.299     0.727    77\n",
      " neutral     0.467     0.919     0.358    95\n",
      "     sad     0.467     0.520     0.703    91\n",
      "surprise     0.467     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.467     0.431     0.438    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 1.577245 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 1.465580  [    0/ 5482]\n",
      "loss: 1.441262  [ 1200/ 5482]\n",
      "loss: 1.669212  [ 2400/ 5482]\n",
      "loss: 1.650730  [ 3600/ 5482]\n",
      "loss: 1.548632  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.464     0.624     0.535    99\n",
      " disgust     0.464     0.484     0.692    107\n",
      "    fear     0.464     0.130     0.037    80\n",
      "   happy     0.464     0.295     0.727    77\n",
      " neutral     0.464     0.919     0.358    95\n",
      "     sad     0.464     0.516     0.692    91\n",
      "surprise     0.464     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.464     0.424     0.435    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.577281 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 1.388541  [    0/ 5482]\n",
      "loss: 1.575634  [ 1200/ 5482]\n",
      "loss: 1.632243  [ 2400/ 5482]\n",
      "loss: 1.487903  [ 3600/ 5482]\n",
      "loss: 1.585443  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.470     0.640     0.576    99\n",
      " disgust     0.470     0.449     0.664    107\n",
      "    fear     0.470     0.200     0.062    80\n",
      "   happy     0.470     0.311     0.675    77\n",
      " neutral     0.470     0.841     0.389    95\n",
      "     sad     0.470     0.512     0.714    91\n",
      "surprise     0.470     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.470     0.422     0.440    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 1.570705 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 1.311734  [    0/ 5482]\n",
      "loss: 1.720007  [ 1200/ 5482]\n",
      "loss: 1.547307  [ 2400/ 5482]\n",
      "loss: 1.463408  [ 3600/ 5482]\n",
      "loss: 1.354275  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.618     0.556    99\n",
      " disgust     0.477     0.481     0.701    107\n",
      "    fear     0.477     0.238     0.062    80\n",
      "   happy     0.477     0.306     0.727    77\n",
      " neutral     0.477     0.881     0.389    95\n",
      "     sad     0.477     0.529     0.692    91\n",
      "surprise     0.477     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.436     0.447    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.571650 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 1.515709  [    0/ 5482]\n",
      "loss: 1.442383  [ 1200/ 5482]\n",
      "loss: 1.467271  [ 2400/ 5482]\n",
      "loss: 1.511043  [ 3600/ 5482]\n",
      "loss: 1.517657  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.472     0.629     0.566    99\n",
      " disgust     0.472     0.490     0.692    107\n",
      "    fear     0.472     0.105     0.025    80\n",
      "   happy     0.472     0.299     0.727    77\n",
      " neutral     0.472     0.900     0.379    95\n",
      "     sad     0.472     0.516     0.703    91\n",
      "surprise     0.472     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.472     0.420     0.442    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.572209 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 1.586087  [    0/ 5482]\n",
      "loss: 1.833097  [ 1200/ 5482]\n",
      "loss: 1.551475  [ 2400/ 5482]\n",
      "loss: 1.542607  [ 3600/ 5482]\n",
      "loss: 1.610110  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.475     0.635     0.545    99\n",
      " disgust     0.475     0.484     0.701    107\n",
      "    fear     0.475     0.136     0.037    80\n",
      "   happy     0.475     0.302     0.740    77\n",
      " neutral     0.475     0.884     0.400    95\n",
      "     sad     0.475     0.543     0.692    91\n",
      "surprise     0.475     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.475     0.426     0.445    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.569334 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 1.550888  [    0/ 5482]\n",
      "loss: 1.504795  [ 1200/ 5482]\n",
      "loss: 1.919925  [ 2400/ 5482]\n",
      "loss: 1.623871  [ 3600/ 5482]\n",
      "loss: 1.722040  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.628     0.545    99\n",
      " disgust     0.479     0.487     0.692    107\n",
      "    fear     0.479     0.160     0.050    80\n",
      "   happy     0.479     0.298     0.727    77\n",
      " neutral     0.479     0.872     0.432    95\n",
      "     sad     0.479     0.562     0.692    91\n",
      "surprise     0.479     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.430     0.448    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.567341 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 1.585220  [    0/ 5482]\n",
      "loss: 1.813840  [ 1200/ 5482]\n",
      "loss: 1.616589  [ 2400/ 5482]\n",
      "loss: 1.541759  [ 3600/ 5482]\n",
      "loss: 1.898564  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.466     0.644     0.586    99\n",
      " disgust     0.466     0.449     0.654    107\n",
      "    fear     0.466     0.200     0.062    80\n",
      "   happy     0.466     0.307     0.662    77\n",
      " neutral     0.466     0.783     0.379    95\n",
      "     sad     0.466     0.504     0.703    91\n",
      "surprise     0.466     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.466     0.412     0.435    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 1.565043 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 1.419093  [    0/ 5482]\n",
      "loss: 1.492599  [ 1200/ 5482]\n",
      "loss: 1.588473  [ 2400/ 5482]\n",
      "loss: 1.481157  [ 3600/ 5482]\n",
      "loss: 1.533080  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.472     0.633     0.576    99\n",
      " disgust     0.472     0.487     0.682    107\n",
      "    fear     0.472     0.083     0.025    80\n",
      "   happy     0.472     0.304     0.727    77\n",
      " neutral     0.472     0.902     0.389    95\n",
      "     sad     0.472     0.521     0.692    91\n",
      "surprise     0.472     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.472     0.419     0.442    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.564958 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 1.525496  [    0/ 5482]\n",
      "loss: 1.418832  [ 1200/ 5482]\n",
      "loss: 1.683628  [ 2400/ 5482]\n",
      "loss: 1.540334  [ 3600/ 5482]\n",
      "loss: 1.299991  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.663     0.576    99\n",
      " disgust     0.489     0.463     0.701    107\n",
      "    fear     0.489     0.250     0.050    80\n",
      "   happy     0.489     0.308     0.727    77\n",
      " neutral     0.489     0.840     0.442    95\n",
      "     sad     0.489     0.561     0.703    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.441     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.560918 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 1.489527  [    0/ 5482]\n",
      "loss: 1.514573  [ 1200/ 5482]\n",
      "loss: 1.446668  [ 2400/ 5482]\n",
      "loss: 1.684121  [ 3600/ 5482]\n",
      "loss: 1.666926  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.475     0.655     0.576    99\n",
      " disgust     0.475     0.452     0.654    107\n",
      "    fear     0.475     0.263     0.062    80\n",
      "   happy     0.475     0.308     0.727    77\n",
      " neutral     0.475     0.812     0.411    95\n",
      "     sad     0.475     0.529     0.692    91\n",
      "surprise     0.475     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.475     0.431     0.446    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.560775 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 1.513814  [    0/ 5482]\n",
      "loss: 1.555679  [ 1200/ 5482]\n",
      "loss: 1.797639  [ 2400/ 5482]\n",
      "loss: 1.774688  [ 3600/ 5482]\n",
      "loss: 1.576704  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.620     0.576    99\n",
      " disgust     0.484     0.500     0.692    107\n",
      "    fear     0.484     0.120     0.037    80\n",
      "   happy     0.484     0.303     0.740    77\n",
      " neutral     0.484     0.837     0.432    95\n",
      "     sad     0.484     0.583     0.692    91\n",
      "surprise     0.484     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.423     0.453    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.563585 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 1.574388  [    0/ 5482]\n",
      "loss: 1.349461  [ 1200/ 5482]\n",
      "loss: 1.646765  [ 2400/ 5482]\n",
      "loss: 1.500964  [ 3600/ 5482]\n",
      "loss: 1.547445  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.480     0.617     0.586    99\n",
      " disgust     0.480     0.507     0.692    107\n",
      "    fear     0.480     0.125     0.037    80\n",
      "   happy     0.480     0.305     0.740    77\n",
      " neutral     0.480     0.925     0.389    95\n",
      "     sad     0.480     0.538     0.703    91\n",
      "surprise     0.480     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.480     0.431     0.450    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.563875 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 1.789571  [    0/ 5482]\n",
      "loss: 1.583717  [ 1200/ 5482]\n",
      "loss: 1.414628  [ 2400/ 5482]\n",
      "loss: 1.562498  [ 3600/ 5482]\n",
      "loss: 1.426395  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.624     0.586    99\n",
      " disgust     0.477     0.493     0.654    107\n",
      "    fear     0.477     0.200     0.062    80\n",
      "   happy     0.477     0.315     0.727    77\n",
      " neutral     0.477     0.841     0.389    95\n",
      "     sad     0.477     0.508     0.714    91\n",
      "surprise     0.477     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.426     0.448    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.558917 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 1.598469  [    0/ 5482]\n",
      "loss: 1.692376  [ 1200/ 5482]\n",
      "loss: 1.458152  [ 2400/ 5482]\n",
      "loss: 1.497366  [ 3600/ 5482]\n",
      "loss: 1.559035  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.608     0.596    99\n",
      " disgust     0.482     0.474     0.673    107\n",
      "    fear     0.482     0.200     0.050    80\n",
      "   happy     0.482     0.333     0.714    77\n",
      " neutral     0.482     0.812     0.411    95\n",
      "     sad     0.482     0.508     0.714    91\n",
      "surprise     0.482     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.419     0.451    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.554510 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 1.444893  [    0/ 5482]\n",
      "loss: 1.642687  [ 1200/ 5482]\n",
      "loss: 1.701453  [ 2400/ 5482]\n",
      "loss: 1.363078  [ 3600/ 5482]\n",
      "loss: 1.578453  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.648     0.576    99\n",
      " disgust     0.485     0.503     0.692    107\n",
      "    fear     0.485     0.130     0.037    80\n",
      "   happy     0.485     0.304     0.753    77\n",
      " neutral     0.485     0.872     0.432    95\n",
      "     sad     0.485     0.553     0.692    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.430     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.559657 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 1.838525  [    0/ 5482]\n",
      "loss: 1.652517  [ 1200/ 5482]\n",
      "loss: 1.820604  [ 2400/ 5482]\n",
      "loss: 1.544593  [ 3600/ 5482]\n",
      "loss: 1.669194  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.644     0.586    99\n",
      " disgust     0.489     0.468     0.682    107\n",
      "    fear     0.489     0.240     0.075    80\n",
      "   happy     0.489     0.312     0.714    77\n",
      " neutral     0.489     0.811     0.453    95\n",
      "     sad     0.489     0.573     0.692    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.436     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.550471 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 1.465877  [    0/ 5482]\n",
      "loss: 1.667825  [ 1200/ 5482]\n",
      "loss: 1.353983  [ 2400/ 5482]\n",
      "loss: 1.695008  [ 3600/ 5482]\n",
      "loss: 1.309485  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.600     0.606    99\n",
      " disgust     0.485     0.473     0.664    107\n",
      "    fear     0.485     0.250     0.062    80\n",
      "   happy     0.485     0.329     0.714    77\n",
      " neutral     0.485     0.816     0.421    95\n",
      "     sad     0.485     0.524     0.714    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.428     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.550021 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 1.668939  [    0/ 5482]\n",
      "loss: 1.494838  [ 1200/ 5482]\n",
      "loss: 1.285770  [ 2400/ 5482]\n",
      "loss: 1.657503  [ 3600/ 5482]\n",
      "loss: 1.792882  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.670     0.596    99\n",
      " disgust     0.489     0.470     0.664    107\n",
      "    fear     0.489     0.235     0.050    80\n",
      "   happy     0.489     0.311     0.740    77\n",
      " neutral     0.489     0.768     0.453    95\n",
      "     sad     0.489     0.557     0.703    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.430     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.548840 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 1.508324  [    0/ 5482]\n",
      "loss: 1.676027  [ 1200/ 5482]\n",
      "loss: 1.596559  [ 2400/ 5482]\n",
      "loss: 1.355124  [ 3600/ 5482]\n",
      "loss: 1.381031  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.587     0.616    99\n",
      " disgust     0.479     0.515     0.636    107\n",
      "    fear     0.479     0.083     0.025    80\n",
      "   happy     0.479     0.326     0.740    77\n",
      " neutral     0.479     0.848     0.411    95\n",
      "     sad     0.479     0.504     0.714    91\n",
      "surprise     0.479     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.409     0.449    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.554647 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 1.658282  [    0/ 5482]\n",
      "loss: 1.588271  [ 1200/ 5482]\n",
      "loss: 1.405022  [ 2400/ 5482]\n",
      "loss: 1.716115  [ 3600/ 5482]\n",
      "loss: 1.571065  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.639     0.626    99\n",
      " disgust     0.490     0.470     0.664    107\n",
      "    fear     0.490     0.190     0.050    80\n",
      "   happy     0.490     0.333     0.727    77\n",
      " neutral     0.490     0.750     0.442    95\n",
      "     sad     0.490     0.547     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.419     0.459    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.544727 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 1.544851  [    0/ 5482]\n",
      "loss: 1.563997  [ 1200/ 5482]\n",
      "loss: 1.548645  [ 2400/ 5482]\n",
      "loss: 1.508138  [ 3600/ 5482]\n",
      "loss: 1.535888  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.598     0.616    99\n",
      " disgust     0.484     0.490     0.664    107\n",
      "    fear     0.484     0.045     0.013    80\n",
      "   happy     0.484     0.320     0.727    77\n",
      " neutral     0.484     0.778     0.442    95\n",
      "     sad     0.484     0.571     0.703    91\n",
      "surprise     0.484     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.400     0.452    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.545777 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 1.747650  [    0/ 5482]\n",
      "loss: 1.772709  [ 1200/ 5482]\n",
      "loss: 1.482093  [ 2400/ 5482]\n",
      "loss: 1.275384  [ 3600/ 5482]\n",
      "loss: 1.599373  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.610     0.616    99\n",
      " disgust     0.484     0.500     0.654    107\n",
      "    fear     0.484     0.077     0.025    80\n",
      "   happy     0.484     0.315     0.727    77\n",
      " neutral     0.484     0.840     0.442    95\n",
      "     sad     0.484     0.552     0.703    91\n",
      "surprise     0.484     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.413     0.453    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.547106 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 1.487376  [    0/ 5482]\n",
      "loss: 1.493412  [ 1200/ 5482]\n",
      "loss: 1.243398  [ 2400/ 5482]\n",
      "loss: 1.798832  [ 3600/ 5482]\n",
      "loss: 1.466200  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.497     0.663     0.616    99\n",
      " disgust     0.497     0.471     0.682    107\n",
      "    fear     0.497     0.217     0.062    80\n",
      "   happy     0.497     0.327     0.727    77\n",
      " neutral     0.497     0.759     0.463    95\n",
      "     sad     0.497     0.577     0.703    91\n",
      "surprise     0.497     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.497     0.431     0.465    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.540138 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 1.395625  [    0/ 5482]\n",
      "loss: 1.379299  [ 1200/ 5482]\n",
      "loss: 1.485554  [ 2400/ 5482]\n",
      "loss: 1.429627  [ 3600/ 5482]\n",
      "loss: 1.571981  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.480     0.585     0.626    99\n",
      " disgust     0.480     0.500     0.654    107\n",
      "    fear     0.480     0.100     0.025    80\n",
      "   happy     0.480     0.316     0.701    77\n",
      " neutral     0.480     0.833     0.421    95\n",
      "     sad     0.480     0.520     0.714    91\n",
      "surprise     0.480     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.480     0.408     0.449    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.544921 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 1.627385  [    0/ 5482]\n",
      "loss: 1.552460  [ 1200/ 5482]\n",
      "loss: 1.297686  [ 2400/ 5482]\n",
      "loss: 1.504073  [ 3600/ 5482]\n",
      "loss: 1.399839  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.646     0.626    99\n",
      " disgust     0.490     0.467     0.654    107\n",
      "    fear     0.490     0.158     0.037    80\n",
      "   happy     0.490     0.318     0.740    77\n",
      " neutral     0.490     0.782     0.453    95\n",
      "     sad     0.490     0.577     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.421     0.459    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.540179 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 1.717784  [    0/ 5482]\n",
      "loss: 1.770979  [ 1200/ 5482]\n",
      "loss: 1.413517  [ 2400/ 5482]\n",
      "loss: 1.598031  [ 3600/ 5482]\n",
      "loss: 1.484741  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.642     0.616    99\n",
      " disgust     0.492     0.500     0.692    107\n",
      "    fear     0.492     0.080     0.025    80\n",
      "   happy     0.492     0.314     0.753    77\n",
      " neutral     0.492     0.808     0.442    95\n",
      "     sad     0.492     0.600     0.692    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.420     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.542369 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 1.420570  [    0/ 5482]\n",
      "loss: 1.371310  [ 1200/ 5482]\n",
      "loss: 1.308943  [ 2400/ 5482]\n",
      "loss: 1.584194  [ 3600/ 5482]\n",
      "loss: 1.749796  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.634     0.596    99\n",
      " disgust     0.489     0.483     0.673    107\n",
      "    fear     0.489     0.143     0.037    80\n",
      "   happy     0.489     0.308     0.779    77\n",
      " neutral     0.489     0.837     0.432    95\n",
      "     sad     0.489     0.612     0.692    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.431     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.545325 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 1.868985  [    0/ 5482]\n",
      "loss: 1.440235  [ 1200/ 5482]\n",
      "loss: 1.539663  [ 2400/ 5482]\n",
      "loss: 1.649603  [ 3600/ 5482]\n",
      "loss: 1.511439  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.633     0.626    99\n",
      " disgust     0.492     0.507     0.682    107\n",
      "    fear     0.492     0.136     0.037    80\n",
      "   happy     0.492     0.311     0.727    77\n",
      " neutral     0.492     0.808     0.442    95\n",
      "     sad     0.492     0.561     0.703    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.422     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.540170 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 1.437914  [    0/ 5482]\n",
      "loss: 1.980579  [ 1200/ 5482]\n",
      "loss: 1.462851  [ 2400/ 5482]\n",
      "loss: 1.480016  [ 3600/ 5482]\n",
      "loss: 1.467217  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.646     0.626    99\n",
      " disgust     0.487     0.507     0.664    107\n",
      "    fear     0.487     0.087     0.025    80\n",
      "   happy     0.487     0.309     0.727    77\n",
      " neutral     0.487     0.778     0.442    95\n",
      "     sad     0.487     0.552     0.703    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.411     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.537532 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 1.651013  [    0/ 5482]\n",
      "loss: 1.478153  [ 1200/ 5482]\n",
      "loss: 1.660784  [ 2400/ 5482]\n",
      "loss: 1.574544  [ 3600/ 5482]\n",
      "loss: 1.891185  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.653     0.626    99\n",
      " disgust     0.492     0.470     0.654    107\n",
      "    fear     0.492     0.227     0.062    80\n",
      "   happy     0.492     0.320     0.714    77\n",
      " neutral     0.492     0.733     0.463    95\n",
      "     sad     0.492     0.571     0.703    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.425     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.532369 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 1.507528  [    0/ 5482]\n",
      "loss: 1.600664  [ 1200/ 5482]\n",
      "loss: 1.461349  [ 2400/ 5482]\n",
      "loss: 1.465564  [ 3600/ 5482]\n",
      "loss: 1.531616  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.618     0.636    99\n",
      " disgust     0.482     0.469     0.645    107\n",
      "    fear     0.482     0.176     0.037    80\n",
      "   happy     0.482     0.312     0.688    77\n",
      " neutral     0.482     0.724     0.442    95\n",
      "     sad     0.482     0.552     0.703    91\n",
      "surprise     0.482     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.407     0.450    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.531388 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 1.482244  [    0/ 5482]\n",
      "loss: 1.329822  [ 1200/ 5482]\n",
      "loss: 1.480680  [ 2400/ 5482]\n",
      "loss: 1.479118  [ 3600/ 5482]\n",
      "loss: 1.432653  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.606     0.636    99\n",
      " disgust     0.493     0.510     0.682    107\n",
      "    fear     0.493     0.130     0.037    80\n",
      "   happy     0.493     0.311     0.714    77\n",
      " neutral     0.493     0.782     0.453    95\n",
      "     sad     0.493     0.593     0.703    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.419     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.534426 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 1.513115  [    0/ 5482]\n",
      "loss: 1.542507  [ 1200/ 5482]\n",
      "loss: 1.476862  [ 2400/ 5482]\n",
      "loss: 1.443426  [ 3600/ 5482]\n",
      "loss: 1.782842  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.608     0.626    99\n",
      " disgust     0.479     0.471     0.607    107\n",
      "    fear     0.479     0.136     0.037    80\n",
      "   happy     0.479     0.316     0.714    77\n",
      " neutral     0.479     0.717     0.453    95\n",
      "     sad     0.479     0.561     0.703    91\n",
      "surprise     0.479     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.401     0.449    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.531142 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 1.607195  [    0/ 5482]\n",
      "loss: 1.422811  [ 1200/ 5482]\n",
      "loss: 1.705455  [ 2400/ 5482]\n",
      "loss: 1.470595  [ 3600/ 5482]\n",
      "loss: 1.422817  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.612     0.636    99\n",
      " disgust     0.482     0.469     0.645    107\n",
      "    fear     0.482     0.158     0.037    80\n",
      "   happy     0.482     0.306     0.675    77\n",
      " neutral     0.482     0.741     0.453    95\n",
      "     sad     0.482     0.566     0.703    91\n",
      "surprise     0.482     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.408     0.450    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.528379 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 1.710642  [    0/ 5482]\n",
      "loss: 1.631632  [ 1200/ 5482]\n",
      "loss: 1.479718  [ 2400/ 5482]\n",
      "loss: 1.390393  [ 3600/ 5482]\n",
      "loss: 1.734639  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.480     0.600     0.636    99\n",
      " disgust     0.480     0.472     0.626    107\n",
      "    fear     0.480     0.222     0.050    80\n",
      "   happy     0.480     0.319     0.662    77\n",
      " neutral     0.480     0.717     0.453    95\n",
      "     sad     0.480     0.520     0.714    91\n",
      "surprise     0.480     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.480     0.407     0.449    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.525990 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 1.414533  [    0/ 5482]\n",
      "loss: 1.635071  [ 1200/ 5482]\n",
      "loss: 1.744774  [ 2400/ 5482]\n",
      "loss: 1.567478  [ 3600/ 5482]\n",
      "loss: 1.751227  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.630     0.636    99\n",
      " disgust     0.485     0.456     0.636    107\n",
      "    fear     0.485     0.150     0.037    80\n",
      "   happy     0.485     0.309     0.701    77\n",
      " neutral     0.485     0.733     0.463    95\n",
      "     sad     0.485     0.604     0.703    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.412     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.525736 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 1.563506  [    0/ 5482]\n",
      "loss: 1.485802  [ 1200/ 5482]\n",
      "loss: 1.579114  [ 2400/ 5482]\n",
      "loss: 1.595489  [ 3600/ 5482]\n",
      "loss: 1.652345  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.612     0.636    99\n",
      " disgust     0.484     0.471     0.617    107\n",
      "    fear     0.484     0.185     0.062    80\n",
      "   happy     0.484     0.317     0.662    77\n",
      " neutral     0.484     0.692     0.474    95\n",
      "     sad     0.484     0.570     0.714    91\n",
      "surprise     0.484     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.407     0.452    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.523686 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 1.334020  [    0/ 5482]\n",
      "loss: 1.618317  [ 1200/ 5482]\n",
      "loss: 1.616351  [ 2400/ 5482]\n",
      "loss: 1.402743  [ 3600/ 5482]\n",
      "loss: 1.582566  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.589     0.636    99\n",
      " disgust     0.489     0.507     0.682    107\n",
      "    fear     0.489     0.083     0.025    80\n",
      "   happy     0.489     0.306     0.688    77\n",
      " neutral     0.489     0.811     0.453    95\n",
      "     sad     0.489     0.587     0.703    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.412     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.524994 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 1.687241  [    0/ 5482]\n",
      "loss: 1.588778  [ 1200/ 5482]\n",
      "loss: 1.420559  [ 2400/ 5482]\n",
      "loss: 1.535812  [ 3600/ 5482]\n",
      "loss: 1.541753  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.583     0.636    99\n",
      " disgust     0.477     0.482     0.617    107\n",
      "    fear     0.477     0.167     0.037    80\n",
      "   happy     0.477     0.309     0.662    77\n",
      " neutral     0.477     0.721     0.463    95\n",
      "     sad     0.477     0.529     0.703    91\n",
      "surprise     0.477     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.399     0.446    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.522268 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 1.467668  [    0/ 5482]\n",
      "loss: 1.419646  [ 1200/ 5482]\n",
      "loss: 1.464866  [ 2400/ 5482]\n",
      "loss: 1.411657  [ 3600/ 5482]\n",
      "loss: 1.558478  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.475     0.562     0.636    99\n",
      " disgust     0.475     0.504     0.607    107\n",
      "    fear     0.475     0.045     0.013    80\n",
      "   happy     0.475     0.303     0.701    77\n",
      " neutral     0.475     0.754     0.453    95\n",
      "     sad     0.475     0.571     0.703    91\n",
      "surprise     0.475     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.475     0.392     0.445    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.528702 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 1.436924  [    0/ 5482]\n",
      "loss: 1.774963  [ 1200/ 5482]\n",
      "loss: 1.724937  [ 2400/ 5482]\n",
      "loss: 1.519847  [ 3600/ 5482]\n",
      "loss: 1.569987  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.606     0.636    99\n",
      " disgust     0.489     0.464     0.664    107\n",
      "    fear     0.489     0.105     0.025    80\n",
      "   happy     0.489     0.306     0.675    77\n",
      " neutral     0.489     0.742     0.484    95\n",
      "     sad     0.489     0.627     0.703    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.407     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.518047 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 1.620693  [    0/ 5482]\n",
      "loss: 1.602080  [ 1200/ 5482]\n",
      "loss: 1.706322  [ 2400/ 5482]\n",
      "loss: 1.311647  [ 3600/ 5482]\n",
      "loss: 1.565004  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.618     0.636    99\n",
      " disgust     0.487     0.473     0.654    107\n",
      "    fear     0.487     0.105     0.025    80\n",
      "   happy     0.487     0.306     0.675    77\n",
      " neutral     0.487     0.750     0.474    95\n",
      "     sad     0.487     0.586     0.714    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.405     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.515702 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 1.380840  [    0/ 5482]\n",
      "loss: 1.495904  [ 1200/ 5482]\n",
      "loss: 1.501571  [ 2400/ 5482]\n",
      "loss: 1.507444  [ 3600/ 5482]\n",
      "loss: 1.453291  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.618     0.636    99\n",
      " disgust     0.485     0.469     0.636    107\n",
      "    fear     0.485     0.143     0.037    80\n",
      "   happy     0.485     0.305     0.688    77\n",
      " neutral     0.485     0.726     0.474    95\n",
      "     sad     0.485     0.604     0.703    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.409     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.516510 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 1.652985  [    0/ 5482]\n",
      "loss: 1.603684  [ 1200/ 5482]\n",
      "loss: 1.795777  [ 2400/ 5482]\n",
      "loss: 1.406415  [ 3600/ 5482]\n",
      "loss: 1.458190  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.622     0.616    99\n",
      " disgust     0.487     0.479     0.654    107\n",
      "    fear     0.487     0.083     0.025    80\n",
      "   happy     0.487     0.303     0.740    77\n",
      " neutral     0.487     0.763     0.474    95\n",
      "     sad     0.487     0.653     0.681    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.415     0.456    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.521725 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 1.626722  [    0/ 5482]\n",
      "loss: 1.479135  [ 1200/ 5482]\n",
      "loss: 1.366752  [ 2400/ 5482]\n",
      "loss: 1.637314  [ 3600/ 5482]\n",
      "loss: 1.677875  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.618     0.636    99\n",
      " disgust     0.485     0.476     0.645    107\n",
      "    fear     0.485     0.143     0.037    80\n",
      "   happy     0.485     0.308     0.675    77\n",
      " neutral     0.485     0.721     0.463    95\n",
      "     sad     0.485     0.580     0.714    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.407     0.453    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.512527 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 1.511219  [    0/ 5482]\n",
      "loss: 1.315271  [ 1200/ 5482]\n",
      "loss: 1.528208  [ 2400/ 5482]\n",
      "loss: 1.464999  [ 3600/ 5482]\n",
      "loss: 1.694658  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.583     0.636    99\n",
      " disgust     0.484     0.482     0.626    107\n",
      "    fear     0.484     0.125     0.037    80\n",
      "   happy     0.484     0.301     0.675    77\n",
      " neutral     0.484     0.730     0.484    95\n",
      "     sad     0.484     0.621     0.703    91\n",
      "surprise     0.484     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.406     0.452    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.516357 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 1.520860  [    0/ 5482]\n",
      "loss: 1.377543  [ 1200/ 5482]\n",
      "loss: 1.378328  [ 2400/ 5482]\n",
      "loss: 1.546014  [ 3600/ 5482]\n",
      "loss: 1.481992  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.636     0.636    99\n",
      " disgust     0.489     0.459     0.626    107\n",
      "    fear     0.489     0.105     0.025    80\n",
      "   happy     0.489     0.305     0.688    77\n",
      " neutral     0.489     0.696     0.505    95\n",
      "     sad     0.489     0.631     0.714    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.405     0.456    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.509820 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 1.521039  [    0/ 5482]\n",
      "loss: 1.597032  [ 1200/ 5482]\n",
      "loss: 1.683904  [ 2400/ 5482]\n",
      "loss: 1.433833  [ 3600/ 5482]\n",
      "loss: 1.800977  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.624     0.636    99\n",
      " disgust     0.489     0.479     0.645    107\n",
      "    fear     0.489     0.087     0.025    80\n",
      "   happy     0.489     0.306     0.714    77\n",
      " neutral     0.489     0.738     0.474    95\n",
      "     sad     0.489     0.634     0.703    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.410     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.514512 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 1.170392  [    0/ 5482]\n",
      "loss: 1.659346  [ 1200/ 5482]\n",
      "loss: 1.499544  [ 2400/ 5482]\n",
      "loss: 1.749134  [ 3600/ 5482]\n",
      "loss: 1.554641  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.620     0.626    99\n",
      " disgust     0.487     0.486     0.636    107\n",
      "    fear     0.487     0.080     0.025    80\n",
      "   happy     0.487     0.302     0.701    77\n",
      " neutral     0.487     0.712     0.495    95\n",
      "     sad     0.487     0.640     0.703    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.406     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.511732 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 1.670020  [    0/ 5482]\n",
      "loss: 1.563808  [ 1200/ 5482]\n",
      "loss: 1.615923  [ 2400/ 5482]\n",
      "loss: 1.272660  [ 3600/ 5482]\n",
      "loss: 1.458985  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.608     0.626    99\n",
      " disgust     0.489     0.479     0.636    107\n",
      "    fear     0.489     0.059     0.013    80\n",
      "   happy     0.489     0.297     0.740    77\n",
      " neutral     0.489     0.754     0.484    95\n",
      "     sad     0.489     0.667     0.703    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.409     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.516424 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 1.482677  [    0/ 5482]\n",
      "loss: 1.341688  [ 1200/ 5482]\n",
      "loss: 1.681686  [ 2400/ 5482]\n",
      "loss: 1.594196  [ 3600/ 5482]\n",
      "loss: 1.549911  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.589     0.636    99\n",
      " disgust     0.479     0.468     0.607    107\n",
      "    fear     0.479     0.158     0.037    80\n",
      "   happy     0.479     0.306     0.675    77\n",
      " neutral     0.479     0.682     0.474    95\n",
      "     sad     0.479     0.587     0.703    91\n",
      "surprise     0.479     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.398     0.448    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.508041 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 1.523651  [    0/ 5482]\n",
      "loss: 1.515179  [ 1200/ 5482]\n",
      "loss: 1.563556  [ 2400/ 5482]\n",
      "loss: 1.414604  [ 3600/ 5482]\n",
      "loss: 1.574514  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.573     0.636    99\n",
      " disgust     0.477     0.493     0.626    107\n",
      "    fear     0.477     0.087     0.025    80\n",
      "   happy     0.477     0.298     0.662    77\n",
      " neutral     0.477     0.710     0.463    95\n",
      "     sad     0.477     0.593     0.703    91\n",
      "surprise     0.477     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.393     0.445    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.508494 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 1.584821  [    0/ 5482]\n",
      "loss: 1.445847  [ 1200/ 5482]\n",
      "loss: 1.631203  [ 2400/ 5482]\n",
      "loss: 1.615529  [ 3600/ 5482]\n",
      "loss: 1.601782  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.594     0.636    99\n",
      " disgust     0.487     0.466     0.645    107\n",
      "    fear     0.487     0.150     0.037    80\n",
      "   happy     0.487     0.306     0.675    77\n",
      " neutral     0.487     0.719     0.484    95\n",
      "     sad     0.487     0.627     0.703    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.409     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.503389 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 1.586276  [    0/ 5482]\n",
      "loss: 1.498406  [ 1200/ 5482]\n",
      "loss: 1.338844  [ 2400/ 5482]\n",
      "loss: 1.560174  [ 3600/ 5482]\n",
      "loss: 1.352560  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.589     0.636    99\n",
      " disgust     0.482     0.500     0.626    107\n",
      "    fear     0.482     0.087     0.025    80\n",
      "   happy     0.482     0.305     0.701    77\n",
      " neutral     0.482     0.733     0.463    95\n",
      "     sad     0.482     0.587     0.703    91\n",
      "surprise     0.482     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.400     0.451    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.508628 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 1.721843  [    0/ 5482]\n",
      "loss: 1.561767  [ 1200/ 5482]\n",
      "loss: 1.746749  [ 2400/ 5482]\n",
      "loss: 1.630299  [ 3600/ 5482]\n",
      "loss: 1.394070  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.542     0.646    99\n",
      " disgust     0.479     0.508     0.617    107\n",
      "    fear     0.479     0.062     0.013    80\n",
      "   happy     0.479     0.313     0.675    77\n",
      " neutral     0.479     0.759     0.463    95\n",
      "     sad     0.479     0.533     0.714    91\n",
      "surprise     0.479     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.388     0.447    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.505161 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 1.606573  [    0/ 5482]\n",
      "loss: 1.397551  [ 1200/ 5482]\n",
      "loss: 1.299133  [ 2400/ 5482]\n",
      "loss: 1.457622  [ 3600/ 5482]\n",
      "loss: 1.514287  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.589     0.636    99\n",
      " disgust     0.487     0.479     0.645    107\n",
      "    fear     0.487     0.167     0.037    80\n",
      "   happy     0.487     0.312     0.688    77\n",
      " neutral     0.487     0.721     0.463    95\n",
      "     sad     0.487     0.591     0.714    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.408     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.499588 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 1.575759  [    0/ 5482]\n",
      "loss: 1.629388  [ 1200/ 5482]\n",
      "loss: 1.586160  [ 2400/ 5482]\n",
      "loss: 1.411878  [ 3600/ 5482]\n",
      "loss: 1.555148  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.589     0.636    99\n",
      " disgust     0.485     0.486     0.636    107\n",
      "    fear     0.485     0.105     0.025    80\n",
      "   happy     0.485     0.314     0.688    77\n",
      " neutral     0.485     0.682     0.474    95\n",
      "     sad     0.485     0.596     0.714    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.396     0.453    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.498589 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 1.407928  [    0/ 5482]\n",
      "loss: 1.469062  [ 1200/ 5482]\n",
      "loss: 1.504510  [ 2400/ 5482]\n",
      "loss: 1.714446  [ 3600/ 5482]\n",
      "loss: 1.342524  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.583     0.636    99\n",
      " disgust     0.493     0.497     0.664    107\n",
      "    fear     0.493     0.053     0.013    80\n",
      "   happy     0.493     0.310     0.701    77\n",
      " neutral     0.493     0.774     0.505    95\n",
      "     sad     0.493     0.615     0.703    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.405     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.500312 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 1.448848  [    0/ 5482]\n",
      "loss: 1.777120  [ 1200/ 5482]\n",
      "loss: 1.540952  [ 2400/ 5482]\n",
      "loss: 1.618654  [ 3600/ 5482]\n",
      "loss: 1.657558  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.480     0.573     0.636    99\n",
      " disgust     0.480     0.471     0.607    107\n",
      "    fear     0.480     0.056     0.013    80\n",
      "   happy     0.480     0.303     0.688    77\n",
      " neutral     0.480     0.746     0.495    95\n",
      "     sad     0.480     0.604     0.703    91\n",
      "surprise     0.480     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.480     0.393     0.449    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.499816 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 1.658350  [    0/ 5482]\n",
      "loss: 1.781312  [ 1200/ 5482]\n",
      "loss: 1.359681  [ 2400/ 5482]\n",
      "loss: 1.664627  [ 3600/ 5482]\n",
      "loss: 1.454874  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.612     0.636    99\n",
      " disgust     0.482     0.475     0.626    107\n",
      "    fear     0.482     0.111     0.025    80\n",
      "   happy     0.482     0.305     0.688    77\n",
      " neutral     0.482     0.692     0.474    95\n",
      "     sad     0.482     0.587     0.703    91\n",
      "surprise     0.482     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.397     0.450    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.496506 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 1.501825  [    0/ 5482]\n",
      "loss: 1.790319  [ 1200/ 5482]\n",
      "loss: 1.405665  [ 2400/ 5482]\n",
      "loss: 1.481749  [ 3600/ 5482]\n",
      "loss: 1.303221  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.612     0.636    99\n",
      " disgust     0.487     0.462     0.626    107\n",
      "    fear     0.487     0.143     0.037    80\n",
      "   happy     0.487     0.301     0.688    77\n",
      " neutral     0.487     0.727     0.505    95\n",
      "     sad     0.487     0.636     0.692    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.412     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.495350 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 1.242566  [    0/ 5482]\n",
      "loss: 1.507198  [ 1200/ 5482]\n",
      "loss: 1.542663  [ 2400/ 5482]\n",
      "loss: 1.535923  [ 3600/ 5482]\n",
      "loss: 1.567587  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.472     0.508     0.646    99\n",
      " disgust     0.472     0.517     0.579    107\n",
      "    fear     0.472     0.000     0.000    80\n",
      "   happy     0.472     0.301     0.688    77\n",
      " neutral     0.472     0.815     0.463    95\n",
      "     sad     0.472     0.533     0.714    91\n",
      "surprise     0.472     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.472     0.382     0.442    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.508935 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 1.414095  [    0/ 5482]\n",
      "loss: 1.646238  [ 1200/ 5482]\n",
      "loss: 1.265588  [ 2400/ 5482]\n",
      "loss: 1.443470  [ 3600/ 5482]\n",
      "loss: 1.492614  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.594     0.636    99\n",
      " disgust     0.490     0.472     0.626    107\n",
      "    fear     0.490     0.111     0.025    80\n",
      "   happy     0.490     0.303     0.688    77\n",
      " neutral     0.490     0.725     0.526    95\n",
      "     sad     0.490     0.640     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.406     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.492044 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 1.446832  [    0/ 5482]\n",
      "loss: 1.393154  [ 1200/ 5482]\n",
      "loss: 1.281223  [ 2400/ 5482]\n",
      "loss: 1.318042  [ 3600/ 5482]\n",
      "loss: 1.349983  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.583     0.636    99\n",
      " disgust     0.477     0.504     0.617    107\n",
      "    fear     0.477     0.053     0.013    80\n",
      "   happy     0.477     0.298     0.701    77\n",
      " neutral     0.477     0.741     0.453    95\n",
      "     sad     0.477     0.566     0.703    91\n",
      "surprise     0.477     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.392     0.446    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.497269 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 1.689524  [    0/ 5482]\n",
      "loss: 1.292907  [ 1200/ 5482]\n",
      "loss: 1.527083  [ 2400/ 5482]\n",
      "loss: 1.606596  [ 3600/ 5482]\n",
      "loss: 1.493822  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.552     0.646    99\n",
      " disgust     0.487     0.500     0.645    107\n",
      "    fear     0.487     0.125     0.025    80\n",
      "   happy     0.487     0.317     0.662    77\n",
      " neutral     0.487     0.719     0.484    95\n",
      "     sad     0.487     0.565     0.714    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.397     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.488077 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 1.618524  [    0/ 5482]\n",
      "loss: 1.390935  [ 1200/ 5482]\n",
      "loss: 1.312097  [ 2400/ 5482]\n",
      "loss: 1.401347  [ 3600/ 5482]\n",
      "loss: 1.468423  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.598     0.646    99\n",
      " disgust     0.498     0.479     0.645    107\n",
      "    fear     0.498     0.200     0.037    80\n",
      "   happy     0.498     0.305     0.688    77\n",
      " neutral     0.498     0.725     0.526    95\n",
      "     sad     0.498     0.644     0.714    91\n",
      "surprise     0.498     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.421     0.465    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.486455 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 1.502050  [    0/ 5482]\n",
      "loss: 1.333496  [ 1200/ 5482]\n",
      "loss: 1.414313  [ 2400/ 5482]\n",
      "loss: 1.413749  [ 3600/ 5482]\n",
      "loss: 1.558015  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.593     0.646    99\n",
      " disgust     0.489     0.478     0.617    107\n",
      "    fear     0.489     0.133     0.025    80\n",
      "   happy     0.489     0.302     0.675    77\n",
      " neutral     0.489     0.721     0.516    95\n",
      "     sad     0.489     0.596     0.714    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.403     0.456    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.485497 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 1.506766  [    0/ 5482]\n",
      "loss: 1.376439  [ 1200/ 5482]\n",
      "loss: 1.585747  [ 2400/ 5482]\n",
      "loss: 1.489727  [ 3600/ 5482]\n",
      "loss: 1.309867  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.568     0.636    99\n",
      " disgust     0.479     0.474     0.598    107\n",
      "    fear     0.479     0.118     0.025    80\n",
      "   happy     0.479     0.313     0.662    77\n",
      " neutral     0.479     0.716     0.505    95\n",
      "     sad     0.479     0.547     0.703    91\n",
      "surprise     0.479     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.391     0.447    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.485402 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 1.499686  [    0/ 5482]\n",
      "loss: 1.335187  [ 1200/ 5482]\n",
      "loss: 1.474937  [ 2400/ 5482]\n",
      "loss: 1.622211  [ 3600/ 5482]\n",
      "loss: 1.624334  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.618     0.636    99\n",
      " disgust     0.490     0.482     0.626    107\n",
      "    fear     0.490     0.188     0.037    80\n",
      "   happy     0.490     0.301     0.688    77\n",
      " neutral     0.490     0.700     0.516    95\n",
      "     sad     0.490     0.598     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.412     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.485244 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 1.556443  [    0/ 5482]\n",
      "loss: 1.348732  [ 1200/ 5482]\n",
      "loss: 1.777085  [ 2400/ 5482]\n",
      "loss: 1.564045  [ 3600/ 5482]\n",
      "loss: 1.300496  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.593     0.646    99\n",
      " disgust     0.490     0.485     0.598    107\n",
      "    fear     0.490     0.118     0.025    80\n",
      "   happy     0.490     0.301     0.688    77\n",
      " neutral     0.490     0.712     0.547    95\n",
      "     sad     0.490     0.615     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.403     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.484752 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 1.732710  [    0/ 5482]\n",
      "loss: 1.271757  [ 1200/ 5482]\n",
      "loss: 1.471879  [ 2400/ 5482]\n",
      "loss: 1.743949  [ 3600/ 5482]\n",
      "loss: 1.539086  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.557     0.646    99\n",
      " disgust     0.485     0.496     0.607    107\n",
      "    fear     0.485     0.062     0.013    80\n",
      "   happy     0.485     0.297     0.701    77\n",
      " neutral     0.485     0.762     0.505    95\n",
      "     sad     0.485     0.621     0.703    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.399     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.492590 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 1.672893  [    0/ 5482]\n",
      "loss: 1.356071  [ 1200/ 5482]\n",
      "loss: 1.317304  [ 2400/ 5482]\n",
      "loss: 1.257775  [ 3600/ 5482]\n",
      "loss: 1.689252  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.587     0.646    99\n",
      " disgust     0.490     0.475     0.626    107\n",
      "    fear     0.490     0.125     0.025    80\n",
      "   happy     0.490     0.298     0.688    77\n",
      " neutral     0.490     0.746     0.526    95\n",
      "     sad     0.490     0.636     0.692    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.410     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.483394 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 1.506140  [    0/ 5482]\n",
      "loss: 1.463716  [ 1200/ 5482]\n",
      "loss: 1.715964  [ 2400/ 5482]\n",
      "loss: 1.598835  [ 3600/ 5482]\n",
      "loss: 1.399971  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.606     0.636    99\n",
      " disgust     0.502     0.460     0.645    107\n",
      "    fear     0.502     0.211     0.050    80\n",
      "   happy     0.502     0.319     0.688    77\n",
      " neutral     0.502     0.693     0.547    95\n",
      "     sad     0.502     0.677     0.714    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.424     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.476422 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 1.515505  [    0/ 5482]\n",
      "loss: 1.679968  [ 1200/ 5482]\n",
      "loss: 1.416283  [ 2400/ 5482]\n",
      "loss: 1.323701  [ 3600/ 5482]\n",
      "loss: 1.378729  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.477     0.546     0.657    99\n",
      " disgust     0.477     0.492     0.589    107\n",
      "    fear     0.477     0.077     0.013    80\n",
      "   happy     0.477     0.304     0.662    77\n",
      " neutral     0.477     0.750     0.474    95\n",
      "     sad     0.477     0.541     0.725    91\n",
      "surprise     0.477     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.477     0.387     0.446    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.480624 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 1.477193  [    0/ 5482]\n",
      "loss: 1.657029  [ 1200/ 5482]\n",
      "loss: 1.549361  [ 2400/ 5482]\n",
      "loss: 1.327566  [ 3600/ 5482]\n",
      "loss: 1.368522  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.598     0.646    99\n",
      " disgust     0.500     0.486     0.626    107\n",
      "    fear     0.500     0.214     0.037    80\n",
      "   happy     0.500     0.317     0.675    77\n",
      " neutral     0.500     0.684     0.568    95\n",
      "     sad     0.500     0.602     0.714    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.414     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.475294 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 1.659751  [    0/ 5482]\n",
      "loss: 1.414291  [ 1200/ 5482]\n",
      "loss: 1.438286  [ 2400/ 5482]\n",
      "loss: 1.600335  [ 3600/ 5482]\n",
      "loss: 1.556715  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.475     0.524     0.657    99\n",
      " disgust     0.475     0.500     0.579    107\n",
      "    fear     0.475     0.071     0.013    80\n",
      "   happy     0.475     0.304     0.623    77\n",
      " neutral     0.475     0.706     0.505    95\n",
      "     sad     0.475     0.541     0.725    91\n",
      "surprise     0.475     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.475     0.378     0.443    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.480261 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 1.281370  [    0/ 5482]\n",
      "loss: 1.272148  [ 1200/ 5482]\n",
      "loss: 1.575746  [ 2400/ 5482]\n",
      "loss: 1.682192  [ 3600/ 5482]\n",
      "loss: 1.656705  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.600     0.636    99\n",
      " disgust     0.492     0.483     0.654    107\n",
      "    fear     0.492     0.118     0.025    80\n",
      "   happy     0.492     0.287     0.727    77\n",
      " neutral     0.492     0.825     0.495    95\n",
      "     sad     0.492     0.681     0.681    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.428     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.487609 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 1.559559  [    0/ 5482]\n",
      "loss: 1.626429  [ 1200/ 5482]\n",
      "loss: 1.368140  [ 2400/ 5482]\n",
      "loss: 1.438930  [ 3600/ 5482]\n",
      "loss: 1.496111  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.612     0.636    99\n",
      " disgust     0.493     0.466     0.645    107\n",
      "    fear     0.493     0.167     0.037    80\n",
      "   happy     0.493     0.292     0.701    77\n",
      " neutral     0.493     0.766     0.516    95\n",
      "     sad     0.493     0.685     0.692    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.427     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.478322 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 1.624178  [    0/ 5482]\n",
      "loss: 1.541011  [ 1200/ 5482]\n",
      "loss: 1.574402  [ 2400/ 5482]\n",
      "loss: 1.666226  [ 3600/ 5482]\n",
      "loss: 1.251419  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.561     0.646    99\n",
      " disgust     0.493     0.471     0.607    107\n",
      "    fear     0.493     0.167     0.025    80\n",
      "   happy     0.493     0.302     0.714    77\n",
      " neutral     0.493     0.746     0.526    95\n",
      "     sad     0.493     0.670     0.714    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.417     0.462    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.477721 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 1.294866  [    0/ 5482]\n",
      "loss: 1.373135  [ 1200/ 5482]\n",
      "loss: 1.546035  [ 2400/ 5482]\n",
      "loss: 1.804994  [ 3600/ 5482]\n",
      "loss: 1.498049  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.610     0.646    99\n",
      " disgust     0.503     0.496     0.636    107\n",
      "    fear     0.503     0.250     0.050    80\n",
      "   happy     0.503     0.311     0.662    77\n",
      " neutral     0.503     0.679     0.579    95\n",
      "     sad     0.503     0.607     0.714    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.422     0.470    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.470253 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 1.269243  [    0/ 5482]\n",
      "loss: 1.165219  [ 1200/ 5482]\n",
      "loss: 1.326948  [ 2400/ 5482]\n",
      "loss: 1.223242  [ 3600/ 5482]\n",
      "loss: 1.645763  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.538     0.646    99\n",
      " disgust     0.487     0.485     0.607    107\n",
      "    fear     0.487     0.056     0.013    80\n",
      "   happy     0.487     0.304     0.662    77\n",
      " neutral     0.487     0.739     0.537    95\n",
      "     sad     0.487     0.637     0.714    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.394     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.473670 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 1.356081  [    0/ 5482]\n",
      "loss: 1.496803  [ 1200/ 5482]\n",
      "loss: 1.475842  [ 2400/ 5482]\n",
      "loss: 1.439882  [ 3600/ 5482]\n",
      "loss: 1.732148  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.484     0.556     0.657    99\n",
      " disgust     0.484     0.477     0.570    107\n",
      "    fear     0.484     0.083     0.013    80\n",
      "   happy     0.484     0.310     0.675    77\n",
      " neutral     0.484     0.718     0.537    95\n",
      "     sad     0.484     0.570     0.714    91\n",
      "surprise     0.484     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.484     0.388     0.452    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.469710 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 1.479125  [    0/ 5482]\n",
      "loss: 1.296518  [ 1200/ 5482]\n",
      "loss: 1.475534  [ 2400/ 5482]\n",
      "loss: 1.498227  [ 3600/ 5482]\n",
      "loss: 1.282058  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.569     0.667    99\n",
      " disgust     0.485     0.485     0.589    107\n",
      "    fear     0.485     0.125     0.013    80\n",
      "   happy     0.485     0.296     0.688    77\n",
      " neutral     0.485     0.750     0.505    95\n",
      "     sad     0.485     0.575     0.714    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.400     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.472191 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 1.464785  [    0/ 5482]\n",
      "loss: 1.570914  [ 1200/ 5482]\n",
      "loss: 1.448824  [ 2400/ 5482]\n",
      "loss: 1.467690  [ 3600/ 5482]\n",
      "loss: 1.284484  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.495     0.574     0.667    99\n",
      " disgust     0.495     0.493     0.636    107\n",
      "    fear     0.495     0.167     0.025    80\n",
      "   happy     0.495     0.289     0.701    77\n",
      " neutral     0.495     0.778     0.516    95\n",
      "     sad     0.495     0.663     0.692    91\n",
      "surprise     0.495     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.495     0.423     0.462    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.476410 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 1.548673  [    0/ 5482]\n",
      "loss: 1.130723  [ 1200/ 5482]\n",
      "loss: 1.632609  [ 2400/ 5482]\n",
      "loss: 1.462041  [ 3600/ 5482]\n",
      "loss: 1.437038  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.482     0.591     0.657    99\n",
      " disgust     0.482     0.480     0.551    107\n",
      "    fear     0.482     0.133     0.025    80\n",
      "   happy     0.482     0.302     0.662    77\n",
      " neutral     0.482     0.689     0.537    95\n",
      "     sad     0.482     0.555     0.725    91\n",
      "surprise     0.482     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.482     0.393     0.451    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.466933 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 1.320795  [    0/ 5482]\n",
      "loss: 1.331334  [ 1200/ 5482]\n",
      "loss: 1.639069  [ 2400/ 5482]\n",
      "loss: 1.619981  [ 3600/ 5482]\n",
      "loss: 1.415188  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.610     0.616    99\n",
      " disgust     0.490     0.466     0.636    107\n",
      "    fear     0.490     0.182     0.025    80\n",
      "   happy     0.490     0.282     0.714    77\n",
      " neutral     0.490     0.758     0.526    95\n",
      "     sad     0.490     0.685     0.692    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.426     0.459    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.470728 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 1.518787  [    0/ 5482]\n",
      "loss: 1.456946  [ 1200/ 5482]\n",
      "loss: 1.497104  [ 2400/ 5482]\n",
      "loss: 1.530363  [ 3600/ 5482]\n",
      "loss: 1.384392  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.574     0.667    99\n",
      " disgust     0.490     0.477     0.589    107\n",
      "    fear     0.490     0.071     0.013    80\n",
      "   happy     0.490     0.301     0.688    77\n",
      " neutral     0.490     0.722     0.547    95\n",
      "     sad     0.490     0.634     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.397     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.464429 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 1.479818  [    0/ 5482]\n",
      "loss: 1.615223  [ 1200/ 5482]\n",
      "loss: 1.397221  [ 2400/ 5482]\n",
      "loss: 1.110321  [ 3600/ 5482]\n",
      "loss: 1.414085  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.546     0.657    99\n",
      " disgust     0.485     0.496     0.589    107\n",
      "    fear     0.485     0.071     0.013    80\n",
      "   happy     0.485     0.305     0.688    77\n",
      " neutral     0.485     0.725     0.526    95\n",
      "     sad     0.485     0.598     0.703    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.392     0.454    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.465892 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 1.353452  [    0/ 5482]\n",
      "loss: 1.663391  [ 1200/ 5482]\n",
      "loss: 1.346302  [ 2400/ 5482]\n",
      "loss: 1.798599  [ 3600/ 5482]\n",
      "loss: 1.438348  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.550     0.667    99\n",
      " disgust     0.487     0.512     0.589    107\n",
      "    fear     0.487     0.083     0.013    80\n",
      "   happy     0.487     0.306     0.688    77\n",
      " neutral     0.487     0.770     0.495    95\n",
      "     sad     0.487     0.554     0.736    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.397     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.467335 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 1.555528  [    0/ 5482]\n",
      "loss: 1.715657  [ 1200/ 5482]\n",
      "loss: 1.757540  [ 2400/ 5482]\n",
      "loss: 1.649367  [ 3600/ 5482]\n",
      "loss: 1.813348  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.495     0.570     0.657    99\n",
      " disgust     0.495     0.482     0.617    107\n",
      "    fear     0.495     0.083     0.013    80\n",
      "   happy     0.495     0.290     0.688    77\n",
      " neutral     0.495     0.768     0.558    95\n",
      "     sad     0.495     0.674     0.703    91\n",
      "surprise     0.495     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.495     0.410     0.462    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.468940 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 1.287450  [    0/ 5482]\n",
      "loss: 1.383392  [ 1200/ 5482]\n",
      "loss: 1.408684  [ 2400/ 5482]\n",
      "loss: 1.676395  [ 3600/ 5482]\n",
      "loss: 1.395190  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.607     0.657    99\n",
      " disgust     0.502     0.489     0.617    107\n",
      "    fear     0.502     0.300     0.037    80\n",
      "   happy     0.502     0.308     0.675    77\n",
      " neutral     0.502     0.711     0.568    95\n",
      "     sad     0.502     0.584     0.725    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.428     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.459691 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 1.697275  [    0/ 5482]\n",
      "loss: 1.568950  [ 1200/ 5482]\n",
      "loss: 1.275490  [ 2400/ 5482]\n",
      "loss: 1.506465  [ 3600/ 5482]\n",
      "loss: 1.459805  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.596     0.657    99\n",
      " disgust     0.489     0.467     0.589    107\n",
      "    fear     0.489     0.091     0.013    80\n",
      "   happy     0.489     0.294     0.688    77\n",
      " neutral     0.489     0.729     0.537    95\n",
      "     sad     0.489     0.619     0.714    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.399     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.460232 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 1.351743  [    0/ 5482]\n",
      "loss: 1.353961  [ 1200/ 5482]\n",
      "loss: 1.459083  [ 2400/ 5482]\n",
      "loss: 1.394220  [ 3600/ 5482]\n",
      "loss: 1.188557  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.580     0.657    99\n",
      " disgust     0.498     0.511     0.636    107\n",
      "    fear     0.498     0.071     0.013    80\n",
      "   happy     0.498     0.301     0.688    77\n",
      " neutral     0.498     0.746     0.558    95\n",
      "     sad     0.498     0.615     0.703    91\n",
      "surprise     0.498     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.404     0.465    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.460019 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 1.382727  [    0/ 5482]\n",
      "loss: 1.478948  [ 1200/ 5482]\n",
      "loss: 1.575743  [ 2400/ 5482]\n",
      "loss: 1.315899  [ 3600/ 5482]\n",
      "loss: 1.338858  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.591     0.657    99\n",
      " disgust     0.490     0.492     0.598    107\n",
      "    fear     0.490     0.083     0.013    80\n",
      "   happy     0.490     0.296     0.688    77\n",
      " neutral     0.490     0.729     0.537    95\n",
      "     sad     0.490     0.596     0.714    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.398     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.458116 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 1.584864  [    0/ 5482]\n",
      "loss: 1.274727  [ 1200/ 5482]\n",
      "loss: 1.227732  [ 2400/ 5482]\n",
      "loss: 1.290969  [ 3600/ 5482]\n",
      "loss: 1.502482  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.613     0.657    99\n",
      " disgust     0.492     0.496     0.589    107\n",
      "    fear     0.492     0.125     0.025    80\n",
      "   happy     0.492     0.297     0.675    77\n",
      " neutral     0.492     0.736     0.558    95\n",
      "     sad     0.492     0.570     0.714    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.405     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.456474 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 1.444543  [    0/ 5482]\n",
      "loss: 1.337148  [ 1200/ 5482]\n",
      "loss: 1.717434  [ 2400/ 5482]\n",
      "loss: 1.481572  [ 3600/ 5482]\n",
      "loss: 1.735964  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.635     0.616    99\n",
      " disgust     0.493     0.468     0.607    107\n",
      "    fear     0.493     0.235     0.050    80\n",
      "   happy     0.493     0.294     0.688    77\n",
      " neutral     0.493     0.691     0.589    95\n",
      "     sad     0.493     0.639     0.681    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.423     0.462    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.453136 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 1.250130  [    0/ 5482]\n",
      "loss: 1.437463  [ 1200/ 5482]\n",
      "loss: 1.588428  [ 2400/ 5482]\n",
      "loss: 1.472935  [ 3600/ 5482]\n",
      "loss: 1.546258  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.621     0.646    99\n",
      " disgust     0.502     0.496     0.645    107\n",
      "    fear     0.502     0.083     0.013    80\n",
      "   happy     0.502     0.303     0.688    77\n",
      " neutral     0.502     0.700     0.589    95\n",
      "     sad     0.502     0.624     0.692    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.404     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.451706 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 1.325858  [    0/ 5482]\n",
      "loss: 1.302604  [ 1200/ 5482]\n",
      "loss: 1.416285  [ 2400/ 5482]\n",
      "loss: 1.292299  [ 3600/ 5482]\n",
      "loss: 1.567681  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.495     0.596     0.657    99\n",
      " disgust     0.495     0.496     0.607    107\n",
      "    fear     0.495     0.083     0.013    80\n",
      "   happy     0.495     0.301     0.688    77\n",
      " neutral     0.495     0.716     0.558    95\n",
      "     sad     0.495     0.602     0.714    91\n",
      "surprise     0.495     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.495     0.399     0.462    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.453064 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 1.612213  [    0/ 5482]\n",
      "loss: 1.358492  [ 1200/ 5482]\n",
      "loss: 1.645102  [ 2400/ 5482]\n",
      "loss: 1.333104  [ 3600/ 5482]\n",
      "loss: 1.503925  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.587     0.646    99\n",
      " disgust     0.493     0.474     0.607    107\n",
      "    fear     0.493     0.083     0.013    80\n",
      "   happy     0.493     0.292     0.701    77\n",
      " neutral     0.493     0.746     0.558    95\n",
      "     sad     0.493     0.667     0.703    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.407     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.454901 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 1.716394  [    0/ 5482]\n",
      "loss: 1.327315  [ 1200/ 5482]\n",
      "loss: 1.357388  [ 2400/ 5482]\n",
      "loss: 1.518982  [ 3600/ 5482]\n",
      "loss: 1.327724  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.584     0.667    99\n",
      " disgust     0.492     0.481     0.589    107\n",
      "    fear     0.492     0.077     0.013    80\n",
      "   happy     0.492     0.296     0.688    77\n",
      " neutral     0.492     0.722     0.547    95\n",
      "     sad     0.492     0.637     0.714    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.400     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.452494 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 1.263103  [    0/ 5482]\n",
      "loss: 1.688769  [ 1200/ 5482]\n",
      "loss: 1.398770  [ 2400/ 5482]\n",
      "loss: 1.275482  [ 3600/ 5482]\n",
      "loss: 1.398902  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.582     0.646    99\n",
      " disgust     0.493     0.504     0.636    107\n",
      "    fear     0.493     0.077     0.013    80\n",
      "   happy     0.493     0.282     0.714    77\n",
      " neutral     0.493     0.797     0.537    95\n",
      "     sad     0.493     0.667     0.681    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.415     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.463907 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 1.534823  [    0/ 5482]\n",
      "loss: 1.660205  [ 1200/ 5482]\n",
      "loss: 1.573060  [ 2400/ 5482]\n",
      "loss: 1.468763  [ 3600/ 5482]\n",
      "loss: 1.368182  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.495     0.558     0.677    99\n",
      " disgust     0.495     0.516     0.598    107\n",
      "    fear     0.495     0.083     0.013    80\n",
      "   happy     0.495     0.299     0.688    77\n",
      " neutral     0.495     0.750     0.537    95\n",
      "     sad     0.495     0.606     0.725    91\n",
      "surprise     0.495     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.495     0.402     0.463    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.452396 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 1.228719  [    0/ 5482]\n",
      "loss: 1.869190  [ 1200/ 5482]\n",
      "loss: 1.610036  [ 2400/ 5482]\n",
      "loss: 1.444204  [ 3600/ 5482]\n",
      "loss: 1.486984  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.485     0.606     0.606    99\n",
      " disgust     0.485     0.464     0.607    107\n",
      "    fear     0.485     0.231     0.037    80\n",
      "   happy     0.485     0.279     0.714    77\n",
      " neutral     0.485     0.732     0.547    95\n",
      "     sad     0.485     0.678     0.670    91\n",
      "surprise     0.485     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.485     0.427     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.454081 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 1.434541  [    0/ 5482]\n",
      "loss: 1.745001  [ 1200/ 5482]\n",
      "loss: 1.268140  [ 2400/ 5482]\n",
      "loss: 1.448386  [ 3600/ 5482]\n",
      "loss: 1.338247  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.657     0.657    99\n",
      " disgust     0.500     0.450     0.589    107\n",
      "    fear     0.500     0.250     0.050    80\n",
      "   happy     0.500     0.308     0.675    77\n",
      " neutral     0.500     0.659     0.611    95\n",
      "     sad     0.500     0.643     0.692    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.424     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.442490 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 1.772073  [    0/ 5482]\n",
      "loss: 1.367820  [ 1200/ 5482]\n",
      "loss: 1.600569  [ 2400/ 5482]\n",
      "loss: 1.595136  [ 3600/ 5482]\n",
      "loss: 1.017290  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.565     0.657    99\n",
      " disgust     0.503     0.500     0.598    107\n",
      "    fear     0.503     0.250     0.037    80\n",
      "   happy     0.503     0.311     0.675    77\n",
      " neutral     0.503     0.718     0.589    95\n",
      "     sad     0.503     0.609     0.736    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.422     0.470    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.445247 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 1.585890  [    0/ 5482]\n",
      "loss: 1.151876  [ 1200/ 5482]\n",
      "loss: 1.582934  [ 2400/ 5482]\n",
      "loss: 1.312585  [ 3600/ 5482]\n",
      "loss: 1.428977  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.625     0.657    99\n",
      " disgust     0.511     0.481     0.607    107\n",
      "    fear     0.511     0.308     0.050    80\n",
      "   happy     0.511     0.313     0.675    77\n",
      " neutral     0.511     0.674     0.632    95\n",
      "     sad     0.511     0.641     0.725    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.435     0.478    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.441984 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 1.300263  [    0/ 5482]\n",
      "loss: 1.607309  [ 1200/ 5482]\n",
      "loss: 1.621757  [ 2400/ 5482]\n",
      "loss: 1.608417  [ 3600/ 5482]\n",
      "loss: 1.288184  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.579     0.667    99\n",
      " disgust     0.492     0.496     0.551    107\n",
      "    fear     0.492     0.200     0.025    80\n",
      "   happy     0.492     0.309     0.662    77\n",
      " neutral     0.492     0.675     0.568    95\n",
      "     sad     0.492     0.557     0.747    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.402     0.460    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.448367 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 1.505861  [    0/ 5482]\n",
      "loss: 1.252941  [ 1200/ 5482]\n",
      "loss: 1.666266  [ 2400/ 5482]\n",
      "loss: 1.318551  [ 3600/ 5482]\n",
      "loss: 1.303961  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.612     0.636    99\n",
      " disgust     0.493     0.493     0.626    107\n",
      "    fear     0.493     0.083     0.013    80\n",
      "   happy     0.493     0.277     0.688    77\n",
      " neutral     0.493     0.757     0.558    95\n",
      "     sad     0.493     0.653     0.703    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.411     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.449405 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 1.307840  [    0/ 5482]\n",
      "loss: 1.212583  [ 1200/ 5482]\n",
      "loss: 1.807783  [ 2400/ 5482]\n",
      "loss: 1.609619  [ 3600/ 5482]\n",
      "loss: 1.672005  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.637     0.657    99\n",
      " disgust     0.508     0.463     0.589    107\n",
      "    fear     0.508     0.267     0.050    80\n",
      "   happy     0.508     0.323     0.675    77\n",
      " neutral     0.508     0.642     0.642    95\n",
      "     sad     0.508     0.644     0.714    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.425     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.439568 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 1.676319  [    0/ 5482]\n",
      "loss: 1.136056  [ 1200/ 5482]\n",
      "loss: 1.437888  [ 2400/ 5482]\n",
      "loss: 1.223138  [ 3600/ 5482]\n",
      "loss: 1.276510  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.487     0.614     0.626    99\n",
      " disgust     0.487     0.457     0.589    107\n",
      "    fear     0.487     0.083     0.013    80\n",
      "   happy     0.487     0.284     0.675    77\n",
      " neutral     0.487     0.688     0.579    95\n",
      "     sad     0.487     0.667     0.703    91\n",
      "surprise     0.487     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.487     0.399     0.455    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.439654 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 1.604757  [    0/ 5482]\n",
      "loss: 1.229391  [ 1200/ 5482]\n",
      "loss: 1.317369  [ 2400/ 5482]\n",
      "loss: 1.399848  [ 3600/ 5482]\n",
      "loss: 1.288183  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.602     0.657    99\n",
      " disgust     0.500     0.481     0.598    107\n",
      "    fear     0.500     0.100     0.013    80\n",
      "   happy     0.500     0.298     0.688    77\n",
      " neutral     0.500     0.700     0.589    95\n",
      "     sad     0.500     0.653     0.725    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.405     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.440849 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 1.302306  [    0/ 5482]\n",
      "loss: 1.240601  [ 1200/ 5482]\n",
      "loss: 1.515369  [ 2400/ 5482]\n",
      "loss: 1.247249  [ 3600/ 5482]\n",
      "loss: 1.800510  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.631     0.657    99\n",
      " disgust     0.505     0.479     0.626    107\n",
      "    fear     0.505     0.250     0.025    80\n",
      "   happy     0.505     0.299     0.688    77\n",
      " neutral     0.505     0.687     0.600    95\n",
      "     sad     0.505     0.646     0.703    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.427     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.436859 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 1.668743  [    0/ 5482]\n",
      "loss: 1.451060  [ 1200/ 5482]\n",
      "loss: 1.260593  [ 2400/ 5482]\n",
      "loss: 1.765466  [ 3600/ 5482]\n",
      "loss: 1.450481  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.625     0.657    99\n",
      " disgust     0.502     0.492     0.570    107\n",
      "    fear     0.502     0.214     0.037    80\n",
      "   happy     0.502     0.306     0.675    77\n",
      " neutral     0.502     0.634     0.621    95\n",
      "     sad     0.502     0.629     0.725    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.414     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.434952 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 1.123524  [    0/ 5482]\n",
      "loss: 1.453240  [ 1200/ 5482]\n",
      "loss: 1.364123  [ 2400/ 5482]\n",
      "loss: 1.378099  [ 3600/ 5482]\n",
      "loss: 1.241990  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.497     0.578     0.677    99\n",
      " disgust     0.497     0.517     0.579    107\n",
      "    fear     0.497     0.091     0.013    80\n",
      "   happy     0.497     0.305     0.688    77\n",
      " neutral     0.497     0.716     0.558    95\n",
      "     sad     0.497     0.583     0.736    91\n",
      "surprise     0.497     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.497     0.398     0.464    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.439897 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 1.411213  [    0/ 5482]\n",
      "loss: 1.379713  [ 1200/ 5482]\n",
      "loss: 1.230780  [ 2400/ 5482]\n",
      "loss: 1.745139  [ 3600/ 5482]\n",
      "loss: 1.306022  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.559     0.667    99\n",
      " disgust     0.500     0.511     0.636    107\n",
      "    fear     0.500     0.250     0.025    80\n",
      "   happy     0.500     0.288     0.688    77\n",
      " neutral     0.500     0.726     0.558    95\n",
      "     sad     0.500     0.670     0.692    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.429     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.441558 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 1.235141  [    0/ 5482]\n",
      "loss: 1.158213  [ 1200/ 5482]\n",
      "loss: 1.747361  [ 2400/ 5482]\n",
      "loss: 1.195358  [ 3600/ 5482]\n",
      "loss: 1.371732  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.564     0.667    99\n",
      " disgust     0.503     0.513     0.570    107\n",
      "    fear     0.503     0.250     0.037    80\n",
      "   happy     0.503     0.323     0.688    77\n",
      " neutral     0.503     0.675     0.589    95\n",
      "     sad     0.503     0.591     0.747    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.417     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.437601 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 1.317375  [    0/ 5482]\n",
      "loss: 1.202211  [ 1200/ 5482]\n",
      "loss: 1.498311  [ 2400/ 5482]\n",
      "loss: 1.354614  [ 3600/ 5482]\n",
      "loss: 1.549288  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.555     0.667    99\n",
      " disgust     0.502     0.525     0.579    107\n",
      "    fear     0.502     0.182     0.025    80\n",
      "   happy     0.502     0.317     0.688    77\n",
      " neutral     0.502     0.692     0.568    95\n",
      "     sad     0.502     0.590     0.758    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.409     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.435968 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 1.576818  [    0/ 5482]\n",
      "loss: 1.207066  [ 1200/ 5482]\n",
      "loss: 1.097167  [ 2400/ 5482]\n",
      "loss: 1.178162  [ 3600/ 5482]\n",
      "loss: 1.380291  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.588     0.677    99\n",
      " disgust     0.503     0.530     0.579    107\n",
      "    fear     0.503     0.154     0.025    80\n",
      "   happy     0.503     0.325     0.688    77\n",
      " neutral     0.503     0.720     0.568    95\n",
      "     sad     0.503     0.539     0.758    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.408     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.434315 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 1.551300  [    0/ 5482]\n",
      "loss: 1.363132  [ 1200/ 5482]\n",
      "loss: 1.363011  [ 2400/ 5482]\n",
      "loss: 1.445502  [ 3600/ 5482]\n",
      "loss: 1.183503  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.613     0.657    99\n",
      " disgust     0.502     0.475     0.626    107\n",
      "    fear     0.502     0.200     0.025    80\n",
      "   happy     0.502     0.288     0.688    77\n",
      " neutral     0.502     0.724     0.579    95\n",
      "     sad     0.502     0.688     0.703    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.427     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.434787 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 1.390770  [    0/ 5482]\n",
      "loss: 1.318490  [ 1200/ 5482]\n",
      "loss: 1.362282  [ 2400/ 5482]\n",
      "loss: 1.478278  [ 3600/ 5482]\n",
      "loss: 1.677365  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.615     0.646    99\n",
      " disgust     0.502     0.508     0.598    107\n",
      "    fear     0.502     0.167     0.025    80\n",
      "   happy     0.502     0.303     0.688    77\n",
      " neutral     0.502     0.709     0.589    95\n",
      "     sad     0.502     0.588     0.736    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.413     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.429301 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 1.255820  [    0/ 5482]\n",
      "loss: 1.508498  [ 1200/ 5482]\n",
      "loss: 1.456851  [ 2400/ 5482]\n",
      "loss: 1.339848  [ 3600/ 5482]\n",
      "loss: 1.617448  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.589     0.636    99\n",
      " disgust     0.493     0.517     0.561    107\n",
      "    fear     0.493     0.231     0.037    80\n",
      "   happy     0.493     0.299     0.688    77\n",
      " neutral     0.493     0.688     0.558    95\n",
      "     sad     0.493     0.575     0.758    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.414     0.463    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.435792 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 1.354866  [    0/ 5482]\n",
      "loss: 1.445506  [ 1200/ 5482]\n",
      "loss: 1.239585  [ 2400/ 5482]\n",
      "loss: 1.663041  [ 3600/ 5482]\n",
      "loss: 1.478049  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.631     0.657    99\n",
      " disgust     0.508     0.486     0.626    107\n",
      "    fear     0.508     0.231     0.037    80\n",
      "   happy     0.508     0.306     0.688    77\n",
      " neutral     0.508     0.704     0.600    95\n",
      "     sad     0.508     0.637     0.714    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.428     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.426549 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 1.414888  [    0/ 5482]\n",
      "loss: 1.418836  [ 1200/ 5482]\n",
      "loss: 1.363745  [ 2400/ 5482]\n",
      "loss: 1.222643  [ 3600/ 5482]\n",
      "loss: 1.533661  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.545     0.677    99\n",
      " disgust     0.500     0.540     0.626    107\n",
      "    fear     0.500     0.182     0.025    80\n",
      "   happy     0.500     0.303     0.688    77\n",
      " neutral     0.500     0.735     0.526    95\n",
      "     sad     0.500     0.606     0.725    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.416     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.431865 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 1.451216  [    0/ 5482]\n",
      "loss: 1.390393  [ 1200/ 5482]\n",
      "loss: 1.226882  [ 2400/ 5482]\n",
      "loss: 1.458703  [ 3600/ 5482]\n",
      "loss: 1.402649  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.604     0.677    99\n",
      " disgust     0.505     0.512     0.598    107\n",
      "    fear     0.505     0.100     0.013    80\n",
      "   happy     0.505     0.299     0.688    77\n",
      " neutral     0.505     0.743     0.579    95\n",
      "     sad     0.505     0.602     0.747    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.409     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.427149 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 1.454478  [    0/ 5482]\n",
      "loss: 1.176434  [ 1200/ 5482]\n",
      "loss: 1.315899  [ 2400/ 5482]\n",
      "loss: 1.618202  [ 3600/ 5482]\n",
      "loss: 1.397465  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.589     0.667    99\n",
      " disgust     0.511     0.489     0.607    107\n",
      "    fear     0.511     0.300     0.037    80\n",
      "   happy     0.511     0.312     0.688    77\n",
      " neutral     0.511     0.694     0.621    95\n",
      "     sad     0.511     0.660     0.725    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.435     0.478    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.428157 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 1.277529  [    0/ 5482]\n",
      "loss: 1.300984  [ 1200/ 5482]\n",
      "loss: 1.292942  [ 2400/ 5482]\n",
      "loss: 1.730122  [ 3600/ 5482]\n",
      "loss: 1.669767  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.611     0.667    99\n",
      " disgust     0.510     0.508     0.617    107\n",
      "    fear     0.510     0.071     0.013    80\n",
      "   happy     0.510     0.306     0.688    77\n",
      " neutral     0.510     0.716     0.611    95\n",
      "     sad     0.510     0.644     0.736    91\n",
      "surprise     0.510     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.408     0.476    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.426049 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 1.409182  [    0/ 5482]\n",
      "loss: 1.614159  [ 1200/ 5482]\n",
      "loss: 1.288157  [ 2400/ 5482]\n",
      "loss: 1.817211  [ 3600/ 5482]\n",
      "loss: 1.514904  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.615     0.677    99\n",
      " disgust     0.513     0.520     0.607    107\n",
      "    fear     0.513     0.167     0.025    80\n",
      "   happy     0.513     0.325     0.675    77\n",
      " neutral     0.513     0.682     0.611    95\n",
      "     sad     0.513     0.580     0.758    91\n",
      "surprise     0.513     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.413     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.420404 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 1.198361  [    0/ 5482]\n",
      "loss: 1.530712  [ 1200/ 5482]\n",
      "loss: 1.656468  [ 2400/ 5482]\n",
      "loss: 1.415304  [ 3600/ 5482]\n",
      "loss: 1.833235  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.588     0.677    99\n",
      " disgust     0.518     0.500     0.664    107\n",
      "    fear     0.518     0.250     0.025    80\n",
      "   happy     0.518     0.317     0.688    77\n",
      " neutral     0.518     0.725     0.611    95\n",
      "     sad     0.518     0.657     0.714    91\n",
      "surprise     0.518     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.434     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.419918 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 1.586305  [    0/ 5482]\n",
      "loss: 1.391453  [ 1200/ 5482]\n",
      "loss: 1.363303  [ 2400/ 5482]\n",
      "loss: 1.423501  [ 3600/ 5482]\n",
      "loss: 1.308395  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.492     0.600     0.636    99\n",
      " disgust     0.492     0.493     0.645    107\n",
      "    fear     0.492     0.091     0.013    80\n",
      "   happy     0.492     0.276     0.714    77\n",
      " neutral     0.492     0.754     0.547    95\n",
      "     sad     0.492     0.698     0.659    91\n",
      "surprise     0.492     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.492     0.416     0.459    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.432665 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 1.388218  [    0/ 5482]\n",
      "loss: 1.239421  [ 1200/ 5482]\n",
      "loss: 1.303184  [ 2400/ 5482]\n",
      "loss: 1.408780  [ 3600/ 5482]\n",
      "loss: 1.465530  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.586     0.657    99\n",
      " disgust     0.503     0.496     0.617    107\n",
      "    fear     0.503     0.250     0.037    80\n",
      "   happy     0.503     0.301     0.688    77\n",
      " neutral     0.503     0.684     0.568    95\n",
      "     sad     0.503     0.667     0.725    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.426     0.470    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.425291 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 1.224752  [    0/ 5482]\n",
      "loss: 1.452473  [ 1200/ 5482]\n",
      "loss: 1.257672  [ 2400/ 5482]\n",
      "loss: 1.484938  [ 3600/ 5482]\n",
      "loss: 1.495594  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.584     0.667    99\n",
      " disgust     0.503     0.500     0.654    107\n",
      "    fear     0.503     0.125     0.013    80\n",
      "   happy     0.503     0.285     0.688    77\n",
      " neutral     0.503     0.761     0.568    95\n",
      "     sad     0.503     0.685     0.692    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.420     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.426894 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 1.173511  [    0/ 5482]\n",
      "loss: 1.344166  [ 1200/ 5482]\n",
      "loss: 1.429627  [ 2400/ 5482]\n",
      "loss: 1.747878  [ 3600/ 5482]\n",
      "loss: 1.361953  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.629     0.667    99\n",
      " disgust     0.511     0.512     0.617    107\n",
      "    fear     0.511     0.300     0.037    80\n",
      "   happy     0.511     0.308     0.688    77\n",
      " neutral     0.511     0.679     0.600    95\n",
      "     sad     0.511     0.609     0.736    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.434     0.478    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.416277 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 1.152399  [    0/ 5482]\n",
      "loss: 1.308687  [ 1200/ 5482]\n",
      "loss: 1.655009  [ 2400/ 5482]\n",
      "loss: 1.403504  [ 3600/ 5482]\n",
      "loss: 1.318626  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.593     0.646    99\n",
      " disgust     0.503     0.523     0.626    107\n",
      "    fear     0.503     0.111     0.013    80\n",
      "   happy     0.503     0.290     0.688    77\n",
      " neutral     0.503     0.709     0.589    95\n",
      "     sad     0.503     0.641     0.725    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.409     0.470    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.424217 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 1.457841  [    0/ 5482]\n",
      "loss: 1.313198  [ 1200/ 5482]\n",
      "loss: 1.378050  [ 2400/ 5482]\n",
      "loss: 1.378421  [ 3600/ 5482]\n",
      "loss: 1.489088  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.593     0.677    99\n",
      " disgust     0.505     0.525     0.598    107\n",
      "    fear     0.505     0.154     0.025    80\n",
      "   happy     0.505     0.301     0.688    77\n",
      " neutral     0.505     0.730     0.568    95\n",
      "     sad     0.505     0.607     0.747    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.416     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.419043 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 1.564066  [    0/ 5482]\n",
      "loss: 1.154483  [ 1200/ 5482]\n",
      "loss: 1.422631  [ 2400/ 5482]\n",
      "loss: 1.355665  [ 3600/ 5482]\n",
      "loss: 1.022007  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.636     0.636    99\n",
      " disgust     0.507     0.496     0.636    107\n",
      "    fear     0.507     0.167     0.025    80\n",
      "   happy     0.507     0.285     0.688    77\n",
      " neutral     0.507     0.723     0.632    95\n",
      "     sad     0.507     0.677     0.692    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.426     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.416775 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 1.176963  [    0/ 5482]\n",
      "loss: 1.487454  [ 1200/ 5482]\n",
      "loss: 1.609444  [ 2400/ 5482]\n",
      "loss: 1.654074  [ 3600/ 5482]\n",
      "loss: 1.410355  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.520     0.674     0.626    99\n",
      " disgust     0.520     0.503     0.673    107\n",
      "    fear     0.520     0.100     0.013    80\n",
      "   happy     0.520     0.302     0.714    77\n",
      " neutral     0.520     0.733     0.663    95\n",
      "     sad     0.520     0.660     0.703    91\n",
      "surprise     0.520     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.520     0.425     0.485    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.418426 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 1.250467  [    0/ 5482]\n",
      "loss: 1.441006  [ 1200/ 5482]\n",
      "loss: 1.431297  [ 2400/ 5482]\n",
      "loss: 1.563137  [ 3600/ 5482]\n",
      "loss: 1.544433  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.630     0.636    99\n",
      " disgust     0.498     0.482     0.636    107\n",
      "    fear     0.498     0.125     0.013    80\n",
      "   happy     0.498     0.282     0.688    77\n",
      " neutral     0.498     0.733     0.579    95\n",
      "     sad     0.498     0.653     0.703    91\n",
      "surprise     0.498     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.415     0.465    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.415554 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 1.278647  [    0/ 5482]\n",
      "loss: 1.222947  [ 1200/ 5482]\n",
      "loss: 1.308239  [ 2400/ 5482]\n",
      "loss: 1.305892  [ 3600/ 5482]\n",
      "loss: 1.208671  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.606     0.636    99\n",
      " disgust     0.503     0.504     0.626    107\n",
      "    fear     0.503     0.143     0.013    80\n",
      "   happy     0.503     0.288     0.688    77\n",
      " neutral     0.503     0.699     0.611    95\n",
      "     sad     0.503     0.657     0.714    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.414     0.470    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.417577 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 1.515018  [    0/ 5482]\n",
      "loss: 1.453514  [ 1200/ 5482]\n",
      "loss: 1.356058  [ 2400/ 5482]\n",
      "loss: 1.363626  [ 3600/ 5482]\n",
      "loss: 1.295051  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.612     0.636    99\n",
      " disgust     0.500     0.512     0.617    107\n",
      "    fear     0.500     0.200     0.037    80\n",
      "   happy     0.500     0.281     0.740    77\n",
      " neutral     0.500     0.768     0.558    95\n",
      "     sad     0.500     0.692     0.692    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.438     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.426698 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 1.112971  [    0/ 5482]\n",
      "loss: 1.350300  [ 1200/ 5482]\n",
      "loss: 1.412351  [ 2400/ 5482]\n",
      "loss: 1.310616  [ 3600/ 5482]\n",
      "loss: 1.300045  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.627     0.646    99\n",
      " disgust     0.507     0.496     0.607    107\n",
      "    fear     0.507     0.222     0.025    80\n",
      "   happy     0.507     0.291     0.688    77\n",
      " neutral     0.507     0.702     0.621    95\n",
      "     sad     0.507     0.647     0.725    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.427     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.410826 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 1.387214  [    0/ 5482]\n",
      "loss: 1.210253  [ 1200/ 5482]\n",
      "loss: 1.479143  [ 2400/ 5482]\n",
      "loss: 1.386962  [ 3600/ 5482]\n",
      "loss: 1.444258  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.660     0.626    99\n",
      " disgust     0.505     0.512     0.607    107\n",
      "    fear     0.505     0.077     0.013    80\n",
      "   happy     0.505     0.298     0.688    77\n",
      " neutral     0.505     0.694     0.621    95\n",
      "     sad     0.505     0.602     0.747    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.406     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.411379 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 1.469869  [    0/ 5482]\n",
      "loss: 1.337176  [ 1200/ 5482]\n",
      "loss: 1.474707  [ 2400/ 5482]\n",
      "loss: 1.391859  [ 3600/ 5482]\n",
      "loss: 1.236130  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.563     0.677    99\n",
      " disgust     0.500     0.539     0.579    107\n",
      "    fear     0.500     0.200     0.025    80\n",
      "   happy     0.500     0.291     0.675    77\n",
      " neutral     0.500     0.675     0.568    95\n",
      "     sad     0.500     0.636     0.747    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.415     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.415655 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 1.381107  [    0/ 5482]\n",
      "loss: 1.220449  [ 1200/ 5482]\n",
      "loss: 1.423975  [ 2400/ 5482]\n",
      "loss: 1.640408  [ 3600/ 5482]\n",
      "loss: 1.471708  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.490     0.541     0.667    99\n",
      " disgust     0.490     0.546     0.607    107\n",
      "    fear     0.490     0.111     0.013    80\n",
      "   happy     0.490     0.282     0.688    77\n",
      " neutral     0.490     0.725     0.526    95\n",
      "     sad     0.490     0.627     0.703    91\n",
      "surprise     0.490     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.490     0.405     0.458    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.421805 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 1.453936  [    0/ 5482]\n",
      "loss: 1.264859  [ 1200/ 5482]\n",
      "loss: 1.310004  [ 2400/ 5482]\n",
      "loss: 1.217540  [ 3600/ 5482]\n",
      "loss: 1.228567  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.569     0.667    99\n",
      " disgust     0.502     0.547     0.598    107\n",
      "    fear     0.502     0.200     0.025    80\n",
      "   happy     0.502     0.293     0.688    77\n",
      " neutral     0.502     0.726     0.558    95\n",
      "     sad     0.502     0.602     0.747    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.420     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.414911 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 1.542201  [    0/ 5482]\n",
      "loss: 1.475299  [ 1200/ 5482]\n",
      "loss: 1.541532  [ 2400/ 5482]\n",
      "loss: 1.366447  [ 3600/ 5482]\n",
      "loss: 1.448239  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.582     0.646    99\n",
      " disgust     0.493     0.548     0.636    107\n",
      "    fear     0.493     0.083     0.013    80\n",
      "   happy     0.493     0.272     0.727    77\n",
      " neutral     0.493     0.778     0.516    95\n",
      "     sad     0.493     0.663     0.692    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.418     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.429504 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 1.526791  [    0/ 5482]\n",
      "loss: 1.393993  [ 1200/ 5482]\n",
      "loss: 1.266462  [ 2400/ 5482]\n",
      "loss: 1.909144  [ 3600/ 5482]\n",
      "loss: 1.307411  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.497     0.580     0.657    99\n",
      " disgust     0.497     0.512     0.617    107\n",
      "    fear     0.497     0.182     0.025    80\n",
      "   happy     0.497     0.281     0.701    77\n",
      " neutral     0.497     0.716     0.558    95\n",
      "     sad     0.497     0.685     0.692    91\n",
      "surprise     0.497     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.497     0.422     0.464    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.415426 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 1.552651  [    0/ 5482]\n",
      "loss: 1.401527  [ 1200/ 5482]\n",
      "loss: 1.371364  [ 2400/ 5482]\n",
      "loss: 1.573696  [ 3600/ 5482]\n",
      "loss: 1.579982  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.613     0.657    99\n",
      " disgust     0.500     0.516     0.598    107\n",
      "    fear     0.500     0.091     0.013    80\n",
      "   happy     0.500     0.290     0.688    77\n",
      " neutral     0.500     0.730     0.568    95\n",
      "     sad     0.500     0.607     0.747    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.407     0.467    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.407741 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 1.518356  [    0/ 5482]\n",
      "loss: 1.490245  [ 1200/ 5482]\n",
      "loss: 1.452135  [ 2400/ 5482]\n",
      "loss: 1.089116  [ 3600/ 5482]\n",
      "loss: 1.358128  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.593     0.677    99\n",
      " disgust     0.503     0.524     0.617    107\n",
      "    fear     0.503     0.000     0.000    80\n",
      "   happy     0.503     0.288     0.688    77\n",
      " neutral     0.503     0.736     0.558    95\n",
      "     sad     0.503     0.642     0.747    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.397     0.470    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.413005 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 1.282611  [    0/ 5482]\n",
      "loss: 1.594866  [ 1200/ 5482]\n",
      "loss: 1.564252  [ 2400/ 5482]\n",
      "loss: 1.383008  [ 3600/ 5482]\n",
      "loss: 1.402762  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.639     0.626    99\n",
      " disgust     0.507     0.519     0.636    107\n",
      "    fear     0.507     0.111     0.013    80\n",
      "   happy     0.507     0.293     0.714    77\n",
      " neutral     0.507     0.734     0.611    95\n",
      "     sad     0.507     0.613     0.714    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.416     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.409332 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 1.222742  [    0/ 5482]\n",
      "loss: 1.168741  [ 1200/ 5482]\n",
      "loss: 1.463700  [ 2400/ 5482]\n",
      "loss: 1.663189  [ 3600/ 5482]\n",
      "loss: 1.338841  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.589     0.667    99\n",
      " disgust     0.508     0.524     0.617    107\n",
      "    fear     0.508     0.000     0.000    80\n",
      "   happy     0.508     0.303     0.688    77\n",
      " neutral     0.508     0.728     0.621    95\n",
      "     sad     0.508     0.617     0.725    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.394     0.474    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.401984 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 1.553383  [    0/ 5482]\n",
      "loss: 1.524682  [ 1200/ 5482]\n",
      "loss: 1.544282  [ 2400/ 5482]\n",
      "loss: 1.426973  [ 3600/ 5482]\n",
      "loss: 1.266233  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.602     0.657    99\n",
      " disgust     0.511     0.524     0.617    107\n",
      "    fear     0.511     0.273     0.037    80\n",
      "   happy     0.511     0.289     0.714    77\n",
      " neutral     0.511     0.738     0.621    95\n",
      "     sad     0.511     0.674     0.703    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.443     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.407322 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 1.356116  [    0/ 5482]\n",
      "loss: 1.335038  [ 1200/ 5482]\n",
      "loss: 1.442343  [ 2400/ 5482]\n",
      "loss: 1.600945  [ 3600/ 5482]\n",
      "loss: 1.548700  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.644     0.657    99\n",
      " disgust     0.508     0.500     0.579    107\n",
      "    fear     0.508     0.182     0.025    80\n",
      "   happy     0.508     0.299     0.727    77\n",
      " neutral     0.508     0.690     0.632    95\n",
      "     sad     0.508     0.650     0.714    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.424     0.476    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.402382 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 1.530528  [    0/ 5482]\n",
      "loss: 1.541058  [ 1200/ 5482]\n",
      "loss: 1.397896  [ 2400/ 5482]\n",
      "loss: 1.316589  [ 3600/ 5482]\n",
      "loss: 1.689411  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.692     0.636    99\n",
      " disgust     0.507     0.471     0.598    107\n",
      "    fear     0.507     0.125     0.013    80\n",
      "   happy     0.507     0.293     0.727    77\n",
      " neutral     0.507     0.698     0.632    95\n",
      "     sad     0.507     0.660     0.703    91\n",
      "surprise     0.507     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.563     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.402670 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 1.210561  [    0/ 5482]\n",
      "loss: 1.280633  [ 1200/ 5482]\n",
      "loss: 1.230916  [ 2400/ 5482]\n",
      "loss: 1.466498  [ 3600/ 5482]\n",
      "loss: 1.357176  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.600     0.667    99\n",
      " disgust     0.505     0.519     0.626    107\n",
      "    fear     0.505     0.000     0.000    80\n",
      "   happy     0.505     0.287     0.701    77\n",
      " neutral     0.505     0.740     0.600    95\n",
      "     sad     0.505     0.640     0.703    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.398     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.404507 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 1.375158  [    0/ 5482]\n",
      "loss: 1.417913  [ 1200/ 5482]\n",
      "loss: 1.155506  [ 2400/ 5482]\n",
      "loss: 1.240518  [ 3600/ 5482]\n",
      "loss: 1.270483  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.493     0.527     0.687    99\n",
      " disgust     0.493     0.570     0.570    107\n",
      "    fear     0.493     0.091     0.013    80\n",
      "   happy     0.493     0.286     0.675    77\n",
      " neutral     0.493     0.761     0.537    95\n",
      "     sad     0.493     0.596     0.747    91\n",
      "surprise     0.493     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.493     0.405     0.461    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.416787 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 1.607425  [    0/ 5482]\n",
      "loss: 1.176959  [ 1200/ 5482]\n",
      "loss: 1.245398  [ 2400/ 5482]\n",
      "loss: 1.205940  [ 3600/ 5482]\n",
      "loss: 1.408844  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.593     0.677    99\n",
      " disgust     0.507     0.583     0.589    107\n",
      "    fear     0.507     0.062     0.013    80\n",
      "   happy     0.507     0.310     0.675    77\n",
      " neutral     0.507     0.740     0.600    95\n",
      "     sad     0.507     0.539     0.758    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.404     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.402281 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 1.416865  [    0/ 5482]\n",
      "loss: 1.337132  [ 1200/ 5482]\n",
      "loss: 1.239496  [ 2400/ 5482]\n",
      "loss: 1.521134  [ 3600/ 5482]\n",
      "loss: 1.495771  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.570     0.657    99\n",
      " disgust     0.507     0.531     0.645    107\n",
      "    fear     0.507     0.000     0.000    80\n",
      "   happy     0.507     0.290     0.701    77\n",
      " neutral     0.507     0.767     0.589    95\n",
      "     sad     0.507     0.670     0.714    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.404     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.406309 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 1.428308  [    0/ 5482]\n",
      "loss: 1.722792  [ 1200/ 5482]\n",
      "loss: 1.306746  [ 2400/ 5482]\n",
      "loss: 1.496439  [ 3600/ 5482]\n",
      "loss: 1.347609  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.595     0.697    99\n",
      " disgust     0.507     0.520     0.607    107\n",
      "    fear     0.507     0.125     0.013    80\n",
      "   happy     0.507     0.295     0.701    77\n",
      " neutral     0.507     0.730     0.568    95\n",
      "     sad     0.507     0.635     0.725    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.414     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.400170 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 1.156756  [    0/ 5482]\n",
      "loss: 1.348441  [ 1200/ 5482]\n",
      "loss: 1.447936  [ 2400/ 5482]\n",
      "loss: 1.629126  [ 3600/ 5482]\n",
      "loss: 1.381215  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.577     0.646    99\n",
      " disgust     0.489     0.525     0.598    107\n",
      "    fear     0.489     0.000     0.000    80\n",
      "   happy     0.489     0.276     0.714    77\n",
      " neutral     0.489     0.735     0.526    95\n",
      "     sad     0.489     0.657     0.714    91\n",
      "surprise     0.489     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.396     0.457    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.413189 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 1.561209  [    0/ 5482]\n",
      "loss: 1.337725  [ 1200/ 5482]\n",
      "loss: 1.470616  [ 2400/ 5482]\n",
      "loss: 1.570539  [ 3600/ 5482]\n",
      "loss: 1.420902  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.613     0.657    99\n",
      " disgust     0.508     0.512     0.598    107\n",
      "    fear     0.508     0.182     0.025    80\n",
      "   happy     0.508     0.303     0.727    77\n",
      " neutral     0.508     0.695     0.600    95\n",
      "     sad     0.508     0.653     0.725    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.423     0.476    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.397435 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 1.258297  [    0/ 5482]\n",
      "loss: 1.412405  [ 1200/ 5482]\n",
      "loss: 1.407853  [ 2400/ 5482]\n",
      "loss: 1.125082  [ 3600/ 5482]\n",
      "loss: 1.512014  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.520     0.650     0.657    99\n",
      " disgust     0.520     0.508     0.617    107\n",
      "    fear     0.520     0.000     0.000    80\n",
      "   happy     0.520     0.320     0.727    77\n",
      " neutral     0.520     0.699     0.684    95\n",
      "     sad     0.520     0.631     0.714    91\n",
      "surprise     0.520     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.520     0.401     0.486    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.392501 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 1.568042  [    0/ 5482]\n",
      "loss: 1.399082  [ 1200/ 5482]\n",
      "loss: 1.387880  [ 2400/ 5482]\n",
      "loss: 1.263368  [ 3600/ 5482]\n",
      "loss: 1.319024  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.589     0.667    99\n",
      " disgust     0.505     0.520     0.598    107\n",
      "    fear     0.505     0.182     0.025    80\n",
      "   happy     0.505     0.306     0.675    77\n",
      " neutral     0.505     0.674     0.611    95\n",
      "     sad     0.505     0.611     0.725    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.412     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.392249 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 1.450854  [    0/ 5482]\n",
      "loss: 1.415012  [ 1200/ 5482]\n",
      "loss: 1.414110  [ 2400/ 5482]\n",
      "loss: 1.434195  [ 3600/ 5482]\n",
      "loss: 1.333537  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.500     0.554     0.677    99\n",
      " disgust     0.500     0.525     0.579    107\n",
      "    fear     0.500     0.167     0.025    80\n",
      "   happy     0.500     0.298     0.701    77\n",
      " neutral     0.500     0.730     0.568    95\n",
      "     sad     0.500     0.635     0.725    91\n",
      "surprise     0.500     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.500     0.415     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.400130 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 1.327079  [    0/ 5482]\n",
      "loss: 1.252690  [ 1200/ 5482]\n",
      "loss: 1.597224  [ 2400/ 5482]\n",
      "loss: 1.162994  [ 3600/ 5482]\n",
      "loss: 1.399493  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.617     0.667    99\n",
      " disgust     0.505     0.555     0.570    107\n",
      "    fear     0.505     0.250     0.025    80\n",
      "   happy     0.505     0.312     0.701    77\n",
      " neutral     0.505     0.655     0.600    95\n",
      "     sad     0.505     0.544     0.747    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.419     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.392725 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 1.318512  [    0/ 5482]\n",
      "loss: 1.356993  [ 1200/ 5482]\n",
      "loss: 1.265544  [ 2400/ 5482]\n",
      "loss: 1.172627  [ 3600/ 5482]\n",
      "loss: 1.771357  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.507     0.634     0.646    99\n",
      " disgust     0.507     0.522     0.664    107\n",
      "    fear     0.507     0.000     0.000    80\n",
      "   happy     0.507     0.284     0.714    77\n",
      " neutral     0.507     0.775     0.579    95\n",
      "     sad     0.507     0.660     0.703    91\n",
      "surprise     0.507     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.507     0.411     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.399371 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 1.271055  [    0/ 5482]\n",
      "loss: 1.294846  [ 1200/ 5482]\n",
      "loss: 1.596365  [ 2400/ 5482]\n",
      "loss: 1.238803  [ 3600/ 5482]\n",
      "loss: 1.230913  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.617     0.667    99\n",
      " disgust     0.503     0.496     0.579    107\n",
      "    fear     0.503     0.250     0.037    80\n",
      "   happy     0.503     0.310     0.701    77\n",
      " neutral     0.503     0.679     0.579    95\n",
      "     sad     0.503     0.604     0.736    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.422     0.471    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.391284 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 1.310201  [    0/ 5482]\n",
      "loss: 1.367621  [ 1200/ 5482]\n",
      "loss: 1.409589  [ 2400/ 5482]\n",
      "loss: 1.526526  [ 3600/ 5482]\n",
      "loss: 1.245990  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.497     0.584     0.667    99\n",
      " disgust     0.497     0.591     0.514    107\n",
      "    fear     0.497     0.143     0.025    80\n",
      "   happy     0.497     0.313     0.675    77\n",
      " neutral     0.497     0.659     0.632    95\n",
      "     sad     0.497     0.511     0.747    91\n",
      "surprise     0.497     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.497     0.400     0.466    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.393073 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 1.268374  [    0/ 5482]\n",
      "loss: 1.210363  [ 1200/ 5482]\n",
      "loss: 1.382846  [ 2400/ 5482]\n",
      "loss: 1.156392  [ 3600/ 5482]\n",
      "loss: 1.509172  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.629     0.667    99\n",
      " disgust     0.511     0.548     0.589    107\n",
      "    fear     0.511     0.250     0.025    80\n",
      "   happy     0.511     0.310     0.675    77\n",
      " neutral     0.511     0.632     0.632    95\n",
      "     sad     0.511     0.576     0.747    91\n",
      "surprise     0.511     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.563     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.388464 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 1.360822  [    0/ 5482]\n",
      "loss: 1.469290  [ 1200/ 5482]\n",
      "loss: 1.448545  [ 2400/ 5482]\n",
      "loss: 1.193521  [ 3600/ 5482]\n",
      "loss: 1.326931  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.596     0.687    99\n",
      " disgust     0.505     0.541     0.495    107\n",
      "    fear     0.505     0.182     0.025    80\n",
      "   happy     0.505     0.333     0.662    77\n",
      " neutral     0.505     0.645     0.632    95\n",
      "     sad     0.505     0.525     0.813    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.403     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.393456 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 1.403386  [    0/ 5482]\n",
      "loss: 1.273100  [ 1200/ 5482]\n",
      "loss: 1.278373  [ 2400/ 5482]\n",
      "loss: 1.099148  [ 3600/ 5482]\n",
      "loss: 1.582429  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.602     0.657    99\n",
      " disgust     0.498     0.516     0.607    107\n",
      "    fear     0.498     0.000     0.000    80\n",
      "   happy     0.498     0.288     0.688    77\n",
      " neutral     0.498     0.750     0.568    95\n",
      "     sad     0.498     0.609     0.736    91\n",
      "surprise     0.498     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.395     0.465    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.395863 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 1.093052  [    0/ 5482]\n",
      "loss: 1.255012  [ 1200/ 5482]\n",
      "loss: 1.439014  [ 2400/ 5482]\n",
      "loss: 1.209414  [ 3600/ 5482]\n",
      "loss: 1.407009  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.600     0.667    99\n",
      " disgust     0.511     0.516     0.607    107\n",
      "    fear     0.511     0.222     0.025    80\n",
      "   happy     0.511     0.306     0.675    77\n",
      " neutral     0.511     0.706     0.632    95\n",
      "     sad     0.511     0.609     0.736    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.423     0.477    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.385936 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 1.390975  [    0/ 5482]\n",
      "loss: 1.207119  [ 1200/ 5482]\n",
      "loss: 1.420993  [ 2400/ 5482]\n",
      "loss: 1.482583  [ 3600/ 5482]\n",
      "loss: 1.594279  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.585     0.697    99\n",
      " disgust     0.513     0.552     0.542    107\n",
      "    fear     0.513     0.375     0.037    80\n",
      "   happy     0.513     0.329     0.662    77\n",
      " neutral     0.513     0.681     0.653    95\n",
      "     sad     0.513     0.526     0.769    91\n",
      "surprise     0.513     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.436     0.480    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.389421 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 1.255404  [    0/ 5482]\n",
      "loss: 1.194927  [ 1200/ 5482]\n",
      "loss: 1.578306  [ 2400/ 5482]\n",
      "loss: 1.598526  [ 3600/ 5482]\n",
      "loss: 1.440194  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.617     0.667    99\n",
      " disgust     0.502     0.524     0.607    107\n",
      "    fear     0.502     0.000     0.000    80\n",
      "   happy     0.502     0.289     0.701    77\n",
      " neutral     0.502     0.750     0.568    95\n",
      "     sad     0.502     0.615     0.736    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.399     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.392385 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 1.857754  [    0/ 5482]\n",
      "loss: 1.364671  [ 1200/ 5482]\n",
      "loss: 1.310367  [ 2400/ 5482]\n",
      "loss: 1.399956  [ 3600/ 5482]\n",
      "loss: 1.495089  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.657     0.657    99\n",
      " disgust     0.518     0.519     0.645    107\n",
      "    fear     0.518     0.100     0.013    80\n",
      "   happy     0.518     0.300     0.740    77\n",
      " neutral     0.518     0.738     0.621    95\n",
      "     sad     0.518     0.660     0.703    91\n",
      "surprise     0.518     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.568     0.485    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.387285 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 1.518595  [    0/ 5482]\n",
      "loss: 1.164010  [ 1200/ 5482]\n",
      "loss: 1.479322  [ 2400/ 5482]\n",
      "loss: 1.228032  [ 3600/ 5482]\n",
      "loss: 1.340699  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.619     0.657    99\n",
      " disgust     0.510     0.531     0.645    107\n",
      "    fear     0.510     0.000     0.000    80\n",
      "   happy     0.510     0.291     0.688    77\n",
      " neutral     0.510     0.750     0.600    95\n",
      "     sad     0.510     0.620     0.736    91\n",
      "surprise     0.510     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.402     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.386236 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 1.327744  [    0/ 5482]\n",
      "loss: 1.464098  [ 1200/ 5482]\n",
      "loss: 1.606827  [ 2400/ 5482]\n",
      "loss: 1.465800  [ 3600/ 5482]\n",
      "loss: 1.229806  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.620     0.677    99\n",
      " disgust     0.515     0.538     0.598    107\n",
      "    fear     0.515     0.091     0.013    80\n",
      "   happy     0.515     0.312     0.701    77\n",
      " neutral     0.515     0.698     0.632    95\n",
      "     sad     0.515     0.602     0.747    91\n",
      "surprise     0.515     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.409     0.481    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.381908 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 1.372173  [    0/ 5482]\n",
      "loss: 1.246749  [ 1200/ 5482]\n",
      "loss: 1.465829  [ 2400/ 5482]\n",
      "loss: 1.116396  [ 3600/ 5482]\n",
      "loss: 1.280161  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.570     0.697    99\n",
      " disgust     0.513     0.566     0.598    107\n",
      "    fear     0.513     0.143     0.025    80\n",
      "   happy     0.513     0.317     0.688    77\n",
      " neutral     0.513     0.770     0.600    95\n",
      "     sad     0.513     0.562     0.747    91\n",
      "surprise     0.513     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.418     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.387944 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 1.151633  [    0/ 5482]\n",
      "loss: 1.317580  [ 1200/ 5482]\n",
      "loss: 1.205238  [ 2400/ 5482]\n",
      "loss: 1.411313  [ 3600/ 5482]\n",
      "loss: 1.187575  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.637     0.657    99\n",
      " disgust     0.511     0.524     0.617    107\n",
      "    fear     0.511     0.083     0.013    80\n",
      "   happy     0.511     0.303     0.701    77\n",
      " neutral     0.511     0.704     0.600    95\n",
      "     sad     0.511     0.618     0.747    91\n",
      "surprise     0.511     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.553     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.382505 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 1.279581  [    0/ 5482]\n",
      "loss: 1.521977  [ 1200/ 5482]\n",
      "loss: 1.337154  [ 2400/ 5482]\n",
      "loss: 1.385107  [ 3600/ 5482]\n",
      "loss: 1.326966  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.528     0.663     0.657    99\n",
      " disgust     0.528     0.507     0.636    107\n",
      "    fear     0.528     0.333     0.037    80\n",
      "   happy     0.528     0.316     0.714    77\n",
      " neutral     0.528     0.681     0.653    95\n",
      "     sad     0.528     0.660     0.747    91\n",
      "surprise     0.528     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.528     0.595     0.494    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.380360 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 1.227987  [    0/ 5482]\n",
      "loss: 1.364578  [ 1200/ 5482]\n",
      "loss: 1.460000  [ 2400/ 5482]\n",
      "loss: 1.407446  [ 3600/ 5482]\n",
      "loss: 1.321694  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.502     0.593     0.677    99\n",
      " disgust     0.502     0.538     0.598    107\n",
      "    fear     0.502     0.000     0.000    80\n",
      "   happy     0.502     0.283     0.701    77\n",
      " neutral     0.502     0.750     0.568    95\n",
      "     sad     0.502     0.644     0.736    91\n",
      "surprise     0.502     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.502     0.401     0.469    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.391093 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 1.565635  [    0/ 5482]\n",
      "loss: 1.417069  [ 1200/ 5482]\n",
      "loss: 1.265172  [ 2400/ 5482]\n",
      "loss: 1.311979  [ 3600/ 5482]\n",
      "loss: 1.145068  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.674     0.646    99\n",
      " disgust     0.510     0.515     0.626    107\n",
      "    fear     0.510     0.000     0.000    80\n",
      "   happy     0.510     0.299     0.727    77\n",
      " neutral     0.510     0.718     0.589    95\n",
      "     sad     0.510     0.607     0.747    91\n",
      "surprise     0.510     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.402     0.477    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.384086 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 1.322941  [    0/ 5482]\n",
      "loss: 1.548628  [ 1200/ 5482]\n",
      "loss: 1.289241  [ 2400/ 5482]\n",
      "loss: 1.433881  [ 3600/ 5482]\n",
      "loss: 1.266947  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.598     0.677    99\n",
      " disgust     0.505     0.533     0.607    107\n",
      "    fear     0.505     0.077     0.013    80\n",
      "   happy     0.505     0.293     0.701    77\n",
      " neutral     0.505     0.753     0.579    95\n",
      "     sad     0.505     0.629     0.725    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.412     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.386078 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 1.507142  [    0/ 5482]\n",
      "loss: 1.076381  [ 1200/ 5482]\n",
      "loss: 1.648701  [ 2400/ 5482]\n",
      "loss: 1.233823  [ 3600/ 5482]\n",
      "loss: 1.387911  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.521     0.699     0.657    99\n",
      " disgust     0.521     0.517     0.570    107\n",
      "    fear     0.521     0.364     0.050    80\n",
      "   happy     0.521     0.310     0.740    77\n",
      " neutral     0.521     0.639     0.653    95\n",
      "     sad     0.521     0.642     0.747    91\n",
      "surprise     0.521     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.521     0.596     0.490    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.378442 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 1.280953  [    0/ 5482]\n",
      "loss: 1.549180  [ 1200/ 5482]\n",
      "loss: 1.593016  [ 2400/ 5482]\n",
      "loss: 1.346860  [ 3600/ 5482]\n",
      "loss: 1.385809  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.637     0.657    99\n",
      " disgust     0.516     0.504     0.607    107\n",
      "    fear     0.516     0.091     0.013    80\n",
      "   happy     0.516     0.306     0.714    77\n",
      " neutral     0.516     0.735     0.642    95\n",
      "     sad     0.516     0.644     0.736    91\n",
      "surprise     0.516     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.560     0.484    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.379111 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 1.413532  [    0/ 5482]\n",
      "loss: 1.216778  [ 1200/ 5482]\n",
      "loss: 1.431703  [ 2400/ 5482]\n",
      "loss: 1.364756  [ 3600/ 5482]\n",
      "loss: 1.168414  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.503     0.598     0.677    99\n",
      " disgust     0.503     0.561     0.514    107\n",
      "    fear     0.503     0.250     0.037    80\n",
      "   happy     0.503     0.315     0.688    77\n",
      " neutral     0.503     0.674     0.632    95\n",
      "     sad     0.503     0.531     0.758    91\n",
      "surprise     0.503     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.503     0.419     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.379383 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 1.275833  [    0/ 5482]\n",
      "loss: 1.060780  [ 1200/ 5482]\n",
      "loss: 1.470930  [ 2400/ 5482]\n",
      "loss: 1.529283  [ 3600/ 5482]\n",
      "loss: 1.431219  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.620     0.677    99\n",
      " disgust     0.511     0.524     0.607    107\n",
      "    fear     0.511     0.083     0.013    80\n",
      "   happy     0.511     0.302     0.714    77\n",
      " neutral     0.511     0.731     0.600    95\n",
      "     sad     0.511     0.629     0.725    91\n",
      "surprise     0.511     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.556     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.376364 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 1.379434  [    0/ 5482]\n",
      "loss: 1.358234  [ 1200/ 5482]\n",
      "loss: 1.305038  [ 2400/ 5482]\n",
      "loss: 1.341088  [ 3600/ 5482]\n",
      "loss: 1.498410  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.632     0.677    99\n",
      " disgust     0.508     0.532     0.617    107\n",
      "    fear     0.508     0.000     0.000    80\n",
      "   happy     0.508     0.297     0.701    77\n",
      " neutral     0.508     0.747     0.589    95\n",
      "     sad     0.508     0.615     0.736    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.403     0.474    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.380412 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 1.021305  [    0/ 5482]\n",
      "loss: 1.563483  [ 1200/ 5482]\n",
      "loss: 1.308625  [ 2400/ 5482]\n",
      "loss: 1.660591  [ 3600/ 5482]\n",
      "loss: 1.529254  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.635     0.667    99\n",
      " disgust     0.518     0.555     0.617    107\n",
      "    fear     0.518     0.286     0.025    80\n",
      "   happy     0.518     0.318     0.701    77\n",
      " neutral     0.518     0.659     0.632    95\n",
      "     sad     0.518     0.568     0.736    91\n",
      "surprise     0.518     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.574     0.485    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.373834 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 1.710702  [    0/ 5482]\n",
      "loss: 1.431583  [ 1200/ 5482]\n",
      "loss: 1.020209  [ 2400/ 5482]\n",
      "loss: 1.574442  [ 3600/ 5482]\n",
      "loss: 1.737209  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.650     0.677    99\n",
      " disgust     0.518     0.488     0.561    107\n",
      "    fear     0.518     0.333     0.050    80\n",
      "   happy     0.518     0.305     0.740    77\n",
      " neutral     0.518     0.701     0.642    95\n",
      "     sad     0.518     0.680     0.725    91\n",
      "surprise     0.518     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.594     0.487    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.375526 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 1.286981  [    0/ 5482]\n",
      "loss: 1.166846  [ 1200/ 5482]\n",
      "loss: 1.241016  [ 2400/ 5482]\n",
      "loss: 1.312298  [ 3600/ 5482]\n",
      "loss: 1.362625  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.653     0.646    99\n",
      " disgust     0.511     0.527     0.636    107\n",
      "    fear     0.511     0.000     0.000    80\n",
      "   happy     0.511     0.302     0.740    77\n",
      " neutral     0.511     0.757     0.589    95\n",
      "     sad     0.511     0.620     0.736    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.408     0.478    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.380126 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 1.389434  [    0/ 5482]\n",
      "loss: 1.594830  [ 1200/ 5482]\n",
      "loss: 1.480547  [ 2400/ 5482]\n",
      "loss: 1.246826  [ 3600/ 5482]\n",
      "loss: 1.679235  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.656     0.636    99\n",
      " disgust     0.525     0.526     0.664    107\n",
      "    fear     0.525     0.083     0.013    80\n",
      "   happy     0.525     0.310     0.740    77\n",
      " neutral     0.525     0.756     0.621    95\n",
      "     sad     0.525     0.654     0.747    91\n",
      "surprise     0.525     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.569     0.491    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.372673 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 1.919978  [    0/ 5482]\n",
      "loss: 1.149295  [ 1200/ 5482]\n",
      "loss: 1.421418  [ 2400/ 5482]\n",
      "loss: 1.321235  [ 3600/ 5482]\n",
      "loss: 1.535492  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.670     0.657    99\n",
      " disgust     0.516     0.511     0.654    107\n",
      "    fear     0.516     0.091     0.013    80\n",
      "   happy     0.516     0.297     0.740    77\n",
      " neutral     0.516     0.731     0.600    95\n",
      "     sad     0.516     0.684     0.714    91\n",
      "surprise     0.516     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.426     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.376868 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 1.115771  [    0/ 5482]\n",
      "loss: 1.513570  [ 1200/ 5482]\n",
      "loss: 1.439436  [ 2400/ 5482]\n",
      "loss: 1.292431  [ 3600/ 5482]\n",
      "loss: 1.193641  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.582     0.717    99\n",
      " disgust     0.510     0.589     0.589    107\n",
      "    fear     0.510     0.083     0.013    80\n",
      "   happy     0.510     0.305     0.662    77\n",
      " neutral     0.510     0.750     0.600    95\n",
      "     sad     0.510     0.544     0.747    91\n",
      "surprise     0.510     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.408     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.377970 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 1.328651  [    0/ 5482]\n",
      "loss: 1.047904  [ 1200/ 5482]\n",
      "loss: 1.240715  [ 2400/ 5482]\n",
      "loss: 1.066501  [ 3600/ 5482]\n",
      "loss: 1.352557  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.518     0.647     0.667    99\n",
      " disgust     0.518     0.571     0.673    107\n",
      "    fear     0.518     0.000     0.000    80\n",
      "   happy     0.518     0.285     0.714    77\n",
      " neutral     0.518     0.812     0.589    95\n",
      "     sad     0.518     0.626     0.736    91\n",
      "surprise     0.518     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.518     0.420     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.380528 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 1.624455  [    0/ 5482]\n",
      "loss: 1.498194  [ 1200/ 5482]\n",
      "loss: 1.336636  [ 2400/ 5482]\n",
      "loss: 1.329043  [ 3600/ 5482]\n",
      "loss: 1.472416  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.644     0.657    99\n",
      " disgust     0.516     0.508     0.598    107\n",
      "    fear     0.516     0.091     0.013    80\n",
      "   happy     0.516     0.307     0.714    77\n",
      " neutral     0.516     0.721     0.653    95\n",
      "     sad     0.516     0.632     0.736    91\n",
      "surprise     0.516     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.558     0.484    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.370499 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 1.672816  [    0/ 5482]\n",
      "loss: 1.361833  [ 1200/ 5482]\n",
      "loss: 1.602909  [ 2400/ 5482]\n",
      "loss: 1.261445  [ 3600/ 5482]\n",
      "loss: 1.184095  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.623     0.667    99\n",
      " disgust     0.510     0.552     0.542    107\n",
      "    fear     0.510     0.286     0.050    80\n",
      "   happy     0.510     0.321     0.688    77\n",
      " neutral     0.510     0.642     0.642    95\n",
      "     sad     0.510     0.548     0.747    91\n",
      "surprise     0.510     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.567     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.367270 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 1.586089  [    0/ 5482]\n",
      "loss: 1.676314  [ 1200/ 5482]\n",
      "loss: 1.266923  [ 2400/ 5482]\n",
      "loss: 1.190913  [ 3600/ 5482]\n",
      "loss: 1.353133  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.619     0.707    99\n",
      " disgust     0.508     0.535     0.570    107\n",
      "    fear     0.508     0.067     0.013    80\n",
      "   happy     0.508     0.299     0.727    77\n",
      " neutral     0.508     0.737     0.589    95\n",
      "     sad     0.508     0.635     0.725    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.413     0.476    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.375286 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 1.499261  [    0/ 5482]\n",
      "loss: 1.384246  [ 1200/ 5482]\n",
      "loss: 1.263706  [ 2400/ 5482]\n",
      "loss: 1.166384  [ 3600/ 5482]\n",
      "loss: 1.576611  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.636     0.687    99\n",
      " disgust     0.513     0.516     0.617    107\n",
      "    fear     0.513     0.000     0.000    80\n",
      "   happy     0.513     0.303     0.727    77\n",
      " neutral     0.513     0.737     0.589    95\n",
      "     sad     0.513     0.667     0.725    91\n",
      "surprise     0.513     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.551     0.480    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.371180 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 1.767014  [    0/ 5482]\n",
      "loss: 1.466125  [ 1200/ 5482]\n",
      "loss: 1.155910  [ 2400/ 5482]\n",
      "loss: 1.135838  [ 3600/ 5482]\n",
      "loss: 1.214319  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.590     0.697    99\n",
      " disgust     0.508     0.533     0.607    107\n",
      "    fear     0.508     0.111     0.013    80\n",
      "   happy     0.508     0.288     0.740    77\n",
      " neutral     0.508     0.757     0.589    95\n",
      "     sad     0.508     0.697     0.681    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.425     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.379808 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 1.320809  [    0/ 5482]\n",
      "loss: 1.307304  [ 1200/ 5482]\n",
      "loss: 1.430512  [ 2400/ 5482]\n",
      "loss: 1.345678  [ 3600/ 5482]\n",
      "loss: 1.208068  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.508     0.663     0.657    99\n",
      " disgust     0.508     0.508     0.617    107\n",
      "    fear     0.508     0.000     0.000    80\n",
      "   happy     0.508     0.285     0.740    77\n",
      " neutral     0.508     0.750     0.600    95\n",
      "     sad     0.508     0.670     0.714    91\n",
      "surprise     0.508     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.508     0.411     0.475    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.378184 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 1.294510  [    0/ 5482]\n",
      "loss: 1.632539  [ 1200/ 5482]\n",
      "loss: 1.329527  [ 2400/ 5482]\n",
      "loss: 1.187958  [ 3600/ 5482]\n",
      "loss: 1.181608  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.584     0.667    99\n",
      " disgust     0.513     0.542     0.607    107\n",
      "    fear     0.513     0.182     0.025    80\n",
      "   happy     0.513     0.315     0.688    77\n",
      " neutral     0.513     0.720     0.621    95\n",
      "     sad     0.513     0.586     0.747    91\n",
      "surprise     0.513     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.418     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.374213 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 1.418108  [    0/ 5482]\n",
      "loss: 1.054652  [ 1200/ 5482]\n",
      "loss: 1.378085  [ 2400/ 5482]\n",
      "loss: 1.427168  [ 3600/ 5482]\n",
      "loss: 1.691786  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.505     0.636     0.687    99\n",
      " disgust     0.505     0.558     0.589    107\n",
      "    fear     0.505     0.000     0.000    80\n",
      "   happy     0.505     0.292     0.701    77\n",
      " neutral     0.505     0.753     0.579    95\n",
      "     sad     0.505     0.586     0.747    91\n",
      "surprise     0.505     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.505     0.404     0.472    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.372730 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 1.256654  [    0/ 5482]\n",
      "loss: 1.674468  [ 1200/ 5482]\n",
      "loss: 1.122438  [ 2400/ 5482]\n",
      "loss: 1.367474  [ 3600/ 5482]\n",
      "loss: 1.712507  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.605     0.697    99\n",
      " disgust     0.516     0.544     0.579    107\n",
      "    fear     0.516     0.154     0.025    80\n",
      "   happy     0.516     0.310     0.688    77\n",
      " neutral     0.516     0.714     0.632    95\n",
      "     sad     0.516     0.611     0.758    91\n",
      "surprise     0.516     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.420     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.361267 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 1.596081  [    0/ 5482]\n",
      "loss: 1.294504  [ 1200/ 5482]\n",
      "loss: 1.409909  [ 2400/ 5482]\n",
      "loss: 1.434245  [ 3600/ 5482]\n",
      "loss: 1.103516  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.521     0.674     0.646    99\n",
      " disgust     0.521     0.508     0.626    107\n",
      "    fear     0.521     0.154     0.025    80\n",
      "   happy     0.521     0.309     0.753    77\n",
      " neutral     0.521     0.697     0.653    95\n",
      "     sad     0.521     0.699     0.714    91\n",
      "surprise     0.521     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.521     0.434     0.488    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.363960 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 1.354912  [    0/ 5482]\n",
      "loss: 1.188026  [ 1200/ 5482]\n",
      "loss: 1.303870  [ 2400/ 5482]\n",
      "loss: 1.365781  [ 3600/ 5482]\n",
      "loss: 1.313006  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.627     0.697    99\n",
      " disgust     0.515     0.538     0.598    107\n",
      "    fear     0.515     0.000     0.000    80\n",
      "   happy     0.515     0.292     0.727    77\n",
      " neutral     0.515     0.770     0.600    95\n",
      "     sad     0.515     0.642     0.747    91\n",
      "surprise     0.515     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.410     0.481    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.372176 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 1.548985  [    0/ 5482]\n",
      "loss: 1.052551  [ 1200/ 5482]\n",
      "loss: 1.289775  [ 2400/ 5482]\n",
      "loss: 1.313971  [ 3600/ 5482]\n",
      "loss: 1.223777  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.657     0.657    99\n",
      " disgust     0.515     0.508     0.607    107\n",
      "    fear     0.515     0.000     0.000    80\n",
      "   happy     0.515     0.300     0.740    77\n",
      " neutral     0.515     0.735     0.642    95\n",
      "     sad     0.515     0.660     0.725    91\n",
      "surprise     0.515     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.408     0.482    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.363244 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 1.356617  [    0/ 5482]\n",
      "loss: 1.505794  [ 1200/ 5482]\n",
      "loss: 1.381336  [ 2400/ 5482]\n",
      "loss: 1.697709  [ 3600/ 5482]\n",
      "loss: 1.575561  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.526     0.602     0.687    99\n",
      " disgust     0.526     0.508     0.626    107\n",
      "    fear     0.526     0.250     0.037    80\n",
      "   happy     0.526     0.322     0.714    77\n",
      " neutral     0.526     0.747     0.653    95\n",
      "     sad     0.526     0.667     0.725    91\n",
      "surprise     0.526     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.526     0.442     0.492    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.360115 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.907295  [    0/ 5482]\n",
      "loss: 1.168345  [ 1200/ 5482]\n",
      "loss: 1.532987  [ 2400/ 5482]\n",
      "loss: 1.632702  [ 3600/ 5482]\n",
      "loss: 1.277167  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.569     0.707    99\n",
      " disgust     0.510     0.555     0.570    107\n",
      "    fear     0.510     0.200     0.025    80\n",
      "   happy     0.510     0.338     0.649    77\n",
      " neutral     0.510     0.655     0.600    95\n",
      "     sad     0.510     0.542     0.780    91\n",
      "surprise     0.510     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.408     0.476    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.362195 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 1.470046  [    0/ 5482]\n",
      "loss: 1.283509  [ 1200/ 5482]\n",
      "loss: 1.288194  [ 2400/ 5482]\n",
      "loss: 1.473087  [ 3600/ 5482]\n",
      "loss: 1.280761  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.520     0.674     0.646    99\n",
      " disgust     0.520     0.504     0.645    107\n",
      "    fear     0.520     0.167     0.025    80\n",
      "   happy     0.520     0.300     0.740    77\n",
      " neutral     0.520     0.726     0.642    95\n",
      "     sad     0.520     0.696     0.703    91\n",
      "surprise     0.520     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.520     0.438     0.486    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.363570 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 1.236263  [    0/ 5482]\n",
      "loss: 1.385651  [ 1200/ 5482]\n",
      "loss: 1.202149  [ 2400/ 5482]\n",
      "loss: 1.237257  [ 3600/ 5482]\n",
      "loss: 1.160539  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.520     0.638     0.677    99\n",
      " disgust     0.520     0.528     0.626    107\n",
      "    fear     0.520     0.091     0.013    80\n",
      "   happy     0.520     0.303     0.688    77\n",
      " neutral     0.520     0.718     0.642    95\n",
      "     sad     0.520     0.632     0.736    91\n",
      "surprise     0.520     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.520     0.558     0.486    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.356132 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 1.286120  [    0/ 5482]\n",
      "loss: 1.206577  [ 1200/ 5482]\n",
      "loss: 1.212683  [ 2400/ 5482]\n",
      "loss: 1.220750  [ 3600/ 5482]\n",
      "loss: 1.060544  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.620     0.677    99\n",
      " disgust     0.516     0.565     0.607    107\n",
      "    fear     0.516     0.250     0.037    80\n",
      "   happy     0.516     0.312     0.688    77\n",
      " neutral     0.516     0.694     0.621    95\n",
      "     sad     0.516     0.563     0.736    91\n",
      "surprise     0.516     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.572     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.357916 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 1.401209  [    0/ 5482]\n",
      "loss: 1.289421  [ 1200/ 5482]\n",
      "loss: 1.490704  [ 2400/ 5482]\n",
      "loss: 1.281485  [ 3600/ 5482]\n",
      "loss: 1.422063  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.530     0.605     0.697    99\n",
      " disgust     0.530     0.524     0.617    107\n",
      "    fear     0.530     0.273     0.037    80\n",
      "   happy     0.530     0.327     0.714    77\n",
      " neutral     0.530     0.724     0.663    95\n",
      "     sad     0.530     0.641     0.725    91\n",
      "surprise     0.530     1.000     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.530     0.585     0.496    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.353910 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 1.316991  [    0/ 5482]\n",
      "loss: 1.199776  [ 1200/ 5482]\n",
      "loss: 1.494514  [ 2400/ 5482]\n",
      "loss: 1.047477  [ 3600/ 5482]\n",
      "loss: 1.190659  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.642     0.687    99\n",
      " disgust     0.513     0.532     0.617    107\n",
      "    fear     0.513     0.000     0.000    80\n",
      "   happy     0.513     0.286     0.714    77\n",
      " neutral     0.513     0.760     0.600    95\n",
      "     sad     0.513     0.641     0.725    91\n",
      "surprise     0.513     0.500     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.480     0.480    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.362533 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 1.484035  [    0/ 5482]\n",
      "loss: 1.172015  [ 1200/ 5482]\n",
      "loss: 1.444807  [ 2400/ 5482]\n",
      "loss: 1.923286  [ 3600/ 5482]\n",
      "loss: 1.204150  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.516     0.657     0.657    99\n",
      " disgust     0.516     0.530     0.654    107\n",
      "    fear     0.516     0.000     0.000    80\n",
      "   happy     0.516     0.286     0.727    77\n",
      " neutral     0.516     0.800     0.589    95\n",
      "     sad     0.516     0.648     0.747    91\n",
      "surprise     0.516     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.516     0.417     0.482    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.364022 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 1.128854  [    0/ 5482]\n",
      "loss: 1.333661  [ 1200/ 5482]\n",
      "loss: 1.355757  [ 2400/ 5482]\n",
      "loss: 1.456123  [ 3600/ 5482]\n",
      "loss: 1.292889  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.521     0.632     0.677    99\n",
      " disgust     0.521     0.548     0.636    107\n",
      "    fear     0.521     0.182     0.025    80\n",
      "   happy     0.521     0.311     0.714    77\n",
      " neutral     0.521     0.723     0.632    95\n",
      "     sad     0.521     0.606     0.725    91\n",
      "surprise     0.521     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.521     0.429     0.487    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.353067 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 1.417884  [    0/ 5482]\n",
      "loss: 1.541062  [ 1200/ 5482]\n",
      "loss: 1.100648  [ 2400/ 5482]\n",
      "loss: 1.242784  [ 3600/ 5482]\n",
      "loss: 1.301984  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.670     0.657    99\n",
      " disgust     0.513     0.527     0.645    107\n",
      "    fear     0.513     0.154     0.025    80\n",
      "   happy     0.513     0.284     0.753    77\n",
      " neutral     0.513     0.778     0.589    95\n",
      "     sad     0.513     0.685     0.692    91\n",
      "surprise     0.513     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.443     0.480    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.373179 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 1.223238  [    0/ 5482]\n",
      "loss: 1.172560  [ 1200/ 5482]\n",
      "loss: 1.761080  [ 2400/ 5482]\n",
      "loss: 1.475309  [ 3600/ 5482]\n",
      "loss: 1.100678  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.660     0.667    99\n",
      " disgust     0.513     0.554     0.579    107\n",
      "    fear     0.513     0.429     0.037    80\n",
      "   happy     0.513     0.312     0.701    77\n",
      " neutral     0.513     0.674     0.611    95\n",
      "     sad     0.513     0.535     0.758    91\n",
      "surprise     0.513     0.333     0.016    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.500     0.481    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.356931 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.841624  [    0/ 5482]\n",
      "loss: 1.420967  [ 1200/ 5482]\n",
      "loss: 1.281902  [ 2400/ 5482]\n",
      "loss: 1.217422  [ 3600/ 5482]\n",
      "loss: 1.460642  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.648     0.687    99\n",
      " disgust     0.515     0.546     0.607    107\n",
      "    fear     0.515     0.059     0.013    80\n",
      "   happy     0.515     0.303     0.740    77\n",
      " neutral     0.515     0.767     0.589    95\n",
      "     sad     0.515     0.626     0.736    91\n",
      "surprise     0.515     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.421     0.482    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.360851 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 1.413937  [    0/ 5482]\n",
      "loss: 1.382739  [ 1200/ 5482]\n",
      "loss: 1.264786  [ 2400/ 5482]\n",
      "loss: 1.250840  [ 3600/ 5482]\n",
      "loss: 1.242127  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.511     0.657     0.657    99\n",
      " disgust     0.511     0.496     0.579    107\n",
      "    fear     0.511     0.091     0.013    80\n",
      "   happy     0.511     0.310     0.740    77\n",
      " neutral     0.511     0.705     0.653    95\n",
      "     sad     0.511     0.637     0.714    91\n",
      "surprise     0.511     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.511     0.414     0.479    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.351703 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 1.169230  [    0/ 5482]\n",
      "loss: 1.314778  [ 1200/ 5482]\n",
      "loss: 1.310095  [ 2400/ 5482]\n",
      "loss: 1.104522  [ 3600/ 5482]\n",
      "loss: 1.413021  [ 4800/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.510     0.663     0.657    99\n",
      " disgust     0.510     0.496     0.598    107\n",
      "    fear     0.510     0.083     0.013    80\n",
      "   happy     0.510     0.302     0.714    77\n",
      " neutral     0.510     0.702     0.621    95\n",
      "     sad     0.510     0.644     0.736    91\n",
      "surprise     0.510     0.000     0.000    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.510     0.413     0.477    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.349415 \n",
      "\n",
      "Generating Report... \n",
      "\n",
      "Report saved to: /home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/notebooks/content/soundstream/experiments/experiment_v_12_1_NR1/Run_Nr_0/dimred.md \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.822004  [    0/ 5482]\n",
      "loss: 1.988740  [  600/ 5482]\n",
      "loss: 1.847151  [ 1200/ 5482]\n",
      "loss: 1.762228  [ 1800/ 5482]\n",
      "loss: 1.979193  [ 2400/ 5482]\n",
      "loss: 1.923629  [ 3000/ 5482]\n",
      "loss: 1.756514  [ 3600/ 5482]\n",
      "loss: 1.950403  [ 4200/ 5482]\n",
      "loss: 1.819160  [ 4800/ 5482]\n",
      "loss: 1.944856  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.152     0.333     0.081    99\n",
      " disgust     0.152     0.000     0.000    107\n",
      "    fear     0.152     0.141     0.963    80\n",
      "   happy     0.152     0.000     0.000    77\n",
      " neutral     0.152     0.000     0.000    95\n",
      "     sad     0.152     0.176     0.033    91\n",
      "surprise     0.152     0.208     0.082    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.152     0.123     0.165    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 1.912352 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.943381  [    0/ 5482]\n",
      "loss: 1.856444  [  600/ 5482]\n",
      "loss: 1.777243  [ 1200/ 5482]\n",
      "loss: 2.018300  [ 1800/ 5482]\n",
      "loss: 2.093305  [ 2400/ 5482]\n",
      "loss: 1.948958  [ 3000/ 5482]\n",
      "loss: 1.989530  [ 3600/ 5482]\n",
      "loss: 1.779854  [ 4200/ 5482]\n",
      "loss: 2.014181  [ 4800/ 5482]\n",
      "loss: 1.843021  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.179     0.354     0.172    99\n",
      " disgust     0.179     0.000     0.000    107\n",
      "    fear     0.179     0.147     0.863    80\n",
      "   happy     0.179     0.000     0.000    77\n",
      " neutral     0.179     0.000     0.000    95\n",
      "     sad     0.179     0.185     0.132    91\n",
      "surprise     0.179     0.379     0.180    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.179     0.152     0.192    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 17.9%, Avg loss: 1.888334 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.986214  [    0/ 5482]\n",
      "loss: 1.940924  [  600/ 5482]\n",
      "loss: 2.012872  [ 1200/ 5482]\n",
      "loss: 1.809199  [ 1800/ 5482]\n",
      "loss: 1.813030  [ 2400/ 5482]\n",
      "loss: 1.850199  [ 3000/ 5482]\n",
      "loss: 1.986532  [ 3600/ 5482]\n",
      "loss: 1.862440  [ 4200/ 5482]\n",
      "loss: 1.910911  [ 4800/ 5482]\n",
      "loss: 1.754029  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.226     0.432     0.192    99\n",
      " disgust     0.226     0.000     0.000    107\n",
      "    fear     0.226     0.176     0.875    80\n",
      "   happy     0.226     0.000     0.000    77\n",
      " neutral     0.226     0.000     0.000    95\n",
      "     sad     0.226     0.277     0.418    91\n",
      "surprise     0.226     0.355     0.180    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.226     0.177     0.238    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 22.6%, Avg loss: 1.874977 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.937744  [    0/ 5482]\n",
      "loss: 1.883158  [  600/ 5482]\n",
      "loss: 2.052261  [ 1200/ 5482]\n",
      "loss: 1.933834  [ 1800/ 5482]\n",
      "loss: 1.929294  [ 2400/ 5482]\n",
      "loss: 1.789737  [ 3000/ 5482]\n",
      "loss: 1.961410  [ 3600/ 5482]\n",
      "loss: 1.985514  [ 4200/ 5482]\n",
      "loss: 1.854624  [ 4800/ 5482]\n",
      "loss: 2.017554  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.256     0.435     0.303    99\n",
      " disgust     0.256     0.000     0.000    107\n",
      "    fear     0.256     0.199     0.775    80\n",
      "   happy     0.256     0.000     0.000    77\n",
      " neutral     0.256     0.000     0.000    95\n",
      "     sad     0.256     0.322     0.527    91\n",
      "surprise     0.256     0.200     0.262    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.256     0.165     0.267    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.6%, Avg loss: 1.864842 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.773931  [    0/ 5482]\n",
      "loss: 2.098245  [  600/ 5482]\n",
      "loss: 1.981640  [ 1200/ 5482]\n",
      "loss: 1.857073  [ 1800/ 5482]\n",
      "loss: 1.845518  [ 2400/ 5482]\n",
      "loss: 1.770604  [ 3000/ 5482]\n",
      "loss: 1.838708  [ 3600/ 5482]\n",
      "loss: 1.700346  [ 4200/ 5482]\n",
      "loss: 2.043226  [ 4800/ 5482]\n",
      "loss: 2.021851  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.261     0.455     0.303    99\n",
      " disgust     0.261     0.000     0.000    107\n",
      "    fear     0.261     0.198     0.812    80\n",
      "   happy     0.261     0.000     0.000    77\n",
      " neutral     0.261     0.000     0.000    95\n",
      "     sad     0.261     0.311     0.549    91\n",
      "surprise     0.261     0.255     0.230    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.261     0.174     0.271    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 1.856759 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.611658  [    0/ 5482]\n",
      "loss: 1.774899  [  600/ 5482]\n",
      "loss: 1.797589  [ 1200/ 5482]\n",
      "loss: 1.761675  [ 1800/ 5482]\n",
      "loss: 1.917185  [ 2400/ 5482]\n",
      "loss: 1.935032  [ 3000/ 5482]\n",
      "loss: 1.938471  [ 3600/ 5482]\n",
      "loss: 2.039805  [ 4200/ 5482]\n",
      "loss: 1.926817  [ 4800/ 5482]\n",
      "loss: 1.733551  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.254     0.435     0.273    99\n",
      " disgust     0.254     0.000     0.000    107\n",
      "    fear     0.254     0.194     0.887    80\n",
      "   happy     0.254     0.000     0.000    77\n",
      " neutral     0.254     0.000     0.000    95\n",
      "     sad     0.254     0.322     0.527    91\n",
      "surprise     0.254     0.273     0.148    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.254     0.175     0.262    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 1.851516 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.905246  [    0/ 5482]\n",
      "loss: 1.753871  [  600/ 5482]\n",
      "loss: 1.723882  [ 1200/ 5482]\n",
      "loss: 1.857768  [ 1800/ 5482]\n",
      "loss: 1.743893  [ 2400/ 5482]\n",
      "loss: 1.810203  [ 3000/ 5482]\n",
      "loss: 1.835629  [ 3600/ 5482]\n",
      "loss: 1.910636  [ 4200/ 5482]\n",
      "loss: 1.850775  [ 4800/ 5482]\n",
      "loss: 2.049063  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.264     0.438     0.283    99\n",
      " disgust     0.264     0.000     0.000    107\n",
      "    fear     0.264     0.204     0.838    80\n",
      "   happy     0.264     0.000     0.000    77\n",
      " neutral     0.264     0.000     0.000    95\n",
      "     sad     0.264     0.317     0.582    91\n",
      "surprise     0.264     0.255     0.213    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.264     0.173     0.274    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.4%, Avg loss: 1.842944 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.954114  [    0/ 5482]\n",
      "loss: 1.748390  [  600/ 5482]\n",
      "loss: 1.842223  [ 1200/ 5482]\n",
      "loss: 1.927186  [ 1800/ 5482]\n",
      "loss: 1.882001  [ 2400/ 5482]\n",
      "loss: 1.719610  [ 3000/ 5482]\n",
      "loss: 1.659290  [ 3600/ 5482]\n",
      "loss: 1.935554  [ 4200/ 5482]\n",
      "loss: 1.993785  [ 4800/ 5482]\n",
      "loss: 1.846607  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.293     0.427     0.505    99\n",
      " disgust     0.293     0.000     0.000    107\n",
      "    fear     0.293     0.228     0.650    80\n",
      "   happy     0.293     0.000     0.000    77\n",
      " neutral     0.293     0.000     0.000    95\n",
      "     sad     0.293     0.372     0.593    91\n",
      "surprise     0.293     0.192     0.377    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.293     0.174     0.304    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 29.3%, Avg loss: 1.841800 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.035648  [    0/ 5482]\n",
      "loss: 1.737339  [  600/ 5482]\n",
      "loss: 1.693069  [ 1200/ 5482]\n",
      "loss: 1.852855  [ 1800/ 5482]\n",
      "loss: 1.953343  [ 2400/ 5482]\n",
      "loss: 1.780568  [ 3000/ 5482]\n",
      "loss: 1.996064  [ 3600/ 5482]\n",
      "loss: 1.846906  [ 4200/ 5482]\n",
      "loss: 1.653478  [ 4800/ 5482]\n",
      "loss: 1.826064  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.261     0.463     0.253    99\n",
      " disgust     0.261     0.000     0.000    107\n",
      "    fear     0.261     0.202     0.875    80\n",
      "   happy     0.261     0.000     0.000    77\n",
      " neutral     0.261     0.000     0.000    95\n",
      "     sad     0.261     0.316     0.593    91\n",
      "surprise     0.261     0.263     0.164    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.261     0.178     0.269    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 1.831183 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.732483  [    0/ 5482]\n",
      "loss: 1.660982  [  600/ 5482]\n",
      "loss: 1.668430  [ 1200/ 5482]\n",
      "loss: 1.683085  [ 1800/ 5482]\n",
      "loss: 1.831195  [ 2400/ 5482]\n",
      "loss: 1.798609  [ 3000/ 5482]\n",
      "loss: 1.669582  [ 3600/ 5482]\n",
      "loss: 1.955004  [ 4200/ 5482]\n",
      "loss: 1.746792  [ 4800/ 5482]\n",
      "loss: 1.699423  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.310     0.524     0.444    99\n",
      " disgust     0.310     0.000     0.000    107\n",
      "    fear     0.310     0.237     0.787    80\n",
      "   happy     0.310     0.000     0.000    77\n",
      " neutral     0.310     0.000     0.000    95\n",
      "     sad     0.310     0.341     0.670    91\n",
      "surprise     0.310     0.259     0.344    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.310     0.194     0.321    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.0%, Avg loss: 1.824154 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.831099  [    0/ 5482]\n",
      "loss: 1.674081  [  600/ 5482]\n",
      "loss: 1.815037  [ 1200/ 5482]\n",
      "loss: 1.915417  [ 1800/ 5482]\n",
      "loss: 1.929998  [ 2400/ 5482]\n",
      "loss: 1.896212  [ 3000/ 5482]\n",
      "loss: 1.849763  [ 3600/ 5482]\n",
      "loss: 1.707328  [ 4200/ 5482]\n",
      "loss: 1.928502  [ 4800/ 5482]\n",
      "loss: 1.655393  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.285     0.529     0.374    99\n",
      " disgust     0.285     0.000     0.000    107\n",
      "    fear     0.285     0.216     0.863    80\n",
      "   happy     0.285     0.000     0.000    77\n",
      " neutral     0.285     0.000     0.000    95\n",
      "     sad     0.285     0.329     0.604    91\n",
      "surprise     0.285     0.245     0.213    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.285     0.188     0.293    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 1.819923 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.698276  [    0/ 5482]\n",
      "loss: 1.869110  [  600/ 5482]\n",
      "loss: 1.773967  [ 1200/ 5482]\n",
      "loss: 1.740820  [ 1800/ 5482]\n",
      "loss: 2.057305  [ 2400/ 5482]\n",
      "loss: 1.892303  [ 3000/ 5482]\n",
      "loss: 1.758495  [ 3600/ 5482]\n",
      "loss: 1.886319  [ 4200/ 5482]\n",
      "loss: 1.859359  [ 4800/ 5482]\n",
      "loss: 1.837591  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.287     0.519     0.404    99\n",
      " disgust     0.287     0.000     0.000    107\n",
      "    fear     0.287     0.215     0.838    80\n",
      "   happy     0.287     0.000     0.000    77\n",
      " neutral     0.287     0.000     0.000    95\n",
      "     sad     0.287     0.333     0.593    91\n",
      "surprise     0.287     0.237     0.230    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.287     0.186     0.295    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 28.7%, Avg loss: 1.816459 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.620481  [    0/ 5482]\n",
      "loss: 1.840693  [  600/ 5482]\n",
      "loss: 1.902481  [ 1200/ 5482]\n",
      "loss: 1.618958  [ 1800/ 5482]\n",
      "loss: 1.820070  [ 2400/ 5482]\n",
      "loss: 1.962623  [ 3000/ 5482]\n",
      "loss: 2.039839  [ 3600/ 5482]\n",
      "loss: 2.081522  [ 4200/ 5482]\n",
      "loss: 1.709581  [ 4800/ 5482]\n",
      "loss: 1.822604  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.270     0.604     0.293    99\n",
      " disgust     0.270     0.000     0.000    107\n",
      "    fear     0.270     0.202     0.912    80\n",
      "   happy     0.270     0.000     0.000    77\n",
      " neutral     0.270     0.000     0.000    95\n",
      "     sad     0.270     0.317     0.582    91\n",
      "surprise     0.270     0.303     0.164    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.270     0.204     0.279    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 27.0%, Avg loss: 1.813943 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.639284  [    0/ 5482]\n",
      "loss: 1.872690  [  600/ 5482]\n",
      "loss: 1.720846  [ 1200/ 5482]\n",
      "loss: 1.765697  [ 1800/ 5482]\n",
      "loss: 1.794623  [ 2400/ 5482]\n",
      "loss: 1.853331  [ 3000/ 5482]\n",
      "loss: 1.670570  [ 3600/ 5482]\n",
      "loss: 1.772686  [ 4200/ 5482]\n",
      "loss: 1.815612  [ 4800/ 5482]\n",
      "loss: 1.931834  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.311     0.613     0.384    99\n",
      " disgust     0.311     0.000     0.000    107\n",
      "    fear     0.311     0.243     0.875    80\n",
      "   happy     0.311     0.000     0.000    77\n",
      " neutral     0.311     0.000     0.000    95\n",
      "     sad     0.311     0.304     0.692    91\n",
      "surprise     0.311     0.358     0.311    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.311     0.217     0.323    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.1%, Avg loss: 1.801665 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.019884  [    0/ 5482]\n",
      "loss: 1.861227  [  600/ 5482]\n",
      "loss: 1.731308  [ 1200/ 5482]\n",
      "loss: 1.644710  [ 1800/ 5482]\n",
      "loss: 1.769606  [ 2400/ 5482]\n",
      "loss: 1.788338  [ 3000/ 5482]\n",
      "loss: 1.820480  [ 3600/ 5482]\n",
      "loss: 1.845837  [ 4200/ 5482]\n",
      "loss: 1.783005  [ 4800/ 5482]\n",
      "loss: 1.722821  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.581     0.434    99\n",
      " disgust     0.321     0.000     0.000    107\n",
      "    fear     0.321     0.246     0.775    80\n",
      "   happy     0.321     0.000     0.000    77\n",
      " neutral     0.321     0.000     0.000    95\n",
      "     sad     0.321     0.306     0.703    91\n",
      "surprise     0.321     0.360     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.213     0.336    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.795372 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.813641  [    0/ 5482]\n",
      "loss: 1.797315  [  600/ 5482]\n",
      "loss: 1.858374  [ 1200/ 5482]\n",
      "loss: 1.625660  [ 1800/ 5482]\n",
      "loss: 1.762231  [ 2400/ 5482]\n",
      "loss: 1.682040  [ 3000/ 5482]\n",
      "loss: 1.734622  [ 3600/ 5482]\n",
      "loss: 1.687336  [ 4200/ 5482]\n",
      "loss: 1.864178  [ 4800/ 5482]\n",
      "loss: 1.975298  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.321     0.535     0.535    99\n",
      " disgust     0.321     0.000     0.000    107\n",
      "    fear     0.321     0.255     0.613    80\n",
      "   happy     0.321     0.000     0.000    77\n",
      " neutral     0.321     0.000     0.000    95\n",
      "     sad     0.321     0.291     0.681    91\n",
      "surprise     0.321     0.302     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.321     0.198     0.336    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.793037 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.565140  [    0/ 5482]\n",
      "loss: 1.919927  [  600/ 5482]\n",
      "loss: 1.752673  [ 1200/ 5482]\n",
      "loss: 1.945690  [ 1800/ 5482]\n",
      "loss: 1.937162  [ 2400/ 5482]\n",
      "loss: 1.789173  [ 3000/ 5482]\n",
      "loss: 1.584396  [ 3600/ 5482]\n",
      "loss: 1.695611  [ 4200/ 5482]\n",
      "loss: 1.721663  [ 4800/ 5482]\n",
      "loss: 1.606649  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.316     0.641     0.414    99\n",
      " disgust     0.316     0.000     0.000    107\n",
      "    fear     0.316     0.250     0.900    80\n",
      "   happy     0.316     0.000     0.000    77\n",
      " neutral     0.316     0.000     0.000    95\n",
      "     sad     0.316     0.304     0.681    91\n",
      "surprise     0.316     0.333     0.295    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.316     0.218     0.327    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 31.6%, Avg loss: 1.790378 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.666130  [    0/ 5482]\n",
      "loss: 1.730328  [  600/ 5482]\n",
      "loss: 1.725295  [ 1200/ 5482]\n",
      "loss: 1.835732  [ 1800/ 5482]\n",
      "loss: 1.640118  [ 2400/ 5482]\n",
      "loss: 1.872630  [ 3000/ 5482]\n",
      "loss: 1.640713  [ 3600/ 5482]\n",
      "loss: 1.815614  [ 4200/ 5482]\n",
      "loss: 1.764411  [ 4800/ 5482]\n",
      "loss: 1.852841  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.326     0.635     0.475    99\n",
      " disgust     0.326     0.000     0.000    107\n",
      "    fear     0.326     0.252     0.700    80\n",
      "   happy     0.326     0.000     0.000    77\n",
      " neutral     0.326     0.000     0.000    95\n",
      "     sad     0.326     0.281     0.714    91\n",
      "surprise     0.326     0.373     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.326     0.220     0.342    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.6%, Avg loss: 1.781356 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.904072  [    0/ 5482]\n",
      "loss: 1.556851  [  600/ 5482]\n",
      "loss: 1.769364  [ 1200/ 5482]\n",
      "loss: 1.627607  [ 1800/ 5482]\n",
      "loss: 1.842142  [ 2400/ 5482]\n",
      "loss: 1.520946  [ 3000/ 5482]\n",
      "loss: 1.699127  [ 3600/ 5482]\n",
      "loss: 2.056327  [ 4200/ 5482]\n",
      "loss: 1.883524  [ 4800/ 5482]\n",
      "loss: 1.926013  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.325     0.610     0.475    99\n",
      " disgust     0.325     0.000     0.000    107\n",
      "    fear     0.325     0.252     0.713    80\n",
      "   happy     0.325     0.000     0.000    77\n",
      " neutral     0.325     0.000     0.000    95\n",
      "     sad     0.325     0.283     0.714    91\n",
      "surprise     0.325     0.377     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.325     0.217     0.340    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 1.775177 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.657991  [    0/ 5482]\n",
      "loss: 1.747916  [  600/ 5482]\n",
      "loss: 1.945399  [ 1200/ 5482]\n",
      "loss: 1.733999  [ 1800/ 5482]\n",
      "loss: 1.736683  [ 2400/ 5482]\n",
      "loss: 1.884744  [ 3000/ 5482]\n",
      "loss: 1.814862  [ 3600/ 5482]\n",
      "loss: 1.665018  [ 4200/ 5482]\n",
      "loss: 1.748177  [ 4800/ 5482]\n",
      "loss: 1.807242  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.338     0.582     0.535    99\n",
      " disgust     0.338     0.000     0.000    107\n",
      "    fear     0.338     0.260     0.750    80\n",
      "   happy     0.338     0.000     0.000    77\n",
      " neutral     0.338     0.000     0.000    95\n",
      "     sad     0.338     0.311     0.703    91\n",
      "surprise     0.338     0.354     0.475    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.338     0.215     0.352    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.773517 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.649077  [    0/ 5482]\n",
      "loss: 1.953616  [  600/ 5482]\n",
      "loss: 1.544285  [ 1200/ 5482]\n",
      "loss: 1.627187  [ 1800/ 5482]\n",
      "loss: 1.746127  [ 2400/ 5482]\n",
      "loss: 1.783827  [ 3000/ 5482]\n",
      "loss: 1.912462  [ 3600/ 5482]\n",
      "loss: 1.890948  [ 4200/ 5482]\n",
      "loss: 1.769802  [ 4800/ 5482]\n",
      "loss: 1.619941  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.343     0.560     0.616    99\n",
      " disgust     0.343     0.000     0.000    107\n",
      "    fear     0.343     0.265     0.713    80\n",
      "   happy     0.343     0.000     0.000    77\n",
      " neutral     0.343     0.000     0.000    95\n",
      "     sad     0.343     0.315     0.703    91\n",
      "surprise     0.343     0.325     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.343     0.209     0.354    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.772291 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.771585  [    0/ 5482]\n",
      "loss: 1.930709  [  600/ 5482]\n",
      "loss: 1.648871  [ 1200/ 5482]\n",
      "loss: 1.702082  [ 1800/ 5482]\n",
      "loss: 1.735711  [ 2400/ 5482]\n",
      "loss: 1.795920  [ 3000/ 5482]\n",
      "loss: 1.793524  [ 3600/ 5482]\n",
      "loss: 1.715700  [ 4200/ 5482]\n",
      "loss: 1.718114  [ 4800/ 5482]\n",
      "loss: 1.692024  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.336     0.589     0.566    99\n",
      " disgust     0.336     0.000     0.000    107\n",
      "    fear     0.336     0.265     0.662    80\n",
      "   happy     0.336     0.000     0.000    77\n",
      " neutral     0.336     0.000     0.000    95\n",
      "     sad     0.336     0.283     0.714    91\n",
      "surprise     0.336     0.365     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.336     0.215     0.350    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.764008 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.744353  [    0/ 5482]\n",
      "loss: 1.804259  [  600/ 5482]\n",
      "loss: 1.905638  [ 1200/ 5482]\n",
      "loss: 1.660325  [ 1800/ 5482]\n",
      "loss: 1.737007  [ 2400/ 5482]\n",
      "loss: 1.691039  [ 3000/ 5482]\n",
      "loss: 1.709305  [ 3600/ 5482]\n",
      "loss: 1.745034  [ 4200/ 5482]\n",
      "loss: 1.861398  [ 4800/ 5482]\n",
      "loss: 1.623908  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.344     0.602     0.596    99\n",
      " disgust     0.344     0.000     0.000    107\n",
      "    fear     0.344     0.279     0.688    80\n",
      "   happy     0.344     0.000     0.000    77\n",
      " neutral     0.344     0.000     0.000    95\n",
      "     sad     0.344     0.284     0.714    91\n",
      "surprise     0.344     0.360     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.344     0.218     0.358    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.760813 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.730356  [    0/ 5482]\n",
      "loss: 1.897425  [  600/ 5482]\n",
      "loss: 1.757293  [ 1200/ 5482]\n",
      "loss: 1.785967  [ 1800/ 5482]\n",
      "loss: 1.558369  [ 2400/ 5482]\n",
      "loss: 1.719351  [ 3000/ 5482]\n",
      "loss: 1.630006  [ 3600/ 5482]\n",
      "loss: 1.832736  [ 4200/ 5482]\n",
      "loss: 1.795897  [ 4800/ 5482]\n",
      "loss: 1.669962  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.346     0.625     0.606    99\n",
      " disgust     0.346     0.000     0.000    107\n",
      "    fear     0.346     0.262     0.688    80\n",
      "   happy     0.346     0.000     0.000    77\n",
      " neutral     0.346     0.000     0.000    95\n",
      "     sad     0.346     0.296     0.725    91\n",
      "surprise     0.346     0.370     0.492    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.346     0.222     0.359    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 1.756472 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.983451  [    0/ 5482]\n",
      "loss: 1.567403  [  600/ 5482]\n",
      "loss: 1.729060  [ 1200/ 5482]\n",
      "loss: 1.730093  [ 1800/ 5482]\n",
      "loss: 1.703335  [ 2400/ 5482]\n",
      "loss: 1.774937  [ 3000/ 5482]\n",
      "loss: 1.734777  [ 3600/ 5482]\n",
      "loss: 1.628142  [ 4200/ 5482]\n",
      "loss: 1.871767  [ 4800/ 5482]\n",
      "loss: 1.700648  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.357     0.704     0.576    99\n",
      " disgust     0.357     0.000     0.000    107\n",
      "    fear     0.357     0.311     0.637    80\n",
      "   happy     0.357     0.000     0.000    77\n",
      " neutral     0.357     0.000     0.000    95\n",
      "     sad     0.357     0.274     0.824    91\n",
      "surprise     0.357     0.385     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.357     0.239     0.373    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 1.749627 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.764106  [    0/ 5482]\n",
      "loss: 1.819097  [  600/ 5482]\n",
      "loss: 1.612222  [ 1200/ 5482]\n",
      "loss: 1.794767  [ 1800/ 5482]\n",
      "loss: 1.703454  [ 2400/ 5482]\n",
      "loss: 1.612059  [ 3000/ 5482]\n",
      "loss: 1.681351  [ 3600/ 5482]\n",
      "loss: 1.753167  [ 4200/ 5482]\n",
      "loss: 1.582233  [ 4800/ 5482]\n",
      "loss: 1.729052  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.341     0.561     0.606    99\n",
      " disgust     0.341     0.000     0.000    107\n",
      "    fear     0.341     0.260     0.713    80\n",
      "   happy     0.341     0.000     0.000    77\n",
      " neutral     0.341     0.000     0.000    95\n",
      "     sad     0.341     0.310     0.692    91\n",
      "surprise     0.341     0.346     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.341     0.211     0.353    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.753255 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.811189  [    0/ 5482]\n",
      "loss: 1.900884  [  600/ 5482]\n",
      "loss: 1.722766  [ 1200/ 5482]\n",
      "loss: 1.645036  [ 1800/ 5482]\n",
      "loss: 1.866075  [ 2400/ 5482]\n",
      "loss: 1.711552  [ 3000/ 5482]\n",
      "loss: 1.731957  [ 3600/ 5482]\n",
      "loss: 1.893097  [ 4200/ 5482]\n",
      "loss: 1.763380  [ 4800/ 5482]\n",
      "loss: 1.623950  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.348     0.619     0.657    99\n",
      " disgust     0.348     0.000     0.000    107\n",
      "    fear     0.348     0.282     0.600    80\n",
      "   happy     0.348     0.000     0.000    77\n",
      " neutral     0.348     0.000     0.000    95\n",
      "     sad     0.348     0.290     0.714    91\n",
      "surprise     0.348     0.306     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.348     0.214     0.361    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.747574 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.993248  [    0/ 5482]\n",
      "loss: 1.637426  [  600/ 5482]\n",
      "loss: 1.547869  [ 1200/ 5482]\n",
      "loss: 1.641778  [ 1800/ 5482]\n",
      "loss: 1.747069  [ 2400/ 5482]\n",
      "loss: 1.877042  [ 3000/ 5482]\n",
      "loss: 1.710903  [ 3600/ 5482]\n",
      "loss: 1.638773  [ 4200/ 5482]\n",
      "loss: 1.625570  [ 4800/ 5482]\n",
      "loss: 1.608268  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.352     0.625     0.657    99\n",
      " disgust     0.352     0.000     0.000    107\n",
      "    fear     0.352     0.279     0.625    80\n",
      "   happy     0.352     0.000     0.000    77\n",
      " neutral     0.352     0.000     0.000    95\n",
      "     sad     0.352     0.283     0.736    91\n",
      "surprise     0.352     0.367     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.352     0.222     0.366    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 1.738756 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.782228  [    0/ 5482]\n",
      "loss: 1.771986  [  600/ 5482]\n",
      "loss: 1.796256  [ 1200/ 5482]\n",
      "loss: 1.786454  [ 1800/ 5482]\n",
      "loss: 1.829782  [ 2400/ 5482]\n",
      "loss: 1.789996  [ 3000/ 5482]\n",
      "loss: 1.642260  [ 3600/ 5482]\n",
      "loss: 1.620189  [ 4200/ 5482]\n",
      "loss: 1.832093  [ 4800/ 5482]\n",
      "loss: 1.824087  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.348     0.671     0.556    99\n",
      " disgust     0.348     0.000     0.000    107\n",
      "    fear     0.348     0.274     0.762    80\n",
      "   happy     0.348     0.000     0.000    77\n",
      " neutral     0.348     0.000     0.000    95\n",
      "     sad     0.348     0.275     0.758    91\n",
      "surprise     0.348     0.500     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.348     0.246     0.360    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 1.737575 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.793294  [    0/ 5482]\n",
      "loss: 1.596513  [  600/ 5482]\n",
      "loss: 1.596128  [ 1200/ 5482]\n",
      "loss: 1.749898  [ 1800/ 5482]\n",
      "loss: 1.785816  [ 2400/ 5482]\n",
      "loss: 1.587853  [ 3000/ 5482]\n",
      "loss: 1.666291  [ 3600/ 5482]\n",
      "loss: 1.625161  [ 4200/ 5482]\n",
      "loss: 1.687457  [ 4800/ 5482]\n",
      "loss: 1.859195  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.596     0.657    99\n",
      " disgust     0.356     0.000     0.000    107\n",
      "    fear     0.356     0.280     0.650    80\n",
      "   happy     0.356     0.000     0.000    77\n",
      " neutral     0.356     0.000     0.000    95\n",
      "     sad     0.356     0.291     0.736    91\n",
      "surprise     0.356     0.388     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.222     0.369    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.733720 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.638728  [    0/ 5482]\n",
      "loss: 1.733115  [  600/ 5482]\n",
      "loss: 1.568615  [ 1200/ 5482]\n",
      "loss: 1.729659  [ 1800/ 5482]\n",
      "loss: 1.623450  [ 2400/ 5482]\n",
      "loss: 1.679105  [ 3000/ 5482]\n",
      "loss: 1.704121  [ 3600/ 5482]\n",
      "loss: 1.612320  [ 4200/ 5482]\n",
      "loss: 1.667056  [ 4800/ 5482]\n",
      "loss: 1.699419  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.359     0.624     0.636    99\n",
      " disgust     0.359     0.000     0.000    107\n",
      "    fear     0.359     0.292     0.700    80\n",
      "   happy     0.359     0.000     0.000    77\n",
      " neutral     0.359     0.000     0.000    95\n",
      "     sad     0.359     0.282     0.758    91\n",
      "surprise     0.359     0.431     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.359     0.233     0.372    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 1.728183 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.877296  [    0/ 5482]\n",
      "loss: 1.603040  [  600/ 5482]\n",
      "loss: 1.675440  [ 1200/ 5482]\n",
      "loss: 1.806966  [ 1800/ 5482]\n",
      "loss: 1.655173  [ 2400/ 5482]\n",
      "loss: 1.656181  [ 3000/ 5482]\n",
      "loss: 1.607663  [ 3600/ 5482]\n",
      "loss: 1.781317  [ 4200/ 5482]\n",
      "loss: 1.882990  [ 4800/ 5482]\n",
      "loss: 1.666078  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.667     0.606    99\n",
      " disgust     0.366     0.000     0.000    107\n",
      "    fear     0.366     0.306     0.700    80\n",
      "   happy     0.366     0.000     0.000    77\n",
      " neutral     0.366     0.000     0.000    95\n",
      "     sad     0.366     0.284     0.813    91\n",
      "surprise     0.366     0.434     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.241     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.722539 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.648940  [    0/ 5482]\n",
      "loss: 1.724798  [  600/ 5482]\n",
      "loss: 1.777487  [ 1200/ 5482]\n",
      "loss: 1.707698  [ 1800/ 5482]\n",
      "loss: 1.800713  [ 2400/ 5482]\n",
      "loss: 1.721315  [ 3000/ 5482]\n",
      "loss: 1.726020  [ 3600/ 5482]\n",
      "loss: 1.675638  [ 4200/ 5482]\n",
      "loss: 1.670292  [ 4800/ 5482]\n",
      "loss: 1.664795  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.359     0.575     0.697    99\n",
      " disgust     0.359     0.000     0.000    107\n",
      "    fear     0.359     0.291     0.600    80\n",
      "   happy     0.359     0.000     0.000    77\n",
      " neutral     0.359     0.000     0.000    95\n",
      "     sad     0.359     0.309     0.758    91\n",
      "surprise     0.359     0.324     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.359     0.214     0.371    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 1.725523 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.703592  [    0/ 5482]\n",
      "loss: 1.688098  [  600/ 5482]\n",
      "loss: 1.784958  [ 1200/ 5482]\n",
      "loss: 1.645072  [ 1800/ 5482]\n",
      "loss: 1.766735  [ 2400/ 5482]\n",
      "loss: 1.676015  [ 3000/ 5482]\n",
      "loss: 1.524067  [ 3600/ 5482]\n",
      "loss: 1.614504  [ 4200/ 5482]\n",
      "loss: 1.570430  [ 4800/ 5482]\n",
      "loss: 1.819002  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.369     0.660     0.646    99\n",
      " disgust     0.369     0.000     0.000    107\n",
      "    fear     0.369     0.306     0.650    80\n",
      "   happy     0.369     0.000     0.000    77\n",
      " neutral     0.369     0.000     0.000    95\n",
      "     sad     0.369     0.287     0.824    91\n",
      "surprise     0.369     0.415     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.369     0.238     0.383    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.716186 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.567805  [    0/ 5482]\n",
      "loss: 1.576497  [  600/ 5482]\n",
      "loss: 1.736628  [ 1200/ 5482]\n",
      "loss: 2.139468  [ 1800/ 5482]\n",
      "loss: 1.758845  [ 2400/ 5482]\n",
      "loss: 1.758379  [ 3000/ 5482]\n",
      "loss: 1.608966  [ 3600/ 5482]\n",
      "loss: 1.731658  [ 4200/ 5482]\n",
      "loss: 1.477610  [ 4800/ 5482]\n",
      "loss: 1.865367  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.367     0.620     0.677    99\n",
      " disgust     0.367     0.000     0.000    107\n",
      "    fear     0.367     0.318     0.625    80\n",
      "   happy     0.367     0.000     0.000    77\n",
      " neutral     0.367     0.000     0.000    95\n",
      "     sad     0.367     0.288     0.824    91\n",
      "surprise     0.367     0.376     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.367     0.229     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.712645 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.615879  [    0/ 5482]\n",
      "loss: 1.803811  [  600/ 5482]\n",
      "loss: 1.833339  [ 1200/ 5482]\n",
      "loss: 1.795863  [ 1800/ 5482]\n",
      "loss: 1.614770  [ 2400/ 5482]\n",
      "loss: 1.706779  [ 3000/ 5482]\n",
      "loss: 1.662699  [ 3600/ 5482]\n",
      "loss: 1.694877  [ 4200/ 5482]\n",
      "loss: 1.558577  [ 4800/ 5482]\n",
      "loss: 1.672193  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.361     0.595     0.667    99\n",
      " disgust     0.361     0.000     0.000    107\n",
      "    fear     0.361     0.293     0.675    80\n",
      "   happy     0.361     0.000     0.000    77\n",
      " neutral     0.361     0.000     0.000    95\n",
      "     sad     0.361     0.292     0.758    91\n",
      "surprise     0.361     0.392     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.361     0.225     0.373    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 1.711828 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.559861  [    0/ 5482]\n",
      "loss: 1.838322  [  600/ 5482]\n",
      "loss: 1.748435  [ 1200/ 5482]\n",
      "loss: 1.802375  [ 1800/ 5482]\n",
      "loss: 1.709331  [ 2400/ 5482]\n",
      "loss: 1.626887  [ 3000/ 5482]\n",
      "loss: 1.686909  [ 3600/ 5482]\n",
      "loss: 1.942189  [ 4200/ 5482]\n",
      "loss: 1.687440  [ 4800/ 5482]\n",
      "loss: 1.608716  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.641     0.667    99\n",
      " disgust     0.366     0.000     0.000    107\n",
      "    fear     0.366     0.309     0.637    80\n",
      "   happy     0.366     0.000     0.000    77\n",
      " neutral     0.366     0.000     0.000    95\n",
      "     sad     0.366     0.286     0.780    91\n",
      "surprise     0.366     0.372     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.230     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.706188 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 1.656555  [    0/ 5482]\n",
      "loss: 1.626565  [  600/ 5482]\n",
      "loss: 1.698012  [ 1200/ 5482]\n",
      "loss: 1.656569  [ 1800/ 5482]\n",
      "loss: 1.642786  [ 2400/ 5482]\n",
      "loss: 1.576169  [ 3000/ 5482]\n",
      "loss: 1.489916  [ 3600/ 5482]\n",
      "loss: 1.737518  [ 4200/ 5482]\n",
      "loss: 1.679981  [ 4800/ 5482]\n",
      "loss: 1.984768  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.367     0.595     0.697    99\n",
      " disgust     0.367     0.000     0.000    107\n",
      "    fear     0.367     0.297     0.637    80\n",
      "   happy     0.367     0.000     0.000    77\n",
      " neutral     0.367     0.000     0.000    95\n",
      "     sad     0.367     0.303     0.780    91\n",
      "surprise     0.367     0.375     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.367     0.224     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.706399 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 1.866286  [    0/ 5482]\n",
      "loss: 1.699958  [  600/ 5482]\n",
      "loss: 1.788159  [ 1200/ 5482]\n",
      "loss: 1.665333  [ 1800/ 5482]\n",
      "loss: 1.775106  [ 2400/ 5482]\n",
      "loss: 1.567452  [ 3000/ 5482]\n",
      "loss: 1.934656  [ 3600/ 5482]\n",
      "loss: 1.677508  [ 4200/ 5482]\n",
      "loss: 1.680756  [ 4800/ 5482]\n",
      "loss: 1.734541  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.630     0.687    99\n",
      " disgust     0.366     0.000     0.000    107\n",
      "    fear     0.366     0.305     0.625    80\n",
      "   happy     0.366     0.000     0.000    77\n",
      " neutral     0.366     0.000     0.000    95\n",
      "     sad     0.366     0.285     0.780    91\n",
      "surprise     0.366     0.382     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.229     0.378    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.699911 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.744327  [    0/ 5482]\n",
      "loss: 1.601991  [  600/ 5482]\n",
      "loss: 1.819715  [ 1200/ 5482]\n",
      "loss: 1.727975  [ 1800/ 5482]\n",
      "loss: 1.617812  [ 2400/ 5482]\n",
      "loss: 1.696055  [ 3000/ 5482]\n",
      "loss: 1.561265  [ 3600/ 5482]\n",
      "loss: 1.579304  [ 4200/ 5482]\n",
      "loss: 1.765463  [ 4800/ 5482]\n",
      "loss: 1.785897  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.372     0.638     0.677    99\n",
      " disgust     0.372     0.000     0.000    107\n",
      "    fear     0.372     0.307     0.688    80\n",
      "   happy     0.372     0.000     0.000    77\n",
      " neutral     0.372     0.000     0.000    95\n",
      "     sad     0.372     0.288     0.813    91\n",
      "surprise     0.372     0.449     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.372     0.240     0.384    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 1.695761 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 1.749553  [    0/ 5482]\n",
      "loss: 1.642860  [  600/ 5482]\n",
      "loss: 1.567380  [ 1200/ 5482]\n",
      "loss: 1.674518  [ 1800/ 5482]\n",
      "loss: 1.696390  [ 2400/ 5482]\n",
      "loss: 1.459952  [ 3000/ 5482]\n",
      "loss: 1.616296  [ 3600/ 5482]\n",
      "loss: 1.760554  [ 4200/ 5482]\n",
      "loss: 1.670062  [ 4800/ 5482]\n",
      "loss: 1.824932  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.615     0.677    99\n",
      " disgust     0.366     0.000     0.000    107\n",
      "    fear     0.366     0.318     0.588    80\n",
      "   happy     0.366     0.000     0.000    77\n",
      " neutral     0.366     0.000     0.000    95\n",
      "     sad     0.366     0.280     0.802    91\n",
      "surprise     0.366     0.391     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.229     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.692596 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 1.750264  [    0/ 5482]\n",
      "loss: 1.830522  [  600/ 5482]\n",
      "loss: 1.666060  [ 1200/ 5482]\n",
      "loss: 1.606799  [ 1800/ 5482]\n",
      "loss: 1.530672  [ 2400/ 5482]\n",
      "loss: 1.769318  [ 3000/ 5482]\n",
      "loss: 1.815053  [ 3600/ 5482]\n",
      "loss: 1.363735  [ 4200/ 5482]\n",
      "loss: 1.802406  [ 4800/ 5482]\n",
      "loss: 1.813141  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.356     0.561     0.697    99\n",
      " disgust     0.356     0.000     0.000    107\n",
      "    fear     0.356     0.288     0.550    80\n",
      "   happy     0.356     0.000     0.000    77\n",
      " neutral     0.356     0.000     0.000    95\n",
      "     sad     0.356     0.302     0.747    91\n",
      "surprise     0.356     0.330     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.356     0.212     0.369    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 35.6%, Avg loss: 1.696287 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 1.706270  [    0/ 5482]\n",
      "loss: 1.526068  [  600/ 5482]\n",
      "loss: 1.448085  [ 1200/ 5482]\n",
      "loss: 1.824984  [ 1800/ 5482]\n",
      "loss: 1.766388  [ 2400/ 5482]\n",
      "loss: 1.851115  [ 3000/ 5482]\n",
      "loss: 1.609902  [ 3600/ 5482]\n",
      "loss: 1.710478  [ 4200/ 5482]\n",
      "loss: 1.480717  [ 4800/ 5482]\n",
      "loss: 1.683294  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.362     0.542     0.717    99\n",
      " disgust     0.362     0.000     0.000    107\n",
      "    fear     0.362     0.288     0.613    80\n",
      "   happy     0.362     0.000     0.000    77\n",
      " neutral     0.362     0.000     0.000    95\n",
      "     sad     0.362     0.314     0.758    91\n",
      "surprise     0.362     0.360     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.362     0.215     0.373    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 1.692568 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 1.592181  [    0/ 5482]\n",
      "loss: 1.642561  [  600/ 5482]\n",
      "loss: 1.610827  [ 1200/ 5482]\n",
      "loss: 1.550885  [ 1800/ 5482]\n",
      "loss: 1.690391  [ 2400/ 5482]\n",
      "loss: 1.603223  [ 3000/ 5482]\n",
      "loss: 1.553426  [ 3600/ 5482]\n",
      "loss: 1.549238  [ 4200/ 5482]\n",
      "loss: 1.676156  [ 4800/ 5482]\n",
      "loss: 1.643230  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.377     0.667     0.687    99\n",
      " disgust     0.377     0.000     0.000    107\n",
      "    fear     0.377     0.333     0.613    80\n",
      "   happy     0.377     0.000     0.000    77\n",
      " neutral     0.377     1.000     0.011    95\n",
      "     sad     0.377     0.282     0.846    91\n",
      "surprise     0.377     0.402     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.377     0.383     0.390    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.679112 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 1.693982  [    0/ 5482]\n",
      "loss: 1.386092  [  600/ 5482]\n",
      "loss: 1.713226  [ 1200/ 5482]\n",
      "loss: 1.747506  [ 1800/ 5482]\n",
      "loss: 1.550386  [ 2400/ 5482]\n",
      "loss: 1.707383  [ 3000/ 5482]\n",
      "loss: 1.499694  [ 3600/ 5482]\n",
      "loss: 1.710890  [ 4200/ 5482]\n",
      "loss: 1.560895  [ 4800/ 5482]\n",
      "loss: 1.569480  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.568     0.717    99\n",
      " disgust     0.366     0.000     0.000    107\n",
      "    fear     0.366     0.323     0.525    80\n",
      "   happy     0.366     0.000     0.000    77\n",
      " neutral     0.366     1.000     0.011    95\n",
      "     sad     0.366     0.297     0.835    91\n",
      "surprise     0.366     0.337     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.361     0.376    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.682387 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 1.629853  [    0/ 5482]\n",
      "loss: 1.732029  [  600/ 5482]\n",
      "loss: 1.761496  [ 1200/ 5482]\n",
      "loss: 1.629312  [ 1800/ 5482]\n",
      "loss: 1.526729  [ 2400/ 5482]\n",
      "loss: 1.584681  [ 3000/ 5482]\n",
      "loss: 1.571508  [ 3600/ 5482]\n",
      "loss: 1.476163  [ 4200/ 5482]\n",
      "loss: 1.704211  [ 4800/ 5482]\n",
      "loss: 1.627598  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.372     0.645     0.697    99\n",
      " disgust     0.372     0.000     0.000    107\n",
      "    fear     0.372     0.354     0.575    80\n",
      "   happy     0.372     0.000     0.000    77\n",
      " neutral     0.372     0.000     0.000    95\n",
      "     sad     0.372     0.271     0.857    91\n",
      "surprise     0.372     0.400     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.372     0.239     0.384    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 1.672071 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 1.505365  [    0/ 5482]\n",
      "loss: 1.806784  [  600/ 5482]\n",
      "loss: 1.790299  [ 1200/ 5482]\n",
      "loss: 1.741653  [ 1800/ 5482]\n",
      "loss: 1.625463  [ 2400/ 5482]\n",
      "loss: 1.552789  [ 3000/ 5482]\n",
      "loss: 1.571448  [ 3600/ 5482]\n",
      "loss: 1.600339  [ 4200/ 5482]\n",
      "loss: 1.471222  [ 4800/ 5482]\n",
      "loss: 1.607247  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.380     0.657     0.677    99\n",
      " disgust     0.380     0.000     0.000    107\n",
      "    fear     0.380     0.340     0.637    80\n",
      "   happy     0.380     0.000     0.000    77\n",
      " neutral     0.380     0.000     0.000    95\n",
      "     sad     0.380     0.278     0.879    91\n",
      "surprise     0.380     0.486     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.380     0.251     0.393    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.669242 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 1.681755  [    0/ 5482]\n",
      "loss: 1.473253  [  600/ 5482]\n",
      "loss: 1.650275  [ 1200/ 5482]\n",
      "loss: 1.739240  [ 1800/ 5482]\n",
      "loss: 1.738684  [ 2400/ 5482]\n",
      "loss: 1.436774  [ 3000/ 5482]\n",
      "loss: 1.867293  [ 3600/ 5482]\n",
      "loss: 1.468094  [ 4200/ 5482]\n",
      "loss: 1.771385  [ 4800/ 5482]\n",
      "loss: 1.623512  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.377     0.688     0.667    99\n",
      " disgust     0.377     0.000     0.000    107\n",
      "    fear     0.377     0.322     0.725    80\n",
      "   happy     0.377     0.000     0.000    77\n",
      " neutral     0.377     0.000     0.000    95\n",
      "     sad     0.377     0.267     0.824    91\n",
      "surprise     0.377     0.585     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.377     0.266     0.389    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.669822 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 1.657977  [    0/ 5482]\n",
      "loss: 1.670870  [  600/ 5482]\n",
      "loss: 1.638534  [ 1200/ 5482]\n",
      "loss: 1.627912  [ 1800/ 5482]\n",
      "loss: 1.600404  [ 2400/ 5482]\n",
      "loss: 1.751955  [ 3000/ 5482]\n",
      "loss: 1.620198  [ 3600/ 5482]\n",
      "loss: 1.640702  [ 4200/ 5482]\n",
      "loss: 1.521588  [ 4800/ 5482]\n",
      "loss: 1.510743  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.369     0.559     0.717    99\n",
      " disgust     0.369     0.000     0.000    107\n",
      "    fear     0.369     0.305     0.588    80\n",
      "   happy     0.369     0.000     0.000    77\n",
      " neutral     0.369     1.000     0.011    95\n",
      "     sad     0.369     0.302     0.802    91\n",
      "surprise     0.369     0.384     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.369     0.364     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.667808 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 1.618126  [    0/ 5482]\n",
      "loss: 1.747360  [  600/ 5482]\n",
      "loss: 1.627851  [ 1200/ 5482]\n",
      "loss: 1.422831  [ 1800/ 5482]\n",
      "loss: 1.628389  [ 2400/ 5482]\n",
      "loss: 1.957477  [ 3000/ 5482]\n",
      "loss: 1.804817  [ 3600/ 5482]\n",
      "loss: 1.492794  [ 4200/ 5482]\n",
      "loss: 1.729414  [ 4800/ 5482]\n",
      "loss: 1.695630  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.364     0.553     0.737    99\n",
      " disgust     0.364     0.000     0.000    107\n",
      "    fear     0.364     0.286     0.575    80\n",
      "   happy     0.364     0.000     0.000    77\n",
      " neutral     0.364     0.000     0.000    95\n",
      "     sad     0.364     0.315     0.769    91\n",
      "surprise     0.364     0.347     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.364     0.214     0.375    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 1.671259 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 1.672246  [    0/ 5482]\n",
      "loss: 1.632535  [  600/ 5482]\n",
      "loss: 1.765349  [ 1200/ 5482]\n",
      "loss: 1.692935  [ 1800/ 5482]\n",
      "loss: 1.515020  [ 2400/ 5482]\n",
      "loss: 1.753589  [ 3000/ 5482]\n",
      "loss: 1.577878  [ 3600/ 5482]\n",
      "loss: 1.790452  [ 4200/ 5482]\n",
      "loss: 1.658366  [ 4800/ 5482]\n",
      "loss: 1.542684  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.369     0.603     0.707    99\n",
      " disgust     0.369     0.000     0.000    107\n",
      "    fear     0.369     0.304     0.600    80\n",
      "   happy     0.369     0.000     0.000    77\n",
      " neutral     0.369     1.000     0.011    95\n",
      "     sad     0.369     0.307     0.802    91\n",
      "surprise     0.369     0.340     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.369     0.365     0.380    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.9%, Avg loss: 1.662173 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.371814  [    0/ 5482]\n",
      "loss: 1.602702  [  600/ 5482]\n",
      "loss: 1.651834  [ 1200/ 5482]\n",
      "loss: 1.861597  [ 1800/ 5482]\n",
      "loss: 1.685649  [ 2400/ 5482]\n",
      "loss: 1.634564  [ 3000/ 5482]\n",
      "loss: 1.636586  [ 3600/ 5482]\n",
      "loss: 1.544737  [ 4200/ 5482]\n",
      "loss: 1.528096  [ 4800/ 5482]\n",
      "loss: 1.661169  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.367     0.579     0.707    99\n",
      " disgust     0.367     0.000     0.000    107\n",
      "    fear     0.367     0.323     0.525    80\n",
      "   happy     0.367     0.000     0.000    77\n",
      " neutral     0.367     0.000     0.000    95\n",
      "     sad     0.367     0.298     0.846    91\n",
      "surprise     0.367     0.347     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.367     0.221     0.379    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.659292 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 1.504227  [    0/ 5482]\n",
      "loss: 1.566963  [  600/ 5482]\n",
      "loss: 1.535869  [ 1200/ 5482]\n",
      "loss: 1.883250  [ 1800/ 5482]\n",
      "loss: 1.745511  [ 2400/ 5482]\n",
      "loss: 1.525485  [ 3000/ 5482]\n",
      "loss: 1.562410  [ 3600/ 5482]\n",
      "loss: 1.451550  [ 4200/ 5482]\n",
      "loss: 1.688328  [ 4800/ 5482]\n",
      "loss: 1.813213  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.364     0.571     0.727    99\n",
      " disgust     0.364     0.000     0.000    107\n",
      "    fear     0.364     0.297     0.575    80\n",
      "   happy     0.364     0.000     0.000    77\n",
      " neutral     0.364     0.000     0.000    95\n",
      "     sad     0.364     0.289     0.780    91\n",
      "surprise     0.364     0.398     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.364     0.222     0.375    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 1.655945 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.676595  [    0/ 5482]\n",
      "loss: 1.501747  [  600/ 5482]\n",
      "loss: 1.566927  [ 1200/ 5482]\n",
      "loss: 1.494077  [ 1800/ 5482]\n",
      "loss: 1.438528  [ 2400/ 5482]\n",
      "loss: 1.492562  [ 3000/ 5482]\n",
      "loss: 1.566097  [ 3600/ 5482]\n",
      "loss: 1.638417  [ 4200/ 5482]\n",
      "loss: 1.656523  [ 4800/ 5482]\n",
      "loss: 1.648448  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.370     0.603     0.707    99\n",
      " disgust     0.370     0.000     0.000    107\n",
      "    fear     0.370     0.333     0.575    80\n",
      "   happy     0.370     0.000     0.000    77\n",
      " neutral     0.370     0.000     0.000    95\n",
      "     sad     0.370     0.287     0.824    91\n",
      "surprise     0.370     0.368     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.370     0.228     0.383    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 1.649097 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 1.535309  [    0/ 5482]\n",
      "loss: 1.651661  [  600/ 5482]\n",
      "loss: 1.565063  [ 1200/ 5482]\n",
      "loss: 1.522684  [ 1800/ 5482]\n",
      "loss: 1.496535  [ 2400/ 5482]\n",
      "loss: 1.424160  [ 3000/ 5482]\n",
      "loss: 1.684059  [ 3600/ 5482]\n",
      "loss: 1.526407  [ 4200/ 5482]\n",
      "loss: 1.561042  [ 4800/ 5482]\n",
      "loss: 1.561848  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.380     0.670     0.697    99\n",
      " disgust     0.380     0.000     0.000    107\n",
      "    fear     0.380     0.340     0.650    80\n",
      "   happy     0.380     0.000     0.000    77\n",
      " neutral     0.380     0.000     0.000    95\n",
      "     sad     0.380     0.283     0.835    91\n",
      "surprise     0.380     0.412     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.380     0.243     0.394    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 1.646943 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 1.601717  [    0/ 5482]\n",
      "loss: 1.653078  [  600/ 5482]\n",
      "loss: 1.600366  [ 1200/ 5482]\n",
      "loss: 1.408425  [ 1800/ 5482]\n",
      "loss: 1.858992  [ 2400/ 5482]\n",
      "loss: 1.659040  [ 3000/ 5482]\n",
      "loss: 1.809740  [ 3600/ 5482]\n",
      "loss: 1.473244  [ 4200/ 5482]\n",
      "loss: 1.643287  [ 4800/ 5482]\n",
      "loss: 1.592301  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.377     0.648     0.687    99\n",
      " disgust     0.377     0.000     0.000    107\n",
      "    fear     0.377     0.327     0.650    80\n",
      "   happy     0.377     0.000     0.000    77\n",
      " neutral     0.377     1.000     0.011    95\n",
      "     sad     0.377     0.277     0.824    91\n",
      "surprise     0.377     0.459     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.377     0.387     0.390    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.644415 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 1.840147  [    0/ 5482]\n",
      "loss: 1.731816  [  600/ 5482]\n",
      "loss: 1.765178  [ 1200/ 5482]\n",
      "loss: 1.596344  [ 1800/ 5482]\n",
      "loss: 1.654399  [ 2400/ 5482]\n",
      "loss: 1.711354  [ 3000/ 5482]\n",
      "loss: 1.734605  [ 3600/ 5482]\n",
      "loss: 1.651151  [ 4200/ 5482]\n",
      "loss: 1.657587  [ 4800/ 5482]\n",
      "loss: 1.546084  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.374     0.576     0.727    99\n",
      " disgust     0.374     0.000     0.000    107\n",
      "    fear     0.374     0.310     0.613    80\n",
      "   happy     0.374     0.000     0.000    77\n",
      " neutral     0.374     1.000     0.011    95\n",
      "     sad     0.374     0.297     0.813    91\n",
      "surprise     0.374     0.416     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.374     0.371     0.384    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 1.643215 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.766832  [    0/ 5482]\n",
      "loss: 1.710111  [  600/ 5482]\n",
      "loss: 1.820136  [ 1200/ 5482]\n",
      "loss: 1.481662  [ 1800/ 5482]\n",
      "loss: 1.511725  [ 2400/ 5482]\n",
      "loss: 1.591291  [ 3000/ 5482]\n",
      "loss: 1.578103  [ 3600/ 5482]\n",
      "loss: 1.864854  [ 4200/ 5482]\n",
      "loss: 1.512066  [ 4800/ 5482]\n",
      "loss: 1.552523  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.366     0.532     0.747    99\n",
      " disgust     0.366     0.000     0.000    107\n",
      "    fear     0.366     0.297     0.575    80\n",
      "   happy     0.366     0.000     0.000    77\n",
      " neutral     0.366     0.000     0.000    95\n",
      "     sad     0.366     0.314     0.780    91\n",
      "surprise     0.366     0.356     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.366     0.214     0.375    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.643087 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 1.625098  [    0/ 5482]\n",
      "loss: 1.535632  [  600/ 5482]\n",
      "loss: 1.609527  [ 1200/ 5482]\n",
      "loss: 1.538383  [ 1800/ 5482]\n",
      "loss: 1.812780  [ 2400/ 5482]\n",
      "loss: 1.493544  [ 3000/ 5482]\n",
      "loss: 1.477697  [ 3600/ 5482]\n",
      "loss: 1.558925  [ 4200/ 5482]\n",
      "loss: 1.585119  [ 4800/ 5482]\n",
      "loss: 1.544264  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.377     0.593     0.707    99\n",
      " disgust     0.377     0.000     0.000    107\n",
      "    fear     0.377     0.366     0.512    80\n",
      "   happy     0.377     0.000     0.000    77\n",
      " neutral     0.377     1.000     0.011    95\n",
      "     sad     0.377     0.289     0.901    91\n",
      "surprise     0.377     0.379     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.377     0.375     0.389    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.632919 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 1.548379  [    0/ 5482]\n",
      "loss: 1.410977  [  600/ 5482]\n",
      "loss: 1.438346  [ 1200/ 5482]\n",
      "loss: 1.605415  [ 1800/ 5482]\n",
      "loss: 1.478587  [ 2400/ 5482]\n",
      "loss: 1.642255  [ 3000/ 5482]\n",
      "loss: 1.525536  [ 3600/ 5482]\n",
      "loss: 1.698691  [ 4200/ 5482]\n",
      "loss: 1.548978  [ 4800/ 5482]\n",
      "loss: 1.607797  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.367     0.526     0.727    99\n",
      " disgust     0.367     0.000     0.000    107\n",
      "    fear     0.367     0.306     0.650    80\n",
      "   happy     0.367     0.000     0.000    77\n",
      " neutral     0.367     1.000     0.021    95\n",
      "     sad     0.367     0.305     0.780    91\n",
      "surprise     0.367     0.397     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.367     0.362     0.374    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.636782 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.693918  [    0/ 5482]\n",
      "loss: 1.831702  [  600/ 5482]\n",
      "loss: 1.705948  [ 1200/ 5482]\n",
      "loss: 1.556821  [ 1800/ 5482]\n",
      "loss: 1.502813  [ 2400/ 5482]\n",
      "loss: 1.548660  [ 3000/ 5482]\n",
      "loss: 1.559615  [ 3600/ 5482]\n",
      "loss: 1.546079  [ 4200/ 5482]\n",
      "loss: 1.515189  [ 4800/ 5482]\n",
      "loss: 1.604830  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.637     0.727    99\n",
      " disgust     0.390     0.000     0.000    107\n",
      "    fear     0.390     0.340     0.637    80\n",
      "   happy     0.390     0.000     0.000    77\n",
      " neutral     0.390     1.000     0.053    95\n",
      "     sad     0.390     0.288     0.835    91\n",
      "surprise     0.390     0.436     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.386     0.401    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.626046 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 1.543911  [    0/ 5482]\n",
      "loss: 1.437714  [  600/ 5482]\n",
      "loss: 1.639639  [ 1200/ 5482]\n",
      "loss: 1.601176  [ 1800/ 5482]\n",
      "loss: 1.692752  [ 2400/ 5482]\n",
      "loss: 1.505500  [ 3000/ 5482]\n",
      "loss: 1.911541  [ 3600/ 5482]\n",
      "loss: 1.882928  [ 4200/ 5482]\n",
      "loss: 1.573198  [ 4800/ 5482]\n",
      "loss: 1.561956  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.387     0.581     0.727    99\n",
      " disgust     0.387     0.000     0.000    107\n",
      "    fear     0.387     0.308     0.650    80\n",
      "   happy     0.387     0.000     0.000    77\n",
      " neutral     0.387     1.000     0.095    95\n",
      "     sad     0.387     0.298     0.791    91\n",
      "surprise     0.387     0.470     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.387     0.379     0.396    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.626110 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 1.502128  [    0/ 5482]\n",
      "loss: 1.460708  [  600/ 5482]\n",
      "loss: 1.412803  [ 1200/ 5482]\n",
      "loss: 1.537791  [ 1800/ 5482]\n",
      "loss: 1.384138  [ 2400/ 5482]\n",
      "loss: 1.542605  [ 3000/ 5482]\n",
      "loss: 1.624527  [ 3600/ 5482]\n",
      "loss: 1.660586  [ 4200/ 5482]\n",
      "loss: 1.493868  [ 4800/ 5482]\n",
      "loss: 1.585814  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.699     0.727    99\n",
      " disgust     0.393     0.000     0.000    107\n",
      "    fear     0.393     0.331     0.637    80\n",
      "   happy     0.393     0.000     0.000    77\n",
      " neutral     0.393     1.000     0.063    95\n",
      "     sad     0.393     0.283     0.846    91\n",
      "surprise     0.393     0.453     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.395     0.404    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.617133 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 1.604258  [    0/ 5482]\n",
      "loss: 1.489145  [  600/ 5482]\n",
      "loss: 1.524537  [ 1200/ 5482]\n",
      "loss: 1.514439  [ 1800/ 5482]\n",
      "loss: 1.585952  [ 2400/ 5482]\n",
      "loss: 1.365946  [ 3000/ 5482]\n",
      "loss: 1.454274  [ 3600/ 5482]\n",
      "loss: 1.533164  [ 4200/ 5482]\n",
      "loss: 1.743444  [ 4800/ 5482]\n",
      "loss: 1.449797  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.640     0.717    99\n",
      " disgust     0.393     0.000     0.000    107\n",
      "    fear     0.393     0.317     0.637    80\n",
      "   happy     0.393     0.000     0.000    77\n",
      " neutral     0.393     1.000     0.095    95\n",
      "     sad     0.393     0.292     0.846    91\n",
      "surprise     0.393     0.492     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.391     0.403    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.618140 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.597742  [    0/ 5482]\n",
      "loss: 1.673148  [  600/ 5482]\n",
      "loss: 1.429461  [ 1200/ 5482]\n",
      "loss: 1.523501  [ 1800/ 5482]\n",
      "loss: 1.743302  [ 2400/ 5482]\n",
      "loss: 1.857199  [ 3000/ 5482]\n",
      "loss: 1.484063  [ 3600/ 5482]\n",
      "loss: 1.565250  [ 4200/ 5482]\n",
      "loss: 1.511183  [ 4800/ 5482]\n",
      "loss: 1.546982  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.393     0.617     0.747    99\n",
      " disgust     0.393     0.000     0.000    107\n",
      "    fear     0.393     0.349     0.562    80\n",
      "   happy     0.393     0.000     0.000    77\n",
      " neutral     0.393     1.000     0.084    95\n",
      "     sad     0.393     0.295     0.857    91\n",
      "surprise     0.393     0.393     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.393     0.379     0.404    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 1.611163 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 1.630554  [    0/ 5482]\n",
      "loss: 1.422884  [  600/ 5482]\n",
      "loss: 1.575096  [ 1200/ 5482]\n",
      "loss: 1.686933  [ 1800/ 5482]\n",
      "loss: 1.550926  [ 2400/ 5482]\n",
      "loss: 1.577335  [ 3000/ 5482]\n",
      "loss: 1.439687  [ 3600/ 5482]\n",
      "loss: 1.490886  [ 4200/ 5482]\n",
      "loss: 1.379803  [ 4800/ 5482]\n",
      "loss: 1.676599  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.402     0.600     0.727    99\n",
      " disgust     0.402     0.000     0.000    107\n",
      "    fear     0.402     0.371     0.537    80\n",
      "   happy     0.402     0.000     0.000    77\n",
      " neutral     0.402     1.000     0.137    95\n",
      "     sad     0.402     0.301     0.890    91\n",
      "surprise     0.402     0.391     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.402     0.380     0.412    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.609138 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 1.684373  [    0/ 5482]\n",
      "loss: 1.556718  [  600/ 5482]\n",
      "loss: 1.682758  [ 1200/ 5482]\n",
      "loss: 1.772491  [ 1800/ 5482]\n",
      "loss: 1.582571  [ 2400/ 5482]\n",
      "loss: 1.474101  [ 3000/ 5482]\n",
      "loss: 1.558679  [ 3600/ 5482]\n",
      "loss: 1.669698  [ 4200/ 5482]\n",
      "loss: 1.535848  [ 4800/ 5482]\n",
      "loss: 1.502780  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.400     0.642     0.707    99\n",
      " disgust     0.400     0.000     0.000    107\n",
      "    fear     0.400     0.317     0.637    80\n",
      "   happy     0.400     0.000     0.000    77\n",
      " neutral     0.400     1.000     0.137    95\n",
      "     sad     0.400     0.293     0.846    91\n",
      "surprise     0.400     0.516     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.400     0.395     0.410    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.607532 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 1.557261  [    0/ 5482]\n",
      "loss: 1.812269  [  600/ 5482]\n",
      "loss: 1.651158  [ 1200/ 5482]\n",
      "loss: 1.741357  [ 1800/ 5482]\n",
      "loss: 1.549602  [ 2400/ 5482]\n",
      "loss: 1.541678  [ 3000/ 5482]\n",
      "loss: 1.487842  [ 3600/ 5482]\n",
      "loss: 1.629858  [ 4200/ 5482]\n",
      "loss: 1.547664  [ 4800/ 5482]\n",
      "loss: 1.653846  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.390     0.539     0.768    99\n",
      " disgust     0.390     0.000     0.000    107\n",
      "    fear     0.390     0.317     0.550    80\n",
      "   happy     0.390     0.000     0.000    77\n",
      " neutral     0.390     1.000     0.116    95\n",
      "     sad     0.390     0.314     0.791    91\n",
      "surprise     0.390     0.389     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.390     0.366     0.400    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.611422 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 1.560932  [    0/ 5482]\n",
      "loss: 1.555317  [  600/ 5482]\n",
      "loss: 1.741619  [ 1200/ 5482]\n",
      "loss: 1.361603  [ 1800/ 5482]\n",
      "loss: 1.363019  [ 2400/ 5482]\n",
      "loss: 1.693691  [ 3000/ 5482]\n",
      "loss: 1.606792  [ 3600/ 5482]\n",
      "loss: 1.766877  [ 4200/ 5482]\n",
      "loss: 1.439856  [ 4800/ 5482]\n",
      "loss: 1.491301  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.397     0.559     0.768    99\n",
      " disgust     0.397     0.000     0.000    107\n",
      "    fear     0.397     0.302     0.650    80\n",
      "   happy     0.397     0.000     0.000    77\n",
      " neutral     0.397     1.000     0.158    95\n",
      "     sad     0.397     0.309     0.802    91\n",
      "surprise     0.397     0.510     0.426    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.397     0.383     0.401    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.604803 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 1.400606  [    0/ 5482]\n",
      "loss: 1.718737  [  600/ 5482]\n",
      "loss: 1.556430  [ 1200/ 5482]\n",
      "loss: 1.447540  [ 1800/ 5482]\n",
      "loss: 1.560200  [ 2400/ 5482]\n",
      "loss: 1.635356  [ 3000/ 5482]\n",
      "loss: 1.497821  [ 3600/ 5482]\n",
      "loss: 2.008858  [ 4200/ 5482]\n",
      "loss: 1.530185  [ 4800/ 5482]\n",
      "loss: 1.461065  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.395     0.517     0.758    99\n",
      " disgust     0.395     0.000     0.000    107\n",
      "    fear     0.395     0.356     0.525    80\n",
      "   happy     0.395     1.000     0.013    77\n",
      " neutral     0.395     0.857     0.126    95\n",
      "     sad     0.395     0.333     0.824    91\n",
      "surprise     0.395     0.336     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.395     0.486     0.405    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 1.603254 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 1.540341  [    0/ 5482]\n",
      "loss: 1.540573  [  600/ 5482]\n",
      "loss: 1.547149  [ 1200/ 5482]\n",
      "loss: 1.532966  [ 1800/ 5482]\n",
      "loss: 1.599633  [ 2400/ 5482]\n",
      "loss: 1.655438  [ 3000/ 5482]\n",
      "loss: 1.644867  [ 3600/ 5482]\n",
      "loss: 1.710953  [ 4200/ 5482]\n",
      "loss: 1.642005  [ 4800/ 5482]\n",
      "loss: 1.872664  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.403     0.545     0.737    99\n",
      " disgust     0.403     0.000     0.000    107\n",
      "    fear     0.403     0.355     0.537    80\n",
      "   happy     0.403     1.000     0.013    77\n",
      " neutral     0.403     1.000     0.168    95\n",
      "     sad     0.403     0.324     0.846    91\n",
      "surprise     0.403     0.360     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.403     0.512     0.413    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.598451 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 1.660606  [    0/ 5482]\n",
      "loss: 1.254822  [  600/ 5482]\n",
      "loss: 1.610662  [ 1200/ 5482]\n",
      "loss: 1.404971  [ 1800/ 5482]\n",
      "loss: 1.524778  [ 2400/ 5482]\n",
      "loss: 1.684273  [ 3000/ 5482]\n",
      "loss: 1.464251  [ 3600/ 5482]\n",
      "loss: 1.527254  [ 4200/ 5482]\n",
      "loss: 1.533508  [ 4800/ 5482]\n",
      "loss: 1.492850  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.402     0.523     0.788    99\n",
      " disgust     0.402     0.000     0.000    107\n",
      "    fear     0.402     0.331     0.537    80\n",
      "   happy     0.402     1.000     0.013    77\n",
      " neutral     0.402     0.944     0.179    95\n",
      "     sad     0.402     0.333     0.780    91\n",
      "surprise     0.402     0.354     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.402     0.498     0.410    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 1.597305 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 1.450770  [    0/ 5482]\n",
      "loss: 1.589474  [  600/ 5482]\n",
      "loss: 1.548634  [ 1200/ 5482]\n",
      "loss: 1.531954  [ 1800/ 5482]\n",
      "loss: 1.398711  [ 2400/ 5482]\n",
      "loss: 1.362715  [ 3000/ 5482]\n",
      "loss: 1.454733  [ 3600/ 5482]\n",
      "loss: 1.411946  [ 4200/ 5482]\n",
      "loss: 1.582028  [ 4800/ 5482]\n",
      "loss: 1.527678  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.436     0.581     0.758    99\n",
      " disgust     0.436     0.000     0.000    107\n",
      "    fear     0.436     0.352     0.562    80\n",
      "   happy     0.436     1.000     0.052    77\n",
      " neutral     0.436     0.966     0.295    95\n",
      "     sad     0.436     0.330     0.846    91\n",
      "surprise     0.436     0.425     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.436     0.522     0.446    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.586953 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 1.601085  [    0/ 5482]\n",
      "loss: 1.463026  [  600/ 5482]\n",
      "loss: 1.566729  [ 1200/ 5482]\n",
      "loss: 1.427009  [ 1800/ 5482]\n",
      "loss: 1.615673  [ 2400/ 5482]\n",
      "loss: 1.367944  [ 3000/ 5482]\n",
      "loss: 1.478390  [ 3600/ 5482]\n",
      "loss: 1.576459  [ 4200/ 5482]\n",
      "loss: 1.581424  [ 4800/ 5482]\n",
      "loss: 1.528296  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.426     0.614     0.707    99\n",
      " disgust     0.426     0.000     0.000    107\n",
      "    fear     0.426     0.402     0.537    80\n",
      "   happy     0.426     1.000     0.052    77\n",
      " neutral     0.426     0.960     0.253    95\n",
      "     sad     0.426     0.314     0.901    91\n",
      "surprise     0.426     0.374     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.426     0.523     0.437    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 1.587632 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 1.345819  [    0/ 5482]\n",
      "loss: 1.559250  [  600/ 5482]\n",
      "loss: 1.469296  [ 1200/ 5482]\n",
      "loss: 1.441600  [ 1800/ 5482]\n",
      "loss: 1.401742  [ 2400/ 5482]\n",
      "loss: 1.548608  [ 3000/ 5482]\n",
      "loss: 1.820042  [ 3600/ 5482]\n",
      "loss: 1.468811  [ 4200/ 5482]\n",
      "loss: 1.341697  [ 4800/ 5482]\n",
      "loss: 1.662802  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.421     0.535     0.768    99\n",
      " disgust     0.421     0.000     0.000    107\n",
      "    fear     0.421     0.355     0.537    80\n",
      "   happy     0.421     0.750     0.039    77\n",
      " neutral     0.421     0.958     0.242    95\n",
      "     sad     0.421     0.328     0.835    91\n",
      "surprise     0.421     0.414     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.421     0.477     0.430    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 1.584802 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 1.433005  [    0/ 5482]\n",
      "loss: 1.632914  [  600/ 5482]\n",
      "loss: 1.723201  [ 1200/ 5482]\n",
      "loss: 1.603213  [ 1800/ 5482]\n",
      "loss: 1.416944  [ 2400/ 5482]\n",
      "loss: 1.589890  [ 3000/ 5482]\n",
      "loss: 1.465521  [ 3600/ 5482]\n",
      "loss: 1.509611  [ 4200/ 5482]\n",
      "loss: 1.634225  [ 4800/ 5482]\n",
      "loss: 1.568716  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.408     0.510     0.788    99\n",
      " disgust     0.408     0.000     0.000    107\n",
      "    fear     0.408     0.314     0.625    80\n",
      "   happy     0.408     0.250     0.013    77\n",
      " neutral     0.408     1.000     0.253    95\n",
      "     sad     0.408     0.321     0.758    91\n",
      "surprise     0.408     0.491     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.408     0.412     0.411    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 1.589857 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 1.513278  [    0/ 5482]\n",
      "loss: 1.404715  [  600/ 5482]\n",
      "loss: 1.351064  [ 1200/ 5482]\n",
      "loss: 1.671993  [ 1800/ 5482]\n",
      "loss: 1.516793  [ 2400/ 5482]\n",
      "loss: 1.670235  [ 3000/ 5482]\n",
      "loss: 1.498281  [ 3600/ 5482]\n",
      "loss: 1.536428  [ 4200/ 5482]\n",
      "loss: 1.554012  [ 4800/ 5482]\n",
      "loss: 1.559075  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.428     0.542     0.788    99\n",
      " disgust     0.428     0.000     0.000    107\n",
      "    fear     0.428     0.329     0.650    80\n",
      "   happy     0.428     0.400     0.052    77\n",
      " neutral     0.428     0.967     0.305    95\n",
      "     sad     0.428     0.324     0.780    91\n",
      "surprise     0.428     0.551     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.428     0.445     0.431    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 42.8%, Avg loss: 1.582441 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 1.854849  [    0/ 5482]\n",
      "loss: 1.737182  [  600/ 5482]\n",
      "loss: 1.412071  [ 1200/ 5482]\n",
      "loss: 1.515268  [ 1800/ 5482]\n",
      "loss: 1.521735  [ 2400/ 5482]\n",
      "loss: 1.505641  [ 3000/ 5482]\n",
      "loss: 1.343645  [ 3600/ 5482]\n",
      "loss: 1.421587  [ 4200/ 5482]\n",
      "loss: 1.478525  [ 4800/ 5482]\n",
      "loss: 1.428151  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.444     0.545     0.788    99\n",
      " disgust     0.444     0.000     0.000    107\n",
      "    fear     0.444     0.381     0.500    80\n",
      "   happy     0.444     0.688     0.143    77\n",
      " neutral     0.444     0.933     0.295    95\n",
      "     sad     0.444     0.335     0.824    91\n",
      "surprise     0.444     0.424     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.444     0.472     0.456    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.572037 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 1.282576  [    0/ 5482]\n",
      "loss: 1.593117  [  600/ 5482]\n",
      "loss: 1.522615  [ 1200/ 5482]\n",
      "loss: 1.643804  [ 1800/ 5482]\n",
      "loss: 1.362170  [ 2400/ 5482]\n",
      "loss: 1.445157  [ 3000/ 5482]\n",
      "loss: 1.557051  [ 3600/ 5482]\n",
      "loss: 1.616352  [ 4200/ 5482]\n",
      "loss: 1.531398  [ 4800/ 5482]\n",
      "loss: 1.389965  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.464     0.629     0.737    99\n",
      " disgust     0.464     0.000     0.000    107\n",
      "    fear     0.464     0.387     0.600    80\n",
      "   happy     0.464     0.727     0.104    77\n",
      " neutral     0.464     0.952     0.421    95\n",
      "     sad     0.464     0.313     0.857    91\n",
      "surprise     0.464     0.529     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.464     0.506     0.473    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.569381 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 1.547351  [    0/ 5482]\n",
      "loss: 1.417206  [  600/ 5482]\n",
      "loss: 1.532488  [ 1200/ 5482]\n",
      "loss: 1.334842  [ 1800/ 5482]\n",
      "loss: 1.287599  [ 2400/ 5482]\n",
      "loss: 1.530297  [ 3000/ 5482]\n",
      "loss: 1.627174  [ 3600/ 5482]\n",
      "loss: 1.699025  [ 4200/ 5482]\n",
      "loss: 1.487367  [ 4800/ 5482]\n",
      "loss: 1.404280  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.472     0.679     0.747    99\n",
      " disgust     0.472     0.000     0.000    107\n",
      "    fear     0.472     0.402     0.588    80\n",
      "   happy     0.472     0.625     0.195    77\n",
      " neutral     0.472     1.000     0.379    95\n",
      "     sad     0.472     0.309     0.879    91\n",
      "surprise     0.472     0.554     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.472     0.510     0.483    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.561125 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 1.362617  [    0/ 5482]\n",
      "loss: 1.324294  [  600/ 5482]\n",
      "loss: 1.780817  [ 1200/ 5482]\n",
      "loss: 1.431697  [ 1800/ 5482]\n",
      "loss: 1.680616  [ 2400/ 5482]\n",
      "loss: 1.510270  [ 3000/ 5482]\n",
      "loss: 1.647866  [ 3600/ 5482]\n",
      "loss: 1.547549  [ 4200/ 5482]\n",
      "loss: 1.567090  [ 4800/ 5482]\n",
      "loss: 1.369401  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.479     0.609     0.788    99\n",
      " disgust     0.479     0.000     0.000    107\n",
      "    fear     0.479     0.380     0.650    80\n",
      "   happy     0.479     0.594     0.247    77\n",
      " neutral     0.479     0.974     0.400    95\n",
      "     sad     0.479     0.330     0.813    91\n",
      "surprise     0.479     0.620     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.479     0.501     0.487    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.562788 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 1.624657  [    0/ 5482]\n",
      "loss: 1.723272  [  600/ 5482]\n",
      "loss: 1.632020  [ 1200/ 5482]\n",
      "loss: 1.510450  [ 1800/ 5482]\n",
      "loss: 1.330742  [ 2400/ 5482]\n",
      "loss: 1.331472  [ 3000/ 5482]\n",
      "loss: 1.371529  [ 3600/ 5482]\n",
      "loss: 1.740629  [ 4200/ 5482]\n",
      "loss: 1.459925  [ 4800/ 5482]\n",
      "loss: 1.625412  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.459     0.512     0.828    99\n",
      " disgust     0.459     0.000     0.000    107\n",
      "    fear     0.459     0.381     0.537    80\n",
      "   happy     0.459     0.659     0.351    77\n",
      " neutral     0.459     1.000     0.316    95\n",
      "     sad     0.459     0.349     0.736    91\n",
      "surprise     0.459     0.419     0.508    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.459     0.474     0.468    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 1.575627 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 1.541895  [    0/ 5482]\n",
      "loss: 1.442045  [  600/ 5482]\n",
      "loss: 1.390103  [ 1200/ 5482]\n",
      "loss: 1.420643  [ 1800/ 5482]\n",
      "loss: 1.499573  [ 2400/ 5482]\n",
      "loss: 1.345310  [ 3000/ 5482]\n",
      "loss: 1.361479  [ 3600/ 5482]\n",
      "loss: 1.289079  [ 4200/ 5482]\n",
      "loss: 1.407085  [ 4800/ 5482]\n",
      "loss: 1.375329  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.495     0.617     0.798    99\n",
      " disgust     0.495     0.000     0.000    107\n",
      "    fear     0.495     0.429     0.562    80\n",
      "   happy     0.495     0.653     0.416    77\n",
      " neutral     0.495     0.975     0.411    95\n",
      "     sad     0.495     0.318     0.813    91\n",
      "surprise     0.495     0.600     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.495     0.513     0.506    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.554202 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 1.346812  [    0/ 5482]\n",
      "loss: 1.517347  [  600/ 5482]\n",
      "loss: 1.501229  [ 1200/ 5482]\n",
      "loss: 1.449963  [ 1800/ 5482]\n",
      "loss: 1.484467  [ 2400/ 5482]\n",
      "loss: 1.344839  [ 3000/ 5482]\n",
      "loss: 1.420224  [ 3600/ 5482]\n",
      "loss: 1.364304  [ 4200/ 5482]\n",
      "loss: 1.361752  [ 4800/ 5482]\n",
      "loss: 1.495636  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.489     0.582     0.788    99\n",
      " disgust     0.489     0.000     0.000    107\n",
      "    fear     0.489     0.404     0.525    80\n",
      "   happy     0.489     0.635     0.429    77\n",
      " neutral     0.489     0.976     0.421    95\n",
      "     sad     0.489     0.329     0.802    91\n",
      "surprise     0.489     0.561     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.489     0.498     0.498    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.551887 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 1.633474  [    0/ 5482]\n",
      "loss: 1.388823  [  600/ 5482]\n",
      "loss: 1.493729  [ 1200/ 5482]\n",
      "loss: 1.322349  [ 1800/ 5482]\n",
      "loss: 1.617921  [ 2400/ 5482]\n",
      "loss: 1.759952  [ 3000/ 5482]\n",
      "loss: 1.289647  [ 3600/ 5482]\n",
      "loss: 1.379428  [ 4200/ 5482]\n",
      "loss: 1.388439  [ 4800/ 5482]\n",
      "loss: 1.284458  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.523     0.604     0.818    99\n",
      " disgust     0.523     0.000     0.000    107\n",
      "    fear     0.523     0.506     0.500    80\n",
      "   happy     0.523     0.814     0.455    77\n",
      " neutral     0.523     0.922     0.495    95\n",
      "     sad     0.523     0.345     0.857    91\n",
      "surprise     0.523     0.494     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.523     0.526     0.535    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.551152 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.480530  [    0/ 5482]\n",
      "loss: 1.513029  [  600/ 5482]\n",
      "loss: 1.415419  [ 1200/ 5482]\n",
      "loss: 1.789198  [ 1800/ 5482]\n",
      "loss: 1.844017  [ 2400/ 5482]\n",
      "loss: 1.407500  [ 3000/ 5482]\n",
      "loss: 1.471490  [ 3600/ 5482]\n",
      "loss: 1.268023  [ 4200/ 5482]\n",
      "loss: 1.508290  [ 4800/ 5482]\n",
      "loss: 1.533852  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.513     0.612     0.798    99\n",
      " disgust     0.513     0.000     0.000    107\n",
      "    fear     0.513     0.455     0.500    80\n",
      "   happy     0.513     0.750     0.506    77\n",
      " neutral     0.513     0.978     0.474    95\n",
      "     sad     0.513     0.323     0.802    91\n",
      "surprise     0.513     0.536     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.513     0.522     0.527    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.546588 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 1.614924  [    0/ 5482]\n",
      "loss: 1.452673  [  600/ 5482]\n",
      "loss: 1.702536  [ 1200/ 5482]\n",
      "loss: 1.411780  [ 1800/ 5482]\n",
      "loss: 1.482991  [ 2400/ 5482]\n",
      "loss: 1.413639  [ 3000/ 5482]\n",
      "loss: 1.659470  [ 3600/ 5482]\n",
      "loss: 1.666253  [ 4200/ 5482]\n",
      "loss: 1.683087  [ 4800/ 5482]\n",
      "loss: 1.267747  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.533     0.583     0.818    99\n",
      " disgust     0.533     0.000     0.000    107\n",
      "    fear     0.533     0.513     0.487    80\n",
      "   happy     0.533     0.774     0.532    77\n",
      " neutral     0.533     0.981     0.537    95\n",
      "     sad     0.533     0.339     0.824    91\n",
      "surprise     0.533     0.551     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.533     0.534     0.546    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.541794 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 1.358144  [    0/ 5482]\n",
      "loss: 1.606196  [  600/ 5482]\n",
      "loss: 1.690225  [ 1200/ 5482]\n",
      "loss: 1.275477  [ 1800/ 5482]\n",
      "loss: 1.150180  [ 2400/ 5482]\n",
      "loss: 1.390235  [ 3000/ 5482]\n",
      "loss: 1.549584  [ 3600/ 5482]\n",
      "loss: 1.563805  [ 4200/ 5482]\n",
      "loss: 1.776931  [ 4800/ 5482]\n",
      "loss: 1.472648  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.498     0.552     0.808    99\n",
      " disgust     0.498     0.000     0.000    107\n",
      "    fear     0.498     0.530     0.438    80\n",
      "   happy     0.498     0.736     0.506    77\n",
      " neutral     0.498     0.841     0.389    95\n",
      "     sad     0.498     0.372     0.780    91\n",
      "surprise     0.498     0.378     0.689    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.498     0.487     0.516    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 1.560493 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 1.588359  [    0/ 5482]\n",
      "loss: 1.328684  [  600/ 5482]\n",
      "loss: 1.428501  [ 1200/ 5482]\n",
      "loss: 1.239495  [ 1800/ 5482]\n",
      "loss: 1.547583  [ 2400/ 5482]\n",
      "loss: 1.483360  [ 3000/ 5482]\n",
      "loss: 1.344614  [ 3600/ 5482]\n",
      "loss: 1.400511  [ 4200/ 5482]\n",
      "loss: 1.328424  [ 4800/ 5482]\n",
      "loss: 1.338555  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.515     0.583     0.778    99\n",
      " disgust     0.515     0.000     0.000    107\n",
      "    fear     0.515     0.470     0.487    80\n",
      "   happy     0.515     0.657     0.597    77\n",
      " neutral     0.515     1.000     0.474    95\n",
      "     sad     0.515     0.325     0.813    91\n",
      "surprise     0.515     0.635     0.541    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.515     0.524     0.527    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.534740 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 1.293332  [    0/ 5482]\n",
      "loss: 1.540041  [  600/ 5482]\n",
      "loss: 1.488364  [ 1200/ 5482]\n",
      "loss: 1.509996  [ 1800/ 5482]\n",
      "loss: 1.271644  [ 2400/ 5482]\n",
      "loss: 1.679155  [ 3000/ 5482]\n",
      "loss: 1.709863  [ 3600/ 5482]\n",
      "loss: 1.306468  [ 4200/ 5482]\n",
      "loss: 1.355921  [ 4800/ 5482]\n",
      "loss: 1.413865  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.528     0.658     0.778    99\n",
      " disgust     0.528     0.000     0.000    107\n",
      "    fear     0.528     0.476     0.487    80\n",
      "   happy     0.528     0.644     0.610    77\n",
      " neutral     0.528     1.000     0.516    95\n",
      "     sad     0.528     0.323     0.813    91\n",
      "surprise     0.528     0.600     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.528     0.529     0.542    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.528905 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 1.384391  [    0/ 5482]\n",
      "loss: 1.601394  [  600/ 5482]\n",
      "loss: 1.490803  [ 1200/ 5482]\n",
      "loss: 1.601543  [ 1800/ 5482]\n",
      "loss: 1.338591  [ 2400/ 5482]\n",
      "loss: 1.320519  [ 3000/ 5482]\n",
      "loss: 1.631753  [ 3600/ 5482]\n",
      "loss: 1.500682  [ 4200/ 5482]\n",
      "loss: 1.249468  [ 4800/ 5482]\n",
      "loss: 1.271425  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.536     0.702     0.737    99\n",
      " disgust     0.536     0.000     0.000    107\n",
      "    fear     0.536     0.558     0.537    80\n",
      "   happy     0.536     0.605     0.597    77\n",
      " neutral     0.536     0.962     0.526    95\n",
      "     sad     0.536     0.320     0.879    91\n",
      "surprise     0.536     0.686     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.536     0.548     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.529466 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 1.339201  [    0/ 5482]\n",
      "loss: 1.388463  [  600/ 5482]\n",
      "loss: 1.514981  [ 1200/ 5482]\n",
      "loss: 1.284402  [ 1800/ 5482]\n",
      "loss: 1.345656  [ 2400/ 5482]\n",
      "loss: 1.680719  [ 3000/ 5482]\n",
      "loss: 1.440023  [ 3600/ 5482]\n",
      "loss: 1.535996  [ 4200/ 5482]\n",
      "loss: 1.404500  [ 4800/ 5482]\n",
      "loss: 1.370698  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.528     0.667     0.727    99\n",
      " disgust     0.528     0.000     0.000    107\n",
      "    fear     0.528     0.551     0.537    80\n",
      "   happy     0.528     0.656     0.519    77\n",
      " neutral     0.528     1.000     0.526    95\n",
      "     sad     0.528     0.319     0.890    91\n",
      "surprise     0.528     0.610     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.528     0.543     0.542    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.524602 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 1.602888  [    0/ 5482]\n",
      "loss: 1.407936  [  600/ 5482]\n",
      "loss: 1.229772  [ 1200/ 5482]\n",
      "loss: 1.343093  [ 1800/ 5482]\n",
      "loss: 1.657914  [ 2400/ 5482]\n",
      "loss: 1.468888  [ 3000/ 5482]\n",
      "loss: 1.337479  [ 3600/ 5482]\n",
      "loss: 1.330692  [ 4200/ 5482]\n",
      "loss: 1.402941  [ 4800/ 5482]\n",
      "loss: 1.479006  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.549     0.640     0.808    99\n",
      " disgust     0.549     0.000     0.000    107\n",
      "    fear     0.549     0.580     0.500    80\n",
      "   happy     0.549     0.697     0.597    77\n",
      " neutral     0.549     0.981     0.547    95\n",
      "     sad     0.549     0.347     0.857    91\n",
      "surprise     0.549     0.542     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.549     0.541     0.564    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.519761 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 1.617748  [    0/ 5482]\n",
      "loss: 1.473395  [  600/ 5482]\n",
      "loss: 1.450270  [ 1200/ 5482]\n",
      "loss: 1.213694  [ 1800/ 5482]\n",
      "loss: 1.366623  [ 2400/ 5482]\n",
      "loss: 1.581705  [ 3000/ 5482]\n",
      "loss: 1.562119  [ 3600/ 5482]\n",
      "loss: 1.617351  [ 4200/ 5482]\n",
      "loss: 1.539757  [ 4800/ 5482]\n",
      "loss: 1.384830  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.538     0.594     0.828    99\n",
      " disgust     0.538     0.000     0.000    107\n",
      "    fear     0.538     0.475     0.475    80\n",
      "   happy     0.538     0.635     0.610    77\n",
      " neutral     0.538     0.982     0.568    95\n",
      "     sad     0.538     0.350     0.780    91\n",
      "surprise     0.538     0.600     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.538     0.519     0.550    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.522817 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 1.318817  [    0/ 5482]\n",
      "loss: 1.565220  [  600/ 5482]\n",
      "loss: 1.455743  [ 1200/ 5482]\n",
      "loss: 1.357930  [ 1800/ 5482]\n",
      "loss: 1.382302  [ 2400/ 5482]\n",
      "loss: 1.769565  [ 3000/ 5482]\n",
      "loss: 1.567489  [ 3600/ 5482]\n",
      "loss: 1.432191  [ 4200/ 5482]\n",
      "loss: 1.523071  [ 4800/ 5482]\n",
      "loss: 1.359381  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.539     0.640     0.808    99\n",
      " disgust     0.539     0.000     0.000    107\n",
      "    fear     0.539     0.518     0.550    80\n",
      "   happy     0.539     0.618     0.545    77\n",
      " neutral     0.539     0.945     0.547    95\n",
      "     sad     0.539     0.336     0.846    91\n",
      "surprise     0.539     0.708     0.557    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.539     0.538     0.551    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.513517 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 1.489238  [    0/ 5482]\n",
      "loss: 1.491023  [  600/ 5482]\n",
      "loss: 1.574660  [ 1200/ 5482]\n",
      "loss: 1.307451  [ 1800/ 5482]\n",
      "loss: 1.329438  [ 2400/ 5482]\n",
      "loss: 1.190147  [ 3000/ 5482]\n",
      "loss: 1.592604  [ 3600/ 5482]\n",
      "loss: 1.145774  [ 4200/ 5482]\n",
      "loss: 1.464484  [ 4800/ 5482]\n",
      "loss: 1.788737  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.525     0.699     0.727    99\n",
      " disgust     0.525     0.000     0.000    107\n",
      "    fear     0.525     0.446     0.562    80\n",
      "   happy     0.525     0.570     0.636    77\n",
      " neutral     0.525     0.963     0.547    95\n",
      "     sad     0.525     0.325     0.824    91\n",
      "surprise     0.525     0.771     0.443    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.525     0.539     0.534    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.521668 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 1.708677  [    0/ 5482]\n",
      "loss: 1.336016  [  600/ 5482]\n",
      "loss: 1.299900  [ 1200/ 5482]\n",
      "loss: 1.282373  [ 1800/ 5482]\n",
      "loss: 1.676082  [ 2400/ 5482]\n",
      "loss: 1.301685  [ 3000/ 5482]\n",
      "loss: 1.879262  [ 3600/ 5482]\n",
      "loss: 1.346298  [ 4200/ 5482]\n",
      "loss: 1.408777  [ 4800/ 5482]\n",
      "loss: 1.512229  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.551     0.599     0.859    99\n",
      " disgust     0.551     0.000     0.000    107\n",
      "    fear     0.551     0.514     0.450    80\n",
      "   happy     0.551     0.644     0.610    77\n",
      " neutral     0.551     0.983     0.600    95\n",
      "     sad     0.551     0.357     0.824    91\n",
      "surprise     0.551     0.632     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.551     0.533     0.562    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.508607 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 1.437349  [    0/ 5482]\n",
      "loss: 1.470307  [  600/ 5482]\n",
      "loss: 1.399573  [ 1200/ 5482]\n",
      "loss: 1.388764  [ 1800/ 5482]\n",
      "loss: 1.369455  [ 2400/ 5482]\n",
      "loss: 1.407227  [ 3000/ 5482]\n",
      "loss: 1.416946  [ 3600/ 5482]\n",
      "loss: 1.806318  [ 4200/ 5482]\n",
      "loss: 1.368204  [ 4800/ 5482]\n",
      "loss: 1.579275  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.549     0.669     0.798    99\n",
      " disgust     0.549     0.000     0.000    107\n",
      "    fear     0.549     0.575     0.525    80\n",
      "   happy     0.549     0.603     0.610    77\n",
      " neutral     0.549     0.948     0.579    95\n",
      "     sad     0.549     0.342     0.835    91\n",
      "surprise     0.549     0.590     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.549     0.533     0.563    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.510325 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 1.545615  [    0/ 5482]\n",
      "loss: 1.466120  [  600/ 5482]\n",
      "loss: 1.439386  [ 1200/ 5482]\n",
      "loss: 1.373888  [ 1800/ 5482]\n",
      "loss: 1.643333  [ 2400/ 5482]\n",
      "loss: 1.265333  [ 3000/ 5482]\n",
      "loss: 1.503733  [ 3600/ 5482]\n",
      "loss: 1.286215  [ 4200/ 5482]\n",
      "loss: 1.265262  [ 4800/ 5482]\n",
      "loss: 1.116394  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.543     0.698     0.747    99\n",
      " disgust     0.543     0.000     0.000    107\n",
      "    fear     0.543     0.541     0.500    80\n",
      "   happy     0.543     0.643     0.584    77\n",
      " neutral     0.543     0.919     0.600    95\n",
      "     sad     0.543     0.326     0.868    91\n",
      "surprise     0.543     0.643     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.543     0.539     0.556    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.503809 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 1.438318  [    0/ 5482]\n",
      "loss: 1.384505  [  600/ 5482]\n",
      "loss: 1.587623  [ 1200/ 5482]\n",
      "loss: 1.377090  [ 1800/ 5482]\n",
      "loss: 1.510684  [ 2400/ 5482]\n",
      "loss: 1.409824  [ 3000/ 5482]\n",
      "loss: 1.459733  [ 3600/ 5482]\n",
      "loss: 1.474225  [ 4200/ 5482]\n",
      "loss: 1.562457  [ 4800/ 5482]\n",
      "loss: 1.357308  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.530     0.688     0.667    99\n",
      " disgust     0.530     0.000     0.000    107\n",
      "    fear     0.530     0.489     0.550    80\n",
      "   happy     0.530     0.558     0.623    77\n",
      " neutral     0.530     0.982     0.568    95\n",
      "     sad     0.530     0.328     0.868    91\n",
      "surprise     0.530     0.762     0.525    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.530     0.544     0.543    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.505274 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 1.347883  [    0/ 5482]\n",
      "loss: 1.399201  [  600/ 5482]\n",
      "loss: 1.307536  [ 1200/ 5482]\n",
      "loss: 1.526936  [ 1800/ 5482]\n",
      "loss: 1.376899  [ 2400/ 5482]\n",
      "loss: 1.258301  [ 3000/ 5482]\n",
      "loss: 1.325704  [ 3600/ 5482]\n",
      "loss: 1.491982  [ 4200/ 5482]\n",
      "loss: 1.428698  [ 4800/ 5482]\n",
      "loss: 1.391153  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.552     0.688     0.758    99\n",
      " disgust     0.552     0.000     0.000    107\n",
      "    fear     0.552     0.627     0.525    80\n",
      "   happy     0.552     0.667     0.571    77\n",
      " neutral     0.552     0.965     0.579    95\n",
      "     sad     0.552     0.339     0.901    91\n",
      "surprise     0.552     0.565     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.552     0.550     0.568    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.495056 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 1.294381  [    0/ 5482]\n",
      "loss: 1.581482  [  600/ 5482]\n",
      "loss: 1.287291  [ 1200/ 5482]\n",
      "loss: 1.262761  [ 1800/ 5482]\n",
      "loss: 1.175708  [ 2400/ 5482]\n",
      "loss: 1.324187  [ 3000/ 5482]\n",
      "loss: 1.290680  [ 3600/ 5482]\n",
      "loss: 1.118885  [ 4200/ 5482]\n",
      "loss: 1.358588  [ 4800/ 5482]\n",
      "loss: 1.372617  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.552     0.685     0.747    99\n",
      " disgust     0.552     0.000     0.000    107\n",
      "    fear     0.552     0.537     0.537    80\n",
      "   happy     0.552     0.583     0.636    77\n",
      " neutral     0.552     0.951     0.611    95\n",
      "     sad     0.552     0.342     0.846    91\n",
      "surprise     0.552     0.692     0.590    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.552     0.542     0.567    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.496922 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 1.373001  [    0/ 5482]\n",
      "loss: 1.245463  [  600/ 5482]\n",
      "loss: 1.350856  [ 1200/ 5482]\n",
      "loss: 1.262862  [ 1800/ 5482]\n",
      "loss: 1.271156  [ 2400/ 5482]\n",
      "loss: 1.306287  [ 3000/ 5482]\n",
      "loss: 1.296189  [ 3600/ 5482]\n",
      "loss: 1.214593  [ 4200/ 5482]\n",
      "loss: 1.365889  [ 4800/ 5482]\n",
      "loss: 1.295135  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.731     0.798    99\n",
      " disgust     0.557     0.000     0.000    107\n",
      "    fear     0.557     0.538     0.525    80\n",
      "   happy     0.557     0.632     0.623    77\n",
      " neutral     0.557     0.935     0.611    95\n",
      "     sad     0.557     0.343     0.813    91\n",
      "surprise     0.557     0.557     0.639    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.534     0.573    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.496268 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 1.566599  [    0/ 5482]\n",
      "loss: 1.417560  [  600/ 5482]\n",
      "loss: 1.516930  [ 1200/ 5482]\n",
      "loss: 1.179992  [ 1800/ 5482]\n",
      "loss: 1.316866  [ 2400/ 5482]\n",
      "loss: 1.275195  [ 3000/ 5482]\n",
      "loss: 1.341141  [ 3600/ 5482]\n",
      "loss: 1.460298  [ 4200/ 5482]\n",
      "loss: 1.188250  [ 4800/ 5482]\n",
      "loss: 1.194158  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.534     0.582     0.828    99\n",
      " disgust     0.534     0.000     0.000    107\n",
      "    fear     0.534     0.500     0.512    80\n",
      "   happy     0.534     0.551     0.636    77\n",
      " neutral     0.534     0.966     0.589    95\n",
      "     sad     0.534     0.348     0.769    91\n",
      "surprise     0.534     0.718     0.459    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.534     0.523     0.542    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.495570 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 1.639758  [    0/ 5482]\n",
      "loss: 1.374062  [  600/ 5482]\n",
      "loss: 1.555426  [ 1200/ 5482]\n",
      "loss: 1.462348  [ 1800/ 5482]\n",
      "loss: 1.546224  [ 2400/ 5482]\n",
      "loss: 1.270734  [ 3000/ 5482]\n",
      "loss: 1.316432  [ 3600/ 5482]\n",
      "loss: 1.368504  [ 4200/ 5482]\n",
      "loss: 1.476272  [ 4800/ 5482]\n",
      "loss: 1.364183  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.548     0.616     0.778    99\n",
      " disgust     0.548     0.000     0.000    107\n",
      "    fear     0.548     0.548     0.500    80\n",
      "   happy     0.548     0.603     0.610    77\n",
      " neutral     0.548     0.950     0.600    95\n",
      "     sad     0.548     0.342     0.835    91\n",
      "surprise     0.548     0.712     0.607    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.548     0.539     0.561    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.488613 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 1.148025  [    0/ 5482]\n",
      "loss: 1.238763  [  600/ 5482]\n",
      "loss: 1.615894  [ 1200/ 5482]\n",
      "loss: 1.317438  [ 1800/ 5482]\n",
      "loss: 1.241548  [ 2400/ 5482]\n",
      "loss: 1.396013  [ 3000/ 5482]\n",
      "loss: 1.226431  [ 3600/ 5482]\n",
      "loss: 1.878182  [ 4200/ 5482]\n",
      "loss: 1.360017  [ 4800/ 5482]\n",
      "loss: 1.216694  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.629     0.838    99\n",
      " disgust     0.557     0.000     0.000    107\n",
      "    fear     0.557     0.500     0.487    80\n",
      "   happy     0.557     0.618     0.610    77\n",
      " neutral     0.557     0.968     0.632    95\n",
      "     sad     0.557     0.357     0.780    91\n",
      "surprise     0.557     0.635     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.530     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.489626 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 1.438165  [    0/ 5482]\n",
      "loss: 1.226400  [  600/ 5482]\n",
      "loss: 1.313838  [ 1200/ 5482]\n",
      "loss: 1.367312  [ 1800/ 5482]\n",
      "loss: 1.320413  [ 2400/ 5482]\n",
      "loss: 1.289801  [ 3000/ 5482]\n",
      "loss: 1.319766  [ 3600/ 5482]\n",
      "loss: 1.198854  [ 4200/ 5482]\n",
      "loss: 1.550941  [ 4800/ 5482]\n",
      "loss: 1.394949  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.557     0.673     0.747    99\n",
      " disgust     0.557     0.000     0.000    107\n",
      "    fear     0.557     0.571     0.500    80\n",
      "   happy     0.557     0.613     0.636    77\n",
      " neutral     0.557     0.924     0.642    95\n",
      "     sad     0.557     0.348     0.857    91\n",
      "surprise     0.557     0.633     0.623    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.557     0.537     0.572    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.477698 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 1.386143  [    0/ 5482]\n",
      "loss: 1.295838  [  600/ 5482]\n",
      "loss: 1.406400  [ 1200/ 5482]\n",
      "loss: 1.285640  [ 1800/ 5482]\n",
      "loss: 1.305425  [ 2400/ 5482]\n",
      "loss: 1.210543  [ 3000/ 5482]\n",
      "loss: 1.181726  [ 3600/ 5482]\n",
      "loss: 1.473538  [ 4200/ 5482]\n",
      "loss: 1.445949  [ 4800/ 5482]\n",
      "loss: 1.361143  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.546     0.594     0.828    99\n",
      " disgust     0.546     0.000     0.000    107\n",
      "    fear     0.546     0.481     0.487    80\n",
      "   happy     0.546     0.560     0.610    77\n",
      " neutral     0.546     0.967     0.621    95\n",
      "     sad     0.546     0.357     0.780    91\n",
      "surprise     0.546     0.745     0.574    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.546     0.529     0.557    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.479777 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 1.422023  [    0/ 5482]\n",
      "loss: 1.316666  [  600/ 5482]\n",
      "loss: 1.519044  [ 1200/ 5482]\n",
      "loss: 1.311084  [ 1800/ 5482]\n",
      "loss: 1.179303  [ 2400/ 5482]\n",
      "loss: 1.262146  [ 3000/ 5482]\n",
      "loss: 1.367148  [ 3600/ 5482]\n",
      "loss: 1.286564  [ 4200/ 5482]\n",
      "loss: 1.386233  [ 4800/ 5482]\n",
      "loss: 1.325772  [ 5400/ 5482]\n",
      "           accuracy  precision  recall   support\n",
      "   angry     0.548     0.581     0.798    99\n",
      " disgust     0.548     0.000     0.000    107\n",
      "    fear     0.548     0.561     0.463    80\n",
      "   happy     0.548     0.600     0.584    77\n",
      " neutral     0.548     0.922     0.621    95\n",
      "     sad     0.548     0.354     0.813    91\n",
      "surprise     0.548     0.667     0.656    61\n",
      "                                          610\n",
      "\n",
      " \n",
      "     avg     0.548     0.526     0.562    \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.478956 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 1.176845  [    0/ 5482]\n",
      "loss: 1.401602  [  600/ 5482]\n",
      "loss: 1.473442  [ 1200/ 5482]\n",
      "loss: 1.438106  [ 1800/ 5482]\n",
      "loss: 1.352784  [ 2400/ 5482]\n",
      "loss: 1.550492  [ 3000/ 5482]\n",
      "loss: 1.453842  [ 3600/ 5482]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mexp_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_em\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:35\u001B[0m, in \u001B[0;36mExperimentsTrainer.train_em\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m trail \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials_per_model_type):\n\u001B[1;32m     34\u001B[0m     lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_lr \u001B[38;5;241m/\u001B[39m  (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_quotient \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m trail)\n\u001B[0;32m---> 35\u001B[0m     highest_acc_c, higest_epoch_c, higest_true_c, higest_pred_c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_conv_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs_per_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrail\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# generate Report\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     SSGenModelTrainer\u001B[38;5;241m.\u001B[39mgenAndSaveEvaluation(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrail\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/conv\u001B[39m\u001B[38;5;124m\"\u001B[39m, higest_true_c, higest_pred_c, highest_acc_c, higest_epoch_c, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvolutional\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mencoded_dataset\u001B[38;5;241m.\u001B[39mlabel_list)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:54\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_conv_model_test\u001B[0;34m(self, lr, epochs, current_run)\u001B[0m\n\u001B[1;32m     52\u001B[0m model \u001B[38;5;241m=\u001B[39m SSConvModel3Sec(num_emotions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list), xSize\u001B[38;5;241m=\u001B[39mx_size, ySize\u001B[38;5;241m=\u001B[39my_size)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     53\u001B[0m save_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Run_Nr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_run\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/conv/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_model_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_wrapper_trainer_experiments.py:79\u001B[0m, in \u001B[0;36mExperimentsTrainer.run_model_test\u001B[0;34m(self, lr, epochs, model, save_dir, bs)\u001B[0m\n\u001B[1;32m     71\u001B[0m trainDS, testDs \u001B[38;5;241m=\u001B[39m train_val_dataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, val_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m     72\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SSGenModelTrainer(lr\u001B[38;5;241m=\u001B[39mlr, num_epochs\u001B[38;5;241m=\u001B[39mepochs, model\u001B[38;5;241m=\u001B[39mmodel, train_dataset\u001B[38;5;241m=\u001B[39mtrainDS,\n\u001B[1;32m     73\u001B[0m                             eval_dataset\u001B[38;5;241m=\u001B[39mtestDs,\n\u001B[1;32m     74\u001B[0m                             device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, labelList\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_list,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m                             save_highest_acc_min_acc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_highest_acc_min_acc,\n\u001B[1;32m     78\u001B[0m                             model_path \u001B[38;5;241m=\u001B[39m save_dir)\n\u001B[0;32m---> 79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:64\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     62\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memo_reco_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m acc, true, preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loop(test_dataloader, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(acc \u001B[38;5;241m>\u001B[39m highest_acc):\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/soundsream_models_and_utils/ss_trainer_gen_models.py:95\u001B[0m, in \u001B[0;36mSSGenModelTrainer.train_loop\u001B[0;34m(self, dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[1;32m     94\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 95\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "exp_trainer.train_em()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
