{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tess trained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...generating encoding\r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "module_path = str(Path.cwd().parents[0] / \"network_models/w2v_emotion_model\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import utils.wav2Vec_utils as w2vU\n",
    "import utils.audio_dataset_utils as ADU\n",
    "import network_models.soundstream_lstm.LSTM_dataset as lds\n",
    "import network_models.soundstream_lstm.CombinedEmoDataset_7_Emo as ced\n",
    "import  network_models.w2v_emotion_model.custom_collator as cc\n",
    "import network_models.w2v_emotion_model.custom_model as cm\n",
    "import network_models.w2v_emotion_model.model_trainer as ct\n",
    "import gc\n",
    "\n",
    "model_name_or_path = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "pooling_mode = \"mean\"\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "emo_dataset = ced.CombinedEmoDataSet_7_emos(\n",
    "    #directory_tess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/tess\",\n",
    "    directory_cafe=\"/home/ckwdani/Music/emotionDatasets/converted_mono/cafe\",\n",
    "    #directory_ravdess=\"/home/ckwdani/Music/emotionDatasets/converted_mono/RAVDESS Audio_Speech_Actors_01-24\",\n",
    "    #                                         directory_mesd=\"/home/ckwdani/Music/emotionDatasets/converted_mono/mesd\",\n",
    "                                             device=device)\n",
    "\n",
    "processor, sr = w2vU.init_w2v2(num_labels=len(emo_dataset.label_list), label_list=emo_dataset.label_list, device=device)\n",
    "newSet = lds.AudioEmotionTessWav2VecDataset(emo_dataset, processor= processor, sampling_rate=sr)\n",
    "\n",
    "model = cm.Wav2Vec2ForSpeechClassification(model_name_or_path=model_name_or_path, pooling_mode=\"mean\", device=device)\n",
    "model.freeze_feature_extractor()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../content/classifier/W2VClassifier/Nr1/emo_reco_800.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/w2v_emotion_model/custom_model.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y = softmax(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 94\n",
      "80 of 94\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "trainSet, evalSet = ADU.train_val_dataset(newSet, 0.1)\n",
    "batch_size = 8\n",
    "dataset = evalSet\n",
    "data_collator = cc.DataCollatorCTCWithPadding(processor=processor, padding=True, num_labels=len(newSet.dataSet.label_list))\n",
    "loader = DataLoader(dataset, shuffle=False, batch_size=batch_size, num_workers=2 ,collate_fn=data_collator.collate_fn)\n",
    "true, preds = [], []\n",
    "with torch.no_grad():\n",
    "    for batch, (X, z) in enumerate(loader):\n",
    "        X, z = X.to(device),  z.to(device)\n",
    "        pred = model(X)\n",
    "        labels = [torch.squeeze(a.nonzero()).item() for a in z]\n",
    "        true = true + labels\n",
    "        preds = preds + pred.argmax(1).numpy().tolist()\n",
    "        gc.collect()\n",
    "        #preds.append(pred.argmax(1))\n",
    "        if batch % 10 == 0:\n",
    "            print(f\"{batch*batch_size} of {len(dataset)}\")\n",
    "\n",
    "# correct = 0\n",
    "# with torch.no_grad():\n",
    "#     for i in range(len(newSet)):\n",
    "#         tensor = torch.tensor(newSet[i][0])\n",
    "#         if(i%10 == 1):\n",
    "#             print(f\"{i} of {len(newSet)}\")\n",
    "#             print(f\"correct: {correct*100/i} %\")\n",
    "#         if(model(torch.unsqueeze(tensor, dim=0)).argmax(1) == newSet[i][1][0]):\n",
    "#             correct = correct + 1\n",
    "        # print(5 == newSet[500][1][0])\n",
    "        # print(newSet[500][1][0])\n",
    "        # print(model(torch.unsqueeze(tensor, dim=0)))\n",
    "    #print(model(torch.tensor(numpy.expand_dims(newSet[0], axis=0))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.25      0.36      0.29        14\n",
      "     disgust       0.00      0.00      0.00        10\n",
      "        fear       0.12      0.14      0.13        14\n",
      "       happy       0.13      0.31      0.18        13\n",
      "     neutral       0.10      0.25      0.14         8\n",
      "         sad       0.00      0.00      0.00        15\n",
      "    surprise       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.14        94\n",
      "   macro avg       0.09      0.15      0.11        94\n",
      "weighted avg       0.08      0.14      0.10        94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ckwdani/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true, preds, target_names=emo_dataset.label_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: 10 and `sequence_length`: 9`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(newSet[\u001B[38;5;241m324\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/network_models/w2v_emotion_model/custom_model.py:89\u001B[0m, in \u001B[0;36mWav2Vec2ForSpeechClassification.forward\u001B[0;34m(self, input_values)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     87\u001B[0m         input_values,\n\u001B[1;32m     88\u001B[0m ):\n\u001B[0;32m---> 89\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwav2vec2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;66;03m#hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\u001B[39;00m\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1307\u001B[0m, in \u001B[0;36mWav2Vec2Model.forward\u001B[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1302\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_feature_vector_attention_mask(\n\u001B[1;32m   1303\u001B[0m         extract_features\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], attention_mask, add_adapter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1304\u001B[0m     )\n\u001B[1;32m   1306\u001B[0m hidden_states, extract_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_projection(extract_features)\n\u001B[0;32m-> 1307\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mask_hidden_states\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1308\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_time_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask_time_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\n\u001B[1;32m   1309\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1311\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[1;32m   1312\u001B[0m     hidden_states,\n\u001B[1;32m   1313\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1316\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1317\u001B[0m )\n\u001B[1;32m   1319\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1249\u001B[0m, in \u001B[0;36mWav2Vec2Model._mask_hidden_states\u001B[0;34m(self, hidden_states, mask_time_indices, attention_mask)\u001B[0m\n\u001B[1;32m   1247\u001B[0m     hidden_states[mask_time_indices] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasked_spec_embed\u001B[38;5;241m.\u001B[39mto(hidden_states\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   1248\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmask_time_prob \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m-> 1249\u001B[0m     mask_time_indices \u001B[38;5;241m=\u001B[39m \u001B[43m_compute_mask_indices\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1250\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask_prob\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmask_time_prob\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmask_time_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_masks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmask_time_min_masks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1256\u001B[0m     mask_time_indices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(mask_time_indices, device\u001B[38;5;241m=\u001B[39mhidden_states\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[1;32m   1257\u001B[0m     hidden_states[mask_time_indices] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasked_spec_embed\u001B[38;5;241m.\u001B[39mto(hidden_states\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[0;32m~/Programming/Projects/masterarbeit/Jupyter/mainProject/venv/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:165\u001B[0m, in \u001B[0;36m_compute_mask_indices\u001B[0;34m(shape, mask_prob, mask_length, attention_mask, min_masks)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`mask_length` has to be bigger than 0.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask_length \u001B[38;5;241m>\u001B[39m sequence_length:\n\u001B[0;32m--> 165\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    166\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmask_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and `sequence_length`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msequence_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    168\u001B[0m     )\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# epsilon is used for probabilistic rounding\u001B[39;00m\n\u001B[1;32m    171\u001B[0m epsilon \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n",
      "\u001B[0;31mValueError\u001B[0m: `mask_length` has to be smaller than `sequence_length`, but got `mask_length`: 10 and `sequence_length`: 9`"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor = torch.tensor(newSet[324][0])\n",
    "model(torch.unsqueeze(tensor, dim=0)).argmax(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.04325287,  0.08506081,  0.06913736, ..., -0.0217979 ,\n       -0.0183346 , -0.02307062], dtype=float32)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSet[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
